Directory structure:
└── kds_consulting_team-kds-team.ex-hubspot-crm/
    ├── flake8.cfg
    ├── deploy.sh
    ├── docker-compose.yml
    ├── changelog.txt
    ├── sample-data/
    │   ├── HubSpot Example Imports - Deals.csv
    │   ├── HubSpot Example Imports - Contacts.csv
    │   ├── HubSpot Example Imports - Companies.csv
    │   └── HubSpot Example Imports - Tickets.csv
    ├── src/
    │   ├── component.py
    │   ├── hubspot_api/
    │   │   ├── client_service.py
    │   │   ├── __init__.py
    │   │   └── client_v3.py
    │   ├── json_parser.py
    │   └── .DS_Store
    ├── bitbucket-pipelines.yml
    ├── tests/
    │   ├── test_component.py
    │   └── __init__.py
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── configuration_description.md
    │   ├── component_short_description.md
    │   ├── stack_parameters.json
    │   ├── configSchema.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   ├── tables/
    │       │   │   ├── test.csv
    │       │   │   └── test.csv.manifest
    │       │   └── files/
    │       │       └── order1.xml
    │       └── out/
    │           ├── tables/
    │           │   └── test.csv
    │           └── files/
    │               └── order1.xml
    ├── scripts/
    │   ├── update_dev_portal_properties.sh
    │   ├── run_kbc_tests.ps1
    │   ├── run.bat
    │   ├── build_n_run.ps1
    │   └── build_n_test.sh
    ├── requirements.txt
    ├── docs/
    │   └── imgs/
    ├── Dockerfile
    ├── LICENSE.md
    └── README.md

================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi

================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /changelog.txt
================================================
CHANGE LOG
Contacts:
PK 'canonical_vid', 'portal_id' (previously 'vid', 'portal_id')

CONTACT LISTS & SUBMISSINOS
CONTACT_ID renamed to contact_canonical_vid

identity_profiles field split into table contacts_identity_profiles and contacts_identity_profile_identities

DEALS

CONTACT_ID renamed to contact_vid

deals_stages
removed properties_dealstage_versions


pipelines
removed `stages`

pipeline_stages
PIPELINE_ID to pipelineId


================================================
File: /sample-data/HubSpot Example Imports - Deals.csv
================================================
Deal Name,Deal Stage,Pipeline,Amount,Close Date,Deal Owner,Product of Interest,Point of Contact
Dunder Mifflin,Appointment Scheduled,Sales Pipeline,"$3,760",3/16/2018,mmitchell@hubspot.com,Paper,Michael Scott
Luke's Cafe,Proposal Sent,Sales Pipeline,"$2,500",1/23/2019,mmitchell@hubspot.com,CRM,Lorelai Gilmore
Central Perk,Closed Won,Sales Pipeline,"$15,000",11/11/2018,mmitchell@hubspot.com,Marketing Starter,Monica Gellar
,,,,,,,
Default deal property,Default deal property,Default deal property,Default deal property,Default deal property,Default deal property,Custom Property,Custom Property
,,,,,,,
Please note:,,,,,,,
"Deal Stage is a required field, and must match the stages set in the deals pipeline. We recommend that you customize your deal pipeline prior to import by going to Settings > Sales > Deals, then click on the name of the pipeline and editing the pipeline appropriately.",,,,,,,
,,,,,,,
Your Import Checklist Criteria:,,,,,,,
1. Does every piece of information have its own unique column?,,,,,,,
"2. HubSpot default properties have their own values in HubSpot, do the same values appear in the columns?",,,,,,,
3. Do your deal stage values match what's in your current pipeline settings?,,,,,,,
3. Are your custom properties created in HubSpot?,,,,,,,
"4. Are there blank spaces (besides those below your data), stray column headers, or loose information in your spreadsheet? If so, please remove them.",,,,,,,

================================================
File: /sample-data/HubSpot Example Imports - Contacts.csv
================================================
First Name,Last Name,Email Address,Phone Number,Street Address,City,State,Postal Code,Lifecycle Stage,Contact Owner,Favorite Ice Cream Flavor,Product of Interest
Michael,Scott,michael.scott@dundermifflin.com,1(800) 555-5555,16 Scranton Ave,Scranton,PA,18503,Subscriber,mmitchell@hubspot.com,Vanilla,Paper
Mac,Mitchell,macmitch@hubspot.com,1(888) 482-7768,25 First Street,Cambridge,MA,21430,Lead,mmitchell@hubspot.com,Rocky Road,CRM
Lauren,Tzirides,ltziri@hubspot.com,1(888) 482-7768,25 First Street,Cambridge,MA,21430,Sales Qualified Lead,mmitchell@hubspot.com,Coffee,Marketing Starter
,,,,,,,,,,,
Default contact property,Default contact property,Default contact property,Default contact or company property,Default contact or company property,Default contact or company property,Default contact or company property,Default contact or company property,Default contact or company property,Default contact property,Custom Property,Custom Property
,,,,,,,,,,,
Please note:,,,,,,,,,,,
There is a mix of contact and company properties in this spreadsheet. We recommend that you prepare two separate files for contact and company information. You will then have the option to associate two record types during the import.,,,,,,,,,,,
,,,,,,,,,,,
Your Import Checklist Criteria:,,,,,,,,,,,
1. Does every piece of information have its own unique column?,,,,,,,,,,,
"2. HubSpot default properties have their own values that can be accessed within the Properties tool, do the same values appear in the columns?",,,,,,,,,,,
"(You cannot edit some default HubSpot properties; for example, Lifecycle stage has specific values that must match the values in HubSpot. You can check on the values already stored in the default properties in Settings > Properties.)",,,,,,,,,,,
3. Are your custom properties created in HubSpot?,,,,,,,,,,,
"4. Are there blank spaces (besides those below your data), stray column headers, or loose information in your spreadsheet? If so, please remove them.",,,,,,,,,,,

================================================
File: /sample-data/HubSpot Example Imports - Companies.csv
================================================
Name,Company Domain,Phone Number,Street Address,City,State,Postal Code,Lifecycle Stage,Company Owner,Vendor Type,Product of Interest,Point of Contact
Dunder Mifflin,dundermifflin.com,1(800) 555-5555,16 Scranton Ave,Scranton,PA,18503,Subscriber,mmitchell@hubspot.com,Investor,Paper,Michael Scott
Luke's Cafe,lukescafe.com,1(888) 482-7768,16 Main Street,Stars Hollow,CT,16101,Lead,mmitchell@hubspot.com,Partner,CRM,Lorelai Gilmore
Central Perk,centralperk.com,1(888) 495-3402,123 First Street,New York,NY,10001,Sales Qualified Lead,mmitchell@hubspot.com,Agency,Marketing Starter,Monica Gellar
,,,,,,,,,,,
Default company property,Default company property,Default company property,Default company property,Default company property,Default company property,Default company property,Default company property,Default company property,Custom Property,Custom Property,Custom Property
,,,,,,,,,,,
Please note:,,,,,,,,,,,
"Point of Contact is a custom property because contact records cannot be create directly from company records. If you want to create associated contact records, you'll need to do a separate import.",,,,,,,,,,,
,,,,,,,,,,,
Your Import Checklist Criteria:,,,,,,,,,,,
1. Does every piece of information have its own unique column?,,,,,,,,,,,
"2. HubSpot default properties have their own values in HubSpot, do the same values appear in the columns?",,,,,,,,,,,
"(You cannot edit some default HubSpot properties; for example, Lifecycle stage has specific values that must match the values in HubSpot. You can check on the values already stored in the default properties in Settings > Properties.)",,,,,,,,,,,
3. Are your custom properties created in HubSpot?,,,,,,,,,,,
"4. Are there blank spaces (besides those below your data), stray column headers, or loose information in your spreadsheet? If so, please remove them.",,,,,,,,,,,

================================================
File: /sample-data/HubSpot Example Imports - Tickets.csv
================================================
Ticket Name,Associated Contact,Associated Company,Pipeline,Ticket Status,Priority,Ticket Owner,Source,Issue of Interest,Issued Ticket Before?
Free CRM,Brittany Lambert,HubSpot,Support Pipeline,New,Medium,mmitchell@hubspot.com,Phone,App,Yes
Sales Starter,Jack Coopersmith,Dunder Mifflin,Support Pipeline,Waiting on Contact,Low,mmitchell@hubspot.com,Form,Broken icon,Yes
Sales Pro,Sam Cloud,Nike,Support Pipeline,Waiting on Contact,Low,mmitchell@hubspot.com,Email,Import Error,No
Sales Enterprise,Shannon Wade,Gucci,Support Pipeline,Waiting on Us,High,mmitchell@hubspot.com,Chat,Billing Issue,No
Marketing Starter,Blake Toder,Gap,Support Pipeline,Closed,Low,mmitchell@hubspot.com,Chat,Renewal error,Yes
,,,,,,,,,
Default Ticket property,Default Ticket property,Default Ticket property,Default Ticket property,Default Ticket property,Default Ticket property,Default Ticket property,Default Ticket  property,Custom Property,Custom Property
,,,,,,,,,
Import Checklist Criteria:,,,,,,,,,
1. Does every piece of information have it's own unique column and match the information presented in the column header?,,,,,,,,,
"2. HubSpot defualt properties have their own values in HubSpot, do the same values appear in the columns?",,,,,,,,,
"(You cannot edit some default HubSpot properties; for example, Lifecycle stage has specific values that must match the values in HubSpot. You can check on the values already stored in the default properties in Settings > Properties.)",,,,,,,,,
3. Are your custom properties created in HubSpot?,,,,,,,,,
"4. Are there blank spaces (besides those below your data), stray column headers, or loose information in your spreadsheet? If so, please remove them.",,,,,,,,,

================================================
File: /src/component.py
================================================
'''
Template Component main class.

'''
import json
import logging
import os
import sys
import warnings
from typing import Dict, List

import keboola.csvwriter
import keboola.utils as kbcutils
import pandas as pd
from keboola.component import ComponentBase
from keboola.csvwriter import ElasticDictWriter

from hubspot_api.client_service import HubspotClientService, CONTACTS_DEFAULT_COLS
from json_parser import FlattenJsonParser

ENGAGEMENT_ASSOC_COLS = ["contactIds",
                         "companyIds",
                         "dealIds",
                         "ownerIds"]

KEY_CONTACT_VID = 'contact_canonical_vid'

# primary keys
PIPELINE_STAGE_PK = ['pipelineId', 'stageId']
PIPELINE_PK = ['pipelineId']
OWNER_PK = ['ownerId']
LISTS_PK = ['listId']
ACTIVITIES_PK = ['engagement_id ']
EMAIL_EVENTS_PK = ['id', 'created']
CAMPAIGNS_PK = ['id']
DEAL_C_LIST_PK = ['dealId', 'contact_vid']
DEAL_STAGE_HIST_PK = ['dealId', 'sourceVid', 'sourceId', 'timestamp']
DEAL_PK = ['dealId ']
CONTACT_LIST_PK = ['internal_list_id', 'static_list_id', KEY_CONTACT_VID]
C_SUBMISSION_PK = ['form_id', KEY_CONTACT_VID, 'portal_id', 'conversion_id', 'page_id', 'page_url']
CONTACT_PK = ['canonical_vid', 'portal_id']
COMPANY_ID_COL = ['companyId']

# config keys
KEY_API_TOKEN = '#api_token'
KEY_PERIOD_FROM = 'period_from'
KEY_ENDPOINTS = 'endpoints'
KEY_INCR_OUT = 'incremental_output'
KEY_COMPANY_PROPERTIES = 'company_properties'
KEY_CONTACT_PROPERTIES = 'contact_properties'
KEY_DEAL_PROPERTIES = 'deal_properties'
KEY_PROPERTY_ATTRIBUTES = "property_attributes"
# for debug
KEY_STDLOG = 'stdlogging'

SUPPORTED_ENDPOINTS = ['companies', 'campaigns', 'email_events', 'activities', 'lists', 'owners', 'contacts', 'deals',
                       'pipelines']

MANDATORY_PARS = []
MANDATORY_IMAGE_PARS = []

# columns
CONTACT_FORM_SUBISSION_COLS = ["contact-associated-by", "conversion-id", "form-id", "form-type", "meta-data",
                               "page-id", "page-url", "portal-id", "timestamp", "title", KEY_CONTACT_VID]
CONTACT_PROFILES_COLS = ["vid", "saved-at-timestamp", KEY_CONTACT_VID, 'identity_profile_pk']
CONTACT_PROFILE_IDENTITIES_COLS = ['type', 'value', 'timestamp', 'is-primary', 'identity_profile_pk']
CONTACT_LISTS_COLS = ["internal-list-id", "is-member", "static-list-id", "timestamp", "vid", KEY_CONTACT_VID]
DEAL_STAGE_HIST_COLS = ['name', 'source', 'sourceId', 'sourceVid', 'timestamp', 'value', 'dealId']

ENGAGEMENT_COLS = [
    "id",
    "portalId",
    "active",
    "createdAt",
    "lastUpdated",
    "ownerId",
    "type",
    "timestamp",
    "metadata"
]

APP_VERSION = '1.0.1'


class Component(ComponentBase):

    def __init__(self, debug=False):
        ComponentBase.__init__(self)

        # temp suppress pytz warning
        warnings.filterwarnings(
            "ignore",
            message="The localize method is no longer necessary, as this time zone supports the fold attribute",
        )
        logging.info('Loading configuration...')

        try:
            self.validate_configuration_parameters(MANDATORY_PARS)
        except ValueError as ex:
            logging.exception(ex)
            exit(1)

        self.incremental = self.configuration.parameters.get(KEY_INCR_OUT)
        state = self.get_state_file() or {}
        self._object_schemas: dict = state.get('table_schemas') or {}

        # If _object_schemas is empty list [], then it will stay a list instead of being a dict.
        if not self._object_schemas:
            self._object_schemas = {}

        self._writer_cache: Dict[str, ElasticDictWriter] = {}

    def run(self):
        '''
        Main execution code
        '''
        params = self.configuration.parameters  # noqa

        authentication_type = params.get("authentication_type", "API Key")
        if authentication_type == "API Key":
            token = params[KEY_API_TOKEN]
        elif authentication_type == "Private App Token":
            token = params['#private_app_token']
        else:
            raise ValueError(f'Invalid authentication type "{authentication_type}"')

        client_service = HubspotClientService(token, authentication_type=authentication_type)

        if params.get(KEY_PERIOD_FROM):
            import dateparser
            period = params.get(KEY_PERIOD_FROM)
            if not dateparser.parse(period):
                raise ValueError(F'Invalid date from period "{period}", check the supported format')
            start_date, end_date = kbcutils.parse_datetime_interval(period, 'now')
            recent = True
            logging.info(f"Getting data since: {period}")
        else:
            start_date = None
            recent = False

        endpoints = params.get(KEY_ENDPOINTS, SUPPORTED_ENDPOINTS)
        property_attributes = params.get(KEY_PROPERTY_ATTRIBUTES,
                                         {"include_versions": True, "include_source": True, "include_timestamp": True})

        if 'companies' in endpoints:
            logging.info('Extracting Companies')
            res_file_path = os.path.join(self.tables_out_path, 'companies.csv')
            self._get_simple_ds(res_file_path, COMPANY_ID_COL, client_service.get_companies, property_attributes,
                                recent, self._parse_props(params.get(KEY_COMPANY_PROPERTIES)))

        if 'campaigns' in endpoints:
            logging.info('Extracting Campaigns from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'campaigns.csv')
            self._get_simple_ds(res_file_path, CAMPAIGNS_PK, client_service.get_campaigns, recent)

        email_events = [e for e in endpoints if e.startswith('email_events')]
        if email_events:
            email_events = set(email_events)
            # backward compatibility
            if "email_events" in email_events:
                email_events.add('email_events-CLICK')
                email_events.add('email_events-OPEN')
                email_events.remove('email_events')

            logging.info('Extracting Email Events from HubSpot CRM')

            events_list = [e.split('-')[1] for e in email_events]
            res_file_path = os.path.join(self.tables_out_path, 'email_events.csv')
            self._get_simple_ds(res_file_path, EMAIL_EVENTS_PK, client_service.get_email_events, start_date,
                                events_list)

        if 'activities' in endpoints:
            logging.info('Extracting Activities from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'activities.csv')
            self._get_simple_ds(res_file_path, ACTIVITIES_PK, client_service.get_activities, start_date)

        if 'lists' in endpoints:
            logging.info('Extracting Lists from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'lists.csv')
            self._get_simple_ds(res_file_path, LISTS_PK, client_service.get_lists)

        if 'owners' in endpoints:
            logging.info('Extracting Owners from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'owners.csv')
            self._get_simple_ds(res_file_path, OWNER_PK, client_service.get_owners, recent)

        if 'contacts' in endpoints:
            logging.info('Extracting Contacts from HubSpot CRM')
            self.get_contacts(client_service, start_date, self._parse_props(params.get(KEY_CONTACT_PROPERTIES)),
                              property_attributes, params.get('include_contact_list_membership', True))

        if 'deals' in endpoints:
            logging.info('Extracting Deals from HubSpot CRM')
            self.get_deals(client_service, start_date, self._parse_props(params.get(KEY_DEAL_PROPERTIES)),
                           property_attributes)

        if 'pipelines' in endpoints:
            logging.info('Extracting Pipelines from HubSpot CRM')
            self.get_pipelines(client_service)

        if 'dispositions' in endpoints:
            logging.info('Extracting Engagement Dispositons from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'engagement-dispositions.csv')
            self._get_simple_ds(res_file_path, ['id'], client_service.get_owners, recent)

        if 'calls' in endpoints:
            logging.info('Extracting Calls HubSpot CRM')
            self._dowload_crm_v3_object(client_service, 'calls',
                                        properties=self._parse_props(params.get('call_properties', [])))

        if 'emails' in endpoints:
            logging.info('Extracting Emails HubSpot CRM')
            self._dowload_crm_v3_object(client_service, 'emails',
                                        properties=self._parse_props(params.get('email_properties', [])))

        if 'meetings' in endpoints:
            logging.info('Extracting Meetings HubSpot CRM')
            self._dowload_crm_v3_object(client_service, 'meetings',
                                        properties=self._parse_props(params.get('meeting_properties', [])))
        if 'forms' in endpoints:
            logging.info('Extracting Forms HubSpot CRM')
            parser = FlattenJsonParser(child_separator='__', exclude_fields=['displayOptions'],
                                       keys_to_ignore=['fieldGroups'])
            self._download_v3_parsed(client_service.get_forms, parser, 'forms')

        if 'marketing_email_statistics' in endpoints:
            logging.info('Extracting marketing_email_statistics HubSpot')
            parser = FlattenJsonParser(child_separator='__', exclude_fields=['smartEmailFields'],
                                       keys_to_ignore=['styleSettings'])
            updated_since = None
            if start_date:
                updated_since = int(start_date.timestamp() * 1000)
            self._download_v3_parsed(client_service.get_email_statistics, parser, 'marketing_email_statistics',
                                     updated_since=updated_since)

        self._close_files()

    def _get_simple_ds(self, res_file_path, pkey, ds_getter, *fpars):
        """
        Generic method to get simple objects
        :param res_file_path:
        :param pkey:
        :param ds_getter:
        :return:
        """
        res_columns = list()
        counter = 0
        for res in (df for df in ds_getter(*fpars) if not df.empty):
            counter += 1
            self.output_file(res, res_file_path, res.columns)
            res_columns = list(res.columns.values)
            if counter % 100 == 0:
                logging.info(f"Processed {counter} records.")

        # store manifest
        if os.path.isfile(res_file_path):
            cleaned_columns = self._cleanup_col_names(res_columns)
            self._write_table_manifest_legacy(file_name=res_file_path, primary_key=pkey,
                                              incremental=self.incremental,
                                              columns=cleaned_columns)

    # CONTACTS
    def get_contacts(self, client: HubspotClientService, start_time, fields, property_attributes,
                     include_membership: True):
        res_file_path = os.path.join(self.tables_out_path, 'contacts.csv')
        res_columns = []
        counter = 0
        for res in client.get_contacts(property_attributes, start_time, fields, include_membership):
            counter += 100
            if len(res.columns.values) == 0:
                logging.info("No contact records for specified period.")
                continue

            if self.configuration.parameters.get('contact_associations'):
                self._download_contact_associations(client, res)

            if 'form-submissions' in res.columns or 'list-memberships' in res.columns:
                self._store_contact_submission_and_list(res)
                res.drop(['form-submissions', 'list-memberships'], 1, inplace=True, errors='ignore')

            if 'identity-profiles' in res.columns:
                self._store_contact_identity_profiles(res)
                res.drop(['identity-profiles'], 1, inplace=True, errors='ignore')

            if counter % 100 == 0:
                logging.info(f"Processed {counter} Contact records.")

            self._drop_duplicate_properties(res, CONTACTS_DEFAULT_COLS)
            self.output_file(res, res_file_path, res.columns)
            # store columns
            res_columns = list(res.columns.values)
            logging.debug(f"Returned contact columns: {res_columns}")

        # store manifests
        if os.path.isfile(res_file_path):
            cl_cols = self._cleanup_col_names(res_columns)
            self._write_table_manifest_legacy(file_name=res_file_path, primary_key=CONTACT_PK,
                                              incremental=self.incremental,
                                              columns=cl_cols)

    def _download_contact_associations(self, client: HubspotClientService, result: pd.DataFrame):
        vids = result['vid'].tolist()
        for ass in self.configuration.parameters['contact_associations']:
            results = client.get_associations('contact', ass['to_object_type'], vids)
            self._write_associations('contact', ass['to_object_type'], results)

    def _write_associations(self, from_type: str, to_type: str, data: List[dict]):
        result_table = self.create_out_table_definition('object_associations.csv', incremental=self.incremental,
                                                        primary_key=['from_id', 'from_type', 'to_id', 'to_type'])
        result_row = {}
        header = ['from_id',
                  'from_type',
                  'to_id',
                  'to_type',
                  'association_types']
        for row in data:
            for association in row['to']:
                result_row['from_id'] = row['from']['id']
                result_row['from_type'] = from_type
                result_row['to_id'] = association['toObjectId']
                result_row['to_type'] = to_type
                result_row['association_types'] = association['associationTypes']
                self.output_object_dict(result_row, result_table.full_path, header)

        if data:
            self.write_manifest(result_table)

    def _drop_duplicate_properties(self, df, property_names: list):
        columns = list(df.columns.values)
        drop_columns = []
        for c in columns:
            if c.startswith('properties') and c.split('.')[1] in property_names:
                drop_columns.append(c)

        df.drop(columns=drop_columns, inplace=True)

    def _store_contact_submission_and_list(self, contacts):

        c_subform_path = os.path.join(self.tables_out_path, 'contacts_form_submissions.csv')
        c_lists_path = os.path.join(self.tables_out_path, 'contacts_lists.csv')
        # Create table with Contact's form submissions and lists and drop column afterwards
        for index, row in contacts.iterrows():

            if len(row['form-submissions']) > 0:
                temp_contacts_sub_forms = pd.DataFrame(row['form-submissions'])
                temp_contacts_sub_forms[KEY_CONTACT_VID] = row['canonical-vid']
                res_cols = CONTACT_FORM_SUBISSION_COLS
                temp_contacts_sub_forms = temp_contacts_sub_forms.reindex(columns=res_cols).fillna('')

                # save res
                self.output_file(temp_contacts_sub_forms, c_subform_path, temp_contacts_sub_forms.columns)

            if len(row['list-memberships']) > 0:
                temp_contacts_lists = pd.DataFrame(row['list-memberships'])
                temp_contacts_lists[KEY_CONTACT_VID] = row['canonical-vid']
                res_cols = CONTACT_LISTS_COLS
                temp_contacts_lists = temp_contacts_lists.reindex(columns=res_cols).fillna('')
                # save res
                self.output_file(temp_contacts_lists, c_lists_path, temp_contacts_lists.columns)

        if os.path.isfile(c_subform_path):
            self._write_table_manifest_legacy(file_name=c_subform_path, primary_key=C_SUBMISSION_PK,
                                              columns=CONTACT_FORM_SUBISSION_COLS,
                                              incremental=self.incremental)
        if os.path.isfile(c_lists_path):
            self._write_table_manifest_legacy(file_name=c_lists_path, primary_key=CONTACT_LIST_PK,
                                              columns=CONTACT_LISTS_COLS,
                                              incremental=self.incremental)

    def _store_contact_identity_profiles(self, contacts):
        c_profiles = os.path.join(self.tables_out_path, 'contacts_identity_profiles.csv')
        c_identities = os.path.join(self.tables_out_path, 'contacts_identity_profile_identities.csv')
        # Create table with Contact's form submissions and lists and drop column afterwards
        for index, row in contacts.iterrows():

            if len(row['identity-profiles']) > 0:
                tmp_profiles = pd.DataFrame(row['identity-profiles'])
                tmp_profiles[KEY_CONTACT_VID] = row['canonical-vid']
                identity_profile_pk = str(row['canonical-vid']) + '|' + str(row['vid'])
                tmp_profiles['identity_profile_pk'] = identity_profile_pk

                if len(tmp_profiles['identities']) > 0:
                    self._store_identities(tmp_profiles['identities'], c_identities, identity_profile_pk)
                    # tmp_identities = pd.DataFrame(list(tmp_profiles['identities']))
                    # tmp_identities['identity_profile_pk'] = identity_profile_pk
                    # self.output_file(tmp_identities, c_identities, tmp_identities.columns)

                res_cols = CONTACT_PROFILES_COLS
                tmp_profiles = tmp_profiles.reindex(columns=res_cols).fillna('')

                # save res
                self.output_file(tmp_profiles, c_profiles, tmp_profiles.columns)

        if os.path.isfile(c_profiles):
            self._write_table_manifest_legacy(file_name=c_profiles, primary_key=['identity_profile_pk'],
                                              columns=CONTACT_PROFILES_COLS,
                                              incremental=self.incremental)
        if os.path.isfile(c_identities):
            self._write_table_manifest_legacy(file_name=c_identities,
                                              primary_key=['identity_profile_pk', 'type', 'value'],
                                              columns=CONTACT_PROFILE_IDENTITIES_COLS,
                                              incremental=self.incremental)

    def _store_identities(self, identities, res_file, identity_profile_pk):
        for index, row in identities.iteritems():
            tmp_identities = pd.DataFrame(row).copy()
            tmp_identities['identity_profile_pk'] = identity_profile_pk
            tmp_identities = tmp_identities.reindex(columns=CONTACT_PROFILE_IDENTITIES_COLS)
            self.output_file(tmp_identities, res_file, tmp_identities.columns)

    # DEALS
    def get_deals(self, client: HubspotClientService, start_time, fields, property_attributes):
        res_file_path = os.path.join(self.tables_out_path, 'deals.csv')
        res_columns = list()
        counter = 0
        for res in client.get_deals(property_attributes, start_time, fields):
            counter += 1
            self._store_deals_stage_hist_and_list(res)
            res.drop(['properties.dealstage.versions'], 1, inplace=True, errors='ignore')
            res.drop(['associations.associatedVids'], 1, inplace=True, errors='ignore')
            res.drop(['associations.associatedDealIds'], 1, inplace=True, errors='ignore')
            res.drop(['associations.associatedCompanyIds'], 1, inplace=True, errors='ignore')
            self.output_file(res, res_file_path, res.columns)
            # store columns
            if not res.empty:
                res_columns = list(res.columns.values)

            if counter % 100 == 0:
                logging.info(f"Processed {counter} Deals records.")

        # store manifests
        if os.path.isfile(res_file_path):
            cl_cols = self._cleanup_col_names(res_columns)
            self._write_table_manifest_legacy(file_name=res_file_path, primary_key=DEAL_PK,
                                              incremental=self.incremental,
                                              columns=cl_cols)

    def _store_deals_stage_hist_and_list(self, deals):

        stage_hist_path = os.path.join(self.tables_out_path, 'deals_stage_history.csv')
        c_lists_path = os.path.join(self.tables_out_path, 'deals_contacts_list.csv')
        deal_lists_path = os.path.join(self.tables_out_path, 'deals_assoc_deals_list.csv')
        companies_lists_path = os.path.join(self.tables_out_path, 'deals_assoc_companies_list.csv')
        # Create table with Deals' Stage History & Deals' Contacts List
        c_list_cols, stage_his_cols, ass_deal_list_cols = None, None, None
        for index, row in deals.iterrows():

            if row.get('properties.dealstage.versions') and str(
                    row['properties.dealstage.versions']) != 'nan' and len(row['properties.dealstage.versions']) > 0:
                temp_stage_history = pd.DataFrame(row['properties.dealstage.versions'])
                temp_stage_history['dealId'] = row['dealId']
                # fix columns - sometimes there are some missing in the response
                temp_stage_history = temp_stage_history.reindex(columns=DEAL_STAGE_HIST_COLS).fillna('')

                self.output_file(temp_stage_history, stage_hist_path, temp_stage_history.columns)
                if not stage_his_cols:
                    stage_his_cols = list(temp_stage_history.columns.values)

            if row.get('associations.associatedVids') and len(row['associations.associatedVids']) != 0:
                temp_deals_contacts_list = pd.DataFrame(row['associations.associatedVids'],
                                                        columns=['contact_vid'])
                temp_deals_contacts_list['dealId'] = row['dealId']
                self.output_file(temp_deals_contacts_list, c_lists_path, temp_deals_contacts_list.columns)
                if not c_list_cols:
                    c_list_cols = list(temp_deals_contacts_list.columns.values)

            if row.get('associations.associatedCompanyIds') and len(row['associations.associatedCompanyIds']) != 0:
                comp_list = pd.DataFrame(row['associations.associatedCompanyIds'],
                                         columns=['associated_companyId'])
                comp_list['dealId'] = row['dealId']
                logging.debug(f'{list(comp_list.columns.values)}')
                self.output_file(comp_list, companies_lists_path, comp_list.columns)

            if row.get('associations.associatedDealIds') and len(row['associations.associatedDealIds']) != 0:
                ass_deal_list = pd.DataFrame(row['associations.associatedDealIds'],
                                             columns=['associated_dealId'])
                ass_deal_list['dealId'] = row['dealId']
                self.output_file(ass_deal_list, deal_lists_path, ass_deal_list.columns)
                if not ass_deal_list_cols:
                    ass_deal_list_cols = list(ass_deal_list.columns.values)

        if os.path.isfile(stage_hist_path):
            self._write_table_manifest_legacy(file_name=stage_hist_path, primary_key=DEAL_STAGE_HIST_PK,
                                              columns=stage_his_cols,
                                              incremental=self.incremental)
        if os.path.isfile(c_lists_path):
            self._write_table_manifest_legacy(file_name=c_lists_path, primary_key=DEAL_C_LIST_PK,
                                              columns=["contact_vid", "dealId"],
                                              incremental=self.incremental)
        if os.path.isfile(deal_lists_path):
            self._write_table_manifest_legacy(file_name=deal_lists_path,
                                              primary_key=['dealId', 'associated_dealId'],
                                              columns=ass_deal_list_cols,
                                              incremental=self.incremental)
        if os.path.isfile(companies_lists_path):
            self._write_table_manifest_legacy(file_name=companies_lists_path,
                                              primary_key=['dealId', 'associated_companyId'],
                                              columns=["associated_companyId", "dealId"],
                                              incremental=self.incremental)

    # PIPELINES
    def get_pipelines(self, client: HubspotClientService):
        res_file_path = os.path.join(self.tables_out_path, 'pipelines.csv')
        res_columns = list()
        counter = 0
        for res in client.get_pipelines():
            counter += 1
            self._store_pipeline_stages(res)
            res.drop(['stages'], 1, inplace=True, errors='ignore')
            self.output_file(res, res_file_path, res.columns)
            if not res_columns:
                res_columns = list(res.columns.values)

            if counter % 100 == 0:
                logging.info(f"Processed {counter} Pipelines records.")

        # store manifests
        if os.path.isfile(res_file_path):
            cl_cols = self._cleanup_col_names(res_columns)
            self._write_table_manifest_legacy(file_name=res_file_path, primary_key=PIPELINE_PK, columns=cl_cols,
                                              incremental=self.incremental)

    def _store_pipeline_stages(self, pipelines):

        stage_hist_path = os.path.join(self.tables_out_path, 'pipeline_stages.csv')
        # Create table with Pipelines' Stages.
        res_columns = list()
        for index, row in pipelines.iterrows():

            if len(row['stages']) > 0:
                temp_pipelines_stages = pd.DataFrame(row['stages'])
                temp_pipelines_stages['pipelineId'] = row['pipelineId']
                self.output_file(temp_pipelines_stages, stage_hist_path, temp_pipelines_stages.columns)
                if not res_columns:
                    res_columns = list(temp_pipelines_stages.columns.values)

        if os.path.isfile(stage_hist_path):
            self._write_table_manifest_legacy(file_name=stage_hist_path, primary_key=PIPELINE_STAGE_PK,
                                              columns=res_columns,
                                              incremental=self.incremental)

    def _dowload_crm_v3_object(self, client: HubspotClientService, object_name: str, **kwargs):
        result_table = self.create_out_table_definition(f'{object_name}.csv', incremental=self.incremental,
                                                        primary_key=['id'])
        result_path = result_table.full_path
        header_columns = self._object_schemas.get(result_path, ['id'])
        counter = 0
        for res in client.get_v3_engagement_object(object_name, **kwargs):
            if counter % 500 == 0:
                logging.info(f"Downloaded {counter} records.")
            for row in res:
                counter += 1
                self.output_object_dict(row, result_path, header_columns)

        if counter > 0:
            self.write_manifest(result_table)

    def _download_v3_parsed(self, method, parser: FlattenJsonParser, object_name: str, **kwargs):
        result_table = self.create_out_table_definition(f'{object_name}.csv', incremental=self.incremental,
                                                        primary_key=['id'])
        result_path = result_table.full_path
        header_columns = self._object_schemas.get(result_path, ['id'])

        counter = 0
        next_boundary = 500
        total_rows = 0
        for res in method(**kwargs):
            if counter % 500 == 0:
                logging.info(f"Downloading records between {counter} and {next_boundary}.")
                next_boundary = counter + 500
                counter += 1
            for row in res:
                total_rows = + 1
                parsed_row = parser.parse_row(row)
                self.output_object_dict(parsed_row, result_path, header_columns)

        if total_rows > 0:
            self.write_manifest(result_table)

    def output_file(self, data_output, file_output, column_headers):
        """
        Output the dataframe input to destination file
        Append to the file if file does not exist
        * row by row
        """
        if data_output.empty:
            logging.debug("No results for %s", file_output)
            return
        data_output = data_output.astype(str)
        _mode = 'w+' if not os.path.isfile(file_output) else 'a'

        with open(file_output, _mode, encoding='utf-8', newline='') as b:
            data_output.to_csv(b, index=False, header=False, columns=column_headers, line_terminator="")

    def output_object_dict(self, data_output: dict, file_output, column_headers):
        """
        Output the dataframe input to destination file
        Append to the file if file does not exist
        * row by row
        """
        writer = self._get_writer_from_cache(file_output, column_headers)
        data_output = self._flatten_properties(data_output)
        writer.writerow(data_output)

    def _flatten_properties(self, result: dict):
        properties = result.pop('properties', {})
        for p in properties:
            result[p] = properties[p]
        return result

    def _get_writer_from_cache(self, output_path: str, column_headers):
        if not self._writer_cache.get(output_path):
            self._writer_cache[output_path] = keboola.csvwriter.ElasticDictWriter(output_path, column_headers)
            self._writer_cache[output_path].writeheader()

        return self._writer_cache[output_path]

    def _close_files(self):
        for key, f in self._writer_cache.items():
            f.close()
            logging.debug(self._object_schemas)
            self._object_schemas[key] = f.fieldnames

        self.write_state_file({"table_schemas": self._object_schemas})

    def _parse_props(self, param):
        cols = []
        if param:
            cols = [p.strip() for p in param.split(",")]
        return cols

    def _cleanup_col_names(self, columns):
        new_cols = list()
        for col in columns:
            new_col = col.replace('properties.', '', 1).replace('.value', '', 1).replace('.', '_')
            if new_col not in new_cols:
                new_cols.append(new_col)
            else:
                new_cols.append(col)
        return new_cols

    def _write_table_manifest_legacy(self,
                                     file_name,
                                     destination='',
                                     primary_key=None,
                                     columns=None,
                                     incremental=None):
        """
        Write manifest for output table Manifest is used for
        the table to be stored in KBC Storage.
        Args:
            file_name: Local file name of the CSV with table data.
            destination: String name of the table in Storage.
            primary_key: List with names of columns used for primary key.
            columns: List of columns for headless CSV files
            incremental: Set to true to enable incremental loading
        """
        manifest = {}
        if destination:
            if isinstance(destination, str):
                manifest['destination'] = destination
            else:
                raise TypeError("Destination must be a string")
        if primary_key:
            if isinstance(primary_key, list):
                manifest['primary_key'] = primary_key
            else:
                raise TypeError("Primary key must be a list")
        if columns:
            if isinstance(columns, list):
                manifest['columns'] = columns
            else:
                raise TypeError("Columns must by a list")
        if incremental:
            manifest['incremental'] = True
        with open(file_name + '.manifest', 'w') as manifest_file:
            json.dump(manifest, manifest_file)


"""
            Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug_arg = sys.argv[1]
    else:
        debug_arg = False
    try:
        comp = Component(debug_arg)
        comp.run()
    except KeyError as e:
        logging.exception(Exception(F'Invalid Key value:{e}'))
        exit(2)
    except Exception as e:
        detail = ''
        if len(e.args) > 1:
            detail = e.args[1]
        logging.exception(e, extra={"full_message": detail})
        logging.exception(e)
        exit(1)


================================================
File: /src/hubspot_api/client_service.py
================================================
import logging
from collections.abc import Iterable
from datetime import datetime
from json import JSONDecodeError
from typing import List, Optional

import numpy as np
import pandas as pd
from keboola.http_client import HttpClient
from pandas import json_normalize
from requests import Response

from hubspot_api import client_v3

COMPANIES_DEFAULT_COLS = ["additionalDomains", "companyId", "isDeleted", "mergeAudits", "portalId", "stateChanges"]
COMPANY_DEFAULT_PROPERTIES = ['about_us', 'name', 'phone', 'facebook_company_page', 'city', 'country', 'website',
                              'industry', 'annualrevenue', 'linkedin_company_page',
                              'hs_lastmodifieddate', 'hubspot_owner_id', 'notes_last_updated', 'description',
                              'createdate', 'numberofemployees', 'hs_lead_status', 'founded_year',
                              'twitterhandle',
                              'linkedinbio']

DEAL_DEFAULT_COLS = ["associations.associatedCompanyIds",
                     "associations.associatedDealIds",
                     "associations.associatedVids",
                     "dealId",
                     "imports",
                     "isDeleted",
                     "portalId",
                     "stateChanges"]

DEAL_DEFAULT_PROPERTIES = ["hs_object_id", 'authority', 'budget', 'campaign_source', 'hs_analytics_source',
                           'hs_campaign',
                           'hs_lastmodifieddate', 'need', 'timeframe', 'dealname', 'amount', 'closedate', 'pipeline',
                           'createdate', 'engagements_last_meeting_booked', 'dealtype', 'hs_createdate', 'description',
                           'start_date', 'closed_lost_reason', 'closed_won_reason', 'end_date', 'lead_owner',
                           'tech_owner', 'service_amount', 'contract_type',
                           'hubspot_owner_id',
                           'partner_name', 'notes_last_updated']

CONTACTS_DEFAULT_COLS = ["addedAt",
                         "canonical-vid",
                         "form-submissions",
                         "identity-profiles",
                         "is-contact",
                         "list-memberships",
                         "merge-audits",
                         "merged-vids",
                         "portal-id",
                         "profile-token",
                         "profile-url",
                         "vid"]

CONTACT_DEFAULT_PROPERTIES = ['hs_facebookid', 'hs_linkedinid', 'ip_city', 'ip_country',
                              'ip_country_code', 'newsletter_opt_in', 'linkedin_profile',
                              'email', 'mobilephone', 'phone', 'city',
                              'country', 'region', 'jobtitle', 'website', 'numemployees',
                              'industry', 'associatedcompanyid', 'hs_lead_status', 'lastmodifieddate',
                              'source', 'hs_email_optout', 'twitterhandle', 'lead_type',
                              'hubspot_owner_id', 'notes_last_updated', 'hs_analytics_source', 'opt_in',
                              'createdate', 'hs_twitterid', 'lifecyclestage']

LISTS_COLS = ['archived', 'authorId', 'createdAt', 'deleteable', 'dynamic', 'filters',
              'internalListId', 'listId', 'listType', 'metaData.error',
              'metaData.lastProcessingStateChangeAt', 'metaData.lastSizeChangeAt',
              'metaData.listReferencesCount', 'metaData.parentFolderId',
              'metaData.processing', 'metaData.size', 'name', 'portalId', 'readOnly',
              'updatedAt']

EMAIL_EVENTS_COLS = ['appId', 'appName', 'browser', 'browser.family', 'browser.name', 'browser.producer',
                     'browser.producerUrl', 'browser.type', 'browser.url', 'browser.version', 'causedBy.created',
                     'causedBy.id', 'created', 'deviceType', 'duration', 'emailCampaignId', 'filteredEvent', 'id',
                     'ipAddress', 'location', 'location.city', 'location.country', 'location.state', 'portalId',
                     'recipient', 'sentBy.created', 'sentBy.id', 'smtpId', 'type', 'userAgent']

ENGAGEMENTS_COLS = ['metadata.isBot', 'metadata.endTime', 'metadata.postSendStatus', 'associations.quoteIds',
                    'metadata.from.raw', 'engagement.createdBy', 'metadata.to',
                    'metadata.agentResponseTimeMilliseconds', 'metadata.visitorStartTime', 'metadata.messageId',
                    'metadata.durationMilliseconds', 'metadata.externalUrl', 'engagement.source',
                    'associations.contactIds', 'engagement.lastUpdated', 'metadata.body', 'metadata.forObjectType',
                    'metadata.categoryId', 'metadata.sessionClosedAt', 'metadata.visitorEndTime', 'metadata.from.email',
                    'metadata.recordingUrl', 'associations.ticketIds', 'associations.contentIds',
                    'metadata.fromfirstName', 'metadata.numVisitorMessages', 'engagement.ownerId',
                    'metadata.facsimileSendId', 'metadata.createdFromLinkId', 'metadata.sessionDurationMilliseconds',
                    'metadata.startTime', 'metadata.subject', 'associations.ownerIds', 'associations.dealIds',
                    'metadata.html', 'metadata.source', 'metadata.sendDefaultReminder', 'engagement.createdAt',
                    'engagement.sourceId', 'metadata.category', 'metadata.fromemail', 'metadata.sender.email',
                    'metadata.online', 'metadata.from.lastName', 'engagement.uid', 'engagement.allAccessibleTeamIds',
                    'metadata.from.firstName', 'metadata.text', 'metadata.conversationSource', 'metadata.toNumber',
                    'associations.workflowIds', 'metadata.sentVia', 'attachments', 'metadata.numAgentMessages',
                    'metadata.url', 'metadata.agentJoinTime', 'engagement.id', 'engagement.type',
                    'associations.companyIds', 'metadata.title', 'metadata.disposition', 'metadata.state',
                    'engagement.modifiedBy', 'metadata.fromlastName', 'metadata.externalId', 'metadata.sourceId',
                    'metadata.fromNumber', 'metadata.cc', 'metadata.externalAccountId',
                    'metadata.visitorWaitTimeMilliseconds', 'engagement.portalId', 'metadata.trackerKey',
                    'metadata.preMeetingProspectReminders', 'metadata.attachedVideoOpened',
                    'metadata.validationSkipped', 'metadata.loggedFrom', 'metadata.mediaProcessingStatus',
                    'metadata.threadId', 'metadata.reminders', 'metadata.status', 'metadata.name',
                    'engagement.timestamp', 'metadata.contentId', 'metadata.campaignGuid', 'metadata.taskType',
                    'metadata.bcc', 'scheduledTasks', 'engagement.active', 'metadata.attachedVideoWatched',
                    'engagement.teamId', 'metadata.to.email', 'metadata.calleeObjectId', 'metadata.calleeObjectType',
                    'metadata.emailSendEventId.created', 'metadata.emailSendEventId.id', 'metadata.errorMessage']

CAMPAIGNS = 'email/public/v1/campaigns/'

LISTS = 'contacts/v1/lists'

ENGAGEMENTS_PAGED = 'engagements/v1/engagements/paged'
ENGAGEMENTS_PAGED_SINCE = 'engagements/v1/engagements/recent/modified'

EMAIL_EVENTS = 'email/public/v1/events'

CAMPAIGNS_BY_ID = 'email/public/v1/campaigns/by-id'
CAMPAIGNS_BY_ID_RECENT = 'email/public/v1/campaigns'

DEALS_ALL = 'deals/v1/deal/paged'
DEALS_RECENT = 'deals/v1/deal/recent/modified'

COMPANIES_ALL = 'companies/v2/companies/paged'
COMPANIES_RECENT = 'companies/v2/companies/recent/modified'

MAX_RETRIES = 10
BASE_URL = 'https://api.hubapi.com/'

# endpoints
CONTACTS_ALL = 'contacts/v1/lists/all/contacts/all'
CONTACTS_RECENT = 'contacts/v1/lists/recently_updated/contacts/recent'

COMPANY_PROPERTIES = 'properties/v1/companies/properties/'


class HubspotClientService(HttpClient):

    def __init__(self, token, authentication_type: str = "API Key"):
        """

        Args:
            token:
            authentication_type: "API Key" or "Private App Token"
        """
        if authentication_type == "API Key":
            default_params = {"hapikey": token}
            auth_header = {}
        else:
            default_params = {}
            auth_header = {'Authorization': f'Bearer {token}'}

        HttpClient.__init__(self, base_url=BASE_URL, max_retries=MAX_RETRIES, backoff_factor=0.3,
                            status_forcelist=(429, 500, 502, 504, 524), default_params=default_params,
                            auth_header=auth_header)
        self._client_v3 = client_v3.ClientV3(token, authentication_type)

    def _parse_response_text(self, response: Response, endpoint, parameters) -> dict:
        try:
            return response.json()
        except JSONDecodeError as e:
            charp = str(e).split('(char ')
            start_pos = 0
            if len(charp) == 2:
                start_pos = int(charp[1].replace(')', '')) - 100
                if start_pos < 0:
                    start_pos = 0
            raise RuntimeError(f'The HS API response is invalid. enpoint: {endpoint}, parameters: {parameters}. '
                               f'Status: {response.status_code}. '
                               f'Response: {response.text[start_pos:start_pos + 100]}... {e}')

    def _get_paged_result_pages(self, endpoint, parameters, res_obj_name, limit_attr, offset_req_attr, offset_resp_attr,
                                has_more_attr, offset, limit, default_cols=None):

        has_more = True
        while has_more:
            final_df = pd.DataFrame()
            parameters[offset_req_attr] = offset
            parameters[limit_attr] = limit

            req = self.get_raw(self.base_url + endpoint, params=parameters)
            self._check_http_result(req, endpoint)
            req_response = self._parse_response_text(req, endpoint, parameters)
            if req_response.get(has_more_attr):
                has_more = True
                offset = req_response[offset_resp_attr]
            else:
                has_more = False
            if req_response.get(res_obj_name):
                final_df = final_df.append(json_normalize(req_response[res_obj_name]), sort=True)
            else:
                logging.debug(f'Empty response {req_response}')
            if default_cols and not final_df.empty:
                # dedupe
                default_cols = list(set(default_cols))
                final_df = final_df.reindex(columns=default_cols).fillna('')
                # final_df = final_df.loc[:, default_cols].fillna('')
            # sort cols
            final_df = final_df.reindex(sorted(final_df.columns), axis=1)
            yield final_df

    def _get_paged_result_pages_dict(self, endpoint, parameters, res_obj_name, limit_attr, offset_req_attr,
                                     offset_resp_attr, offset, limit, default_cols=None):

        has_more = True
        while has_more:
            final_result = {}
            parameters[offset_req_attr] = offset
            parameters[limit_attr] = limit

            req = self.get_raw(self.base_url + endpoint, params=parameters)
            self._check_http_result(req, endpoint)
            req_response = self._parse_response_text(req, endpoint, parameters)

            # paginate until there are some data
            if req_response.get(res_obj_name):
                logging.debug(
                    f'totalCount:{req_response["totalCount"]}, offset:{req_response["offset"]}, '
                    f'datalen: {len(req_response["objects"])}, total:{req_response["total"]}')
                has_more = True
                # https://legacydocs.hubspot.com/docs/methods/cms_email/get-all-marketing-email-statistics
                # Use the limit of the previous request as the offset to get the next set of results.
                offset = req_response[offset_resp_attr] + limit
            else:
                has_more = False
            if req_response.get(res_obj_name):
                final_result = req_response.get(res_obj_name)
            else:
                logging.debug(f'Empty response {req_response}')

            yield final_result

    def _get_contact_recent_pages(self, parameters, since_time_offset, limit, default_cols=None):
        """
        Recent contacts enpoint paginates backwards, from time offset back to 30 day ago.
        This simulates the expected behaviour -> gets data from now until the point in time

        :param parameters:
        :param since_time_offset:
        :param limit:
        :param default_cols:
        :return:
        """
        res_obj_name = 'contacts'
        endpoint = CONTACTS_RECENT
        # start from today
        timeoffset = int(datetime.utcnow().timestamp() * 1000)

        has_more = True
        while has_more:
            final_df = pd.DataFrame()
            parameters['timeOffset'] = timeoffset
            parameters['count'] = limit

            req = self.get_raw(self.base_url + endpoint, params=parameters)
            self._check_http_result(req, endpoint)
            req_response = self._parse_response_text(req, endpoint, parameters)
            timeoffset = req_response.get('time-offset', since_time_offset)

            if req_response.get('has-more') and timeoffset >= since_time_offset:
                has_more = True
            else:
                has_more = False

            if req_response.get(res_obj_name):
                final_df = final_df.append(json_normalize(req_response[res_obj_name]), sort=True)
            else:
                logging.debug(f'Empty response {req_response}')
            if default_cols and not final_df.empty:
                # dedupe
                default_cols = list(set(default_cols))
                final_df = final_df.reindex(columns=default_cols).fillna('')
                # final_df = final_df.loc[:, default_cols].fillna('')
            # sort cols
            final_df = final_df.reindex(sorted(final_df.columns), axis=1)
            yield final_df

    def _check_http_result(self, response, endpoint):
        http_error_msg = ''
        if isinstance(response.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
            try:
                reason = response.reason.decode('utf-8')
            except UnicodeDecodeError:
                reason = response.reason.decode('iso-8859-1')
        else:
            reason = response.reason
        if 401 == response.status_code:
            http_error_msg = u'Failed to login: %s - Please check your API token' % (reason)
        elif 401 < response.status_code < 500:
            http_error_msg = u'Request to %s failed %s Client Error: %s' % (endpoint, response.status_code, reason)

        elif 500 <= response.status_code < 600:
            http_error_msg = u'Request to %s failed %s Client Error: %s' % (endpoint, response.status_code, reason)

        if http_error_msg:
            raise RuntimeError(http_error_msg, response.text)

    def get_contacts(self, property_attributes, start_time=None, fields=None,
                     show_list_membership: bool = True) -> Iterable:
        """
        Get either all available contacts or recent ones specified by start_time.

        API supports more options, possible to extend in the future
        :type fields: list list of contact properties to get
        :param start_time: datetime
        :return: generator object with all available pages
        """
        offset = -1

        if not fields:
            contact_properties = CONTACT_DEFAULT_PROPERTIES
            expected_contact_cols = CONTACTS_DEFAULT_COLS + self._build_property_cols(
                CONTACTS_DEFAULT_COLS, property_attributes)
        else:
            contact_properties = fields
            expected_contact_cols = CONTACTS_DEFAULT_COLS + self._build_property_cols(fields, property_attributes)

        parameters = {'property': contact_properties, 'formSubmissionMode': 'all',
                      'showListMemberships': show_list_membership}

        # hubspot_api api allows only 30 days back, get all data if larger
        if start_time and (datetime.utcnow() - start_time).days >= 30:
            start_time = None

        parameters['propertyMode'] = 'value_and_history'
        if start_time:
            logging.info('Getting contacts using incremental endpoint (<30 days ago)')
            return self._get_contact_recent_pages(parameters, int(start_time.timestamp() * 1000), 100,
                                                  default_cols=expected_contact_cols)
        else:
            logging.info('Getting ALL contacts using "full scan" endpoint (period >30 days ago)')
            return self._get_paged_result_pages(CONTACTS_ALL, parameters, 'contacts', 'count', 'vidOffset',
                                                'vid-offset', 'has-more', offset, 100,
                                                default_cols=expected_contact_cols)

    def get_companies(self, property_attributes, recent=None, fields=None):

        offset = 0
        if not fields:
            company_properties = COMPANY_DEFAULT_PROPERTIES
            expected_company_cols = COMPANIES_DEFAULT_COLS + self._build_property_cols(
                COMPANY_DEFAULT_PROPERTIES, property_attributes)
        else:
            company_properties = fields
            expected_company_cols = COMPANIES_DEFAULT_COLS + self._build_property_cols(fields, property_attributes)

        parameters = {'properties': company_properties}

        if property_attributes['include_versions']:
            parameters['propertiesWithHistory'] = company_properties

        # # hubspot_api api allows only 30 days back, get all data if larger
        # if start_time and (datetime.utcnow() - start_time).days >= 30:
        #     recent = False
        # else:
        #     recent = True

        if recent:
            return self._get_paged_result_pages(COMPANIES_RECENT, parameters, 'results', 'count', 'offset',
                                                'offset',
                                                'hasMore',
                                                offset, 1000, default_cols=expected_company_cols)
        else:
            return self._get_paged_result_pages(COMPANIES_ALL, parameters, 'companies', 'limit', 'offset',
                                                'offset',
                                                'has-more', offset, 250, default_cols=expected_company_cols)

    def get_company_properties(self):
        req = self.get_raw(self.base_url + COMPANY_PROPERTIES)
        self._check_http_result(req, COMPANY_PROPERTIES)
        req_response = req.json()
        return req_response

    def _build_property_cols(self, properties, property_attributes):
        # get flattened property cols
        prop_cols = []
        for p in properties:
            if property_attributes.get('include_source', True):
                prop_cols.append('properties.' + p + '.source')
                prop_cols.append('properties.' + p + '.sourceId')
            if property_attributes.get('include_timestamp', True):
                prop_cols.append('properties.' + p + '.timestamp')
            if property_attributes.get('include_versions', True):
                prop_cols.append('properties.' + p + '.versions')

            prop_cols.append('properties.' + p + '.value')
        return prop_cols

    def _build_contact_property_cols(self, properties):
        # get flattened property cols
        prop_cols = []
        for p in properties:
            prop_cols.append('properties.' + p + '.value')
            prop_cols.append('properties.' + p + '.versions')
        return prop_cols

    def get_deals(self, property_attributes, start_time=None, fields=None) -> Iterable:
        """
        Get either all available deals or recent ones specified by start_time.

        API supports more options, possible to extend in the future
        :type fields: list list of deal properties to get
        :param start_time: datetime
        :return: generator object with all available pages
        """
        offset = 0
        if not fields:
            deal_properties = DEAL_DEFAULT_PROPERTIES
            expected_deal_cols = DEAL_DEFAULT_COLS + self._build_property_cols(
                DEAL_DEFAULT_PROPERTIES, property_attributes)
        else:
            if 'dealstage' in fields:
                fields.remove('dealstage')
            deal_properties = fields
            expected_deal_cols = DEAL_DEFAULT_COLS + self._build_property_cols(fields, property_attributes)

        property_attributes['include_versions'] = True
        expected_deal_cols += self._build_property_cols(['dealstage'], property_attributes)
        parameters = {'properties': deal_properties,
                      'propertiesWithHistory': 'dealstage',
                      'includeAssociations': 'true'}
        if start_time:
            parameters['since'] = int(start_time.timestamp() * 1000)
            return self._get_paged_result_pages(DEALS_RECENT, parameters, 'results', 'count', 'offset',
                                                'offset',
                                                'hasMore',
                                                offset, 100, default_cols=expected_deal_cols)
        else:
            return self._get_paged_result_pages(DEALS_ALL, parameters, 'deals', 'limit', 'offset', 'offset',
                                                'hasMore',
                                                offset, 250, default_cols=expected_deal_cols)

    def get_campaigns(self, recent=False):
        final_df = pd.DataFrame()
        if recent:
            url = CAMPAIGNS_BY_ID_RECENT
        else:
            url = CAMPAIGNS_BY_ID

        for res in self._get_paged_result_pages(url, {}, 'campaigns', 'limit', 'offset', 'offset', 'hasMore',
                                                None,
                                                1000):

            for index, row in res.iterrows():
                req = self.get_raw(self.base_url + CAMPAIGNS + str(row['id']))
                self._check_http_result(req, CAMPAIGNS)
                req_response = req.json()

                final_df = final_df.append(json_normalize(req_response), sort=True)
            # add missing cols
            columns = ['counters.open', 'counters.click', 'id', 'name', 'counters.delivered',
                       'counters.processed', 'counters.sent', 'lastProcessingFinishedAt',
                       'lastProcessingStartedAt', 'lastProcessingStateChangeAt',
                       'numIncluded', 'processingState', 'subject', 'type', 'appId', 'appName', 'contentId', ]

            if not final_df.empty:
                for c in columns:
                    if c not in final_df:
                        final_df[c] = np.nan

            yield final_df if final_df.empty else final_df[columns]

    def get_email_events(self, start_date: datetime, events_list: list) -> Iterable:
        offset = ''
        timestamp = None
        if start_date:
            timestamp = int(start_date.timestamp() * 1000)

        for event in events_list:
            logging.info(f"Getting {event} events.")
            parameters = {'eventType': event, 'startTimestamp': timestamp}
            for open_ev in self._get_paged_result_pages(EMAIL_EVENTS, parameters, 'events', 'limit', 'offset',
                                                        'offset',
                                                        'hasMore',
                                                        offset, 1000, default_cols=EMAIL_EVENTS_COLS):
                yield open_ev

    def get_activities(self, start_time: datetime) -> Iterable:
        offset = 0

        if start_time:
            pages = self._get_paged_result_pages(ENGAGEMENTS_PAGED_SINCE, {"since": int(start_time.timestamp() * 1000)},
                                                 'results', 'count', 'offset', 'offset', 'hasMore', offset, 250,
                                                 default_cols=ENGAGEMENTS_COLS)
        else:
            pages = self._get_paged_result_pages(ENGAGEMENTS_PAGED, {}, 'results', 'limit', 'offset', 'offset',
                                                 'hasMore',
                                                 offset, 250, default_cols=ENGAGEMENTS_COLS)
        for pg_res in pages:
            if 'metadata.text' in pg_res.columns:
                pg_res.drop(['metadata.text'], 1)
            if 'metadata.html' in pg_res.columns:
                pg_res.drop(['metadata.html'], 1)
            yield pg_res

    def get_lists(self):
        offset = 0

        return self._get_paged_result_pages(LISTS, {}, 'lists', 'limit', 'offset', 'offset', 'has-more',
                                            offset, 250, default_cols=LISTS_COLS)

    def get_pipelines(self, include_inactive=None):
        final_df = pd.DataFrame()

        req = self.get_raw('https://api.hubapi.com/deals/v1/pipelines', params={'include_inactive': include_inactive})
        self._check_http_result(req, 'deals/pipelines')
        req_response = req.json()

        final_df = final_df.append(json_normalize(req_response), sort=True)

        return [final_df]

    def get_owners(self, include_inactive=True):
        final_df = pd.DataFrame()

        req = self.get_raw('https://api.hubapi.com/owners/v2/owners/', params={'include_inactive': include_inactive})
        self._check_http_result(req, 'owners')
        req_response = req.json()

        final_df = final_df.append(json_normalize(req_response), sort=True)

        return [final_df]

    def get_email_statistics(self, include_inactive=True, updated_since: Optional[int] = None):
        parameters = {}
        if updated_since:
            parameters = {"updated__gte": updated_since}
        resp = self._get_paged_result_pages_dict('marketing-emails/v1/emails/with-statistics', parameters, 'objects',
                                                 'limit',
                                                 'offset', 'offset', 0, 250)

        return resp

    def get_v3_engagement_object(self, object_type: str, properties: List[str] = None):
        return self._client_v3.get_engagement_object(object_type, properties)

    def get_forms(self):
        return self._client_v3.get_forms()

    def get_associations(self, from_object_type: str, to_object_type: str, ids: List[str]):
        """

               Args:
                   from_object_type: e.g. company, contact
                   to_object_type: e.g. company, contact
                   ids: List of IDs of from_object type

               Returns: Result as dict

               """
        return self._client_v3.get_associations(from_object_type, to_object_type, ids)


================================================
File: /src/hubspot_api/client_v3.py
================================================
import json
import logging
from enum import Enum
from typing import List, Union, Iterator

from keboola.http_client import HttpClient

MAX_RETRIES = 10
BASE_URL = 'https://api.hubapi.com/'


class EngagementObjects(Enum):
    calls = "calls"
    emails = "emails"
    meetings = "meetings"
    notes = "notes"
    tasks = "tasks"

    @classmethod
    def list(cls):
        return list(map(lambda c: c.name, cls))

    @classmethod
    def validate_fields(cls, fields: List[str]):
        errors = []
        for f in fields:
            if f not in cls.list():
                errors.append(f'"{f}" is not valid {cls.__name__} value!')
        if errors:
            raise ValueError(
                ', '.join(errors) + f'\n Supported {cls.__name__} values are: [{cls.list()}]')

    @classmethod
    def validate_field(cls, field: str):
        if field not in cls.list():
            raise ValueError(
                f'{field}" is not valid {cls.__name__} value! Supported {cls.__name__} values are: [{cls.list()}]')
        return True


class ClientV3(HttpClient):

    def __init__(self, token, authentication_type):
        """

        Args:
            token:
            authentication_type: "API Key" or "Private App Token"
        """
        if authentication_type == "API Key":
            default_params = {"hapikey": token}
            auth_header = {}
        else:
            default_params = {}
            auth_header = {'Authorization': f'Bearer {token}'}
        HttpClient.__init__(self, base_url=BASE_URL, max_retries=MAX_RETRIES, backoff_factor=0.3,
                            status_forcelist=(429, 500, 502, 504, 524), default_params=default_params,
                            auth_header=auth_header)

    def _get_paged_result_pages(self, endpoint, parameters, limit=100, default_cols=None) -> Iterator[List[dict]]:

        has_more = True
        while has_more:
            parameters['limit'] = limit

            req = self.get_raw(self.base_url + endpoint, params=parameters)
            self._check_http_result(req, endpoint)
            resp_text = str.encode(req.text, 'utf-8')
            req_response = json.loads(resp_text)

            if req_response.get('paging', {}).get('next', {}).get('after'):
                has_more = True
                after = req_response['paging']['next']['after']
                parameters['after'] = after
            else:
                has_more = False

            results = []
            if req_response.get('results'):
                results = req_response['results']
            else:
                logging.debug(f'Empty response {req_response}')

            yield results

    def _check_http_result(self, response, endpoint):
        http_error_msg = ''
        error_detail = ''
        if isinstance(response.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
            try:
                reason = response.reason.decode('utf-8')

            except UnicodeDecodeError:
                reason = response.reason.decode('iso-8859-1')
        else:
            reason = response.reason

        if response.status_code >= 400:
            error_detail = response.json()['message'] + f' Errors: {response.json()["errors"]}'
        if 401 == response.status_code:
            http_error_msg = f'Failed to login: {reason} - Please check your API token'
        elif 401 < response.status_code < 500:
            http_error_msg = f'Request to {endpoint} failed {response.status_code} Client Error: {reason} ' \
                             f'Detail: {error_detail}'

        elif 500 <= response.status_code < 600:
            http_error_msg = f'Request to {endpoint} failed {response.status_code} Client Error: {reason} ' \
                             f'Detail: {error_detail}'

        if http_error_msg:
            raise RuntimeError(http_error_msg)

    def get_forms(self, archived: bool = False, form_types: List[str] = None) -> Iterator[List[dict]]:
        request_params = {"archived": archived}
        if form_types:
            form_types_str = ','.join(form_types)
            request_params['formTypes'] = form_types_str

        return self._get_paged_result_pages('marketing/v3/forms', request_params)

    def get_engagement_calls(self, properties: List[str] = None):

        request_params = {}
        if properties:
            properties_str = ','.join(properties)
            request_params['properties'] = properties_str

        return self._get_paged_result_pages('crm/v3/objects/calls', request_params)

    def get_engagement_object(self, object_type: Union[EngagementObjects, str], properties: List[str] = None):

        if isinstance(object_type, str):
            EngagementObjects.validate_field(object_type)
        elif isinstance(object_type, EngagementObjects):
            object_type = object_type.value

        request_params = {}
        if properties:
            properties_str = ','.join(properties)
            request_params['properties'] = properties_str

        return self._get_paged_result_pages(f'crm/v3/objects/{object_type}', request_params)

    def get_associations(self, from_object_type: str, to_object_type: str, ids: List[str]) -> dict:
        """

        Args:
            from_object_type: e.g. company, contact
            to_object_type: e.g. company, contact
            ids: List of IDs of from_object type

        Returns: Result as dict

        """
        body = {'inputs': [{"id": id_value} for id_value in ids]}

        resp = self.post_raw(f'/crm/v4/associations/{from_object_type}/{to_object_type}/batch/read', json=body)

        resp.raise_for_status()

        return resp.json()['results']


================================================
File: /src/json_parser.py
================================================
class FlattenJsonParser:
    def __init__(self, child_separator: str = '_', exclude_fields=None, flatten_lists=False, keys_to_ignore=None):
        self.child_separator = child_separator
        self.exclude_fields = exclude_fields
        self.flatten_lists = flatten_lists
        self.keys_to_ignore = keys_to_ignore
        if self.keys_to_ignore is None:
            self.keys_to_ignore = set()

    def parse_data(self, data):
        for i, row in enumerate(data):
            data[i] = self._flatten_row(row)
        return data

    def parse_row(self, row: dict):
        return self._flatten_row(row)

    @staticmethod
    def _construct_key(parent_key, separator, child_key):
        if parent_key:
            return "".join([parent_key, separator, child_key])
        else:
            return child_key

    def _flatten_row(self, nested_dict):
        if len(nested_dict) == 0:
            return {}

        flattened_dict = dict()

        def _flatten(dict_object, key_name=None, name_with_parent=''):
            if isinstance(dict_object, dict):
                for key in dict_object:
                    if key not in self.keys_to_ignore:
                        new_parent_name = self._construct_key(name_with_parent, self.child_separator, key)
                        _flatten(dict_object[key], key_name=key, name_with_parent=new_parent_name)
                    else:
                        flattened_dict[key] = dict_object[key]
            elif isinstance(dict_object, (list, set, tuple)):
                if self.flatten_lists:
                    for index, item in enumerate(dict_object):
                        new_key_name = self._construct_key(name_with_parent, self.child_separator, str(index))
                        _flatten(item, key_name=new_key_name)
                else:
                    flattened_dict[name_with_parent] = dict_object
            else:
                flattened_dict[name_with_parent] = dict_object

        _flatten(nested_dict, None)
        return flattened_dict


================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - echo 'Pushing test image to repo. [tag=test]'
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:test
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - echo 'Pushing test image to repo. [tag=test]'
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
            - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP 788820312 test
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP 788820312 test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest


class TestComponent(unittest.TestCase):
    pass


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /component_config/component_long_description.md
================================================
This component uses the [HubSpot API](https://developers.hubspot.com/docs/overview) to download the following 
data from [HubSpot](http://www.hubspot.com/):

- Companies
- Contacts
- Deals & Pipelines
- Campaigns
- Email Events
- Engagements
- Contact Lists
- Owners

To configure the extractor, you need to have

a working HubSpot account, and
a [HubSpot API Key](https://app.hubspot.com/keys/get).


The Contacts, Companies, Engagements, Email Events  and Deals support incremental load to limit the API calls.

Additional documentation available [here](https://bitbucket.org/kds_consulting_team/kds-team.ex-hubspot-crm/src/master/README.md)

================================================
File: /component_config/configuration_description.md
================================================
**Working with the `Period from date`**

Only some endpoints support this parameter and different rules applies for each of them:

- `contacts` - The endpoint only scrolls back in time 30 days. **NOTE** that it may happen that returned data contain contacts updated before the specified date, 
this can happen when there is <100 contacts modified since that date. The pagination since date is simulated by the app, so always at least first page (100) of contacts is returned.
- `deals` - This endpoint will only return records modified in the last 30 days, or the 10k most recently modified records.
- `email_events`- not limited
- `activities` - this is [Engagements API](https://developers.hubspot.com/docs/methods/engagements/engagements-overview) in Hubspot jargon. This endpoint will only return records updated in the last 30 days, or the 10k most recently updated records.
- `companies` - This endpoint will only return records updated in the last 30 days, or the 10k most recently updated records.

#### Supported Endpoints

- [Companies](#Companies)  
  **Result tables** : `companies`
- [Contacts](#Contacts)  
  **Result tables** : `contacts`, `contacts_form_submissions`, `contacts_lists`, `contacts_identity_profile_identities`
  , `contacts_identity_profiles`

- [Deals](#Deals)  
  **Result tables** : `deals`, `deals_stage_history`, `deals_contacts_list`, `deals_assoc_deals_list`
  , `deals_assoc_companies_list`
- [Pipelines](#Pipelines)  
  **Result tables** : `pipelines`, `pipeline_stages`

- [Campaigns](#Campaigns)  
  **Result tables** : `campaigns`

- [Email Events](#Email Events)  
  **Result tables** : `email_events`
- [Activities (Engagements)](#Engagements)    
  **Result tables** : `activities`
- [Lists](#Contact Lists)    
  **Result tables** : `lists`
- [Owners](#Owners)    
  **Result tables** : `owners`
- [Meetings](https://developers.hubspot.com/docs/api/crm/meetings) - engagement meetings
- [Calls](https://developers.hubspot.com/docs/api/crm/calls) - engagement calls
- [Emails](https://developers.hubspot.com/docs/api/crm/emails) - engagement emails
- [Forms](https://developers.hubspot.com/docs/api/marketing/forms) - Marketing api Forms
- [Marketing Emails Statistics](https://legacydocs.hubspot.com/docs/methods/cms_email/get-all-marketing-email-statistics) - CMS Marketing Emails Statistics

================================================
File: /component_config/component_short_description.md
================================================
The HubSpot CRM helps companies grow traffic, convert leads, get insights to close more deals, etc

================================================
File: /component_config/stack_parameters.json
================================================
{}

================================================
File: /component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Configuration",
  "required": [
    "period_from",
    "endpoints",
    "company_properties",
    "contact_properties",
    "include_contact_list_membership",
    "deal_properties",
    "property_attributes",
    "incremental_output"
  ],
  "properties": {
    "authentication_type": {
      "type": "string",
      "title": "Authentication Type",
      "description": "<b style='color:red'>NOTE: API Key authentication will be deprecated starting 10/2022.</b>",
      "enum": [
        "API Key",
        "Private App Token"
      ],
      "default": "API Key",
      "propertyOrder": 10
    },
    "endpoints": {
      "type": "array",
      "title": "Endpoints",
      "items": {
        "enum": [
          "companies",
          "campaigns",
          "activities",
          "lists",
          "owners",
          "contacts",
          "deals",
          "pipelines",
          "calls",
          "emails",
          "marketing_email_statistics",
          "meetings",
          "forms",
          "email_events",
          "email_events-SENT",
          "email_events-DROPPED",
          "email_events-PROCESSED",
          "email_events-DELIVERED",
          "email_events-DEFERRED",
          "email_events-BOUNCE",
          "email_events-OPEN",
          "email_events-CLICK"
        ],
        "options": {
          "enum_titles": [
            "companies",
            "campaigns",
            "activities",
            "lists",
            "owners",
            "contacts",
            "deals",
            "pipelines",
            "calls",
            "emails",
            "marketing_email_statistics",
            "meetings",
            "forms",
            "email_events-CLICK&OPEN",
            "email_events-SENT",
            "email_events-DROPPED",
            "email_events-PROCESSED",
            "email_events-DELIVERED",
            "email_events-DEFERRED",
            "email_events-BOUNCE",
            "email_events-OPEN",
            "email_events-CLICK"
          ]
        },
        "type": "string"
      },
      "format": "checkbox",
      "default": [
        "companies",
        "campaigns",
        "activities",
        "lists",
        "owners",
        "contacts",
        "deals",
        "pipelines",
        "email_events-BOUNCE",
        "email_events-OPEN"
      ],
      "uniqueItems": true,
      "propertyOrder": 360
    },
    "#api_token": {
      "type": "string",
      "title": "API token",
      "format": "password",
      "propertyOrder": 200,
      "options": {
        "dependencies": {
          "authentication_type": [
            "API Key",
            null
          ]
        }
      }
    },
    "#private_app_token": {
      "type": "string",
      "title": "Private App Token",
      "description": "Your private app token. More info <a href=\"https://developers.hubspot.com/docs/api/migrate-an-api-key-integration-to-a-private-app\">here</a>",
      "format": "password",
      "propertyOrder": 200,
      "options": {
        "dependencies": {
          "authentication_type": [
            "Private App Token"
          ]
        }
      }
    },
    "period_from": {
      "type": "string",
      "title": "Period from date [including]",
      "description": "Date in YYYY-MM-DD format or dateparser string i.e. 5 days ago, 1 month ago, yesterday, etc. If left empty, all records are downloaded.",
      "propertyOrder": 300
    },
    "incremental_output": {
      "type": "number",
      "enum": [
        0,
        1
      ],
      "options": {
        "enum_titles": [
          "Full Load",
          "Incremental Update"
        ]
      },
      "default": 1,
      "title": "Load type",
      "description": "If set to Incremental update, the result tables will be updated based on primary key. Full load overwrites the destination table each time. NOTE: If you wish to remove deleted records, this needs to be set to Full load and the Period from attribute empty.",
      "propertyOrder": 365
    },
    "property_attributes": {
      "type": "object",
      "title": "Property additional attributes",
      "description": "Add additional attributes to each custom property. NOTE: Applicable for Contacts, Companies and Deals.",
      "options": {
        "expand_height": true
      },
      "propertyOrder": 400,
      "required": [
        "include_versions",
        "include_source",
        "include_timestamp"
      ],
      "format": "grid",
      "properties": {
        "include_versions": {
          "type": "number",
          "enum": [
            0,
            1
          ],
          "options": {
            "enum_titles": [
              "No",
              "Yes"
            ]
          },
          "default": 0,
          "title": "Include attribute versions",
          "description": "A list of previous versions of the property. The first item in the list will be the current version"
        },
        "include_source": {
          "type": "number",
          "enum": [
            0,
            1
          ],
          "options": {
            "enum_titles": [
              "No",
              "Yes"
            ]
          },
          "default": 0,
          "title": "Include source attribute",
          "description": "The method by which this version was set."
        },
        "include_timestamp": {
          "type": "number",
          "enum": [
            0,
            1
          ],
          "options": {
            "enum_titles": [
              "No",
              "Yes"
            ]
          },
          "default": 0,
          "title": "Include timestamp attribute",
          "description": "A Unix timestamp (in milliseconds) of the time when this version was set"
        }
      }
    },
    "deal_properties": {
      "type": "string",
      "title": "Deal additional properties (if selected)",
      "format": "textarea",
      "default": "authority, budget, campaign_source, hs_analytics_source, hs_campaign, hs_lastmodifieddate, need, timeframe, dealname, amount, closedate, pipeline, createdate, engagements_last_meeting_booked, dealtype, hs_createdate, description, start_date, closed_lost_reason, closed_won_reason, end_date, lead_owner, tech_owner, service_amount, contract_type, hubspot_owner_id, partner_name, notes_last_updated",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of Deal properties. The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched.",
      "uniqueItems": true,
      "propertyOrder": 700
    },
    "company_properties": {
      "type": "string",
      "title": "Company additional properties (if selected)",
      "format": "textarea",
      "default": "about_us, name, phone, facebook_company_page, city, country, website, industry, annualrevenue, linkedin_company_page, hs_lastmodifieddate, hubspot_owner_id, notes_last_updated, description, createdate, numberofemployees, hs_lead_status, founded_year, twitterhandle, linkedinbio",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of Company properties. The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched.",
      "uniqueItems": true,
      "propertyOrder": 500
    },
    "contact_properties": {
      "type": "string",
      "title": "Contact additional properties (if selected)",
      "format": "textarea",
      "default": "hs_facebookid, hs_linkedinid, ip_city, ip_country, ip_country_code, newsletter_opt_in, firstname, linkedin_profile, lastname, email, mobilephone, phone, city, country, region, jobtitle, company, website, numemployees, industry, associatedcompanyid, hs_lead_status, lastmodifieddate, source, hs_email_optout, twitterhandle, lead_type, hubspot_owner_id, notes_last_updated, hs_analytics_source, opt_in, createdate, hs_twitterid, lifecyclestage",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of contact properties. The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched.",
      "uniqueItems": true,
      "propertyOrder": 600
    },
    "include_contact_list_membership": {
      "type": "boolean",
      "title": "Include contact list memberships",
      "default": true,
      "description": "If true, list memberships will be downloaded for each contact stored in contact_lists table -> may lead to longer download times and large data",
      "propertyOrder": 610
    },
    "download_contact_associations": {
      "type": "boolean",
      "title": "Fetch contact associations",
      "descriptions": "Fetch associations in case there are multiple associated objects. [BETA API]",
      "default": false,
      "propertyOrder": 611
    },
    "contact_associations": {
      "type": "array",
      "title": "Property additional attributes",
      "description": "Fetch associations in case there are multiple associated objects. [BETA API]",
      "options": {
        "dependencies": {
          "download_contact_associations": true
        },
        "minItems": 1
      },
      "default": "company",
      "format": "grid",
      "propertyOrder": 615,
      "items": {
        "type": "object",
        "title": "Association type",
        "required": [
          "to_object_type"
        ],
        "properties": {
          "to_object_type": {
            "type": "string",
            "title": "To object type",
            "default": "company",
            "propertyOrder": 1
          }
        }
      }
    },
    "call_properties": {
      "type": "string",
      "title": "Call additional properties (if selected)",
      "format": "textarea",
      "default": "",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of call properties. The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched.",
      "uniqueItems": true,
      "propertyOrder": 618
    },
    "email_properties": {
      "type": "string",
      "title": "Email additional properties (if selected)",
      "format": "textarea",
      "default": "",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of email properties. The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched.",
      "uniqueItems": true,
      "propertyOrder": 620
    },
    "meeting_properties": {
      "type": "string",
      "title": "Meetings additional properties (if selected)",
      "format": "textarea",
      "default": "",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of meeting properties. The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched.",
      "uniqueItems": true,
      "propertyOrder": 620
    }
  }
}

================================================
File: /component_config/sample-config/config.json
================================================
{
	"storage": {
		"input": {
			"files": [],
			"tables": [{
					"source": "in.c-test.test",
					"destination": "test.csv",
					"limit": 50,
					"columns": [],
					"where_values": [],
					"where_operator": "eq"
				}
			]
		},
		"output": {
			"files": [],
			"tables": []
		}
	},
	"parameters": {
		"#api_token": "demo",
		"period_from": "2 months ago",
		"endpoints": [
			"companies",
			"campaigns",
			"email_events",
			"activities",
			"lists",
			"owners",
			"contacts",
			"deals",
			"pipelines"
		],
		"company_properties": "about_us, name, phone, facebook_company_page, city, country, website, industry, annualrevenue, linkedin_company_page, hs_lastmodifieddate, hubspot_owner_id, notes_last_updated, description, createdate, numberofemployees, hs_lead_status, founded_year, twitterhandle, linkedinbio",
		"contact_properties": "hs_facebookid, hs_linkedinid, ip_city, ip_country, ip_country_code, newsletter_opt_in, firstname, linkedin_profile, lastname, email, mobilephone, phone, city, country, region, jobtitle, company, website, numemployees, industry, associatedcompanyid, hs_lead_status, lastmodifieddate, source, hs_email_optout, twitterhandle, lead_type, hubspot_owner_id, notes_last_updated, hs_analytics_source, opt_in, createdate, hs_twitterid, lifecyclestage",
		"deal_properties": "authority, budget, campaign_source, hs_analytics_source, hs_campaign, hs_lastmodifieddate, need, timeframe, dealname, amount, closedate, pipeline, createdate, engagements_last_meeting_booked, dealtype, hs_createdate, description, start_date, closed_lost_reason, closed_won_reason, end_date, lead_owner, tech_owner, service_amount, contract_type, hubspot_owner_id, partner_name, notes_last_updated",
		"debug": true
	},
	"image_parameters": {
		"syrup_url": "https://syrup.keboola.com/"
	},
	"authorization": {
		"oauth_api": {
			"id": "OAUTH_API_ID",
			"credentials": {
				"id": "main",
				"authorizedFor": "Myself",
				"creator": {
					"id": "1234",
					"description": "me@keboola.com"
				},
				"created": "2016-01-31 00:13:30",
				"#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
				"oauthVersion": "2.0",
				"appKey": "000000004C184A49",
				"#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
			}
		}
	}
}


================================================
File: /component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: /component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /requirements.txt
================================================
keboola.component
hubspot-api-client
keboola.csvwriter==1.0.1
keboola.http-client
keboola.utils
pytz
python-dateutil
pandas===1.2.4
dateparser


================================================
File: /Dockerfile
================================================
FROM python:3.8.6-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

RUN apt-get update && apt-get install -y build-essential

RUN pip install flake8
RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]


================================================
File: /LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /README.md
================================================
# Hubspot extractor

KBC Component for data retrieval from [Hubspot API](https://developers.hubspot.com/docs/overview).

**Table of contents:**

[TOC]

# Configuration

## Authentication

This component supports also legacy API Key authentication that is being deprecated completely
since [30 November 2022](https://developers.hubspot.com/changelog/upcoming-api-key-sunset)

You need to create
a [Private App](https://developers.hubspot.com/docs/api/migrate-an-api-key-integration-to-a-private-app) in your account
and enable all following scopes:
![scopes](docs/imgs/scopes.png)

### Authentication Method

Two methods supported:

- `Private App Token`
- `API Key`

Please use the `Private App Token` in all cases.

### Private App Token
[REQ] 
Create a [Private App](https://developers.hubspot.com/docs/api/migrate-an-api-key-integration-to-a-private-app) in your
account and enable all following scopes:
![scopes](docs/imgs/scopes.png)

Then use the generated token.

### **API token**

 **NOTE that this method is deprecated since Nov 22**

[REQ] Your Hubspot API token, for more information see
here ([how-do-i-get-my-hubspot-api-key](https://knowledge.hubspot.com/articles/kcs_article/integrations/how-do-i-get-my-hubspot-api-key))

### **Period from date [including]**

[OPT] Date marking start of the period for incremental loads, only records created/modified in that period are
retrieved. Accepts date in `YYYY-MM-DD` format or relative textual values like `today`,`yesterday`, `1 day ago`
, `1 month ago`, etc. If left empty all records are downloaded. **NOTE** that this parameter affects only some of the
endpoints specifically:

- `contacts` - The endpoint only scrolls back in time 30 days. **NOTE** that it may happen that returned data contain
  contacts updated before the specified date, this can happen when there is <100 contacts modified since that date. The
  pagination since date is simulated by the app, so always at least first page (100) of contacts is returned.
- `deals` - This endpoint will only return records modified in the last 30 days, or the 10k most recently modified
  records.
- `email_events`- not limited
- `activities` - this is [Engagements API](https://developers.hubspot.com/docs/methods/engagements/engagements-overview)
  in Hubspot jargon. This endpoint will only return records updated in the last 30 days, or the 10k most recently
  updated records.
- `companies` - This endpoint will only return records updated in the last 30 days, or the 10k most recently updated
  records.

### Endpoints

List of endpoints to download. Currently supported are:

- Deals
- Pipelines
- Campaigns
- Email Events
- Activities (Engagements)
- Contact Lists
- [Owners](https://developers.hubspot.com/docs/api/crm/owners)
- [Meetings](https://developers.hubspot.com/docs/api/crm/meetings) - engagement meetings
- [Calls](https://developers.hubspot.com/docs/api/crm/calls) - engagement calls
- [Emails](https://developers.hubspot.com/docs/api/crm/emails) - engagement emails
- [Forms](https://developers.hubspot.com/docs/api/marketing/forms) - Marketing api Forms
- [Marketing Emails Statistics](https://legacydocs.hubspot.com/docs/methods/cms_email/get-all-marketing-email-statistics)
  - CMS Marketing Emails Statistics

For more info see the **Supported endpoints** section

### Incremental output

Set this parameter to `true` if you wish to output data incrementally - i.e. to upsert new data to the result table
based on the primary key.

#### **Deleted records**

It is important to mention that HubSpot API **does not** return deleted records. That means that even though some tables
like `deals` will contain `isDeleted` field, it's value will be always `False`. To keep track of deleted records you
need to:

- set Incremental Output to False
- remove any value in `Period from date` parameter to retrieve the full history of reccords.

### **Additional Property attributes**

Additional attributes that are fetched for each specified property. Applies only for `companies`,`contacts` and `deals`
endpoint, for which it is possible to define a list of additional properties.

- **`Include source attribute`** - includes additional attributes `[property_name]_source`
  and `[property_name]_sourceId` that specify the method by which the value was set.
- **`Include timestamp attribute`** - includes additional attributes `[property_name]_timestamp` A Unix timestamp (in
  milliseconds) of the time the property was last set.
- **`Include attribute versions`** - includes additional attributes `[property_name]_timestamp` A list of previous
  versions of the property. The first item in the list will be the current version. This field contains a JSON array
  value.

### Additional properties

- **Company properties** - [OPT] Additional list of properties, names must match with api names as specified
  by [Company Properties](https://developers.hubspot.com/docs/methods/companies/company-properties-overview). The
  endpoint must be listed in `endpoints` parameter for this to take effect.
- **Contact properties** -[OPT]  Additional list of properties, names must match with api names as specified
  by [Deal Properties](https://developers.hubspot.com/docs/methods/deals/deal_properties_overview)  The endpoint must be
  listed in `endpoints` parameter for this to take effect.
- **Deal properties** - [OPT] Additional list of properties, names must match with api names as specified
  by [Deal Properties](https://developers.hubspot.com/docs/methods/deals/deal_properties_overview)  The endpoint must be
  listed in `endpoints` parameter for this to take effect.

#### How to find internal name of a property

- In the UI navigate to Contacts, Deals or Companies
- In the top right corner choose `Actions` -> `Edit properties`

![Edit properties](docs/imgs/edit_properties.png)

- Hover over required property and click `Edit`

![Edit property](docs/imgs/edit_property.png)

- Look for `Internal name`, this is what you need to include in the `Additional properties field`

![Internal name](docs/imgs/internal_name.png)

# Functionality

Supports retrieval from several endpoints. Some endpoints allow retrieval of recently updated records,   
this is set by `Date From` parameter. In most of the cases maximum of last 30 days can be retrieved.

# Supported Endpoints

- [Companies](#Companies)  
  **Result tables** : `companies`
- [Contacts](#Contacts)  
  **Result tables** : `contacts`, `contacts_form_submissions`, `contacts_lists`, `contacts_identity_profile_identities`
  , `contacts_identity_profiles`

- [Deals](#Deals)  
  **Result tables** : `deals`, `deals_stage_history`, `deals_contacts_list`, `deals_assoc_deals_list`
  , `deals_assoc_companies_list`
- [Pipelines](#Pipelines)  
  **Result tables** : `pipelines`, `pipeline_stages`

- [Campaigns](#Campaigns)  
  **Result tables** : `campaigns`

- [Email Events](#Email Events)  
  **Result tables** : `email_events`
- [Activities (Engagements)](#Engagements)    
  **Result tables** : `activities`
- [Lists](#Contact Lists)    
  **Result tables** : `lists`
- [Owners](#Owners)    
  **Result tables** : `owners`
- [Meetings](https://developers.hubspot.com/docs/api/crm/meetings) - engagement meetings
- [Calls](https://developers.hubspot.com/docs/api/crm/calls) - engagement calls
- [Emails](https://developers.hubspot.com/docs/api/crm/emails) - engagement emails
- [Forms](https://developers.hubspot.com/docs/api/marketing/forms) - Marketing api Forms
- [Marketing Emails Statistics](https://legacydocs.hubspot.com/docs/methods/cms_email/get-all-marketing-email-statistics)
  - CMS Marketing Emails Statistics

## Companies

[All companies](https://developers.hubspot.com/docs/methods/companies/get-all-companies) or   
[recently modified (last 30 days) ](https://developers.hubspot.com/docs/methods/companies/get_companies_modified) can be
retrieved.   
NOTE: Fetches always 30 day period

Following Company properties are fetched by default:

```python  
 ["about_us", "name", "phone", "facebook_company_page", "city", "country", "website", "industry", "annualrevenue", "linkedin_company_page", "hs_lastmodifieddate", "hubspot_owner_id", "notes_last_updated",   
"description", "createdate", "numberofemployees", "hs_lead_status", "founded_year", "twitterhandle", "linkedinbio"]
```  

Custom properties may be specified in configuration, names must match with api names as specified
by [Company Properties](https://developers.hubspot.com/docs/methods/companies/company-properties-overview)

**Result tables** :

- `companies`

## Contacts

[All contacts](https://developers.hubspot.com/docs/methods/contacts/get_contacts) or   
[recently modified (max last 30 days) ](https://developers.hubspot.com/docs/methods/contacts/get_recently_updated_contacts)
can be retrieved.   
Recently modified period can be limited by `Date From` parameter

Following Contact properties are fetched by default:

```json  
 ["hs_facebookid", "hs_linkedinid", "ip_city", "ip_country", "ip_country_code", "newsletter_opt_in", "firstname", "linkedin_profile", "lastname", "email", "mobilephone", "phone", "city", "country", "region", "jobtitle",   
 "company", "website", "numemployees", "industry", "associatedcompanyid", "hs_lead_status",   
 "lastmodifieddate", "source", "hs_email_optout", "twitterhandle", "lead_type", "hubspot_owner_id",   
 "notes_last_updated", "hs_analytics_source", "opt_in", "createdate", "hs_twitterid", "lifecyclestage"]  
```  

**Note:** Following properties will be fetched each time, regardless configuration and with
option `propertyMode=value_and_history`. This is currently hardcoded and   
all other properties are fetched with value only:

```json  
 ["company", "firstname", "lastmodifieddate", "lastname"]
```

Custom properties may be specified in configuration, names must match with api names as specified
by [Contact Properties](https://developers.hubspot.com/docs/methods/contacts/contact-properties-overview)

**Result tables** :

- `contacts`
- `contacts_form_submissions`
- `contacts_lists`
- `contacts_identity_profile_identities`
- `contacts_identity_profiles`

## Deals

[All deals](https://developers.hubspot.com/docs/methods/deals/get-all-deals) or   
[recently modified (last 30 days) ](https://developers.hubspot.com/docs/methods/deals/get_deals_modified) can be
retrieved.   
NOTE: Fetches max 30 day period, larger periods are cut to match the limit.

Following Deal properties are fetched by default:

```json  
["authority", "budget", "campaign_source", "hs_analytics_source", "hs_campaign", "hs_lastmodifieddate", "need", "timeframe", "dealname", "amount", "closedate", "pipeline", "createdate", "engagements_last_meeting_booked", "dealtype", "hs_createdate", "description", "start_date", "closed_lost_reason", "closed_won_reason", "end_date", "lead_owner", "tech_owner", "service_amount", "contract_type", "hubspot_owner_id", "partner_name", "notes_last_updated"]  
```  

Custom properties may be specified in configuration, names must match with api names as specified
by [Deal Properties](https://developers.hubspot.com/docs/methods/deals/deal_properties_overview)

**Result tables** :

- `deals`
- `deals_stage_history`
- `deals_contacts_list`
- `deals_assoc_deals_list`
- `deals_assoc_companies_list`

## Pipelines

[All pipelines](https://developers.hubspot.com/docs/methods/pipelines/get_pipelines_for_object_type) - gets all
pipelines and its stages.

**Result tables** :

- `pipelines`
- `pipeline_stages`

## Campaigns

[All Campaigns](https://developers.hubspot.com/docs/methods/email/get_campaigns_by_id)

NOTE: Fetches max 30 day period

**Result tables** :

- `campaigns`

## Email Events

[All Email Events](https://developers.hubspot.com/docs/methods/email/get_events)  - possible to limit by `Date From`
parameter.

NOTE: Fetches max 30 day period, larger periods are cut to match the limit.

**Result tables** :

- `email_events`

## Activities (Engagements)

[All Activities](https://developers.hubspot.com/docs/methods/engagements/get-all-engagements) or   
[recently modified (max last 30 days) ](https://developers.hubspot.com/docs/methods/engagements/get-recent-engagements)

-

possible to limit by `Date From` parameter.

NOTE: Fetches max 30 day period, larger periods are cut to match the limit.

**Result tables** :

- `activities`

## Lists

[All Lists](https://developers.hubspot.com/docs/methods/lists/get_lists)

NOTE: Always fetches all available lists

**Result tables** :

- `lists`

## Owners

[All owners](https://developers.hubspot.com/docs/methods/owners/get_owners)

NOTE: Always sets `include_inactive` to `True`

**Result tables** :

- `owners`

# Development

This example contains runnable container with simple unittest. For local testing it is useful to include `data` folder
in the root  
and use docker-compose commands to run the container or execute tests.

If required, change local data folder path to your custom:

```yaml  
 volumes: - ./:/code - ./CUSTOM_FOLDER:/data
```

Clone this repository and init the workspace with following command:

```  
git clone https://bitbucket.org:kds_consulting_team/kds-team.ex-hubspot-crm.git  
cd kds-team.ex-hubspot  
docker-compose build  
docker-compose run --rm dev  
```  

Run the test suite and lint check using this command:

```  
docker-compose run --rm test 
```  

# Integration

For information about deployment and integration with KBC, please refer to
the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/)

