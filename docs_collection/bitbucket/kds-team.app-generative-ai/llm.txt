Directory structure:
└── kds_consulting_team-kds-team.app-generative-ai/
    ├── flake8.cfg
    ├── deploy.sh
    ├── docker-compose.yml
    ├── src/
    │   ├── templates/
    │   │   └── prompts.json
    │   ├── component.py
    │   ├── configuration.py
    │   └── client/
    │       ├── openai_client.py
    │       ├── anthropic_client.py
    │       ├── base.py
    │       ├── googleai_client.py
    │       ├── __init__.py
    │       └── huggingface_client.py
    ├── bitbucket-pipelines.yml
    ├── tests/
    │   ├── test_component.py
    │   └── __init__.py
    ├── component_config/
    │   ├── logger
    │   ├── component_long_description.md
    │   ├── configuration_description.md
    │   ├── component_short_description.md
    │   ├── loggerConfiguration.json
    │   ├── configSchema.json
    │   └── configRowSchema.json
    ├── scripts/
    │   ├── update_dev_portal_properties.sh
    │   ├── run_kbc_tests.ps1
    │   ├── build_n_run.ps1
    │   └── build_n_test.sh
    ├── requirements.txt
    ├── docs/
    │   └── imgs/
    ├── Dockerfile
    ├── LICENSE.md
    └── README.md

================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /src/templates/prompts.json
================================================
{
  "timestamp_from_date": "Convert the date [[INPUT_COLUMN]] into a timestamp. Provide the result in the following format: YYYY-MM-DD HH:MM:SS. Ensure the format is strictly followed.",
  "remove_diacritics": "Carefully remove diacritics from the text: [[INPUT_COLUMN]]. Ensure the original meaning and words remain unchanged.",
  "add_diacritics": "Thoughtfully add appropriate diacritics to the text: [[INPUT_COLUMN]]. Ensure the diacritics match the context of the text.",
  "extract_topics": "Please think carefully and extract main topics from the text: [[INPUT_COLUMN]]. Return them as a JSON array. Only include topics you're confident about.",
  "sentiment_scoring": "Score the sentiment of the text [[INPUT_COLUMN]] on a scale of -1.00 (very negative) to 1.00 (very positive). For example, 'I love this!' should be close to 1.00 and 'I hate this.' should be close to -1.00. Ensure the score accurately reflects the sentiment.",
  "text_shortener": "Shorten the text [[INPUT_COLUMN]] to a maximum of 140 characters. Provide the shortened version and ensure it maintains the core message of the original text. The result must not exceed 140 characters.",
  "grammar_correction": "Correct the grammar of the text [[INPUT_COLUMN]]. It's crucial not to change the overall meaning or add any extra information. Ensure the correction is precise and coherent."
}

================================================
File: /src/component.py
================================================
"""
Template Component main class.

"""
import asyncio
import csv
import dataclasses
import logging
from typing import List
import json
import os
from io import StringIO
from itertools import islice
import pystache as pystache
import requests.exceptions

from keboola.component.base import ComponentBase, sync_action
from keboola.component.sync_actions import ValidationResult, MessageType
from keboola.component.dao import TableDefinition
from keboola.component.exceptions import UserException
from kbcstorage.tables import Tables
from kbcstorage.client import Client

from configuration import Configuration
from client.openai_client import OpenAIClient, AzureOpenAIClient
from client.googleai_client import GoogleAIClient
from client.base import AIClientException
from client.huggingface_client import HuggingfaceClient
from client.anthropic_client import AnthropicClient

# configuration variables
RESULT_COLUMN_NAME = 'result_value'
KEY_API_TOKEN = '#api_token'
KEY_PROMPT = 'prompt'
KEY_DESTINATION = 'destination'

KEY_DEFAULT_API_TOKEN = '#default_api_token'
KEY_DEFAULT_API_TOKEN_HUGGINGFACE = '#default_api_token_huggingface'

KEY_ENDPOINT_URL = 'endpoint_url'

# list of mandatory parameters => if some is missing,
# component will fail with readable message on initialization.
REQUIRED_PARAMETERS = [KEY_PROMPT, KEY_DESTINATION]

PREVIEW_LIMIT = 5
BATCH_SIZE = 10
LOG_EVERY = 100
PROMPT_TEMPLATES = 'templates/prompts.json'


class Component(ComponentBase):

    def __init__(self):
        super().__init__()
        self.table_rows: int = 0
        self.processed_table_rows: int = 0
        self.service = None
        self.api_key = None
        # For Azure OpenAI
        self.deployment_id = None
        self.api_base = None
        self.api_version = None

        self.max_token_spend = 0
        self.model_options = None
        self.input_keys = None
        self.queue_v2 = None
        self.model = None
        self._configuration = None
        self.failed_requests = 0
        self.tokens_used = 0
        self.token_limit_reached = False
        self.out_table_columns = []

        if logging.getLogger().isEnabledFor(logging.INFO):
            httpx_logger = logging.getLogger("httpx")
            httpx_logger.setLevel(logging.ERROR)

    def run(self):
        """
        Main execution code
        """
        self.init_configuration()

        client = self.get_client()
        input_table, out_table = self.prepare_tables()
        self.table_rows = self.count_rows(input_table.full_path)

        asyncio.run(self.process_prompts(client, input_table, out_table))

        self.write_manifest(out_table)

        if self.failed_requests > 0:
            if self.queue_v2:
                self.add_flag_to_manifest()
            raise UserException(f"Component has failed to process {self.failed_requests} records.")
        else:
            if self.token_limit_reached:
                logging.error("Component has been stopped after reaching total token spend limit.")
            else:
                logging.info(f"All rows processed, total token usage = {self.tokens_used}")

    def init_configuration(self):
        self.validate_configuration_parameters(Configuration.get_dataclass_required_parameters())
        self._configuration: Configuration = Configuration.load_from_dict(self.configuration.parameters)

        if self._configuration.max_token_spend > 0:
            self.max_token_spend = self._configuration.max_token_spend
            logging.warning(f"Max token spend has been set to {self.max_token_spend}. If the component reaches "
                            f"this limit, it will exit.")

        self.input_keys = self._get_input_keys(self._configuration.prompt_options.prompt)

        self.queue_v2 = 'queuev2' in os.environ.get('KBC_PROJECT_FEATURE_GATES', '')
        if self.queue_v2:
            logging.info("Component will try to save results even if some queries fail.")
        else:
            logging.warning("Running on old queue, results cannot be stored on failure.")

        self.service = self._configuration.authentication.service
        if self._configuration.authentication.service == "azure_openai":
            self.api_base = self._configuration.authentication.api_base
            self.deployment_id = self._configuration.authentication.deployment_id
            self.api_version = self._configuration.authentication.api_version
        self.api_key = self._configuration.authentication.pswd_api_token

        self.model = self._configuration.model
        logging.info(f"The component is using the model: {self.model}")

        self.model_options = dataclasses.asdict(self._configuration.additional_options)

    def get_client(self):
        if self.service == "openai":
            return OpenAIClient(api_key=self.api_key)

        elif self.service == "azure_openai":
            return AzureOpenAIClient(self.api_key, self.api_base, self.deployment_id, self.api_version)

        elif self.service == "google":
            if self.api_key == "":
                self.api_key = self.configuration.image_parameters.get(KEY_DEFAULT_API_TOKEN)
                logging.info("Using API key provided by Keboola.")
            return GoogleAIClient(self.api_key)

        elif self.service == "huggingface":
            if not self.api_key and self.model != "custom":
                self.api_key = self.configuration.image_parameters.get(KEY_DEFAULT_API_TOKEN_HUGGINGFACE)
                logging.info("Using API key provided by Keboola.")
            return HuggingfaceClient(self.api_key, endpoint_url=self.configuration.parameters.get(KEY_ENDPOINT_URL))

        elif self.service == "anthropic":
            return AnthropicClient(self.api_key)

        else:
            raise UserException(f"{self.service} service is not implemented yet.")

    async def process_prompts(self, client, input_table, out_table) -> None:

        with open(input_table.full_path, 'r') as input_file:
            reader = csv.DictReader(input_file)

            with open(out_table.full_path, 'w+') as out_file:
                writer = csv.DictWriter(out_file, fieldnames=self.out_table_columns)
                writer.writeheader()
                rows = []
                for row in reader:
                    rows.append(row)

                    if len(rows) >= BATCH_SIZE:
                        batch_results = await self.process_batch(client, rows)
                        writer.writerows(batch_results)
                        rows = []

                    if self.max_token_spend != 0 and self.tokens_used >= self.max_token_spend:
                        self.token_limit_reached = True
                        logging.warning(f"The token spend limit of {self.max_token_spend} has been reached. "
                                        f"The component will stop after completing current batch.")
                        break

                # Process remaining rows
                if rows:
                    batch_results = await self.process_batch(client, rows)
                    writer.writerows(batch_results)

    async def process_batch(self, client, rows: list):
        prompts = []
        for row in rows:
            prompt = self._build_prompt(self.input_keys, row)
            prompts.append(prompt)

        tasks = []
        for row, prompt in zip(rows, prompts):
            tasks.append(self._infer(client, row, prompt))

        return await asyncio.gather(*tasks)

    async def _infer(self, client, row, prompt):

        try:
            result, token_usage = await client.infer(model_name=self.model, prompt=prompt, **self.model_options)

        except AIClientException as e:
            if not self.queue_v2:
                if "User location is not supported for the API use" in str(e):
                    raise UserException("Google AI services are only available on US stack.")
                raise UserException(f"Error occured while calling AI API: {e}")

            logging.warning(f"Failed to process row {row}, reason: {e}.")
            return self._build_output_row(row, str(e))

        self.tokens_used += token_usage
        logging.debug(f"Tokens spent: {self.tokens_used}")
        self.processed_table_rows += 1

        if self.processed_table_rows % LOG_EVERY == 0:
            logging.info(f"Processed {self.processed_table_rows} rows. tokens used: {self.tokens_used}")

        if result:
            return self._build_output_row(row, result)
        else:
            self.failed_requests += 1

    def prepare_tables(self):
        input_table = self._get_input_table()
        if missing_keys := [key for key in self.input_keys if key not in input_table.columns]:
            raise UserException(f'The columns "{missing_keys}" need to be present in the input data!')

        out_table = self._build_out_table(input_table)

        if missing_keys := [t for t in out_table.primary_key if t not in input_table.columns]:
            raise UserException(f'Some specified primary keys are not in the input table: {missing_keys}')

        return input_table, out_table

    @staticmethod
    def _build_output_row(input_row: dict, result: str):
        output_row = input_row.copy()
        output_row[RESULT_COLUMN_NAME] = result.strip()
        return output_row

    def _build_out_table(self, input_table: TableDefinition) -> TableDefinition:
        destination_config = self.configuration.parameters['destination']

        if not (out_table_name := destination_config.get("output_table_name")):
            out_table_name = f"app-generative-ai-{self.environment_variables.config_row_id}.csv"
        else:
            out_table_name = f"{out_table_name}.csv"

        self.out_table_columns = input_table.columns + [RESULT_COLUMN_NAME]

        primary_key = destination_config.get('primary_keys_array', [])

        incremental_load = destination_config.get('incremental_load', False)
        return self.create_out_table_definition(out_table_name, columns=[], primary_key=primary_key,
                                                incremental=incremental_load)

    @staticmethod
    def _get_input_keys(prompt: str):
        template = pystache.parse(prompt, delimiters=('[[', ']]'))
        keys = [token.key for token in template._parse_tree if hasattr(token, "key")]  # noqa
        if len(keys) < 1:
            raise UserException('You must provide at least one input placeholder. 0 were found.')

        unique_keys = list(dict.fromkeys(keys))
        return unique_keys

    def _build_prompt(self, input_keys: List[str], row: dict):
        prompt = self._configuration.prompt_options.prompt
        for input_key in input_keys:
            prompt = prompt.replace('[[' + input_key + ']]', row[input_key])
        return prompt

    def _get_input_table(self) -> TableDefinition:
        if not self.get_input_tables_definitions():
            raise UserException("No input table specified. Please provide one input table in the input mapping!")

        if len(self.get_input_tables_definitions()) > 1:
            raise UserException("Only one input table is supported")

        return self.get_input_tables_definitions()[0]

    def add_flag_to_manifest(self):
        manifests = [x for x in os.listdir(self.tables_out_path) if x.endswith('.manifest')]
        if manifests:
            for filename in manifests:
                path = os.path.join(self.tables_out_path, filename)

                with open(path, 'r') as f:
                    data = json.load(f)
                    data['write_always'] = True

                with open(path, 'w') as f:
                    json.dump(data, f)

    def estimate_token_usage(self, preview_size: int, table_size: int) -> int:
        """Estimates token usage based on number of tokens used during test_prompt."""
        if preview_size > 0:
            return int((self.tokens_used / preview_size) * table_size)
        raise UserException("Cannot process tables with no rows.")

    @staticmethod
    def create_markdown_table(data):
        if not data:
            return ""

        table = "| " + RESULT_COLUMN_NAME + " |\n"
        table += "| --- |\n"

        for value in data:
            value = str(value).replace("\n", " ")
            table += "| " + value + " |\n"

        return table

    def _get_table_preview(self, table_id: str, columns: list[str] = None, limit: int = None) -> list[dict]:
        tables = Tables(self._get_kbc_root_url(), self._get_storage_token())
        try:
            preview = tables.preview(table_id, columns=columns)
        except requests.exceptions.HTTPError as e:
            raise UserException(f"Unable to retrieve table preview: {e}")

        data = []
        csv_reader = csv.DictReader(StringIO(preview))

        if limit is None:
            for row in csv_reader:
                data.append(row)
        else:
            for row in islice(csv_reader, limit):
                data.append(row)

        return data

    def _get_table_size(self, table_id: str) -> int:
        """Returns number of rows for specified table_id."""
        tables = Tables(self._get_kbc_root_url(), self._get_storage_token())
        detail = tables.detail(table_id)
        rows_count = int(detail.get("rowsCount"))
        return rows_count

    def _get_kbc_root_url(self) -> str:
        return f'https://{self.environment_variables.stack_id}' if self.environment_variables.stack_id \
            else "https://connection.keboola.com"

    def _get_storage_source(self) -> str:
        storage_config = self.configuration.config_data.get("storage")
        if not storage_config.get("input", {}).get("tables"):
            raise UserException("Input table must be specified.")
        source = storage_config["input"]["tables"][0]["source"]
        return source

    def _get_storage_token(self) -> str:
        return self.configuration.parameters.get('#storage_token') or self.environment_variables.token

    def _set_tokens_limit(self) -> any:
        if self._configuration.max_token_spend > 0:
            self.max_token_spend = self._configuration.max_token_spend
            logging.warning(f"Max token spend has been set to {self.max_token_spend}. If the component reaches "
                            f"this limit, it will exit.")

    def _get_table_columns(self, table_id: str) -> list:
        client = Client(self._get_kbc_root_url(), self._get_storage_token())
        table_detail = client.tables.detail(table_id)
        columns = table_detail.get("columns")
        if not columns:
            raise UserException(f"Cannot fetch list of columns for table {table_id}")
        return columns

    @staticmethod
    def count_rows(file_path):
        with open(file_path, 'r', encoding='utf-8') as file:
            reader = csv.reader(file)
            row_count = sum(1 for _ in reader)-1
        return row_count

    @sync_action('listPkeys')
    def list_table_columns(self):
        """
        Sync action to fill the UI element for primary keys selection.

        Returns:

        """
        self.init_configuration()
        table_id = self._get_storage_source()
        columns = self._get_table_columns(table_id)
        return [{"value": c, "label": c} for c in columns]

    @sync_action('testPrompt')
    def test_prompt(self) -> ValidationResult:
        """
        Uses table preview from sapi to apply prompt for on a few table rows.
        It uses same functions as the run method. The only exception is replacing newlines with spaces to ensure
        proper formatting for ValidationResult.
        """
        self.init_configuration()
        client = self.get_client()

        table_id = self._get_storage_source()
        if len(self.input_keys) > 30:
            raise UserException(f"Test prompt is available only for up to 30 placeholders. "
                                f"You have {len(self.input_keys)} placeholders.")

        table_preview = self._get_table_preview(table_id, columns=self.input_keys, limit=PREVIEW_LIMIT)

        preview_size = len(table_preview)
        table_size = self._get_table_size(table_id)

        if missing_keys := [key for key in self.input_keys if key not in table_preview[0]]:
            raise UserException(f'The columns "{missing_keys}" need to be present in the input data!')

        rows = []
        for row in table_preview:
            rows.append(row)

        results = asyncio.run(self._test_prompt(client, rows))

        output = []
        if len(results) > 0:
            for res in results:
                o = res.get(RESULT_COLUMN_NAME, "")
                output.append(o)

        if output:
            estimated_token_usage = self.estimate_token_usage(preview_size, table_size)

            markdown = self.create_markdown_table(output)
            tokens_used_info = f"\nTokens used during test prompting: {self.tokens_used}"
            token_estimation_info = f"\nEstimated token usage for the whole input table: {estimated_token_usage}"
            markdown += tokens_used_info
            markdown += token_estimation_info
            return ValidationResult(markdown, MessageType.SUCCESS)
        else:
            return ValidationResult("Query returned no data.", MessageType.WARNING)

    async def _test_prompt(self, client, rows):
        tasks = []
        for row in rows:
            prompt = self._build_prompt(self.input_keys, row)
            tasks.append(self._infer(client, row, prompt))

        return await asyncio.gather(*tasks)

    @sync_action('getPromptTemplate')
    def get_prompt_template(self) -> ValidationResult:
        configuration: Configuration = Configuration.load_from_dict(self.configuration.parameters)
        template = configuration.prompt_templates.prompt_template

        with open('src/templates/prompts.json', 'r') as json_file:
            templates = json.load(json_file)

        prompt = templates.get(template)
        if not prompt:
            raise UserException(f"Prompt template {template} does not exist!")
        return ValidationResult(prompt, MessageType.SUCCESS)

    @sync_action('listModels')
    def list_models(self):
        authentication = self.configuration.parameters.get("authentication")
        self.service = authentication.get("service")
        self.api_key = authentication.get(KEY_API_TOKEN)

        if self.service == "azure_openai":
            self.api_base = authentication.get("api_base")
            self.deployment_id = authentication.get("deployment")
            self.api_version = authentication.get("api_version")

        client = self.get_client()
        return asyncio.run(self._list_models(client))

    @staticmethod
    async def _list_models(client):
        models = await client.list_models()
        result = [{"value": m, "label": m} for m in models]
        result.append({"value": "custom_model", "label": "Custom Model"})
        return result


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)


================================================
File: /src/configuration.py
================================================
import dataclasses
import json
from dataclasses import dataclass, field
from typing import List

import dataconf


class ConfigurationBase:
    @staticmethod
    def _convert_private_value(value: str):
        return value.replace('"#', '"pswd_')

    @staticmethod
    def _convert_private_value_inv(value: str):
        if value and value.startswith("pswd_"):
            return value.replace("pswd_", "#", 1)
        else:
            return value

    @classmethod
    def load_from_dict(cls, configuration: dict):
        """
        Initialize the configuration dataclass object from dictionary.
        Args:
            configuration: Dictionary loaded from json configuration.

        Returns:

        """
        json_conf = json.dumps(configuration)
        json_conf = ConfigurationBase._convert_private_value(json_conf)
        return dataconf.loads(json_conf, cls, ignore_unexpected=True)

    @classmethod
    def get_dataclass_required_parameters(cls) -> List[str]:
        """
        Return list of required parameters based on the dataclass definition (no default value)
        Returns: List[str]

        """
        return [cls._convert_private_value_inv(f.name)
                for f in dataclasses.fields(cls)
                if f.default == dataclasses.MISSING
                and f.default_factory == dataclasses.MISSING]


@dataclass
class Destination(ConfigurationBase):
    incremental_load: bool
    output_table_name: str
    primary_keys_array: list[str]


@dataclass
class AdditionalOptions(ConfigurationBase):
    top_p: float
    max_tokens: int
    temperature: float
    presence_penalty: float
    frequency_penalty: float
    timeout: int = 60


@dataclass
class Authentication(ConfigurationBase):
    service: str
    pswd_api_token: str
    api_base: str = ""
    deployment_id: str = ""
    api_version: str = ""


@dataclass
class PromptOptions(ConfigurationBase):
    prompt: str


@dataclass
class PromptTemplates(ConfigurationBase):
    prompt_template: str


@dataclass
class Configuration(ConfigurationBase):
    destination: Destination
    additional_options: AdditionalOptions
    authentication: Authentication
    model: str
    endpoint_url: str = ""
    debug: bool = False
    max_token_spend: int = 0
    prompt_templates: PromptTemplates = field(default_factory=lambda: PromptTemplates(prompt_template=""))
    prompt_options: PromptOptions = field(default_factory=lambda: PromptOptions(prompt=""))


================================================
File: /src/client/openai_client.py
================================================
import logging
import openai
from openai import AsyncOpenAI, AsyncAzureOpenAI
from typing import Optional, Tuple, Callable

from .base import CommonClient, AIClientException


def on_giveup(details: dict):
    raise AIClientException(details.get("exception"))


class OpenAIClient(AsyncOpenAI, CommonClient):
    """
    Implements OpenAI and AzureOpenAI clients.
    TODO: Implement try except using wrapper.
    """

    def __init__(self, api_key: str):
        self.inference_function: callable = None
        super().__init__(api_key=api_key)

    async def infer(self, model_name: str, prompt: str, **model_options) -> Tuple[Optional[str], Optional[int]]:
        if not self.inference_function:
            self.inference_function = await self.get_inference_function(model_name)

        return await self.inference_function(model_name=model_name, prompt=prompt, **model_options)

    async def get_inference_function(self, model_name: str) -> Callable:
        """Returns appropriate inference function (either Completion or ChatCompletion)."""
        try:
            await self.get_chat_completion_result(model_name, prompt="This is a test prompt.", timeout=60,
                                                  max_tokens=20)
            return self.get_chat_completion_result
        except openai.OpenAIError:
            logging.warning(f"Cannot use chat_completion endpoint for model {model_name}, the component will try to use"
                            f"completion_result endpoint.")

        try:
            await self.get_completion_result(model_name, "This is a test prompt.", timeout=60,
                                             max_tokens=20)
            return self.get_completion_result
        except openai.OpenAIError:
            raise AIClientException(f"The component is unable to use model {model_name}. Please check your API key.")

    async def get_completion_result(self, model_name: str, prompt: str, **model_options) \
            -> Tuple[str, Optional[int]]:

        try:
            response = await self.completions.create(model=model_name, prompt=prompt, **model_options)
        except openai.OpenAIError as e:
            raise AIClientException(f"Encountered OpenAIError: {e}")

        content = response.choices[0].text
        token_usage = response.usage.total_tokens

        return content, token_usage

    async def get_chat_completion_result(self, model_name: str, prompt: str, **model_options) \
            -> Tuple[Optional[str], Optional[int]]:

        try:
            response = await self.chat.completions.create(model=model_name,
                                                          messages=[{"role": "user", "content": prompt}],
                                                          **model_options)
        except openai.OpenAIError as e:
            raise AIClientException(f"Encountered OpenAIError: {e}")

        content = response.choices[0].message.content
        token_usage = response.usage.total_tokens

        return content, token_usage

    async def list_models(self) -> list:
        r = await self.models.list()
        return [model.id for model in r.data]


class AzureOpenAIClient(AsyncAzureOpenAI, CommonClient):
    def __init__(self, api_key, api_base, deployment_id, api_version):
        super().__init__(api_key=api_key,
                         api_version=api_version,
                         azure_endpoint=api_base,
                         azure_deployment=deployment_id)

    async def infer(self, model_name: str, prompt: str, **model_options) -> Tuple[str, Optional[int]]:

        try:
            response = await self.chat.completions.create(model=model_name,
                                                          messages=[{"role": "user", "content": prompt}],
                                                          **model_options)
        except openai.BadRequestError as e:
            raise AIClientException(f"BadRequest Error: {e}")
        except openai.OpenAIError as e:
            raise AIClientException(f"Encountered OpenAIError: {e}")

        content = response.choices[0].message.content
        if not content:
            if response.choices[0].finish_reason == "content_filter":
                raise AIClientException(f"Cannot process prompt: {prompt}\nReason: content_filter\n"
                                        f"For more information visit https://learn.microsoft.com/en-us/azure/"
                                        f"ai-services/openai/concepts/content-filter?tabs=warning%2Cpython")
            content = ""

        token_usage = response.usage.total_tokens

        return content, token_usage

    async def list_models(self) -> list:
        r = await self.models.list()
        return [model.id for model in r.data]


================================================
File: /src/client/anthropic_client.py
================================================
import asyncio
from anthropic import AsyncAnthropic, AnthropicError
from typing import Optional, Tuple, Callable

from .base import CommonClient, AIClientException


def on_giveup(details: dict):
    raise AIClientException(details.get("exception"))


class AnthropicClient(CommonClient):
    """
    Implements a client for the Anthropic library.
    """

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.client = AsyncAnthropic(api_key=api_key, max_retries=5)
        self.inference_function: Callable = None
        self.semaphore = asyncio.Semaphore(5)

    async def infer(self, model_name: str, prompt: str, **model_options) -> Tuple[Optional[str], Optional[int]]:
        if not self.inference_function:
            self.inference_function = await self.get_inference_function(model_name)

        return await self.inference_function(model_name=model_name, prompt=prompt, **model_options)

    async def get_inference_function(self, model_name: str) -> Callable:
        """Returns the appropriate inference function."""
        try:
            await self.get_chat_completion_result(model_name, prompt="This is a test prompt.", max_tokens=20)
            return self.get_chat_completion_result
        except AnthropicError:
            raise AIClientException(f"The component is unable to use model {model_name}. Please check your API key.")

    async def get_chat_completion_result(self, model_name: str, prompt: str, **model_options) \
            -> Tuple[Optional[str], Optional[int]]:

        max_tokens = model_options.get('max_tokens', 1024)
        messages = [{"role": "user", "content": prompt}]

        # Use semaphore to limit parallelism due to lower token per minute limit
        async with self.semaphore:
            try:
                response = await self.client.messages.create(
                    max_tokens=max_tokens,
                    model=model_name,
                    messages=messages
                )
            except AnthropicError as e:
                raise AIClientException(f"Encountered AnthropicError: {e}")

            content = response.content[0].text
            token_usage = response.usage.output_tokens

            return content, token_usage

    async def list_models(self) -> list:
        try:
            response = await self.client.models.list()
        except AnthropicError as e:
            raise AIClientException(f"Encountered AnthropicError while listing models: {e}")

        return [r.id for r in response.data]


================================================
File: /src/client/base.py
================================================
from typing import Protocol, Tuple, Optional


class AIClientException(Exception):
    pass


class CommonClient(Protocol):
    """
    Declares default AIClient behaviour
    """

    async def infer(self, model_name: str, prompt: str, **model_options) -> Tuple[Optional[str], Optional[int]]:
        pass

    async def list_models(self) -> list:
        pass


================================================
File: /src/client/googleai_client.py
================================================
import backoff
from typing import Optional, Tuple
import google.api_core.exceptions
import google.generativeai as genai
from google.generativeai.types import AsyncGenerateContentResponse

from .base import CommonClient, AIClientException


class GoogleAIClient(CommonClient):
    def __init__(self, api_key):
        genai.configure(api_key=api_key)
        self.model = None

    @backoff.on_exception(backoff.expo, google.api_core.exceptions.ResourceExhausted, max_time=60)
    async def infer(self, model_name: str, prompt: str, **model_options) \
            -> Tuple[Optional[str], Optional[int]]:
        if not self.model:
            self.model = genai.GenerativeModel(model_name=model_name)

        try:
            response = await self.model.generate_content_async(prompt)
        except google.api_core.exceptions.FailedPrecondition as e:
            raise AIClientException(f"FailedPrecondition: {e}")
        except google.auth.exceptions.GoogleAuthError as e:
            raise AIClientException(f"GoogleAuthError: {e}")

        try:
            content = str(response.text)
        except (IndexError, ValueError) as e:
            feedback = str(response.prompt_feedback) if response.prompt_feedback else ""
            content = f"Failed to process prompt: {prompt}, feedback: {feedback}, reason: {e}"

        token_usage = await self.get_total_tokens(response)

        return content, token_usage

    @staticmethod
    async def get_total_tokens(response: AsyncGenerateContentResponse) -> int:
        usage_metadata = getattr(response, "usage_metadata", 0)
        total_tokens = getattr(usage_metadata, "total_token_count", 0)
        return total_tokens

    async def list_models(self) -> list:
        models_with_generate_content = [
            m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods
        ]
        return models_with_generate_content


================================================
File: /src/client/huggingface_client.py
================================================
import backoff
import logging
from typing import Optional, Tuple
from .base import CommonClient, AIClientException
from httpx import HTTPStatusError

from keboola.http_client import AsyncHttpClient

SUPPORTED_MODELS = {
    "Serverless/Meta-Llama-3-8B-Instruct": "https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct", # noqa
    "Serverless/Mistral-Nemo-Instruct-2407": "https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407", # noqa
    "Serverless/Phi-3-mini-4k-instruct": "https://api-inference.huggingface.co/models/microsoft/Phi-3-mini-4k-instruct"
}


class IsSleepingException(Exception):
    pass


class HuggingfaceClient(CommonClient):
    def __init__(self, api_key: str, endpoint_url: str = ""):
        self.client = None
        self.endpoint_url = endpoint_url
        self.headers = {
            'Accept': 'application/json',
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }

    async def infer(self, model_name: str, prompt: str, **model_options) -> Tuple[Optional[str], Optional[int]]:
        if model_name not in SUPPORTED_MODELS and model_name != "custom_model":
            raise AIClientException(f"Model {model_name} is not supported. "
                                    f"Supported models: {list(SUPPORTED_MODELS.keys())}")

        if not self.client:

            base_url = self._get_base_url(model_name)
            self.client = AsyncHttpClient(base_url=base_url, default_headers=self.headers,
                                          max_requests_per_second=1)

        max_new_tokens = model_options.get("max_tokens")
        model_options = {"max_new_tokens": max_new_tokens} if max_new_tokens else {}

        logging.debug(f"Prompt: {prompt}")
        logging.debug(f"Using model options: {model_options}")

        content = await self.generate_text(prompt=prompt, model_name=model_name, **model_options)
        logging.debug(f"Response: {content}")

        token_usage = await self.get_total_tokens(model_name, prompt, content)

        return content, token_usage

    @backoff.on_exception(backoff.expo, IsSleepingException, max_time=500)
    async def generate_text(self, prompt: str, model_name: str, **model_options) -> str:
        endpoint = SUPPORTED_MODELS.get(model_name)

        data = {
            'inputs': prompt,
            'parameters': model_options
        }

        try:
            result = await self.client.post(endpoint, json=data)
        except HTTPStatusError as e:
            if self._is_asleep(e.response.status_code):
                logging.warning("The model is currently sleeping. The component will now wait "
                                "for the model to wake up.")
                raise IsSleepingException()
            else:
                raise AIClientException(f"HTTP error occurred: {e.response.status_code} {e.response.reason_phrase}")

        return result[0]['generated_text']

    def _get_base_url(self, model_name: str) -> str:
        if model_name == "custom_model":
            if not self.endpoint_url:
                raise AIClientException("Custom model requires endpoint_url")
            base_url = self.endpoint_url
        else:
            try:
                base_url = SUPPORTED_MODELS[model_name]
            except KeyError:
                raise AIClientException(f"Model {model_name} is not supported. "
                                        f"Supported models: {list(SUPPORTED_MODELS.keys())}")

        return base_url

    @staticmethod
    def _is_asleep(status_code: int) -> bool:
        return status_code == 503

    @staticmethod
    async def get_total_tokens(model: str, prompt: str, response: str = "") -> int:
        return 0

    async def list_models(self) -> list:
        return list(SUPPORTED_MODELS.keys())


================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG


  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - echo 'Pushing test image to repo. [tag=test]'
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /component_config/logger
================================================
gelf

================================================
File: /component_config/component_long_description.md
================================================
Query OpenAI, Azure OpenAI and Google Gemini AI services with data provided from your KBC project.

================================================
File: /component_config/configuration_description.md
================================================
### Parameters:

#### AI Service Provider: OpenAI

- **API Key (`#api_token`):** Obtain your API key from the [OpenAI platform settings](https://platform.openai.com/account/api-keys).

#### AI Service Provider: Azure OpenAI

- **API Key (`#api_token`)**
- **API Base (`api_base`)**
- **Deployment ID (`deployment_id`)**
- **API Version (`api_version`):** API version used to call the Completions endpoint. Check the list of supported API versions in the [Microsoft Azure documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796#completions).

For information on retrieving your API key, API Base, and Deployment ID, refer to the [Microsoft Azure documentation](https://learn.microsoft.com/cs-cz/azure/ai-services/openai/quickstart?tabs=command-line&pivots=programming-language-python#retrieve-key-and-endpoint).

- **Model (`prompt`)** You can use the sync action to load available models for your account.
- **Prompt (`prompt`):** The prompt and data input pattern. Use the placeholder [[INPUT_COLUMN]] to refer to the input column. The input table must contain the referenced column.
- **Incremental Load (`incremental load`):** If enabled, the table will update rather than be overwritten. Tables with primary keys will update rows, whereas those without a primary key will append rows.
- **Output Table Name (`output_table_name`)**
- **Primary Keys Array (`primary_keys_array`):** You can input multiple columns separated by commas, e.g., id, other_id. Selecting incremental loads allows for table updates if a primary key is set. A primary key can have multiple columns, and the primary key of an existing table is immutable.
- **Predefined Model (`predefined_model`):** The model that will generate the completion. [Learn more](https://beta.openai.com/docs/models).

**Additional Options:**

- **Max Tokens (`max_tokens`):** Maximum number of tokens for the completion. The token count of your prompt plus `max_tokens` should not exceed the model's context length. Most models support up to 2048 tokens, with the newest ones supporting 4096.
- **Temperature (`temperature`):** Sampling temperature between [0-1]. Higher values result in riskier outputs. Use 0.9 for creativity, and 0 for well-defined answers.
- **Top P (`top_p`):** Nucleus sampling, where only tokens with top_p probability mass are considered. For instance, 0.1 means only the top 10% probability mass tokens are evaluated.
- **Frequency Penalty (`frequency_penalty`):** A number between -2.0 and 2.0. Positive values penalize frequently occurring tokens in the current text, reducing repetition.
- **Presence Penalty (`presence_penalty`):** A number between -2.0 and 2.0. Positive values penalize tokens already present in the text, encouraging diverse topics.
- **Request Timeout (`request_timeout`):** Seconds to wait for API to respond. This is a workaround for OpenAI API not responding sometimes.

---

================================================
File: /component_config/component_short_description.md
================================================
Query OpenAI and Azure OpenAI services with data provided from your KBC project.

================================================
File: /component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}

================================================
File: /component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Data Source Configuration",
  "properties": {
    "authentication": {
      "title": "Authentication",
      "required": [
        "service",
        "#api_token"
      ],
      "properties": {
        "service": {
          "type": "string",
          "title": "AI Service Provider",
          "enum": [
            "openai",
            "azure_openai",
            "google",
            "huggingface",
            "anthropic"
          ],
          "default": "openai",
          "options": {
            "enum_titles": [
              "OpenAI",
              "Azure OpenAI",
              "Google (Only available on US stack)",
              "Hugging Face",
              "Anthropic"
            ]
          },
          "propertyOrder": 1
        },
        "#api_token": {
          "type": "string",
          "format": "password",
          "title": "API Key",
          "propertyOrder": 2
        },
        "api_base": {
          "type": "string",
          "title": "API Base",
          "options": {
            "dependencies": {
              "service": "azure_openai"
            }
          },
          "propertyOrder": 3
        },
        "deployment_id": {
          "type": "string",
          "title": "Deployment ID",
          "options": {
            "dependencies": {
              "service": "azure_openai"
            }
          },
          "propertyOrder": 4
        },
        "api_version": {
          "type": "string",
          "title": "API Version",
          "default": "2023-05-15",
          "options": {
            "dependencies": {
              "service": "azure_openai"
            }
          },
          "propertyOrder": 5
        }
      },
      "propertyOrder": 1
    }
  }
}

================================================
File: /component_config/configRowSchema.json
================================================
{
  "type": "object",
  "title": "Component configuration",
  "required": [
    "additional_options",
    "prompt_options",
    "destination"
  ],
  "properties": {
    "model": {
      "type": "string",
      "title": "Model",
      "enum": [],
      "description": "The model which will generate the completion. <a href=\"https://beta.openai.com/docs/models\">Learn more.</a>",
      "propertyOrder": 1,
      "options": {
        "async": {
          "label": "List models",
          "action": "listModels"
        }
      }
    },
    "endpoint_url": {
      "type": "string",
      "title": "Endpoint URL",
      "description": "You can find the endpoint url in your <a href=\"https://ui.endpoints.huggingface.co/\">HuggingFace web console.</a>",
      "propertyOrder": 2,
      "options": {
        "dependencies": {
          "model": "custom_model"
        }
      }
    },
    "additional_options": {
      "type": "object",
      "title": "Model Options",
      "propertyOrder": 10,
      "required": [
        "temperature",
        "top_p",
        "frequency_penalty",
        "presence_penalty"
      ],
      "properties": {
        "max_tokens": {
          "type": "number",
          "title": "Max Tokens",
          "default": 100,
          "description": "The maximum number of tokens to generate in the completion.\n\nThe token count of your prompt plus max_tokens cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).",
          "propertyOrder": 1
        },
        "temperature": {
          "type": "number",
          "title": "Temperature",
          "default": 0.6,
          "description": "What sampling temperature to use [0-1]. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.",
          "propertyOrder": 2
        },
        "top_p": {
          "type": "number",
          "title": "Top P",
          "default": 1,
          "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.",
          "propertyOrder": 10
        },
        "frequency_penalty": {
          "type": "number",
          "title": "Frequency Penalty",
          "default": 0,
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
          "propertyOrder": 20
        },
        "presence_penalty": {
          "type": "number",
          "title": "Presence Penalty",
          "default": 0,
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
          "propertyOrder": 30
        },
        "timeout": {
          "type": "number",
          "title": "Request Timeout",
          "default": 30,
          "description": "Seconds to wait for API to respond. This is a workaround for OpenAI API not responding sometimes.",
          "propertyOrder": 40
        }
      }
    },
    "prompt_templates": {
      "title": "Prompt Templates",
      "propertyOrder": 15,
      "properties": {
        "prompt_template": {
          "type": "string",
          "title": "Prompt Template",
          "enum": [
            "timestamp_from_date",
            "remove_diacritics",
            "add_diacritics",
            "extract_topics",
            "sentiment_scoring",
            "text_shortener",
            "grammar_correction"
          ],
          "description": "Select the template to load. You can then copy the template into the Prompt window.",
          "propertyOrder": 1,
          "options": {
            "enum_titles": [
              "Timestamp from date",
              "Remove Diacritics",
              "Add Diacritics",
              "Extract Topics",
              "Sentiment Scoring",
              "Text Shortener",
              "Grammar Correction"
            ]
          }
        },
        "load_prompt_template": {
          "type": "button",
          "format": "sync-action",
          "propertyOrder": 2,
          "options": {
            "dependencies": {
              "model_type": "predefined"
            },
            "async": {
              "label": "LOAD PROMPT TEMPLATE",
              "action": "getPromptTemplate"
            }
          }
        }
      }
    },
    "prompt_options": {
      "propertyOrder": 20,
      "type": "object",
      "title": "Prompt Options",
      "properties": {
        "prompt": {
          "type": "string",
          "format": "textarea",
          "title": "Prompt",
          "default": "Extract keywords from this text:\n\n\"\"\"\n[[INPUT_COLUMN]]\n\"\"\"",
          "description": "The prompt and data input pattern. Refer to the input column using placeholder [[INPUT_COLUMN]]. The input table must contain the referenced column. You can find best practices for prompt engineering in <a href=\"https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api\">OpenAI help pages</a>.",
          "propertyOrder": 20,
          "options": {
            "input_height": "250px"
          }
        },
        "validation_button": {
          "type": "button",
          "format": "sync-action",
          "propertyOrder": 30,
          "options": {
            "async": {
              "label": "TEST PROMPT",
              "action": "testPrompt"
            }
          }
        }
      }
    },
    "max_token_spend": {
      "title": "Maximum token spend (Optional)",
      "description": "If set to value greater than 0, the component will stop processing rows after reaching the limit of allowed tokens. This is only supported for OpenAI Service.",
      "default": 0,
      "type": "integer",
      "propertyOrder": 40
    },
    "destination": {
      "title": "Destination",
      "type": "object",
      "required": [
        "output_table_name",
        "incremental_load",
        "primary_keys_array"
      ],
      "properties": {
        "output_table_name": {
          "type": "string",
          "title": "Storage Table Name",
          "description": "Name of the table stored in Storage.",
          "propertyOrder": 100
        },
        "incremental_load": {
          "type": "boolean",
          "format": "checkbox",
          "title": "Incremental Load",
          "description": "If incremental load is turned on, the table will be updated instead of rewritten. Tables with a primary key will have rows updated, tables without a primary key will have rows appended.",
          "propertyOrder": 110
        },
        "primary_keys_array": {
          "type": "array",
          "title": "Primary Keys",
        "format": "select",
          "items": {
            "type": "string"
          },
          "uniqueItems": true,
          "options": {
            "tags": true,
              "async": {
                "label": "Re-load columns",
                "action": "listPkeys"
            }
          },
          "description": "You can enter multiple columns seperated by commas at once e.g., id, other_id. If a primary key is set, updates can be done on the table by selecting incremental loads. The primary key can consist of multiple columns. The primary key of an existing table cannot be changed.",
          "propertyOrder": 120
        }
      }
    }
  }
}

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
fi

echo "Updating logger settings"

value=`cat component_config/logger`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} logger --value="$value"
else
    echo "logger type is empty!"
fi

echo "Updating logger configuration"
value=`cat component_config/loggerConfiguration.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} loggerConfiguration --value="$value"
else
    echo "loggerConfiguration is empty!"
fi

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /requirements.txt
================================================
keboola.component==1.4.4
keboola.utils==1.1.0
kbcstorage==0.7.2
openai==1.58.1
mock==4.0.3
freezegun==1.2.2
pystache
backoff
dataconf
google-generativeai==0.8.3
git+https://github.com/SgtMarmite/python-http-client.git@async-http-client
aiolimiter
anthropic==0.42.0
grpcio==1.67.1

================================================
File: /Dockerfile
================================================
FROM python:3.11.7-slim
ENV PYTHONIOENCODING utf-8

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY requirements.txt /code/requirements.txt
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential git

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]


================================================
File: /LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /README.md
================================================
# Generative AI

This component enables you to query OpenAI, Azure OpenAI, and Llama 2 (coming soon) with data provided from your KBC project.

- [TOC]

---

## Configuration

### Parameters:

#### AI Service Provider: OpenAI

- **API Key (`#api_token`):** Obtain your API key from the [OpenAI platform settings](https://platform.openai.com/account/api-keys).

#### AI Service Provider: Azure OpenAI

- **API Key (`#api_token`)**
- **API Base (`api_base`)**
- **Deployment ID (`deployment_id`)**
- **API Version (`api_version`):** API version used to call the Completions endpoint. Check the list of supported API versions in the [Microsoft Azure documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference?WT.mc_id=AZ-MVP-5004796#completions).

For information on retrieving your API key, API Base, and Deployment ID, refer to the [Microsoft Azure documentation](https://learn.microsoft.com/cs-cz/azure/ai-services/openai/quickstart?tabs=command-line&pivots=programming-language-python#retrieve-key-and-endpoint).

#### AI Service Provider: Google

- Only works on US AWS stack – connection.keboola.com
- Default API Key for this service is provided by Keboola. 

#### AI Service Provider: Hugging Face

- API key is Provided by Keboola if one of the predefined, serverless models is used.
- If you want to use a custom, non-serverless model, you need to provide your own API key.

- **API Key (`#api_token`)**
- **API Key (`endpoint_url`)** - You can use any endpoint url from your <a href="https://ui.endpoints.huggingface.co/">HuggingFace web console.</a> or a serverless deployment.

#### AI Service Provider: Anthropic

- **API Key (`#api_token`)**

- You only need the API key for this service.
- Concurrency is currently limited to 5 requests due to the Anthropic API rate limit for lower-tier accounts.

### Other options:

- **Model (`prompt`)** You can use the sync action to load available models for your account.
- **Prompt (`prompt`):** The prompt and data input pattern. Use the placeholder [[INPUT_COLUMN]] to refer to the input column. The input table must contain the referenced column.
- **Incremental Load (`incremental load`):** If enabled, the table will update rather than be overwritten. Tables with primary keys will update rows, whereas those without a primary key will append rows.
- **Output Table Name (`output_table_name`)**
- **Primary Keys Array (`primary_keys_array`):** You can input multiple columns separated by commas, e.g., id, other_id. Selecting incremental loads allows for table updates if a primary key is set. A primary key can have multiple columns, and the primary key of an existing table is immutable.
- **Predefined Model (`predefined_model`):** The model that will generate the completion. [Learn more](https://beta.openai.com/docs/models).

**Additional Options:**

- **Max Tokens (`max_tokens`):** Maximum number of tokens for the completion. The token count of your prompt plus `max_tokens` should not exceed the model's context length. Most models support up to 2048 tokens, with the newest ones supporting 4096.
- **Temperature (`temperature`):** Sampling temperature between [0-1]. Higher values result in riskier outputs. Use 0.9 for creativity, and 0 for well-defined answers.
- **Top P (`top_p`):** Nucleus sampling, where only tokens with top_p probability mass are considered. For instance, 0.1 means only the top 10% probability mass tokens are evaluated.
- **Frequency Penalty (`frequency_penalty`):** A number between -2.0 and 2.0. Positive values penalize frequently occurring tokens in the current text, reducing repetition.
- **Presence Penalty (`presence_penalty`):** A number between -2.0 and 2.0. Positive values penalize tokens already present in the text, encouraging diverse topics.
- **Request Timeout (`request_timeout`):** Seconds to wait for API to respond. This is a workaround for OpenAI API not responding sometimes.

---

### Component Configuration Example

**Generic configuration**

```json
{
  "parameters": {
    "#api_token": "secret_api_token",
    "sleep": 5
  }
}
```

**Row configuration**

```json
{
  "parameters": {
    "prompt": "Extract keywords from this text:\n\n\"\"\"\n[[INPUT]]\n\"\"\"",
    "model_type": "predefined",
    "destination": {
      "incremental_load": true,
      "output_table_name": "keywords_test",
      "primary_keys_array": [
        "message_id"
      ]
    },
    "predefined_model": "text-davinci-002",
    "additional_options": {
      "top_p": 1,
      "max_tokens": 100,
      "temperature": 0.6,
      "presence_penalty": 0,
      "frequency_penalty": 0
    }
  }
}
```


# Development

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path in
the `docker-compose.yml` file:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Clone this repository, init the workspace and run the component with following command:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
git clone git@bitbucket.org:kds_consulting_team/kds_consulting_team/kds-team.generative-ai.git kds-team.generative-ai
cd kds-team.app-open-ai
docker-compose build
docker-compose run --rm dev
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Run the test suite and lint check using this command:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
docker-compose run --rm test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Integration
===========

For information about deployment and integration with KBC, please refer to the
[deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/)


