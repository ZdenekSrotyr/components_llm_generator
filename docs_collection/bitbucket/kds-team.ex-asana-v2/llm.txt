Directory structure:
└── kds_consulting_team-kds-team.ex-asana-v2/
    ├── flake8.cfg
    ├── deploy.sh
    ├── docker-compose.yml
    ├── change_log.md
    ├── src/
    │   ├── asana_client/
    │   │   ├── mapping_test_script.py
    │   │   ├── mapping_parser.py
    │   │   ├── endpoint_mappings.json
    │   │   └── client.py
    │   ├── component.py
    │   └── __pycache__/
    ├── bitbucket-pipelines.yml
    ├── tests/
    │   ├── test_component.py
    │   └── __init__.py
    ├── component_config/
    │   ├── logger
    │   ├── component_long_description.md
    │   ├── configuration_description.md
    │   ├── configSchema_v1.json
    │   ├── component_short_description.md
    │   ├── loggerConfiguration.json
    │   ├── stack_parameters.json
    │   ├── configSchema.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   ├── tables/
    │       │   │   ├── test.csv
    │       │   │   └── test.csv.manifest
    │       │   └── files/
    │       │       └── order1.xml
    │       └── out/
    │           ├── tables/
    │           │   └── test.csv
    │           └── files/
    │               └── order1.xml
    ├── scripts/
    │   ├── update_dev_portal_properties.sh
    │   ├── run_kbc_tests.ps1
    │   ├── run.bat
    │   ├── build_n_run.ps1
    │   └── build_n_test.sh
    ├── requirements.txt
    ├── docs/
    │   └── imgs/
    ├── Dockerfile
    ├── LICENSE.md
    └── README.md

================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog


================================================
File: /src/asana_client/mapping_test_script.py
================================================
from mapping_parser import MappingParser
import json


endpoint = 'projects_tasks_details'
data_in = {
    "data": {
        "gid": "12345",
        "resource_type": "task",
        "name": "Buy catnip",
        "approval_status": "pending",
        "completed": False,
        "completed_at": "2012-02-22T02:06:58.147Z",
        "completed_by": {
            "gid": "12345",
            "resource_type": "user",
            "name": "Greg Sanchez"
        },
        "created_at": "2012-02-22T02:06:58.147Z",
        "dependencies": [
            {
                "gid": "12345",
                "resource_type": "task"
            }
        ],
        "dependents": [
            {
                "gid": "12345",
                "resource_type": "task"
            }
        ],
        "due_at": "2019-09-15T02:06:58.147Z",
        "due_on": "2019-09-15",
        "external": {
            "data": "A blob of information",
            "gid": "my_gid"
        },
        "hearted": True,
        "hearts": [
            {
                "gid": "12345",
                "user": {
                    "gid": "12345",
                    "resource_type": "user",
                    "name": "Greg Sanchez"
                }
            }
        ],
        "html_notes": "<body>Mittens <em>really</em> likes the stuff from Humboldt.</body>",
        "is_rendered_as_separator": False,
        "liked": True,
        "likes": [
            {
                "gid": "12345",
                "user": {
                    "gid": "12345",
                    "resource_type": "user",
                    "name": "Greg Sanchez"
                }
            }
        ],
        "memberships": [
            {
                "project": {
                    "gid": "12345",
                    "resource_type": "project",
                    "name": "Stuff to buy"
                },
                "section": {
                    "gid": "12345",
                    "resource_type": "section",
                    "name": "Next Actions"
                }
            }
        ],
        "modified_at": "2012-02-22T02:06:58.147Z",
        "notes": "Mittens really likes the stuff from Humboldt.",
        "num_hearts": 5,
        "num_likes": 5,
        "num_subtasks": 3,
        "resource_subtype": "default_task",
        "start_on": "2019-09-14",
        "assignee": {
            "gid": "12345",
            "resource_type": "user",
            "name": "Greg Sanchez"
        },
        "custom_fields": [
            {
                "gid": "12345",
                "resource_type": "custom_field",
                "created_by": {
                    "gid": "12345",
                    "resource_type": "user",
                    "name": "Greg Sanchez"
                },
                "currency_code": "EUR",
                "custom_label": "gold pieces",
                "custom_label_position": "suffix",
                "description": "Development team priority",
                "display_value": "blue",
                "enabled": True,
                "enum_options": [
                    {
                        "gid": "12345",
                        "resource_type": "enum_option",
                        "color": "blue",
                        "enabled": True,
                        "name": "Low"
                    }
                ],
                "enum_value": {
                    "gid": "12345",
                    "resource_type": "enum_option",
                    "color": "blue",
                    "enabled": True,
                    "name": "Low"
                },
                "format": "custom",
                "has_notifications_enabled": True,
                "is_global_to_workspace": True,
                "name": "Status",
                "number_value": 5.2,
                "precision": 2,
                "resource_subtype": "text",
                "text_value": "Some Value",
                "type": "text"
            },
            {
                "gid": "1234567890",
                "resource_type": "custom_field",
                "created_by": {
                    "gid": "12345",
                    "resource_type": "user",
                    "name": "Greg Sanchez"
                },
                "currency_code": "EUR",
                "custom_label": "gold pieces",
                "custom_label_position": "suffix",
                "description": "Development team priority",
                "display_value": "blue",
                "enabled": True,
                "enum_options": [
                    {
                        "gid": "12345",
                        "resource_type": "enum_option",
                        "color": "blue",
                        "enabled": True,
                        "name": "Low"
                    }
                ],
                "enum_value": {
                    "gid": "12345",
                    "resource_type": "enum_option",
                    "color": "blue",
                    "enabled": True,
                    "name": "Low"
                },
                "format": "custom",
                "has_notifications_enabled": True,
                "is_global_to_workspace": True,
                "name": "custom_field",
                "number_value": 5.2,
                "precision": 2,
                "resource_subtype": "text",
                "text_value": "Some Value",
                "type": "text"
            }
        ],
        "followers": [
            {
                "gid": "12345",
                "resource_type": "user",
                "name": "Greg Sanchez"
            }
        ],
        "parent": {
            "gid": "12345",
            "resource_type": "task",
            "name": "Bug Task"
        },
        "permalink_url": "https://app.asana.com/0/resource/123456789/list",
        "projects": [
            {
                "gid": "12345",
                "resource_type": "project",
                "name": "Stuff to buy"
            }
        ],
        "tags": [
            {
                "gid": "59746",
                "name": "Grade A"
            }
        ],
        "workspace": {
            "gid": "12345",
            "resource_type": "workspace",
            "name": "My Company Workspace"
        }
    }
}

with open('src/endpoint_mappings.json', 'r') as m:
    MAPPINGS = json.load(m)

REQUEST_MAP = {
    'workspaces': {
        'endpoint': 'workspaces',
        'mapping': 'workspaces'},
    'users': {
        'endpoint': 'workspaces/{workspaces_id}/users',
        'required': 'workspaces', 'mapping': 'users'},
    'users_details': {
        'endpoint': 'users/{users_id}',
        'required': 'users', 'mapping': 'users_details'},
    'user_defined_projects': {
        'endpoint': 'projects/{projects_id}',
        'required': 'projects', 'mapping': 'projects_details'},
    'projects': {
        'endpoint': 'workspaces/{workspaces_id}/projects',
        'required': 'workspaces', 'mapping': 'projects'},
    'projects_details': {
        'endpoint': 'projects/{projects_id}',
        'required': 'projects', 'mapping': 'projects_details'},
    'projects_sections': {
        'endpoint': 'projects/{projects_id}/sections',
        'required': 'projects', 'mapping': 'sections'},
    'projects_tasks': {
        'endpoint': 'projects/{projects_id}/tasks',
        'required': 'projects', 'mapping': 'tasks'},
    'projects_tasks_details': {
        'endpoint': 'tasks/{projects_tasks_id}',
        'required': 'projects_tasks', 'mapping': 'task_details'},
    'projects_tasks_subtasks': {
        'endpoint': 'tasks/{projects_tasks_id}/subtasks',
        'required': 'projects_tasks', 'mapping': 'task_subtasks'},
    'projects_tasks_stories': {
        'endpoint': 'tasks/{projects_tasks_id}/stories',
        'required': 'projects_tasks', 'mapping': 'task_stories'}
}

endpoint_mapping = MAPPINGS[REQUEST_MAP[endpoint]['mapping']]

MappingParser(
    destination='/data/out/tables/',
    endpoint=REQUEST_MAP[endpoint]['mapping'],
    endpoint_data=data_in['data'],
    mapping=endpoint_mapping,
    incremental=None,
    parent_key='test'
)


================================================
File: /src/asana_client/mapping_parser.py
================================================
import os
import json
import logging  # noqa
import sys  # noqa
import pandas as pd
import time


class MappingParser:
    def __init__(self, destination, endpoint, endpoint_data, mapping, parent_key=None, incremental=False,
                 add_timestamp=False, generate_timestamp=False):

        self.destination = destination
        self.endpoint = endpoint
        self.endpoint_data = endpoint_data
        self.mapping = mapping
        self.parent_key = parent_key
        self.output = []
        self.primary_key = []
        self.incremental = incremental
        self.add_timestamp = add_timestamp

        # Countermeasures for response coming in as DICT
        if isinstance(self.endpoint_data, dict):
            self.endpoint_data = []
            self.endpoint_data.append(endpoint_data)

        # Parsing
        self.parse()
        if self.output:
            pk = self.primary_key
            if generate_timestamp:
                self.output = self._add_timestamp(df_json=self.output)
                pk.append("timestamp")
                pk.remove("section_id")

            self._output(df_json=self.output, filename=self.endpoint)
            self._produce_manifest(filename=self.endpoint, incremental=self.incremental, primary_key=pk)

    def parse(self):
        for row in self.endpoint_data:
            row_json = {}

            for m in self.mapping:
                col_type = self.mapping[m].get('type')

                if col_type == 'column' or not col_type:
                    key = self.mapping[m]['mapping']['destination']
                    # value = row[m]
                    value = self._fetch_value(row=row, key=m)
                    row_json[key] = value

                    # Primary key for incremental load
                    if "primaryKey" in self.mapping[m]['mapping'] and key not in self.primary_key:
                        self.primary_key.append(key)

                elif col_type == 'user':
                    key = self.mapping[m]['mapping']['destination']
                    value = self.parent_key
                    row_json[key] = value

                    # Primary key for incremental load
                    self.primary_key.append(
                        key) if key not in self.primary_key else ''

                elif col_type == 'table':
                    endpoint = self.mapping[m]['destination']
                    mapping = self.mapping[m]['tableMapping']
                    parent_key = row['gid']
                    data = self._fetch_value(row=row, key=m)

                    if endpoint == 'task_details-memberships' and self.add_timestamp:
                        generate_timestamp = True
                    else:
                        generate_timestamp = False

                    MappingParser(
                        destination=self.destination,
                        endpoint=endpoint,
                        endpoint_data=data,
                        mapping=mapping,
                        parent_key=parent_key,
                        incremental=self.incremental,
                        generate_timestamp=generate_timestamp
                    )

            self.output.append(row_json)

    @staticmethod
    def _fetch_value(row, key):
        """
        Fetching value from a nested object
        """
        key_list = key.split('.')
        value = row

        try:
            for k in key_list:
                value = value[k]
        except Exception:
            value = ''

        return value

    def _output(self, df_json, filename):
        output_filename = f'{self.destination}/{filename}.csv'
        if df_json:
            data_output = pd.DataFrame(df_json, dtype=str)
            if not os.path.isfile(output_filename):
                with open(output_filename, 'a') as b:
                    data_output.to_csv(b, index=False)
                b.close()
            else:
                with open(output_filename, 'a') as b:
                    data_output.to_csv(b, index=False, header=False)
                b.close()

    def _produce_manifest(self, filename, incremental, primary_key):
        manifest_filename = f'{self.destination}/{filename}.csv.manifest'
        manifest = {
            'incremental': incremental,
            'primary_key': primary_key,
        }

        with open(manifest_filename, 'w') as file_out:
            json.dump(manifest, file_out)

    @staticmethod
    def _add_timestamp(df_json):
        current_timestamp = time.time()
        for item in df_json:
            item['timestamp'] = current_timestamp

        return df_json


================================================
File: /src/asana_client/endpoint_mappings.json
================================================
{
    "projects": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "resource_type": {
            "type": "column",
            "mapping": {
                "destination": "resource_type"
            }
        },
        "parent_gid": {
            "type": "user",
            "mapping": {
                "destination": "workspace_id"
            }
        }
    },
    "projects_details": {
        "notes": {
            "type": "column",
            "mapping": {
                "destination": "notes"
            }
        },
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "archived": {
            "type": "column",
            "mapping": {
                "destination": "archived"
            }
        },
        "color": {
            "type": "column",
            "mapping": {
                "destination": "color"
            }
        },
        "current_status.text": {
            "type": "column",
            "mapping": {
                "destination": "current_status_text"
            }
        },
        "workspace.name": {
            "type": "column",
            "mapping": {
                "destination": "workspace_name"
            }
        },
        "owner.name": {
            "type": "column",
            "mapping": {
                "destination": "owner_name"
            }
        },
        "owner.gid": {
            "type": "column",
            "mapping": {
                "destination": "owner_id"
            }
        },
        "modified_at": {
            "type": "column",
            "mapping": {
                "destination": "modified_at"
            }
        },
        "public": {
            "type": "column",
            "mapping": {
                "destination": "public"
            }
        },
        "workspace.gid": {
            "type": "column",
            "mapping": {
                "destination": "workspace_id"
            }
        },
        "created_at": {
            "type": "column",
            "mapping": {
                "destination": "created_at"
            }
        },
        "layout": {
            "type": "column",
            "mapping": {
                "destination": "layout"
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "current_status.color": {
            "type": "column",
            "mapping": {
                "destination": "current_status_color"
            }
        },
        "current_status.modified_at": {
            "type": "column",
            "mapping": {
                "destination": "current_status_modified_at"
            }
        },
        "followers": {
            "type": "table",
            "destination": "projects-followers",
            "tableMapping": {
                "gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "id",
                        "primaryKey": true
                    }
                },
                "name": {
                    "type": "column",
                    "mapping": {
                        "destination": "name"
                    }
                },
                "parent_gid": {
                    "type": "user",
                    "mapping": {
                        "destination": "project_id"
                    }
                }
            }
        },
        "current_status.author.gid": {
            "type": "column",
            "mapping": {
                "destination": "current_status_author_id"
            }
        },
        "custom_field_settings": {
            "type": "table",
            "destination": "projects-custom_field_settings",
            "tableMapping": {
                "gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "id",
                        "primaryKey": true
                    }
                },
                "custom_field.gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "custom_field_id"
                    }
                },
                "custom_field.name": {
                    "type": "column",
                    "mapping": {
                        "destination": "custom_field_name"
                    }
                },
                "custom_field.type": {
                    "type": "column",
                    "mapping": {
                        "destination": "custom_field_type"
                    }
                },
                "is_important": {
                    "type": "column",
                    "mapping": {
                        "destination": "is_important"
                    }
                },
                "project.gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "project_id"
                    }
                },
                "project.name": {
                    "type": "column",
                    "mapping": {
                        "destination": "project_name"
                    }
                },
                "custom_field.enum_options": {
                    "type": "table",
                    "destination": "projects-custom_field_settings-enum_options",
                    "tableMapping": {
                        "gid": {
                            "type": "column",
                            "mapping": {
                                "destination": "id",
                                "primaryKey": true
                            }
                        },
                        "name": {
                            "type": "column",
                            "mapping": {
                                "destination": "name"
                            }
                        },
                        "color": {
                            "type": "column",
                            "mapping": {
                                "destination": "color"
                            }
                        },
                        "enabled": {
                            "type": "column",
                            "mapping": {
                                "destination": "enabled"
                            }
                        },
                        "parent_gid": {
                            "type": "user",
                            "mapping": {
                                "destination": "parent_id"
                            }
                        }
                    }
                }
            }
        },
        "current_status.author.name": {
            "type": "column",
            "mapping": {
                "destination": "current_status_author_name"
            }
        },
        "members": {
            "type": "table",
            "destination": "projects-members",
            "tableMapping": {
                "gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "id",
                        "primaryKey": true
                    }
                },
                "name": {
                    "type": "column",
                    "mapping": {
                        "destination": "name"
                    }
                },
                "parent_gid": {
                    "type": "user",
                    "mapping": {
                        "destination": "project_id"
                    }
                }
            }
        },
        "due_date": {
            "type": "column",
            "mapping": {
                "destination": "due_date"
            }
        }
    },
    "sections": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "parent_gid": {
            "type": "user",
            "mapping": {
                "destination": "project_id"
            }
        }
    },
    "section_tasks": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "resource_type": {
            "mapping": {
                "destination": "resource_type"
            }
        },
        "name": {
            "mapping": {
                "destination": "name"
            }
        },
        "parent_gid": {
            "type": "user",
            "mapping": {
                "destination": "section_id"
            }
        }
    },
    "tasks": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "parent_gid": {
            "type": "user",
            "mapping": {
                "destination": "project_id"
            }
        }
    },
    "task_details": {
        "notes": {
            "type": "column",
            "mapping": {
                "destination": "notes"
            }
        },
        "assignee_status": {
            "type": "column",
            "mapping": {
                "destination": "assignee_status"
            }
        },
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "assignee.gid": {
            "type": "column",
            "mapping": {
                "destination": "assignee_id"
            }
        },
        "completed_at": {
            "type": "column",
            "mapping": {
                "destination": "completed_at"
            }
        },
        "custom_fields": {
            "type": "table",
            "destination": "task_details-custom_fields",
            "tableMapping": {
                "parent_gid": {
                    "type": "user",
                    "mapping": {
                        "destination": "task_id"
                    }
                },
                "enum_options": {
                    "type": "table",
                    "destination": "task_details-custom_field-enum_options",
                    "tableMapping": {
                        "gid": {
                            "type": "column",
                            "mapping": {
                                "destination": "id",
                                "primaryKey": true
                            }
                        },
                        "name": {
                            "type": "column",
                            "mapping": {
                                "destination": "name"
                            }
                        },
                        "color": {
                            "type": "column",
                            "mapping": {
                                "destination": "color"
                            }
                        },
                        "enabled": {
                            "type": "column",
                            "mapping": {
                                "destination": "enabled"
                            }
                        },
                        "parent_gid": {
                            "type": "user",
                            "mapping": {
                                "destination": "parent_id"
                            }
                        }
                    }
                },
                "enabled": {
                    "type": "column",
                    "mapping": {
                        "destination": "enabled"
                    }
                },
                "gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "id",
                        "primaryKey": true
                    }
                },
                "display_value": {
                    "type": "column",
                    "mapping": {
                        "destination": "display_value"
                    }
                },
                "text_value": {
                    "type": "column",
                    "mapping": {
                        "destination": "text_value"
                    }
                },
                "enum_value.enabled": {
                    "type": "column",
                    "mapping": {
                        "destination": "enum_value_enabled"
                    }
                },
                "enum_value.name": {
                    "type": "column",
                    "mapping": {
                        "destination": "enum_value_name"
                    }
                },
                "enum_value.gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "enum_value_id"
                    }
                },
                "enum_value.color": {
                    "type": "column",
                    "mapping": {
                        "destination": "enum_value_color"
                    }
                },
                "name": {
                    "type": "column",
                    "mapping": {
                        "destination": "name"
                    }
                },
                "type": {
                    "type": "column",
                    "mapping": {
                        "destination": "type"
                    }
                },
                "number_value": {
                    "type": "column",
                    "mapping": {
                        "destination": "number_value"
                    }
                },
                "precision": {
                    "type": "column",
                    "mapping": {
                        "destination": "precision"
                    }
                }
            }
        },
        "modified_at": {
            "type": "column",
            "mapping": {
                "destination": "modified_at"
            }
        },
        "due_on": {
            "type": "column",
            "mapping": {
                "destination": "due_on"
            }
        },
        "created_at": {
            "type": "column",
            "mapping": {
                "destination": "created_at"
            }
        },
        "assignee.name": {
            "type": "column",
            "mapping": {
                "destination": "assignee_name"
            }
        },
        "completed": {
            "type": "column",
            "mapping": {
                "destination": "completed"
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "parent.name": {
            "type": "column",
            "mapping": {
                "destination": "parent_name"
            }
        },
        "followers": {
            "type": "table",
            "destination": "task_details-followers",
            "tableMapping": {
                "gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "id",
                        "primaryKey": true
                    }
                },
                "name": {
                    "type": "column",
                    "mapping": {
                        "destination": "name"
                    }
                },
                "parent_gid": {
                    "type": "user",
                    "mapping": {
                        "destination": "task_id"
                    }
                }
            }
        },
        "tags": {
            "type": "table",
            "destination": "task_details-tags",
            "tableMapping": {
                "gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "id",
                        "primaryKey": true
                    }
                },
                "name": {
                    "type": "column",
                    "mapping": {
                        "destination": "name"
                    }
                },
                "parent_gid": {
                    "type": "user",
                    "mapping": {
                        "destination": "task_id"
                    }
                }
            }
        },
        "memberships": {
            "type": "table",
            "destination": "task_details-memberships",
            "tableMapping": {
                "project.gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "project_id",
                        "primaryKey": true
                    }
                },
                "project.name": {
                    "type": "column",
                    "mapping": {
                        "destination": "project_name"
                    }
                },
                "section.gid": {
                    "type": "column",
                    "mapping": {
                        "destination": "section_id",
                        "primaryKey": true
                    }
                },
                "section.name": {
                    "type": "column",
                    "mapping": {
                        "destination": "section_name"
                    }
                },
                "parent_gid": {
                    "type": "user",
                    "mapping": {
                        "destination": "task_id"
                    }
                }
            }
        },
        "parent.gid": {
            "type": "column",
            "mapping": {
                "destination": "parent_id"
            }
        },
        "due_at": {
            "type": "column",
            "mapping": {
                "destination": "due_at"
            }
        }
    },
    "task_subtasks": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "parent_id": {
            "type": "user",
            "mapping": {
                "destination": "task_id"
            }
        }
    },
    "task_stories": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "parent_gid": {
            "type": "user",
            "mapping": {
                "destination": "task_id"
            }
        },
        "created_by.gid": {
            "type": "column",
            "mapping": {
                "destination": "created_by_id"
            }
        },
        "created_by.name": {
            "type": "column",
            "mapping": {
                "destination": "created_by_name"
            }
        },
        "created_at": {
            "type": "column",
            "mapping": {
                "destination": "created_at"
            }
        },
        "text": {
            "type": "column",
            "mapping": {
                "destination": "text"
            }
        },
        "type": {
            "type": "column",
            "mapping": {
                "destination": "type"
            }
        },
        "resource_type": {
            "type": "column",
            "mapping": {
                "destination": "resource_type"
            }
        },
        "resource_subtype": {
            "type": "column",
            "mapping": {
                "destination": "resource_subtype"
            }
        }
    },
    "users": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "resource_type": {
            "type": "column",
            "mapping": {
                "destination": "resource_type"
            }
        }
    },
    "users_details": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "email": {
            "type": "column",
            "mapping": {
                "destination": "email"
            }
        },
        "resource_type": {
            "type": "column",
            "mapping": {
                "destination": "resource_type"
            }
        },
        "parent_gid": {
            "type": "user",
            "mapping": {
                "destination": "user_id"
            }
        }
    },
    "workspaces": {
        "gid": {
            "mapping": {
                "destination": "id",
                "primaryKey": true
            }
        },
        "name": {
            "type": "column",
            "mapping": {
                "destination": "name"
            }
        },
        "resource_type": {
            "type": "column",
            "mapping": {
                "destination": "resource_type"
            }
        }
    }
}

================================================
File: /src/asana_client/client.py
================================================
import asyncio
import json
import logging
import os
import time
import random

from httpx import HTTPStatusError
from keboola.http_client.async_client import AsyncHttpClient

from .mapping_parser import MappingParser

MAPPINGS_JSON = 'endpoint_mappings.json'

BASE_URL = 'https://app.asana.com/api/1.0/'

REQUEST_MAP = {
    'workspaces': {
        'level': 0,
        'endpoint': 'workspaces',
        'mapping': 'workspaces'},
    'users': {
        'level': 1,
        'endpoint_batch': '/workspaces/{workspaces_id}/users',
        'endpoint': 'users?workspace={workspaces_id}',
        'required': 'workspaces',
        'mapping': 'users'},
    'users_details': {
        'level': 2,
        'endpoint': 'users/{users_id}',
        'required': 'users',
        'mapping': 'users_details'},
    'projects': {
        'level': 1,
        'endpoint': 'workspaces/{workspaces_id}/projects',
        'required': 'workspaces',
        'mapping': 'projects'},
    'projects_details': {
        'level': 2,
        'endpoint': 'projects/{projects_id}',
        'required': 'projects',
        'mapping': 'projects_details'},
    'user_defined_projects': {
        'level': 2,
        'endpoint': 'projects/{projects_id}',
        'required': 'projects',
        'mapping': 'projects_details'},
    'projects_sections': {
        'level': 3,
        'endpoint': 'projects/{projects_details_id}/sections',
        'required': 'projects_details',
        'mapping': 'sections'},
    "projects_sections_tasks": {
        'level': 4,
        'endpoint': 'sections/{projects_sections_id}/tasks',
        'required': 'projects_sections',
        'mapping': 'section_tasks'},
    'projects_tasks': {
        'level': 3,
        'endpoint': 'projects/{projects_details_id}/tasks',
        'required': 'projects_details',
        'mapping': 'tasks'},
    'projects_tasks_details': {
        'level': 4,
        'endpoint': 'tasks/{projects_tasks_id}',
        'required': 'projects_tasks',
        'mapping': 'task_details'},
    'projects_tasks_subtasks': {
        'level': 4,
        'endpoint': 'tasks/{projects_tasks_id}/subtasks',
        'required': 'projects_tasks',
        'mapping': 'task_subtasks'},
    'projects_tasks_stories': {
        'level': 4,
        'endpoint': 'tasks/{projects_tasks_id}/stories',
        'required': 'projects_tasks',
        'mapping': 'task_stories'}
}

DEFAULT_MAX_REQUESTS_PER_SECOND = 2
DEFAULT_BATCH_SIZE = 100

# The number of objects to return per page. The value must be between 1 and 100.
API_PAGE_LIMIT = 100

KEY_FORBIDDEN_ENDPOINTS = 'forbidden_endpoints'
KEY_GID = 'gid'
KEY_GEN_ID = 'gen_id'
TMP_FOLDER_PATH = '/tmp'


class AsanaClientException(Exception):
    def __init__(self, message, status_code=None):
        super().__init__(message)
        self.status_code = status_code
    pass


class AsanaClient(AsyncHttpClient):
    def __init__(self, destination, api_token, incremental=False, debug: bool = False, skip_unauthorized: bool = False,
                 max_requests_per_second: int = DEFAULT_MAX_REQUESTS_PER_SECOND, membership_timestamp: bool = False,
                 batch_size: int = DEFAULT_BATCH_SIZE):
        self.request_map_levels = None
        self.tables_out_path = destination
        self.incremental = incremental
        self.requested_endpoints = []
        self.root_endpoints_data = {rm_endpoint: [] for rm_endpoint in REQUEST_MAP}
        self.request_map = REQUEST_MAP
        self.counter = 0
        self.skip_unauthorized = skip_unauthorized
        self.membership_timestamp = membership_timestamp
        self.endpoints_needed = set()
        self.completed_since = None
        self.batch_size = batch_size
        super().__init__(base_url=BASE_URL,
                         auth=(api_token, ''),
                         retries=3,
                         retry_status_codes=[400, 402, 429, 500, 502, 503, 504],
                         max_requests_per_second=max_requests_per_second,
                         timeout=10,
                         debug=debug)

        self._init_mappings()
        self._init_tmp_folders()

    def _init_mappings(self):
        json_path = os.path.join(os.path.dirname(__file__), MAPPINGS_JSON)
        with open(json_path, 'r') as m:
            self.mappings = json.load(m)

    async def fetch(self, endpoints, completed_since=None):

        self.endpoints_needed = self.get_endpoints_needed(endpoints)
        self.request_map_levels = self.construct_request_map_with_levels()
        self.requested_endpoints = endpoints
        self.completed_since = completed_since

        for level in self.request_map_levels:
            tasks = []
            logging.debug(f"Fetching level: {level}")
            level_endpoints = self.request_map_levels[level]
            for level_endpoint in level_endpoints:
                if level_endpoint in self.endpoints_needed:
                    tasks.append(self._fetch(level_endpoint, completed_since=self.completed_since))

            await asyncio.gather(*tasks)

    async def _fetch(self, fetched_endpoint, completed_since=None):
        """
        Processing/Fetching data
        """

        logging.info(f'Requesting {fetched_endpoint}...')

        # Prep-ing request parameters
        request_params = {}
        if fetched_endpoint == 'archived_projects':
            fetched_endpoint = 'projects'
            request_params['archived'] = True
        elif fetched_endpoint == 'projects':
            request_params['archived'] = False
        elif fetched_endpoint == 'user_defined_projects':
            fetched_endpoint = 'projects_details'

        # Incremental load
        # Used for endpoint https://developers.asana.com/reference/gettasksforproject
        if fetched_endpoint == "projects_tasks":
            if self.incremental and completed_since:
                request_params['completed_since'] = completed_since

        # Inputs required for the parser and requests
        required_endpoint_data = self.request_map[fetched_endpoint].get('required')

        # For endpoints required data from parent endpoint
        if required_endpoint_data:
            await self._get_multiple_batched(fetched_endpoint, request_params, required_endpoint_data)

        else:
            endpoint_url = self.request_map[fetched_endpoint]['endpoint']
            await self._get_request(endpoint_url=endpoint_url, endpoint_id=await self._generate_root_id(),
                                    endpoint=fetched_endpoint)
        await self._parse_endpoint_data_from_tmp(fetched_endpoint)

    def _generate_batch(self, data):
        for i in range(0, len(data), self.batch_size):
            yield data[i:i + self.batch_size]

    async def _get_multiple_batched(self, fetched_endpoint, request_params, required_endpoint_data):

        # Some endpoint can be forbidden for some parent endpoints type, name etc.
        without_forbidden_endpoints = [endpoint for endpoint in self.root_endpoints_data[required_endpoint_data] if
                                       fetched_endpoint not in endpoint.get(KEY_FORBIDDEN_ENDPOINTS, [])]

        for batch_parent_endpoint_data in self._generate_batch(without_forbidden_endpoints):
            tasks = []
            for parent_endpoint_data in batch_parent_endpoint_data:
                parent_id = parent_endpoint_data[KEY_GID]
                endpoint_url = self.request_map[fetched_endpoint]['endpoint']
                endpoint_url = endpoint_url.replace('{' + f'{required_endpoint_data}' + '_id}', parent_id)

                tasks.append(self._get_request(endpoint_url=endpoint_url, params=request_params, endpoint_id=parent_id,
                                               endpoint=fetched_endpoint))
            await asyncio.gather(*tasks)

    @staticmethod
    async def _generate_root_id():
        gen_index = f"{KEY_GEN_ID}_{int(time.time() * 1000)}_{random.randint(1000, 9999)}"
        return gen_index

    def _init_tmp_folders(self):
        # create file if not exist
        for endpoint in self.root_endpoints_data:
            file_path = self._construct_tmp_folder_name(endpoint)
            if not os.path.exists(file_path):
                os.makedirs(file_path, exist_ok=True)
            else:
                for file in os.listdir(file_path):
                    os.remove(os.path.join(file_path, file))

    @staticmethod
    def _construct_tmp_folder_name(endpoint):
        file_path = f'{TMP_FOLDER_PATH}/{endpoint}'
        return file_path

    def _write_endpoint_data_to_tmp(self, data, endpoint, file_index=None):
        file_path = self._construct_tmp_folder_name(endpoint)
        with open(f'{file_path}/{file_index}.json', 'w') as f:
            json.dump(data, f)

    async def _parse_endpoint_data_from_tmp(self, endpoint):
        # read every file in the endpoint folder
        file_counter = 0
        data_counter = 0
        for file in os.listdir(self._construct_tmp_folder_name(endpoint)):
            file_counter += 1
            with open(f'{self._construct_tmp_folder_name(endpoint)}/{file}', 'r+') as f:
                file_data = json.load(f)
                self._save_parent_endpoint_data(file_data, endpoint)
                file_name = file.split('.')[0]
                data_counter += len(file_data)
                await self._mapping_endpoint_data_to_output(file_data, endpoint, i_id=file_name)

        logging.debug(f"Parsed data count: {data_counter} from tmp files({file_counter}), endpoint: {endpoint}")

    async def _mapping_endpoint_data_to_output(self, data_out, endpoint, i_id=None):
        MappingParser(
            destination=f'{self.tables_out_path}',
            endpoint=self.request_map[endpoint]['mapping'],
            endpoint_data=data_out,
            mapping=self.mappings[self.request_map[endpoint]['mapping']],
            parent_key=i_id,
            incremental=self.incremental,
            add_timestamp=self.membership_timestamp
        )

    def _save_parent_endpoint_data(self, data, endpoint):
        for i in data:
            data_to_save = self._check_endpoint_rules(endpoint, i)
            self.root_endpoints_data[endpoint].append(data_to_save)

    @staticmethod
    def _check_endpoint_rules(endpoint, data):
        data_to_save = {KEY_GID: data[KEY_GID]}

        if endpoint == 'workspaces':
            if data['name'] == 'Personal Projects':
                logging.info(f"Skipping endpoint users for personal workspaces is not allowed: {data['gid']}")
                data_to_save = {KEY_GID: data[KEY_GID], KEY_FORBIDDEN_ENDPOINTS: ['users']}

        return data_to_save

    def add_parent_endpoint_manually(self, id_str, endpoint):
        """
        Delimiting the list of ids and add them into the respective
        endpoint to bypass original request order
        """
        id_str = id_str.replace(' ', '')
        id_list = id_str.split(',')

        for i in id_list:
            tmp = {KEY_GID: i}
            self.root_endpoints_data[endpoint].append(tmp)

    def get_endpoints_needed(self, endpoints):
        endpoints_needed = set()
        for endpoint in endpoints:
            self.find_dependencies(endpoint, endpoints_needed)
        if 'user_defined_projects' in endpoints:
            endpoints_needed.remove('projects')
            endpoints_needed.remove('projects_details')
        return endpoints_needed

    def find_dependencies(self, endpoint, endpoints_needed):
        if endpoint not in endpoints_needed:
            endpoints_needed.add(endpoint)
            required = self.request_map.get(endpoint, {}).get('required')
            if required:
                self.find_dependencies(required, endpoints_needed)

    def construct_request_map_with_levels(self):
        levels = {}
        for endpoint, details in self.request_map.items():
            level = details.get('level', 0)
            level_key = f'level_{level}'
            levels.setdefault(level_key, []).append(endpoint)
        for level in levels:
            levels[level].sort()
        return levels

    async def _get_request(self, endpoint_url, endpoint, endpoint_id, params=None):
        """
        Generic Get request
        """
        # Pagination parameters
        if not params:
            params = {}
        params['limit'] = API_PAGE_LIMIT
        pagination_offset = None

        data = []
        while True:
            # If pagination parameter exist
            if pagination_offset:
                params['offset'] = pagination_offset

            try:
                r = await self._get(endpoint=endpoint_url, params=params)
            except AsanaClientException as e:
                if e.status_code == 403:
                    if self.skip_unauthorized:
                        logging.warning(f"Skipping unauthorized request: {e}")
                        break
                else:
                    raise AsanaClientException(e)

            try:
                data.extend([r['data']] if isinstance(r['data'], dict) else r['data'])
            except KeyError:
                logging.warning(f"Failed to parse data from response: {r}")

            # Loop
            if r.get('next_page'):
                pagination_offset = r['next_page']['offset']
            else:
                params.pop("offset", None)
                break

        self._write_endpoint_data_to_tmp(data, endpoint, endpoint_id)

    async def _get(self, endpoint: str, params=None) -> dict:
        self.counter += 1

        if params is None:
            params = {}

        try:
            logging.debug(f'{endpoint} Parameters: {params}')
            r = await self.get_raw(endpoint, params=params)
            r.raise_for_status()
        except HTTPStatusError as e:
            raise AsanaClientException(f"Cannot fetch resource: {endpoint}, exception: {e}",
                                       status_code=e.response.status_code) from e

        try:
            return r.json()
        except json.decoder.JSONDecodeError as e:
            raise AsanaClientException(f"Cannot parse response for {endpoint}, exception: {e}") from e


================================================
File: /src/component.py
================================================
import asyncio
import logging
import os
import datetime
import pytz
import dateparser
import pandas as pd
from typing import Dict
from keboola.component.base import ComponentBase
from keboola.component.exceptions import UserException

from asana_client.client import AsanaClient, AsanaClientException, DEFAULT_BATCH_SIZE, DEFAULT_MAX_REQUESTS_PER_SECOND

# configuration variables
KEY_DEBUG = 'debug'
KEY_TOKEN = '#token'
KEY_INCREMENTAL_LOAD = 'incremental_load'
KEY_ENDPOINTS = 'endpoints'
KEY_PROJECT_ID = 'project_id'

KEY_LOAD_OPTIONS = "load_options"
KEY_DATE_FROM = "date_from"
KEY_SKIP_UNAUTHORIZED = "skip_unauthorized"
KEY_MAX_REQUESTS_PER_SECOND = "max_requests_per_second"
KEY_BATCH_SIZE = "batch_size"
KEY_TASK_MEMBERSHIP_TIMESTAMP = "task_membership_timestamp"

REQUIRED_PARAMETERS = [
    KEY_ENDPOINTS,
    KEY_INCREMENTAL_LOAD,
    KEY_TOKEN
]
REQUIRED_IMAGE_PARS = []


class Component(ComponentBase):

    def __init__(self):
        super().__init__()
        self.client = None
        self.params = self.configuration.parameters
        self.date_from = self.define_date_from()
        self.now = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ')
        self.skip = self.params.get(KEY_SKIP_UNAUTHORIZED, False)
        self.incremental = self.params.get(KEY_INCREMENTAL_LOAD)
        self.token = self.params.get(KEY_TOKEN)

    def run(self):
        self.validate_configuration_parameters(REQUIRED_PARAMETERS)
        self.validate_image_parameters(REQUIRED_IMAGE_PARS)

        # Initialize the client
        self.client = AsanaClient(destination=self.tables_out_path, api_token=self.token, incremental=self.incremental,
                                  debug=self.params.get(KEY_DEBUG), skip_unauthorized=self.skip,
                                  max_requests_per_second=self.params.get(KEY_MAX_REQUESTS_PER_SECOND,
                                                                          DEFAULT_MAX_REQUESTS_PER_SECOND),
                                  batch_size=self.params.get(KEY_BATCH_SIZE, DEFAULT_BATCH_SIZE),
                                  membership_timestamp=self.params.get(KEY_TASK_MEMBERSHIP_TIMESTAMP, False)
                                  )

        # Validate user inputs
        self.validate_user_inputs(self.params)

        # User input parameters
        endpoints_raw = self.params.get(KEY_ENDPOINTS)

        endpoints = [k for k, v in endpoints_raw.items() if v]

        if 'user_defined_projects' in endpoints:
            self.client.add_parent_endpoint_manually(self.params.get(KEY_PROJECT_ID), 'projects')

        if self.incremental:
            logging.info(f"Timestamp used for incremental fetching: {self.date_from}")

        try:
            asyncio.run(self.client.fetch(endpoints, completed_since=self.date_from))
        except AsanaClientException as e:
            raise UserException(f"Failed to fetch data, exception: {e}")

        # Always storing the last extraction date
        # if self.incremental:
        state = {'last_run': self.now}
        self.write_state_file(state)

        logging.info("Extraction finished")
        logging.debug(f"Requests count: {self.client.counter}")

    def define_date_from(self):
        params = self.configuration.parameters
        load_options = params.get(KEY_LOAD_OPTIONS, {})
        state = self.get_state_file()
        if date_from_raw := load_options.get(KEY_DATE_FROM):
            return self.parse_date(state, date_from_raw)
        return state.get('last_run')

    @staticmethod
    def validate_user_inputs(params):
        """
        Validating user inputs
        """

        # Validate if configuration is empty
        if not params:
            raise UserException('Your configurations are missing.')

        # Validate if nthe API token is missing
        if params[KEY_TOKEN] == '':
            raise UserException('Your API token is missing.')

        # Validate if any endpoints is selected
        endpoint_selected = sum(bool(params[KEY_ENDPOINTS][i]) for i in params[KEY_ENDPOINTS])

        if endpoint_selected == 0:
            raise UserException('Please select at least one endpoint to extract.')

        # Validating if project_ids are defined when
        # endpoint [user_defined_projects] is defined
        if params[KEY_ENDPOINTS]['user_defined_projects']:
            if params[KEY_PROJECT_ID] == '':
                raise UserException(
                    'Parameters are required when [Projects - User Defined] is selected. Please '
                    'define your project IDs.')

    def _output(self, df_json, filename):
        output_filename = f'{self.tables_out_path}/{filename}.csv'
        data_output = pd.DataFrame(df_json, dtype=str)
        if not os.path.isfile(output_filename):
            with open(output_filename, 'a') as b:
                data_output.to_csv(b, index=False)
        else:
            with open(output_filename, 'a') as b:
                data_output.to_csv(b, index=False, header=False)

        b.close()

    @staticmethod
    def parse_date(state: Dict, date_str: str) -> str:
        if date_str.lower() in {"last", "lastrun", "last run"}:
            return state.get('last_run')
        try:
            date_obj = dateparser.parse(date_str, settings={'TIMEZONE': 'UTC'})
            if date_obj is None:
                raise ValueError("Invalid date string")
            date_obj = date_obj.replace(tzinfo=pytz.UTC)
            date_str = date_obj.strftime('%Y-%m-%dT%H:%M:%SZ')
            return date_str
        except ValueError as e:
            raise UserException(f"Parameters Error : Could not parse to date : {date_str}") from e


if __name__ == "__main__":
    try:
        comp = Component()
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)


================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG


  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
            - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:$TEST_TAG
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP <config_id> test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /component_config/logger
================================================
gelf

================================================
File: /component_config/component_long_description.md
================================================
This extractor allows users to extract details and status of your projects and tasks from Asana.



================================================
File: /component_config/configuration_description.md
================================================
## To obtain an API Key ##
1. Login to [Asana](https://app.asana.com/)
2. Click on your account icon at the top right corner of the page
3. Click *My Profile Settings...*.
4. Click on *Apps* tab.
5. Click on *Manage Developer Apps*.
6. Click on *\+ Create New Personal Access Token*.

## Configuration
1. Token
2. Incremental Load
    - *Enable* : The component will start extraction from the last persisted extraction date and load results into respective tables incrementally.
    - *Disable* : The component will extract `everything` from the respective endpoint and full load the respones into the respective output tables in Keboola storage.
3. Endpoints
    1. Users
    2. Users Details
    3. Projects
    4. Projects - User Defined
    5. Archived Projects
    6. Project Sections
    7. Project Tasks
    8. Project Tasks Details
    9. Project Tasks Subtasks
    10. Project Tasks Stories

    **Notes: If `Projects - User Defined` is selected, the component will NOT fetch data from `Projects` endpoint and `Archived Projects` endpoint.

4. Project IDs
    - Required when endpoint `Projects - User Defined` is selected
    - Please enter your values with comma delimiter.

================================================
File: /component_config/configSchema_v1.json
================================================
{
    "type": "object",
    "title": "extractor configuration",
    "required": [
        "#token",
        "incremental_load",
        "endpoints"
    ],
    "properties": {
        "#token": {
            "type": "string",
            "title": "Token",
            "propertyOrder": 100
        },
        "incremental_load": {
            "type": "boolean",
            "title": "Incremental Load",
            "default": false,
            "format": "checkbox",
            "propertyOrder": 200
        },
        "endpoints": {
            "type": "object",
            "required": [
                "users",
                "users_details",
                "projects",
                "archived_projects",
                "projects_sections",
                "projects_tasks",
                "projects_tasks_details",
                "projects_tasks_subtasks",
                "projects_tasks_stories"
            ],
            "title": "Endpoints",
            "propertyOrder": 300,
            "properties": {
                "users": {
                    "type": "boolean",
                    "title": "Users",
                    "format": "checkbox",
                    "propertyOrder": 100
                },
                "users_details": {
                    "type": "boolean",
                    "title": "Users Details",
                    "format": "checkbox",
                    "propertyOrder": 150
                },
                "projects": {
                    "type": "boolean",
                    "title": "Projects",
                    "format": "checkbox",
                    "propertyOrder": 200
                },
                "archived_projects": {
                    "type": "boolean",
                    "title": "Archived Projects",
                    "format": "checkbox",
                    "propertyOrder": 250
                },
                "projects_sections": {
                    "type": "boolean",
                    "title": "Project Sections",
                    "format": "checkbox",
                    "propertyOrder": 300
                },
                "projects_tasks": {
                    "type": "boolean",
                    "title": "Project Tasks",
                    "format": "checkbox",
                    "propertyOrder": 400
                },
                "projects_tasks_details": {
                    "type": "boolean",
                    "title": "Project Tasks Details",
                    "format": "checkbox",
                    "propertyOrder": 500
                },
                "projects_tasks_subtasks": {
                    "type": "boolean",
                    "title": "Project Tasks Subtasks",
                    "format": "checkbox",
                    "propertyOrder": 600
                },
                "projects_tasks_stories": {
                    "type": "boolean",
                    "title": "Project Tasks Stories",
                    "format": "checkbox",
                    "propertOrder": 700
                }
            }
        }
    }
}

================================================
File: /component_config/component_short_description.md
================================================
Asana is a web and mobile application designed to help teams organize, track, and manage their work

================================================
File: /component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}

================================================
File: /component_config/stack_parameters.json
================================================
{}

================================================
File: /component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "extractor configuration",
  "required": [
    "#token",
    "incremental_load",
    "endpoints"
  ],
  "properties": {
    "#token": {
      "type": "string",
      "format": "password",
      "title": "Token",
      "propertyOrder": 100
    },
    "endpoints": {
      "type": "object",
      "required": [
        "users",
        "users_details",
        "projects",
        "user_defined_projects",
        "archived_projects",
        "projects_sections",
        "projects_sections_tasks",
        "projects_tasks",
        "projects_tasks_details",
        "projects_tasks_subtasks",
        "projects_tasks_stories"
      ],
      "title": "Endpoints",
      "propertyOrder": 300,
      "properties": {
        "users": {
          "type": "boolean",
          "title": "Users",
          "format": "checkbox",
          "propertyOrder": 100
        },
        "users_details": {
          "type": "boolean",
          "title": "Users Details",
          "format": "checkbox",
          "propertyOrder": 150
        },
        "projects": {
          "type": "boolean",
          "title": "Projects",
          "format": "checkbox",
          "propertyOrder": 200
        },
        "user_defined_projects": {
          "type": "boolean",
          "title": "Projects - User Defined",
          "format": "checkbox",
          "propertyOrder": 210
        },
        "archived_projects": {
          "type": "boolean",
          "title": "Archived Projects",
          "format": "checkbox",
          "propertyOrder": 250
        },
        "projects_sections": {
          "type": "boolean",
          "title": "Project Sections",
          "format": "checkbox",
          "propertyOrder": 300
        },
        "projects_sections_tasks": {
          "type": "boolean",
          "title": "Project Sections Tasks",
          "format": "checkbox",
          "propertyOrder": 350
        },
        "projects_tasks": {
          "type": "boolean",
          "title": "Project Tasks",
          "format": "checkbox",
          "propertyOrder": 400
        },
        "projects_tasks_details": {
          "type": "boolean",
          "title": "Project Tasks Details",
          "format": "checkbox",
          "propertyOrder": 500
        },
        "projects_tasks_subtasks": {
          "type": "boolean",
          "title": "Project Tasks Subtasks",
          "format": "checkbox",
          "propertyOrder": 600
        },
        "projects_tasks_stories": {
          "type": "boolean",
          "title": "Project Tasks Stories",
          "format": "checkbox",
          "propertyOrder": 700
        }
      }
    },
    "project_id": {
      "type": "string",
      "title": "Project IDs",
      "propertyOrder": 300,
      "options": {
        "dependencies": {
          "endpoints.user_defined_projects" : true
        }
      },
      "description": "Required when endpoint [Projects - User Defined] is selected. Please enter your values with comma delimiter."
    },
    "task_membership_timestamp": {
      "type": "boolean",
      "title": "Add timestamp column to task membership table",
      "propertyOrder": 310,
      "options": {
        "dependencies": {
          "endpoints.projects_tasks_details" : true
        }
      },
      "description": "Allows you to track changes in task membership over time."
    },
    "incremental_load": {
      "type": "boolean",
      "title": "Incremental Load",
      "default": false,
      "format": "checkbox",
      "propertyOrder": 400
    },
    "load_options": {
      "type": "object",
      "title": "Load Options",
      "propertyOrder": 500,
      "properties": {
        "date_from": {
          "type": "string",
          "title": "Date From",
          "default": "last run",
          "description": "Date from which data is downloaded. Either date in YYYY-MM-DD format or dateparser string i.e. 5 days ago, 1 month ago, yesterday, etc. You can also set this as last run, which will fetch data from the last run of the component.",
          "propertyOrder": 20
        }
      },
      "options": {
        "dependencies": {
          "incremental_load": true
        }
      }
    },
    "skip_unauthorized": {
      "type": "boolean",
      "title": "Skip unsuccessful requests",
      "default": false,
      "format": "checkbox",
      "description": "If set to true, the component will skip objects that could not be retrieved from the API.",
      "propertyOrder": 600
    },
    "max_requests_per_second": {
      "type": "number",
      "default": 2.5,
      "title": "Maximum number of requests per second for your licence (default 2.5)",
      "propertyOrder": 700
    }
  }
}

================================================
File: /component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}


================================================
File: /component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: /component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
fi

echo "Updating logger settings"

value=`cat component_config/logger`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} logger --value="$value"
else
    echo "logger type is empty!"
fi

echo "Updating logger configuration"
value=`cat component_config/loggerConfiguration.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} loggerConfiguration --value="$value"
else
    echo "loggerConfiguration is empty!"
fi

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /requirements.txt
================================================
retry==0.9.2
keboola.component==1.4.3
dateparser==1.1.8
keboola.utils
git+https://github.com/SgtMarmite/python-http-client.git@async-http-client
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.10.zip#egg=kbc
logging_gelf==0.0.18
mock
freezegun
pandas
aiolimiter
httpx
pytz

================================================
File: /Dockerfile
================================================
FROM python:3.12-slim
ENV PYTHONIOENCODING utf-8

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY requirements.txt /code/requirements.txt
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential git

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
File: /LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /README.md
================================================
# Asana Extractor
Asana extractor allows you to extract details and status of your projects and tasks.

## To obtain an API Key ##
1. Login to [Asana](https://app.asana.com/)
2. Click on your account icon at the top right corner of the page
3. Click *My Profile Settings...*.
4. Click on *Apps* tab.
5. Click on *Manage Developer Apps*.
6. Click on *\+ Create New Personal Access Token*.

## Configuration
1. Token
2. Endpoints
    1. Users
    2. Users Details
    3. Projects
    4. Projects - User Defined
    5. Archived Projects
    6. Project Sections
    7. Project Tasks
    8. Project Tasks Details
    9. Project Tasks Subtasks
    10. Project Tasks Stories
3. Incremental Load
    - *Enable* : The component will start extraction from the last persisted extraction date and load results into respective tables incrementally. You can also override the start extraction date with the Date From Parameter
    - *Disable* : The component will extract `everything` from the respective endpoint and full load the responses into the respective output tables in Keboola storage.
4. Load options
    - *Date From* : Date from which data is downloaded (only affects Tasks endpoint). Either date in YYYY-MM-DD format or dateparser string i.e. 5 days ago, 1 month ago, yesterday, etc. You can also set this as last run, which will fetch data from the last run of the component. The component uses completed_since parameter which only return tasks that are either incomplete or that have been completed since this time.
5. Project IDs
    - Required when endpoint `Projects - User Defined` is selected
    - Please enter your values with comma delimiter.

    **Notes: If `Projects - User Defined` is selected, the component will NOT fetch data from `Projects` endpoint and `Archived Projects` endpoint.


