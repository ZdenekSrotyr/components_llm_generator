Directory structure:
└── kds_consulting_team-kds-team.processor-parquet2csv/
    ├── .travis.yml
    ├── flake8.cfg
    ├── deploy.sh
    ├── docker-compose.yml
    ├── change_log.md
    ├── src/
    │   └── component.py
    ├── bitbucket-pipelines.yml
    ├── tests/
    │   ├── functional_dtypes/
    │   │   ├── fast/
    │   │   │   ├── expected/
    │   │   │   │   └── data/
    │   │   │   │       └── out/
    │   │   │   │           └── tables/
    │   │   │   │               ├── table.csv
    │   │   │   │               └── table.csv.manifest
    │   │   │   └── source/
    │   │   │       └── data/
    │   │   │           ├── config.json
    │   │   │           ├── in/
    │   │   │           │   └── files/
    │   │   │           │       ├── table2.parquet
    │   │   │           │       └── table.parquet
    │   │   │           └── out/
    │   │   │               └── tables/
    │   │   │                   └── .gitkeep
    │   │   ├── strict/
    │   │   │   ├── expected/
    │   │   │   │   └── data/
    │   │   │   │       └── out/
    │   │   │   │           └── tables/
    │   │   │   │               ├── table.csv
    │   │   │   │               └── table.csv.manifest
    │   │   │   └── source/
    │   │   │       └── data/
    │   │   │           ├── config.json
    │   │   │           ├── in/
    │   │   │           │   └── files/
    │   │   │           │       ├── table2.parquet
    │   │   │           │       └── table1.parquet
    │   │   │           └── out/
    │   │   │               └── tables/
    │   │   │                   └── .gitkeep
    │   │   └── fill/
    │   │       ├── expected/
    │   │       │   └── data/
    │   │       │       └── out/
    │   │       │           └── tables/
    │   │       │               ├── table.csv
    │   │       │               └── table.csv.manifest
    │   │       └── source/
    │   │           └── data/
    │   │               ├── config.json
    │   │               ├── in/
    │   │               │   └── files/
    │   │               │       ├── table2.parquet
    │   │               │       └── table1.parquet
    │   │               └── out/
    │   │                   └── tables/
    │   │                       └── .gitkeep
    │   ├── test_component.py
    │   ├── __init__.py
    │   ├── functional/
    │   │   ├── fast/
    │   │   │   ├── expected/
    │   │   │   │   └── data/
    │   │   │   │       └── out/
    │   │   │   │           └── tables/
    │   │   │   │               ├── table.csv
    │   │   │   │               └── table.csv.manifest
    │   │   │   └── source/
    │   │   │       └── data/
    │   │   │           ├── config.json
    │   │   │           ├── in/
    │   │   │           │   └── files/
    │   │   │           │       ├── table2.parquet
    │   │   │           │       └── table.parquet
    │   │   │           └── out/
    │   │   │               └── tables/
    │   │   │                   └── .gitkeep
    │   │   ├── strict/
    │   │   │   ├── expected/
    │   │   │   │   └── data/
    │   │   │   │       └── out/
    │   │   │   │           └── tables/
    │   │   │   │               ├── table.csv
    │   │   │   │               └── table.csv.manifest
    │   │   │   └── source/
    │   │   │       └── data/
    │   │   │           ├── config.json
    │   │   │           ├── in/
    │   │   │           │   └── files/
    │   │   │           │       ├── table2.parquet
    │   │   │           │       └── table1.parquet
    │   │   │           └── out/
    │   │   │               └── tables/
    │   │   │                   └── .gitkeep
    │   │   └── fill/
    │   │       ├── expected/
    │   │       │   └── data/
    │   │       │       └── out/
    │   │       │           └── tables/
    │   │       │               ├── table.csv
    │   │       │               └── table.csv.manifest
    │   │       └── source/
    │   │           └── data/
    │   │               ├── config.json
    │   │               ├── in/
    │   │               │   └── files/
    │   │               │       ├── table2.parquet
    │   │               │       └── table1.parquet
    │   │               └── out/
    │   │                   └── tables/
    │   │                       └── .gitkeep
    │   └── test_functional.py
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── configuration_description.md
    │   ├── component_short_description.md
    │   ├── stack_parameters.json
    │   ├── configSchema.json
    │   └── sample-config/
    │       └── config.json
    ├── scripts/
    │   ├── update_dev_portal_properties.sh
    │   └── build_n_test.sh
    ├── requirements.txt
    ├── Dockerfile
    ├── LICENSE.md
    └── README.md

================================================
File: /.travis.yml
================================================
sudo: false

services:
  - docker

jobs:
  include:
    - stage: tests
      script:
        - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
        - docker run $APP_IMAGE python -m unittest discover
      # push test image to ECR - uncomment for testing before deployment
      #  - docker pull quay.io/keboola/developer-portal-cli-v2:latest
      #  - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
      #  - docker tag $APP_IMAGE:latest $REPOSITORY:test
      #  - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
      #  - docker push $REPOSITORY:test
      #  - docker pull quay.io/keboola/syrup-cli:latest
    - stage: deploy_dev_portal
      if: branch = master
      script: "./scripts/update_dev_portal_properties.sh"
    - stage: deploy
      if: tag IS present AND  branch = master
      script: "./deploy.sh"


before_script:
  - export APP_IMAGE=keboola-component
  - docker -v
  - docker build -t $APP_IMAGE .


after_success:
  - docker images


================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
    mem_limit: 4096m
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /src/component.py
================================================
import glob
import json
import logging
import os

import pyarrow
from pyarrow import parquet as pq, types, string
import pandas as pd
from keboola.component import ComponentBase, UserException
from keboola.component.dao import BaseType, ColumnDefinition

KEY_MODE = 'mode'
KEY_TABLE_COLUMNS = 'columns'
KEY_TABLE_NAME = 'table_name'
KEY_INCREMENTAL = 'incremental'
KEY_PRIMARY_KEYS = 'primary_keys'
KEY_FILENAME = 'include_filename'
KEY_CHUNKSIZE = 'chunk_size'
KEY_DEBUG = 'debug'
KEY_EXTENSION_MASK = 'file_mask'

MANDATORY_PARAMETERS = [KEY_MODE, KEY_TABLE_NAME]
SUPPORTED_MODES = ["fast", "fill", "strict"]  # , "pandas"]
FILENAME_COLUMN = 'parquet_filename'

DEFAULT_CHUNK_SIZE = 10000


class Component(ComponentBase):

    def __init__(self):
        ComponentBase.__init__(self)
        self.cfg_params = self.configuration.parameters

        try:
            self.validate_configuration_parameters(MANDATORY_PARAMETERS)

        except ValueError as e:
            raise UserException(f"Missing mandatory fields {e} in configuration.")

        self.par_mode = self.cfg_params[KEY_MODE]
        self.par_table_name = self.cfg_params[KEY_TABLE_NAME]
        self.par_table_columns = self.cfg_params.get(KEY_TABLE_COLUMNS, [])
        self.par_incremental = bool(self.cfg_params.get(KEY_INCREMENTAL, False))
        self.par_primary_keys = self.cfg_params.get(KEY_PRIMARY_KEYS, [])
        self.par_include_filename = bool(self.cfg_params.get(KEY_FILENAME, False))
        self.par_chunk_size = self.cfg_params.get(KEY_CHUNKSIZE, None)
        self.par_debug = self.cfg_params.get(KEY_DEBUG, False)
        self.par_extension_mask = self.cfg_params.get(KEY_EXTENSION_MASK, '*.parquet')

        if self.par_debug is True:
            logging.getLogger().setLevel('DEBUG')
        else:
            pass

        self.validateParameters()

    def validateParameters(self):

        # mode validation
        if self.par_mode not in SUPPORTED_MODES:
            raise UserException(f"Unsupported mode {self.par_mode}. Supported modes are: {SUPPORTED_MODES}.")

        # table name validation
        if not isinstance(self.par_table_name, str):
            raise UserException("Parameter \"table_name\" must be of type string.")

        elif self.par_table_name.strip() == '':
            raise UserException("No table name provided.")

        elif self.par_table_name.endswith('.csv') is False:
            self.par_table_name = self.par_table_name + '.csv'

        else:
            pass

        # table columns validation
        if not isinstance(self.par_table_columns, list):
            raise UserException("Parameter \"columns\" must be of type list.")

        elif len(self.par_table_columns) == 0:
            self.par_table_columns = None

        if not isinstance(self.par_primary_keys, list):
            raise UserException("Parameter \"primary_keys\" must be of type list.")

        if self.par_chunk_size is None:
            self.par_chunk_size = DEFAULT_CHUNK_SIZE

        else:
            try:
                _cs = int(self.par_chunk_size)

            except ValueError:
                raise UserException("Parameter \"chunk_size\" must be either an integer or \"null\".")

            self.par_chunk_size = _cs if _cs > 0 else DEFAULT_CHUNK_SIZE

    def getParquetFiles(self):

        all_parquet_files = glob.glob(os.path.join(self.files_in_path, '**', self.par_extension_mask), recursive=True)
        # to ensure consistent order on all platforms
        all_parquet_files.sort()

        if len(all_parquet_files) == 0:
            raise UserException("No parquet files found.")

        else:
            nonempty_parquet_files = [path for path in all_parquet_files if os.path.getsize(path) > 0]
            empty_parquet_files = [path for path in all_parquet_files if os.path.getsize(path) == 0]

            logging.info(f"Skipping {len(empty_parquet_files)} empty files.")
            # logging.debug(f"Paths of empty files: {[x.replace(self.files_in_path, '') for x in empty_parquet_files]}.") # noqa

            self.var_pq_files_paths = nonempty_parquet_files
            self.var_pq_files_names = [x.replace(self.files_in_path, '') for x in nonempty_parquet_files]

            logging.debug(f"Processing {len(self.var_pq_files_names)} files. Names:\n{self.var_pq_files_names}.")

    def processParquet(self):

        path_table = os.path.join(self.tables_out_path, self.par_table_name)

        if self.par_mode == 'fast':
            _schema = self._fastProcess(path_table)

        elif self.par_mode == 'fill':
            _schema = self._fillProcess(path_table)

        elif self.par_mode == 'strict':
            _schema = self._strictProces(path_table)

        # elif self.par_mode == 'pandas':
        #     self._pandasProcess(path_table)

        else:
            raise UserException(f"Unsupported mode {self.par_mode}.")

        schema = {k: ColumnDefinition(data_types=self.convert_dtypes(v)) for k, v in _schema.items()}
        out_table = self.create_out_table_definition(self.par_table_name, schema=schema,
                                                     primary_key=self.par_primary_keys)
        self.write_manifest(out_table)

        logging.info(f"Converted {len(self.var_pq_files_names)} Parquet files to csv.")

    def convert_dtypes(self, dtype: pyarrow.DataType) -> BaseType:
        type_mapping = {
            types.is_integer: BaseType.integer,
            types.is_floating: BaseType.float,
            types.is_boolean: BaseType.boolean,
            types.is_date: BaseType.date,
            types.is_timestamp: BaseType.timestamp,
            types.is_decimal: BaseType.numeric,
        }

        for type_check, base_type in type_mapping.items():
            if type_check(dtype):
                return base_type()

        return BaseType.string()

    def createManifest(self, table_path, columns):

        with open(table_path + '.manifest', 'w') as _man_file:

            json.dump(
                {
                    'columns': columns,
                    'incremental': self.par_incremental,
                    'primary_key': self.par_primary_keys
                },
                _man_file
            )

    # def _pandasProcess(self, table_path):

    #     with open(table_path, 'w') as out_results:

    #         for path, filename in zip(self.var_pq_files_paths, self.var_pq_files_names):

    #             logging.info(f"Converting file {path} to csv.")
    #             _df = pd.read_parquet(path, memory_map=False)
    #             _df.to_csv(out_results, index=False, header=False)

    def _processFastBatch(self, pq_path, out_table, columns, filename):

        _pq_file = pq.read_table(pq_path, columns=self.par_table_columns)
        _pq_batches = _pq_file.to_batches(max_chunksize=self.par_chunk_size)

        for _pq_batch in _pq_batches:
            _df_batch = pd.DataFrame(_pq_batch.to_pydict(), dtype=str)

            for _c in columns:

                if _c not in _df_batch.columns:
                    if _c == FILENAME_COLUMN:
                        _df_batch[_c] = filename

                    else:
                        _df_batch[_c] = ''

            _df_batch[columns].to_csv(out_table, header=False, index=False, na_rep='')

        logging.debug(f"Converted {pq_path} to csv. Rows: {_pq_file.num_rows}.")

    def _fastProcess(self, table_path):

        schema = None
        columns = None

        with open(table_path, 'w') as out_results:

            for path, filename in zip(self.var_pq_files_paths, self.var_pq_files_names):
                logging.info(f"Converting file {path} to csv.")

                _pq_file_schema = pq.read_schema(path)

                if schema is None:
                    schema = _pq_file_schema

                    if schema.names == [] and self.par_table_columns is not None:
                        raise UserException("Schema is empty. Make sure parameter \"columns\" specifies" +
                                            "correct columns present in schema.")

                    elif schema.names == []:
                        raise UserException("Schema is empty.")

                    else:
                        pass

                    logging.debug(f"Using following schema to parse the files:\n{schema}.")

                    if self.par_table_columns is None:
                        columns = schema.names
                    else:
                        columns = self.par_table_columns

                    if self.par_include_filename is True:
                        columns += [FILENAME_COLUMN]

                else:
                    pass

                self._processFastBatch(path, out_results, columns, filename)

                # _pq_file = pq.read_table(path, columns=self.par_table_columns)
                # _pq_batches = _pq_file.to_batches(max_chunksize=self.par_chunk_size)

                # for _pq_batch in _pq_batches:
                #     _df_batch = pd.DataFrame(_pq_batch.to_pydict(), dtype=str)

                #     for _c in columns:

                #         if _c not in _df_batch.columns:
                #             if _c == FILENAME_COLUMN:
                #                 _df_batch[_c] = filename

                #             else:
                #                 _df_batch[_c] = ''

                #     _df_batch[columns].to_csv(out_results, header=False, index=False, na_rep='')

                # logging.debug(f"Converted {filename} to csv. Rows: {_pq_file.num_rows}.")

        return dict(zip(schema.names, schema.types))

    def _fillProcess(self, table_path):

        schema = {}

        for path in self.var_pq_files_paths:

            _pq_file = pq.read_table(path, columns=self.par_table_columns)
            schema.update(dict(zip(_pq_file.schema.names, _pq_file.schema.types)))

        logging.debug(schema.keys())

        if self.par_include_filename is True:
            schema.update({FILENAME_COLUMN: string()})

        with open(table_path, 'w') as out_results:

            for path, filename in zip(self.var_pq_files_paths, self.var_pq_files_names):

                _pq_file = pq.read_table(path)

                _pq_batches = _pq_file.to_batches(max_chunksize=self.par_chunk_size)

                for _pq_batch in _pq_batches:

                    _df_batch = pd.DataFrame(_pq_batch.to_pydict(), dtype=str)

                    for _c in schema:

                        if _c not in _df_batch.columns:
                            if _c == FILENAME_COLUMN:
                                _df_batch[_c] = filename

                            else:
                                _df_batch[_c] = ''

                    _df_batch[schema.keys()].to_csv(out_results, header=False, index=False, na_rep='')

                logging.debug(f"Converted {filename} to csv. Rows: {_pq_file.num_rows}.")

        return schema

    def _strictProces(self, table_path):

        if self.par_table_columns is None:
            raise UserException("Parameter \"columns\" must be specified for strict mode.")

        columns = self.par_table_columns
        if self.par_include_filename is True:
            columns += [FILENAME_COLUMN]

        with open(table_path, 'w') as out_results:

            for path, filename in zip(self.var_pq_files_paths, self.var_pq_files_names):

                _pq_file = pq.read_table(path)

                missing_columns = list(set(columns) - set(_pq_file.schema.names) - set([FILENAME_COLUMN]))
                if missing_columns != []:
                    raise UserException(f"Missing columns {missing_columns} in file {filename}, which were defined " +
                                        "in configuration parameter \"columns\".\n" +
                                        f"Available columns are {_pq_file.schema.names}.")

                _pq_batches = _pq_file.to_batches(max_chunksize=self.par_chunk_size)

                for _pq_batch in _pq_batches:

                    _df_batch = pd.DataFrame(_pq_batch.to_pydict(), dtype=str)
                    if FILENAME_COLUMN in columns:
                        _df_batch[FILENAME_COLUMN] = filename

                    _df_batch[columns].to_csv(out_results, header=False, index=False, na_rep='')

                logging.debug(f"Converted {filename} to csv. Rows: {_pq_file.num_rows}.")

                schema = dict(zip(_pq_file.schema.names, _pq_file.schema.types))

                filtered_schema = {k: v for k, v in schema.items() if k in columns}

        return filtered_schema

    def run(self):
        self.getParquetFiles()
        self.processParquet()


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)


================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
#          - echo 'Pushing test image to repo. [tag=test]'
#          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#          - docker tag $APP_IMAGE:latest $REPOSITORY:test
#          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /tests/functional_dtypes/fast/expected/data/out/tables/table.csv
================================================
one,1,1.1,1.11,True,2023-01-01,2023-01-01 12:00:00,/table.parquet
two,2,2.2,2.22,False,2023-02-01,2023-02-01 12:00:00,/table.parquet
three,3,3.3,3.33,True,2023-03-01,2023-03-01 12:00:00,/table.parquet
four,4,4.4,4.44,False,2023-04-01,2023-04-01 12:00:00,/table.parquet
five,5,5.5,5.55,True,2023-05-01,2023-05-01 12:00:00,/table.parquet
six,6,6.1,6.11,True,2023-01-01,2023-01-01 12:00:00,/table2.parquet
seven,7,7.2,7.22,False,2023-02-01,2023-02-01 12:00:00,/table2.parquet
eight,8,8.3,8.33,True,2023-03-01,2023-03-01 12:00:00,/table2.parquet
nine,9,9.4,9.44,False,2023-04-01,2023-04-01 12:00:00,/table2.parquet
ten,10,10.5,10.55,True,2023-05-01,2023-05-01 12:00:00,/table2.parquet


================================================
File: /tests/functional_dtypes/fast/expected/data/out/tables/table.csv.manifest
================================================
{"write_always": false, "delimiter": ",", "enclosure": "\"", "manifest_type": "out", "has_header": false, "schema": [{"name": "STRING", "data_type": {"base": {"type": "STRING"}}, "nullable": true}, {"name": "INTEGER", "data_type": {"base": {"type": "INTEGER"}}, "nullable": true}, {"name": "NUMERIC", "data_type": {"base": {"type": "FLOAT"}}, "nullable": true}, {"name": "FLOAT", "data_type": {"base": {"type": "FLOAT"}}, "nullable": true}, {"name": "BOOLEAN", "data_type": {"base": {"type": "BOOLEAN"}}, "nullable": true}, {"name": "DATE", "data_type": {"base": {"type": "DATE"}}, "nullable": true}, {"name": "TIMESTAMP", "data_type": {"base": {"type": "TIMESTAMP"}}, "nullable": true, "primary_key": true}]}

================================================
File: /tests/functional_dtypes/fast/source/data/config.json
================================================
{
  "parameters": {
    "mode": "fast",
    "table_name": "table.csv",
    "incremental": true,
    "primary_keys": [
      "TIMESTAMP"
    ],
    "include_filename": true,
    "debug": true,
    "columns": []
  },
  "image_parameters": {}
}

================================================
File: /tests/functional_dtypes/strict/expected/data/out/tables/table.csv
================================================
2023-01-01,2023-01-01 12:00:00,True,1.11,1,1.1,/table1.parquet
2023-02-01,2023-02-01 12:00:00,False,2.22,2,2.2,/table1.parquet
2023-03-01,2023-03-01 12:00:00,True,3.33,3,3.3,/table1.parquet
2023-04-01,2023-04-01 12:00:00,False,4.44,4,4.4,/table1.parquet
2023-05-01,2023-05-01 12:00:00,True,5.55,5,5.5,/table1.parquet
2023-01-01,2023-01-01 12:00:00,True,6.11,6,6.1,/table2.parquet
2023-02-01,2023-02-01 12:00:00,False,7.22,7,7.2,/table2.parquet
2023-03-01,2023-03-01 12:00:00,True,8.33,8,8.3,/table2.parquet
2023-04-01,2023-04-01 12:00:00,False,9.44,9,9.4,/table2.parquet
2023-05-01,2023-05-01 12:00:00,True,10.55,10,10.5,/table2.parquet


================================================
File: /tests/functional_dtypes/strict/expected/data/out/tables/table.csv.manifest
================================================
{"write_always": false, "delimiter": ",", "enclosure": "\"", "manifest_type": "out", "has_header": false, "schema": [{"name": "INTEGER", "data_type": {"base": {"type": "INTEGER"}}, "nullable": true}, {"name": "NUMERIC", "data_type": {"base": {"type": "FLOAT"}}, "nullable": true}, {"name": "FLOAT", "data_type": {"base": {"type": "FLOAT"}}, "nullable": true}, {"name": "BOOLEAN", "data_type": {"base": {"type": "BOOLEAN"}}, "nullable": true}, {"name": "DATE", "data_type": {"base": {"type": "DATE"}}, "nullable": true}, {"name": "TIMESTAMP", "data_type": {"base": {"type": "TIMESTAMP"}}, "nullable": true, "primary_key": true}]}

================================================
File: /tests/functional_dtypes/strict/source/data/config.json
================================================
{
  "parameters": {
    "mode": "strict",
    "columns": ["DATE", "TIMESTAMP", "BOOLEAN", "FLOAT", "INTEGER", "NUMERIC"],
    "table_name": "table.csv",
    "incremental": true,
    "primary_keys": [
      "TIMESTAMP"
    ],
    "include_filename": true,
    "debug": true
  },
  "image_parameters": {}
}

================================================
File: /tests/functional_dtypes/fill/expected/data/out/tables/table.csv
================================================
one,1,1.1,1.11,True,2023-01-01,2023-01-01 12:00:00,,/table1.parquet
two,2,2.2,2.22,False,2023-02-01,2023-02-01 12:00:00,,/table1.parquet
three,3,3.3,3.33,True,2023-03-01,2023-03-01 12:00:00,,/table1.parquet
four,4,4.4,4.44,False,2023-04-01,2023-04-01 12:00:00,,/table1.parquet
five,5,5.5,5.55,True,2023-05-01,2023-05-01 12:00:00,,/table1.parquet
,6,6.1,6.11,True,2023-01-01,2023-01-01 12:00:00,six,/table2.parquet
,7,7.2,7.22,False,2023-02-01,2023-02-01 12:00:00,seven,/table2.parquet
,8,8.3,8.33,True,2023-03-01,2023-03-01 12:00:00,eight,/table2.parquet
,9,9.4,9.44,False,2023-04-01,2023-04-01 12:00:00,nine,/table2.parquet
,10,10.5,10.55,True,2023-05-01,2023-05-01 12:00:00,ten,/table2.parquet


================================================
File: /tests/functional_dtypes/fill/expected/data/out/tables/table.csv.manifest
================================================
{"write_always": false, "delimiter": ",", "enclosure": "\"", "manifest_type": "out", "has_header": false, "schema": [{"name": "STRING_A", "data_type": {"base": {"type": "STRING"}}, "nullable": true}, {"name": "INTEGER", "data_type": {"base": {"type": "INTEGER"}}, "nullable": true}, {"name": "NUMERIC", "data_type": {"base": {"type": "FLOAT"}}, "nullable": true}, {"name": "FLOAT", "data_type": {"base": {"type": "FLOAT"}}, "nullable": true}, {"name": "BOOLEAN", "data_type": {"base": {"type": "BOOLEAN"}}, "nullable": true}, {"name": "DATE", "data_type": {"base": {"type": "DATE"}}, "nullable": true}, {"name": "TIMESTAMP", "data_type": {"base": {"type": "TIMESTAMP"}}, "nullable": true, "primary_key": true}, {"name": "STRING_B", "data_type": {"base": {"type": "STRING"}}, "nullable": true}, {"name": "parquet_filename", "data_type": {"base": {"type": "STRING"}}, "nullable": true}]}

================================================
File: /tests/functional_dtypes/fill/source/data/config.json
================================================
{
  "parameters": {
    "mode": "fill",
    "table_name": "table.csv",
    "incremental": true,
    "primary_keys": [
      "TIMESTAMP"
    ],
    "include_filename": true,
    "debug": true,
    "columns": []
  },
  "image_parameters": {}
}

================================================
File: /tests/functional/fast/expected/data/out/tables/table.csv
================================================
one,1,1.1,1.11,True,2023-01-01,2023-01-01 12:00:00,/table.parquet
two,2,2.2,2.22,False,2023-02-01,2023-02-01 12:00:00,/table.parquet
three,3,3.3,3.33,True,2023-03-01,2023-03-01 12:00:00,/table.parquet
four,4,4.4,4.44,False,2023-04-01,2023-04-01 12:00:00,/table.parquet
five,5,5.5,5.55,True,2023-05-01,2023-05-01 12:00:00,/table.parquet
six,6,6.1,6.11,True,2023-01-01,2023-01-01 12:00:00,/table2.parquet
seven,7,7.2,7.22,False,2023-02-01,2023-02-01 12:00:00,/table2.parquet
eight,8,8.3,8.33,True,2023-03-01,2023-03-01 12:00:00,/table2.parquet
nine,9,9.4,9.44,False,2023-04-01,2023-04-01 12:00:00,/table2.parquet
ten,10,10.5,10.55,True,2023-05-01,2023-05-01 12:00:00,/table2.parquet


================================================
File: /tests/functional/fast/expected/data/out/tables/table.csv.manifest
================================================
{"primary_key": ["TIMESTAMP"], "write_always": false, "delimiter": ",", "enclosure": "\"", "columns": ["STRING", "INTEGER", "NUMERIC", "FLOAT", "BOOLEAN", "DATE", "TIMESTAMP"]}

================================================
File: /tests/functional/fast/source/data/config.json
================================================
{
  "parameters": {
    "mode": "fast",
    "table_name": "table.csv",
    "incremental": true,
    "primary_keys": [
      "TIMESTAMP"
    ],
    "include_filename": true,
    "debug": true,
    "columns": []
  },
  "image_parameters": {}
}

================================================
File: /tests/functional/strict/expected/data/out/tables/table.csv
================================================
2023-01-01,2023-01-01 12:00:00,True,1.11,1,1.1,/table1.parquet
2023-02-01,2023-02-01 12:00:00,False,2.22,2,2.2,/table1.parquet
2023-03-01,2023-03-01 12:00:00,True,3.33,3,3.3,/table1.parquet
2023-04-01,2023-04-01 12:00:00,False,4.44,4,4.4,/table1.parquet
2023-05-01,2023-05-01 12:00:00,True,5.55,5,5.5,/table1.parquet
2023-01-01,2023-01-01 12:00:00,True,6.11,6,6.1,/table2.parquet
2023-02-01,2023-02-01 12:00:00,False,7.22,7,7.2,/table2.parquet
2023-03-01,2023-03-01 12:00:00,True,8.33,8,8.3,/table2.parquet
2023-04-01,2023-04-01 12:00:00,False,9.44,9,9.4,/table2.parquet
2023-05-01,2023-05-01 12:00:00,True,10.55,10,10.5,/table2.parquet


================================================
File: /tests/functional/strict/expected/data/out/tables/table.csv.manifest
================================================
{"primary_key": ["TIMESTAMP"], "write_always": false, "delimiter": ",", "enclosure": "\"", "columns": ["INTEGER", "NUMERIC", "FLOAT", "BOOLEAN", "DATE", "TIMESTAMP"]}

================================================
File: /tests/functional/strict/source/data/config.json
================================================
{
  "parameters": {
    "mode": "strict",
    "columns": ["DATE", "TIMESTAMP", "BOOLEAN", "FLOAT", "INTEGER", "NUMERIC"],
    "table_name": "table.csv",
    "incremental": true,
    "primary_keys": [
      "TIMESTAMP"
    ],
    "include_filename": true,
    "debug": true
  },
  "image_parameters": {}
}

================================================
File: /tests/functional/fill/expected/data/out/tables/table.csv
================================================
one,1,1.1,1.11,True,2023-01-01,2023-01-01 12:00:00,,/table1.parquet
two,2,2.2,2.22,False,2023-02-01,2023-02-01 12:00:00,,/table1.parquet
three,3,3.3,3.33,True,2023-03-01,2023-03-01 12:00:00,,/table1.parquet
four,4,4.4,4.44,False,2023-04-01,2023-04-01 12:00:00,,/table1.parquet
five,5,5.5,5.55,True,2023-05-01,2023-05-01 12:00:00,,/table1.parquet
,6,6.1,6.11,True,2023-01-01,2023-01-01 12:00:00,six,/table2.parquet
,7,7.2,7.22,False,2023-02-01,2023-02-01 12:00:00,seven,/table2.parquet
,8,8.3,8.33,True,2023-03-01,2023-03-01 12:00:00,eight,/table2.parquet
,9,9.4,9.44,False,2023-04-01,2023-04-01 12:00:00,nine,/table2.parquet
,10,10.5,10.55,True,2023-05-01,2023-05-01 12:00:00,ten,/table2.parquet


================================================
File: /tests/functional/fill/expected/data/out/tables/table.csv.manifest
================================================
{"primary_key": ["TIMESTAMP"], "write_always": false, "delimiter": ",", "enclosure": "\"", "columns": ["STRING_A", "INTEGER", "NUMERIC", "FLOAT", "BOOLEAN", "DATE", "TIMESTAMP", "STRING_B", "parquet_filename"]}

================================================
File: /tests/functional/fill/source/data/config.json
================================================
{
  "parameters": {
    "mode": "fill",
    "table_name": "table.csv",
    "incremental": true,
    "primary_keys": [
      "TIMESTAMP"
    ],
    "include_filename": true,
    "debug": true,
    "columns": []
  },
  "image_parameters": {}
}

================================================
File: /tests/test_functional.py
================================================
import os
import unittest
import logging

from datadirtest import DataDirTester


class TestComponent(unittest.TestCase):

    def test_functional(self):
        logging.info('Running functional tests')
        os.environ['KBC_STACKID'] = 'connection.keboola.com'
        os.environ['KBC_PROJECT_FEATURE_GATES'] = 'queuev2'
        functional_tests = DataDirTester()
        functional_tests.run()

    def test_functional_dtypes(self):
        logging.info('Running functional tests with dtypes')
        os.environ['KBC_DATA_TYPE_SUPPORT'] = 'authoritative'
        functional_tests = DataDirTester(data_dir='./tests/functional_dtypes')
        functional_tests.run()


if __name__ == "__main__":
    unittest.main()


================================================
File: /component_config/component_long_description.md
================================================
# Parquet Processor

Parquet processor utilizes the [pyarrow](https://pypi.org/project/pyarrow/) library to process Parquet files and convert them into a single .csv file. A single configuration, in which the Parquet processor is used, should contain the files which belong to one table only and thus maintaining one configuration, one table rule.

Parquet processor automatically filters all files from `/data/in/files` folder, converts them to a .csv format, and outputs to `/data/out/tables` folder.

## Configuration:

A sample of the configuration object:

```json
{
    "definition": {
        "component": "kds-team.processor-parquet2csv"
    },
    "parameters": {
        "mode": "fast",
        "table_name": "table.csv",
        "incremental": true,
        "primary_keys": [
            "order_id"
        ],
        "include_filename": false,
        "debug": true,
        "chunk_size": 5000,
        "file_mask": "export*.parquet",
        "columns": [
            "id",
            "order_id",
            "dwh_created",
            "created_at"
        ]
    }
}
```

## Parameters

- `mode` (required) - A mode, which will be utilized in running of the processor. Must be one of `fast`, `fill` or `strict`. See below for further clarification.
- `table_name` (required) - A name of the table in storage.
- `incremental` (optional) - A boolean value marking, whether to utilize incremental load to storage. If not specified, full load is performed.
- `primary_keys` (optional) - An array of primary keys.
- `include_filename` (optional) - A boolean value. If `true`, an extra column `parquet_filename` with name of data parquet origin file will be included in the output table. Default is `false`.
- `chunk_size` (optional) - A positive integer specifying the size of a chunk, which should be processed in memory. In general, the lower the chunk size, the lower memory consumption, but slower processing and vice versa. If `chunk_size` is not specified or is a negative number the batch size defaults to 10000.
- `columns` (optional) - An array of columns, which will be read from the Parquet file. If any of the columns specified here is not present in the Parquet file, it will be ignored. If mode is set to `strict`, this parameter is required, since it defines the schema, which should be checked.
  - `file_mask` (optional) - A glob-like syntax defining files, which should be included. Can be used for filtering files, or extensions. When not specified, defaults to `*.parquet`, i.e. all files with `.parquet` extension are included.
- `debug` - A boolean value. If `true`, extra logging is added. Default is `false`.

### Different `mode` parameter specification

Since the processor will be processing files continuously, some change in schema of Parquet files may occur with different exports (e.g. new column added or removed). The processor offers different modes to treat the issue that may arise.

#### Mode `fast`

The `fast` mode tries to be as fast in processing files as possible. It determines the schema from the first Parquet file it reads and applies to schema to all remaining Parquet files which are processed, regardless of what is their actual schema. This process is fast, but may lead to some data being lost, if schema of processed files is different.
If parameter `columns` is specified, the processor will read only specified columns and apply them to followin files.

#### Mode `fill`

The `fill` mode reads the schemas of all files in the processing queue and creates a singular schema which is applied to all files. If some columns are missing in one of the files, those will be filled with blanks.

#### Mode `strict`

Strict mode makes sure that all of the files adhere to the same schema, which is defined by `columns` parameters. If any of the columns defined in `columns` parameters is missing in one of the files, an error is raised and conversion to a csv is halted.

## Development

To build and run a docker image, use following commands:

```
docker-compose build dev
docker-compose run --rm dev
```

================================================
File: /component_config/configuration_description.md
================================================
Testing configuration description.

================================================
File: /component_config/component_short_description.md
================================================
Apache Parquet is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem.

================================================
File: /component_config/stack_parameters.json
================================================
{}

================================================
File: /component_config/configSchema.json
================================================
{}

================================================
File: /component_config/sample-config/config.json
================================================
{
  "parameters": {
    "mode": "fast",
    "table_name": "table.csv",
    "incremental": true,
    "primary_keys": [
      "order_id"
    ],
    "include_filename": true,
    "debug": true,
    "columns": []
  },
  "image_parameters": {}
}

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /requirements.txt
================================================
keboola.component==1.6.7
python-snappy==0.7.2
fastparquet==2024.5.0
pyarrow==17.0.0
https://bitbucket.org/kds_consulting_team/datadirtest/get/1.8.2.zip#egg=datadirtest


================================================
File: /Dockerfile
================================================
FROM python:3.12-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential libsnappy-dev
RUN pip install flake8
RUN pip install -r /code/requirements.txt
WORKDIR /code/

CMD ["python", "-W", "ignore", "-u", "/code/src/component.py"]


================================================
File: /LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /README.md
================================================
# Parquet Processor

Parquet processor utilizes the [pyarrow](https://pypi.org/project/pyarrow/) library to process Parquet files and convert them into a single .csv file. A single configuration, in which the Parquet processor is used, should contain the files which belong to one table only and thus maintaining one configuration, one table rule.

Parquet processor automatically filters all files from `/data/in/files` folder, converts them to a .csv format, and outputs to `/data/out/tables` folder.

## Configuration:

A sample of the configuration object:

```json
{
    "definition": {
        "component": "kds-team.processor-parquet2csv"
    },
    "parameters": {
        "mode": "fast",
        "table_name": "table.csv",
        "incremental": true,
        "primary_keys": [
            "order_id"
        ],
        "include_filename": false,
        "debug": true,
        "chunk_size": 5000,
        "columns": [
            "id",
            "order_id",
            "dwh_created",
            "created_at"
        ]
    }
}
```

## Parameters

- `mode` (required) - A mode, which will be utilized in running of the processor. Must be one of `fast`, `fill` or `strict`. See below for further clarification.
- `table_name` (required) - A name of the table in storage.
- `incremental` (optional) - A boolean value marking, whether to utilize incremental load to storage. If not specified, full load is performed.
- `primary_keys` (optional) - An array of primary keys.
- `include_filename` (optional) - A boolean value. If `true`, an extra column `parquet_filename` with name of data parquet origin file will be included in the output table. Default is `false`.
- `chunk_size` (optional) - A positive integer specifying the size of a chunk, which should be processed in memory. In general, the lower the chunk size, the lower memory consumption, but slower processing and vice versa. If `chunk_size` is not specified or is a negative number the batch size defaults to 10000.
- `columns` (optional) - An array of columns, which will be read from the Parquet file. If any of the columns specified here is not present in the Parquet file, it will be ignored. If mode is set to `strict`, this parameter is required, since it defines the schema, which should be checked.
- `file_mask` (optional) - A glob-like syntax defining files, which should be included. Can be used for filtering files, or extensions. When not specified, defaults to `*.parquet`, i.e. all files with `.parquet` extension are included.
- `debug` - A boolean value. If `true`, extra logging is added. Default is `false`.

### Different `mode` parameter specification

Since the processor will be processing files continuously, some change in schema of Parquet files may occur with different exports (e.g. new column added or removed). The processor offers different modes to treat the issue that may arise.

#### Mode `fast`

The `fast` mode tries to be as fast in processing files as possible. It determines the schema from the first Parquet file it reads and applies to schema to all remaining Parquet files which are processed, regardless of what is their actual schema. This process is fast, but may lead to some data being lost, if schema of processed files is different.
If parameter `columns` is specified, the processor will read only specified columns and apply them to followin files.

#### Mode `fill`

The `fill` mode reads the schemas of all files in the processing queue and creates a singular schema which is applied to all files. If some columns are missing in one of the files, those will be filled with blanks.

#### Mode `strict`

Strict mode makes sure that all of the files adhere to the same schema, which is defined by `columns` parameters. If any of the columns defined in `columns` parameters is missing in one of the files, an error is raised and conversion to a csv is halted.

## Development

To build and run a docker image, use following commands:

```
docker-compose build dev
docker-compose run --rm dev
```

