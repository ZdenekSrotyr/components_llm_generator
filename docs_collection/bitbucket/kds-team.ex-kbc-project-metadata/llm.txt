Directory structure:
└── kds_consulting_team-kds-team.ex-kbc-project-metadata/
    ├── README.md
    ├── bitbucket-pipelines.yml
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run.bat
    │   ├── run_kbc_tests.ps1
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── component.py
    │   └── result/
    │       ├── __init__.py
    │       └── kbc_result.py
    └── tests/
        ├── __init__.py
        └── test_component.py

================================================
FILE: README.md
================================================
# KBC project metadata extractor

A custom KBC component that allows scanning project Transformations, Orchestrations, Tokens and others and retrieving metadata useful for on-demand health check analysis.

A helpful tool for quantitative health check analysis. Discover unused code, I/O, documentation coverage and more.

As any project grows it is a good idea to perform a regular reviews of its state, in particular over time there may be introduced some processes that are not efficiently designed and consume a lot of time and credit resources. This tool helps identifying transformations that are eligible for possible optimization in terms of more effective incremental processing. But also identifying configurations that contain non-existent tables on the inputs or inconsistency in the input mapping.

Lot of the indicators of a ineffectively designed transformations may be identified from the Input / Output mapping of the configuration itself.

In general these are:

- Copy or Clone mode?
- Is there any column filters?
- Is there any data filters?
- Is there incremental setup (Changed in Last x)
- Is there incremental output
- How large the tables on the input are and what is the estimated credit consumption?

All above can be retrieved using this extractor.

## Setup

To create a new configuration enter URL:

[https://connection.eu-central-1.keboola.com/admin/projects/[PROJECT\_ID]/extractors/kds-team.ex-kbc-project-metadata](https://connection.eu-central-1.keboola.com/admin/projects/196/extractors/kds-team.ex-kbc-project-metadata)

 and replace the [PROJECT\_ID] with ID of the project you want the configuration in.
## Output result

Extractor outputs following tables:

- **tr_details** - details of transformation configs and its' I/O mappings
- **orchestrations** - orchestration metadata, last run, activity
- **orchestration_tasks** - tasks of orchestrations
- **tokens** - Storage tokens present in the project, last activity, permissions
- **token_bucket_permissions** - bucket permissions of tokens
- **tokens_componentAccess**
- **component_config_details** - component config metadata (last modified, state, creator, etc)
- **table_details** - Storage tables metadata (created, updated, size, cols, 
description coverage, etc)

**TR_details column description:**

| **project\_id** | project id |
| --- | --- |
| **region** | project region |
| **tr\_bucket\_name** | Bucket name |
| **tr\_bucket\_id** | [https://connection.eu-central-1.keboola.com/admin/projects/project\_id/transformations/bucket/](https://connection.eu-central-1.keboola.com/admin/projects/project_id/transformations/bucket/)bucket\_id/transformation/tr\_id |
| **tr\_name** | Transformation name |
| **tr\_id** | [https://connection.eu-central-1.keboola.com/admin/projects/project\_id/transformations/bucket/bucket\_id/transformation/](https://connection.eu-central-1.keboola.com/admin/projects/project_id/transformations/bucket/bucket_id/transformation/)tr\_id |
| **conf\_type** | Defines type of output mapping [Input, output] |
| **incremental** | Is incremental? [False, True] |
| **credits\_est** | Estimated credit consumption. Calculated using table size 1GB = 1CR (In reality this might be more)  In case on subset of columns used on input the number is adjusted according the ratio of the column number used (filtered\_cols / total\_cols ). NOTE: this number is an approximation and does not reflect the filters applied and actual column sizes. In case of output mapping the number is always the actual size of the output table (incorrect in case of incremental output) |
| **table\_size** | Table size in Bytes |
| **table\_exists** | table exists |
| **is\_disabled** |  |
| **table\_id** | E.g. in.c-codelists\_manual\_input.CALENDAR |
| **load\_type** | Load type of the tabel: Input mapping [Copy Table, Clone Table]; Output mapping [full, incremental] |
| **changed\_since** | Changed since filter if specified. E.g. 1 day ago |
| **columns** | Number of filtered columns. If all columns are used, the value is 0 |
| **total\_src\_cols** | Total number of columns in the source table. (0 if table does not exist) |
| **filter** | Filter on input mapping if applies. E.g. department eq [], COM\_DIR eq [&#39;0&#39;] |
| **table\_exists** | Does table exist or not [TRUE,FALSE](../../C:%5Cpages%5Ccreatepage.action%3FspaceKey=BIEXT&amp;title=TRUE,FALSE&amp;linkCreation=true&amp;fromPageId=62768246) |

## **Configuration**

The app works in two modes:

Providing **Master Token:**

Once the master token is provided, the application automatically scans all projects in specified organization that the token has access to. The app scans through each project
in specified organisation, creates a temporary token named `[project_name]_Telemetry_token` with validity for 30 minutes.

To get the master token, you need to be an organisation admin and create one by navigating to [https://connection.eu-central-1.keboola.com/admin/account/access-tokens](https://connection.eu-central-1.keboola.com/admin/account/access-tokens) and clicking the &quot;New token&quot; token button.


You need to fill following configuration fields:

- **Token key ** - your generated master token
- **Organization id -**  Id of the organisation you want to scan ([https://connection.eu-central-1.keboola.com/admin/organizations/](https://connection.eu-central-1.keboola.com/admin/organizations/77)[**[**](https://connection.eu-central-1.keboola.com/admin/organizations/77)**ORGANISATION\_ID]**)
- **Project region - ** region of you organisation / projects

**NOTE: ** Specifying master token will override any configuration you have in the &quot;_List of Storage tokens_&quot; section



Providing **List of Storage tokens** explicitly:

In this mode the app will go through each project specified by its Storage token and region.
You need to fill following configuration fields:

- **Token key** - your generated master token
- **Project region** -  region of you organisation / projects
- **Project ID / Name** - An optional column, for UI reference about the token. 

### Supported metadata

Specify **Project data to download** parameter to pick what data you want to fetch.tokens

**Supported:**

- Transformation details
- Token details
- Orchestration details
- Component config metadata \[extractors, writers, applications\]
- Table details






## Development

This example contains runnable container with simple unittest. For local testing it is useful to include `data` folder in the root
and use docker-compose commands to run the container or execute tests.

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path:
```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone https://bitbucket.org:kds_consulting_team/kbc-python-template.git my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/)


================================================
FILE: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - echo 'Pushing test image to repo. [tag=test]'
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:test
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
FILE: change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.7.2-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential
RUN apt-get install -y git
RUN pip install --upgrade pip

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.0.zip#egg=kbc
mock
freezegun
git+https://github.com/keboola/sapi-python-client.git#egg=kbcstorage


================================================
FILE: component_config/component_long_description.md
================================================
Analyse transformation configurations in your projects. See what tables are used on the input, 
what input mapping setting is being used and what may cause unnecessary increased I/O usage.


================================================
FILE: component_config/component_short_description.md
================================================
Analyse configurations and project metadata in your project.


================================================
FILE: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "App configuration",
  "required": [
    "tokens",
    "master_token",
    "datasets"
  ],
  "properties": {
    "tokens": {
      "type": "array",
      "title": "List of Storage tokens",
      "description": "List of Storage tokens, defining the projects that will be scanned. Transformation Component and Storage access needed.",
      "items": {
        "format": "grid",
        "type": "object",
        "title": "Token",
        "required": [
          "#key",
          "region"
        ],
        "properties": {
          "#key": {
            "type": "string",
            "format": "password",
            "title": "Token Key",
            "propertyOrder": 1000
          },
          "region": {
            "enum": [
              "eu-central-1",
              "us-east-1"
            ],
            "options": {
              "enum_titles": [
                "EU",
                "US"
              ]
            },
            "type": "string",
            "title": "Project region",
            "default": "EU",
            "propertyOrder": 2000
          }
        }
      }
    },
    "master_token": {
      "type": "array",
      "propertyOrder": 100,
      "title": "Master token",
      "description": "Set master token to analyze all project available in the organization. Note that this overrides any tokens explicitly specified in the List of Storage tokens section.",
      "maxItems": 1,
      "items": {
        "format": "grid",
        "type": "object",
        "title": "Token",
        "required": [
          "#token",
          "org_id",
          "region"
        ],
        "properties": {
          "#token": {
            "type": "string",
            "format": "password",
            "title": "Token Key",
            "propertyOrder": 1000
          },
          "org_id": {
            "type": "string",
            "title": "Organization id",
            "propertyOrder": 1200
          },
          "region": {
            "enum": [
              "eu-central-1",
              "us-east-1"
            ],
            "options": {
              "enum_titles": [
                "EU",
                "US"
              ]
            },
            "type": "string",
            "title": "Project region",
            "default": "EU",
            "propertyOrder": 2000
          }
        }
      }
    },
    "datasets": {
      "type": "object",
      "required": [
        "get_transformations",
        "get_tokens",
        "get_orchestrations",
        "get_component_cfg_details",
        "get_tables"
      ],
      "title": "Project data to download",
      "properties": {
        "get_component_cfg_details": {
          "type": "array",
          "uniqueItems": true,
          "items": {
            "type": "string",
            "enum": [
              "extractor", "writer", "application"
            ],"options": {
              "enum_titles": [
                "Extractors",
                "Writers",
                "Applications"
              ]
            }
          },
          "title": "Get Component config metadata",
          "propertyOrder": 5000
        },
        "get_transformations": {
          "type": "boolean",
          "format": "checkbox",
          "title": "Get Transformation details",
          "propertyOrder": 1000
        },
        "get_tokens": {
          "type": "boolean",
          "format": "checkbox",
          "title": "Get Token  details",
          "propertyOrder": 2000
        },
        "get_orchestrations": {
          "type": "boolean",
          "format": "checkbox",
          "title": "Get Orchestration details",
          "propertyOrder": 2500
        },
        "get_tables": {
          "type": "boolean",
          "format": "checkbox",
          "title": "Get Table details",
          "propertyOrder": 3000
        }
      }
    }
  }
}


================================================
FILE: component_config/configuration_description.md
================================================
**NOTE:** When in mode with `Master token`, the application creates a temporary Storage token in the destination project that has access to `all buckets` and components and
is named `[project_name]_Telemetry_token`. The token expires after `30 Minutes`.




================================================
FILE: component_config/stack_parameters.json
================================================
{}


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}



================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
FILE: src/component.py
================================================
'''
Template Component main class.

'''

import csv
import json
import logging
import os
import sys

import requests
from kbc.env_handler import KBCEnvHandler
from kbcstorage.base import Endpoint
from kbcstorage.tables import Tables

from result.kbc_result import TokensWriter, OrchWriter

# configuration variables
KEY_MASTER_TOKEN_KEY = '#token'
KEY_TOKEN_KEY = '#key'

KEY_GET_COMPONENT_CFGS = 'get_component_cfg_details'
KEY_GET_TR = 'get_transformations'
KEY_GET_TOKENS = 'get_tokens'
KEY_GET_ORCHS = 'get_orchestrations'
KEY_GET_TABLE = 'get_tables'
KEY_ORG_ID = 'org_id'
KEY_PROJECTID = 'project_id'
DEAFAULT_TOKEN_EXPIRATION = 1800
KEY_REGION = 'region'
KEY_MASTER_TOKEN = 'master_token'
KEY_TOKENS = 'tokens'
KEY_DATASETS = 'datasets'

OR_GROUP = [[KEY_TOKENS, KEY_MASTER_TOKEN]]

MANDATORY_PARS = [OR_GROUP]
MANDATORY_IMAGE_PARS = []

APP_VERSION = '0.0.1'

TR_CONFIG_FIELDS = [
    "project_id",
    "region",
    "tr_bucket_name",
    "tr_bucket_id",
    "tr_name",
    "tr_id",
    "phase",
    "is_disabled",
    "conf_type",
    "incremental",
    "credits_est",
    "table_size",
    "table_exists",
    "table_id",
    "table_name",
    "load_type",
    "changed_since",
    'columns',
    'total_src_cols',
    'filter',
    'last_run',
    "component_type",
    "has_description",
    "bucket_has_description",
    "no_queries"
]

COMPONENT_CONFIG_FIELDS = [
    "project_id",
    "region",
    "component_type",
    "component_id",
    "component_name",
    "config_id",
    "config_name",
    "description",
    "created_at",
    "created_by",
    "version",
    "lastUpdated_at",
    "lastUpdated_by",
    "changeDescription"
]

COMPONENT_CONFIG__INPUT_FIELDS = ['project_id', 'region', 'component_type', 'config_name', 'config_id', 'name',
                                  'row_id', 'is_disabled', 'conf_type', 'table_id', 'table_name', 'changed_since',
                                  'incremental', 'table_size', 'table_exists', 'columns', 'total_src_cols',
                                  'credits_est', 'has_description']

TABLE_CONFIG_FIELDS = [
    "project_id",
    "region",
    "id",
    "name",
    "transactional",
    "primaryKey",
    "indexedColumns",
    "created",
    "lastImportDate",
    "lastChangeDate",
    "rowsCount",
    "dataSizeBytes",
    "isAlias",
    "isAliasable",
    "attributes",
    "columns",
    "description",
    "documented_columns"
]

EMPTY_TR_TE = '-- This is a sample query.\n-- Adjust accordingly to your input mapping, output mapping\n-- ' \
              'and desired functionality.\n-- CREATE TABLE "out_table" AS SELECT * FROM "in_table";'

REGIONS = {'us-east-1': 'https://connection.keboola.com',
           'eu-central-1': 'https://connection.eu-central-1.keboola.com'}

REGION_SUFFIXES = {'us-east-1': '.keboola.com',
                   'eu-central-1': '.eu-central-1.keboola.com'}


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS)
        # override debug from config
        if self.cfg_params.get('debug'):
            debug = True

        # self.set_default_logger(logging.DEBUG if debug else logging.INFO)
        # fix logging before lib fix
        log_level = logging.DEBUG if debug else logging.INFO

        class InfoFilter(logging.Filter):
            def filter(self, rec):
                return rec.levelno in (logging.DEBUG, logging.INFO)

        hd1 = logging.StreamHandler(sys.stdout)
        hd1.addFilter(InfoFilter())
        hd2 = logging.StreamHandler(sys.stderr)
        hd2.setLevel(logging.WARNING)

        logging.getLogger().setLevel(log_level)
        # remove default handler
        logging.getLogger().removeHandler(logging.getLogger().handlers[0])
        logging.getLogger().addHandler(hd1)
        logging.getLogger().addHandler(hd2)

        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        try:
            self.validate_config()
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as e:
            logging.error(e)
            exit(1)

    def run(self):
        '''
        Main execution code
        '''
        params = self.cfg_params  # noqa

        path = self.tables_out_path

        master_token = params.get(KEY_MASTER_TOKEN, [])
        datasets = params.get(KEY_DATASETS, {})
        if master_token:
            tokens = self._get_project_tokens(master_token[0], params)
        else:
            tokens = params.get(KEY_TOKENS)

        token_wr = None
        orch_writer = None
        if datasets.get(KEY_GET_TOKENS):
            # writer setup
            token_wr = TokensWriter(self.tables_out_path)
        if datasets.get(KEY_GET_ORCHS):
            orch_writer = OrchWriter(self.tables_out_path)

        for idx, tok in enumerate(tokens):
            # add token details
            tok = self.set_token_details(tok)

            if datasets.get(KEY_GET_TR):
                logging.info("Collecting transformation details for project_id %s in region %r",
                             tok[KEY_PROJECTID], tok[KEY_REGION])
                with open(os.path.join(path, 'tr_details.csv'), 'a+', newline='') as out:
                    tr_writer = csv.DictWriter(out, TR_CONFIG_FIELDS)
                    if idx == 0:
                        tr_writer.writeheader()
                    self.write_all_tr_details(tr_writer, tok)

            if datasets.get(KEY_GET_TOKENS):
                logging.info("Collecting token details")
                self.write_all_token_details(token_wr, tok)

            if datasets.get(KEY_GET_ORCHS):
                logging.info("Collecting orchestration details for project_id %s in region %r",
                             tok[KEY_PROJECTID], tok[KEY_REGION])
                test = self.get_orchestration_details(tok)
                orch_writer.write_all(test, object_from_arrays=True,
                                      user_values={'project_pk': str(tok[KEY_PROJECTID]) + '|' + tok['region']})

            if datasets.get(KEY_GET_COMPONENT_CFGS):
                logging.info("Collecting extractor details for project_id %s in region %r",
                             tok[KEY_PROJECTID], tok[KEY_REGION])
                with open(os.path.join(path, 'component_config_details.csv'), 'a+', newline='') as cfg_details, open(
                        os.path.join(path, 'component_config_inputs.csv'), 'a+', newline='') as cfg_inputs:
                    ex_writer = csv.DictWriter(cfg_details, COMPONENT_CONFIG_FIELDS)
                    input_writer = csv.DictWriter(cfg_inputs, COMPONENT_CONFIG__INPUT_FIELDS)
                    if idx == 0:
                        ex_writer.writeheader()
                        input_writer.writeheader()
                    self.write_all_component_cfg_details(ex_writer, input_writer, tok,
                                                         datasets.get(KEY_GET_COMPONENT_CFGS))

            if datasets.get(KEY_GET_TABLE):
                logging.info("Collecting table details for project_id %s in region %r",
                             tok[KEY_PROJECTID], tok[KEY_REGION])
                with open(os.path.join(path, 'table_details.csv'), 'a+', newline='') as out:
                    table_writer = csv.DictWriter(out, TABLE_CONFIG_FIELDS)
                    if idx == 0:
                        table_writer.writeheader()
                    self.write_all_table_details(table_writer, tok)

        if datasets.get(KEY_GET_TOKENS):
            logging.info("Storing manifest files.")
            self.create_manifests(
                token_wr.collect_results(), incremental=False)

        if datasets.get(KEY_GET_ORCHS):
            self.create_manifests(
                orch_writer.collect_results(), incremental=False)

    logging.info("Extraction finished")

    def get_project_tokens(self, token):
        headers = {
            'Content-Type': 'application/json',
            'X-StorageApi-Token': token[KEY_TOKEN_KEY],
        }

        response = requests.get(
            REGIONS[token[KEY_REGION]] + '/v2/storage/tokens',
            headers=headers)
        try:
            response.raise_for_status()
        except requests.HTTPError as e:
            raise e
        else:
            return response.json()

    def _get_std_token_name(self, project_name):
        return project_name + '_Telemetry_token'

    def set_token_details(self, token):
        det = self.get_token_details(token)
        token['project_id'] = det['id']
        return token

    def get_token_details(self, token):
        if REGIONS.get(token['region']):
            url = REGIONS[token['region']]
        else:
            raise ValueError('Unsupported region {}'.format(token['region']))

        token_validation = Endpoint(url, 'storage', token[KEY_TOKEN_KEY])
        token_detail = token_validation._get(
            url + '/v2/storage/tokens/verify')

        return token_detail['owner']

    def get_table_detail(self, tables: Tables, _id):
        detail = {}
        try:
            detail = tables.detail(_id)
            detail['exists'] = True
        except Exception as e:
            detail['exists'] = False
            logging.warning('Failed to get table {} [{}]'.format(_id, e))

        return detail

    def get_input_data(self, tables: Tables, cfg, r, project_id, region, component_type, last_job):
        res = []
        # backward compatibility to allow generic output/input data in the future
        if component_type == 'transformation':
            key_bucket_name = 'tr_bucket_name'
            key_bucket_id = 'tr_bucket_id'
            key_name = 'tr_name'
            key_id = 'tr_id'
        else:
            key_bucket_name = 'bucket_name'
            key_bucket_id = 'bucket_id'
            key_name = 'name'
            key_id = 'id'

        for inp in r['configuration']['input']:
            conf_row = dict()
            conf_row['project_id'] = project_id
            conf_row['region'] = region
            conf_row['component_type'] = component_type
            conf_row[key_bucket_name] = cfg['name']
            conf_row[key_bucket_id] = cfg['id']
            conf_row[key_name] = r['name']
            conf_row[key_id] = r['id']
            conf_row['phase'] = r['configuration'].get('phase', 'N/A')
            conf_row['is_disabled'] = r['configuration'].get('disabled', 'N/A')
            conf_row['conf_type'] = 'input'
            conf_row['table_id'] = inp.get('source', 'N/A')
            conf_row['table_name'] = inp.get('destination', '')
            conf_row['load_type'] = inp.get('loadType') if inp.get(
                'loadType') else 'Copy Table'
            conf_row['changed_since'] = inp.get('changedSince')
            conf_row['incremental'] = True if inp.get(
                'changedSince') else False
            tb_detail = self.get_table_detail(tables, inp.get('source', ''))
            conf_row['table_size'] = 0 if not tb_detail.get(
                'dataSizeBytes') else tb_detail.get('dataSizeBytes', 0)
            conf_row['table_exists'] = tb_detail.get('exists')
            conf_row['columns'] = len(inp.get('columns', []))
            conf_row['total_src_cols'] = len(tb_detail.get('columns', []))
            if conf_row['columns'] > 0 and conf_row['total_src_cols'] > 0:
                conf_row['credits_est'] = (int(conf_row['table_size']) / 1000000000) * (
                        conf_row['columns'] / conf_row['total_src_cols'])
            else:
                conf_row['credits_est'] = (
                        int(conf_row['table_size']) / 1000000000)
            if inp.get('whereColumn'):
                conf_row['filter'] = ' '.join(
                    [inp.get('whereColumn'), inp.get('whereOperator'), str(inp.get('whereValues'))])

            conf_row['last_run'] = last_job.get('startTime', 'N/A')
            conf_row['has_description'] = r['description'] != ''
            conf_row['bucket_has_description'] = cfg['description'] != ''
            res.append(conf_row)
        return res

    def get_storage_input_data(self, tables: Tables, cfg, r, project_id, region, component_type):
        res = []
        key_bucket_name = 'config_name'
        key_bucket_id = 'config_id'
        key_name = 'name'
        key_id = 'row_id'

        for inp in r.get('configuration', {}).get('storage', {}).get('input', {}).get('tables', []):
            conf_row = dict()
            conf_row['project_id'] = project_id
            conf_row['region'] = region
            conf_row['component_type'] = component_type
            conf_row[key_bucket_name] = cfg['name']
            conf_row[key_bucket_id] = cfg['id']
            conf_row[key_name] = r.get('name', 'N/A')
            conf_row[key_id] = r.get('id', 'N/A')
            conf_row['is_disabled'] = r.get('configuration', {}).get('disabled', 'N/A')
            conf_row['conf_type'] = 'input'
            conf_row['table_id'] = inp.get('source', 'N/A')
            conf_row['table_name'] = inp.get('destination', '')
            conf_row['changed_since'] = inp.get('changed_since')
            conf_row['incremental'] = True if inp.get(
                'changed_since') else False
            tb_detail = self.get_table_detail(tables, inp.get('source', ''))
            conf_row['table_size'] = 0 if not tb_detail.get(
                'dataSizeBytes') else tb_detail.get('dataSizeBytes', 0)
            conf_row['table_exists'] = tb_detail.get('exists')
            conf_row['columns'] = len(inp.get('columns', []))
            conf_row['total_src_cols'] = len(tb_detail.get('columns', []))
            if conf_row['columns'] > 0 and conf_row['total_src_cols'] > 0:
                conf_row['credits_est'] = (int(conf_row['table_size']) / 1000000000) * (
                        conf_row['columns'] / conf_row['total_src_cols'])
            else:
                conf_row['credits_est'] = (
                        int(conf_row['table_size']) / 1000000000)
            if inp.get('whereColumn'):
                conf_row['filter'] = ' '.join(
                    [inp.get('whereColumn'), inp.get('whereOperator'), str(inp.get('whereValues'))])
            conf_row['has_description'] = r.get('description', '') != ''
            res.append(conf_row)
        return res

    def get_output_data(self, tables: Tables, cfg, r, project_id, region, component_type, last_job):
        res = []
        # backward compatibility to allow generic output/input data in the future
        if component_type == 'transformation':
            key_bucket_name = 'tr_bucket_name'
            key_bucket_id = 'tr_bucket_id'
            key_name = 'tr_name'
            key_id = 'tr_id'
        else:
            key_bucket_name = 'bucket_name'
            key_bucket_id = 'bucket_id'
            key_name = 'name'
            key_id = 'id'

        for inp in r['configuration']['output']:
            conf_row = dict()
            conf_row['project_id'] = project_id
            conf_row['region'] = region
            conf_row['component_type'] = component_type
            conf_row[key_bucket_name] = cfg['name']
            conf_row[key_bucket_id] = cfg['id']
            conf_row[key_name] = r['name']
            conf_row[key_id] = r['id']
            conf_row['phase'] = r['configuration'].get('phase', 'N/A')
            conf_row['is_disabled'] = r['configuration'].get('disabled', 'N/A')
            conf_row['conf_type'] = 'output'
            conf_row['table_id'] = inp['destination']
            conf_row['table_name'] = inp['source']
            conf_row['load_type'] = 'incremental' if inp.get(
                'incremental') else 'full'
            conf_row['changed_since'] = ''
            conf_row['incremental'] = inp.get(
                'incremental') if inp.get('incremental') else False
            tb_detail = self.get_table_detail(tables, inp.get('destination', ''))
            conf_row['table_size'] = 0 if not tb_detail.get(
                'dataSizeBytes') else tb_detail.get('dataSizeBytes', 0)
            conf_row['table_exists'] = tb_detail.get('exists')
            conf_row['credits_est'] = int(conf_row['table_size']) / 1000000000
            conf_row['last_run'] = last_job.get('startTime', 'N/A')
            conf_row['has_description'] = r['description'] != ''
            conf_row['bucket_has_description'] = cfg['description'] != ''
            res.append(conf_row)
        return res

    def get_transformation_detail_data(self, tables, components, bucket_id, token):

        project_id = token['project_id']
        region = token['region']
        cfg = components._get(
            components.base_url + '/transformation/configs/' + bucket_id)
        rows = cfg['rows']
        component_type = 'transformation'

        res = []
        for r in rows:
            inputs, outputs = None, None

            last_job = self._get_last_config_job(token[KEY_TOKEN_KEY], REGION_SUFFIXES[token['region']],
                                                 'transformation', bucket_id)
            if r.get('configuration', {}).get('input'):
                inputs = self.get_input_data(
                    tables, cfg, r, project_id, region, component_type, last_job)
                res.extend(inputs)
            if r.get('configuration', {}).get('output'):
                outputs = self.get_output_data(
                    tables, cfg, r, project_id, region, component_type, last_job)
                res.extend(outputs)
            if not inputs and not outputs:
                res.append(
                    {'tr_bucket_name': cfg['name'],
                     'tr_bucket_id': cfg['id'],
                     'tr_name': r['name'],
                     'tr_id': r['id'],
                     'conf_type': 'no_in_out',
                     'table_id': '',
                     'table_name': '',
                     'load_type': '',
                     'changed_since': '',
                     'incremental': '',
                     'filter': '',
                     'last_run': last_job.get('startTime', 'N/A'),
                     'is_disabled': r['configuration'].get('disabled', 'N/A'),
                     'table_size': '',
                     'table_exists': '',
                     'region': region,
                     'project_id': project_id,
                     'component_type': component_type,
                     'has_description': r['description'] != '',
                     'bucket_has_description': cfg['description'] != '',
                     'no_queries': ''.join(r.get('configuration', {}).get('queries', [])).strip() != ''
                     })

        return res

    def write_all_tr_details(self, writer, token):
        if REGIONS.get(token['region']):
            url = REGIONS[token['region']]
        else:
            raise ValueError('Unsupported region {}'.format(token['region']))

        sourceEndpoint = Endpoint(url, 'components', token[KEY_TOKEN_KEY])
        tb = Tables(url, token[KEY_TOKEN_KEY])
        all_cfgs = sourceEndpoint._get(
            url + '/v2/storage/components/transformation/configs')

        for cfg in all_cfgs:
            writer.writerows(
                self.get_transformation_detail_data(tb, sourceEndpoint, cfg['id'], token))

    def get_component_config_detail_data(self, components, project_id, region, tables):
        rows = components["configurations"]
        cfg = {
            "project_id": project_id,
            "region": region,
            "component_type": components["type"],
            "component_id": components["id"],
            "component_name": components["name"]
        }

        res = []
        rows_input = []
        for r in rows:
            row_cfg = cfg
            row_cfg["config_id"] = r["id"]
            row_cfg["config_name"] = r["name"]
            row_cfg["description"] = r["description"]
            row_cfg["created_at"] = r["created"]
            row_cfg["created_by"] = r["creatorToken"]["description"]
            row_cfg["version"] = r["version"]
            row_cfg["lastUpdated_at"] = r["currentVersion"]["created"]
            row_cfg["lastUpdated_by"] = r["currentVersion"]["creatorToken"]["description"]
            row_cfg["changeDescription"] = r["changeDescription"]
            res.append(row_cfg)
            rows_input.extend(self.get_storage_input_data(tables, r, r, project_id, region, components["type"]))
            for cfgrow in r.get('rows', []):
                rows_input.extend(
                    self.get_storage_input_data(tables, r, cfgrow, project_id, region, components["type"]))
        return res, rows_input

    def write_all_component_cfg_details(self, detail_writer, input_writer, token, component_types):
        if REGIONS.get(token['region']):
            url = REGIONS[token['region']]
        else:
            raise ValueError('Unsupported region {}'.format(token['region']))
        sourceEndpoint = Endpoint(url, 'components', token[KEY_TOKEN_KEY])
        tb = Tables(url, token[KEY_TOKEN_KEY])
        for comp_type in component_types:
            all_cfgs = sourceEndpoint._get(
                url + '/v2/storage/components', params={"componentType": comp_type, "include": "rows,configuration"})

            for cfg in all_cfgs:
                cfg_details, rowinputs = self.get_component_config_detail_data(cfg, token['project_id'],
                                                                               token['region'], tb)
                detail_writer.writerows(cfg_details)
                input_writer.writerows(rowinputs)

    def get_all_table_details_data(self, table_details, region, project_id):
        res = []
        data = {
            'project_id': project_id,
            'region': region,
            'id': table_details['id'],
            'name': table_details['name'],
            'transactional': table_details['transactional'],
            'primaryKey': ', '.join(table_details['primaryKey']),
            'indexedColumns': ', '.join(table_details['indexedColumns']),
            'created': table_details['created'],
            'lastImportDate': table_details['lastImportDate'],
            'lastChangeDate': table_details['lastChangeDate'],
            'rowsCount': table_details['rowsCount'],
            'dataSizeBytes': table_details['dataSizeBytes'],
            'isAlias': table_details['isAlias'],
            'isAliasable': table_details['isAliasable'],
            # isn't this field deprecated?
            'attributes': str(table_details['attributes']),
            'columns': ', '.join(table_details['columns']),
            'documented_columns': ', '.join(self.get_col_meta_documented_cols(table_details['columnMetadata'])),
            'description': self.get_meta_description(table_details['metadata'])
        }
        res.append(data)

        return res

    def get_col_meta_documented_cols(self, colmeta):
        doc_cols = []
        for c in colmeta:
            if self.get_meta_description(colmeta[c]):
                doc_cols.append(c)
        return doc_cols

    def get_meta_description(self, metadata):
        for m in metadata:
            if m.get('key', '') == 'KBC.description':
                return m['value']

    def write_all_table_details(self, writer, token):
        if REGIONS.get(token['region']):
            url = REGIONS[token['region']]
        else:
            raise ValueError('Unsupported region {}'.format(token['region']))

        tb = Tables(url, token[KEY_TOKEN_KEY])
        args = ["attributes", "columns", "metadata", "columnMetadata"]
        all_tb = tb.list(include=args)

        for table in all_tb:
            writer.writerows(self.get_all_table_details_data(
                table, token['project_id'], token['region']))

    def generate_token(self, decription, token, proj_id, region, expires_in=DEAFAULT_TOKEN_EXPIRATION):
        headers = {
            'Content-Type': 'application/json',
            'X-KBC-ManageApiToken': token,
        }

        data = {
            "description": decription,
            "canManageBuckets": True,
            "canReadAllFileUploads": False,
            "canPurgeTrash": False,
            "canManageTokens": True,
            "componentAccess": ["transformation"],
            "bucketPermissions": {"*": "read"},
            "expiresIn": expires_in
        }

        response = requests.post(REGIONS[region] + '/manage/projects/' + str(proj_id) + '/tokens',
                                 headers=headers,
                                 data=json.dumps(data))
        try:
            response.raise_for_status()
        except requests.HTTPError as e:
            raise e
        else:
            return response.json()

    def get_organization(self, master_token):
        headers = {
            'Content-Type': 'application/json',
            'X-KBC-ManageApiToken': master_token[KEY_MASTER_TOKEN_KEY],
        }

        response = requests.get(
            REGIONS[master_token[KEY_REGION]] +
            '/manage/organizations/' + str(master_token[KEY_ORG_ID]),
            headers=headers)
        try:
            response.raise_for_status()
        except requests.HTTPError as e:
            raise e
        else:
            return response.json()

    def get_orchestrations(self, token_key, url_suffix):
        syrup_cl = Endpoint('https://syrup' + url_suffix,
                            'orchestrator', token_key)

        url = syrup_cl.root_url + '/orchestrator/orchestrations'
        res = syrup_cl._get(url)
        return res

    def get_orch_detail(self, config_id, token_key, url_suffix):
        cl = Endpoint('https://connection' + url_suffix,
                      'components', token_key)
        url = '{}/{}/configs/{}'.format(cl.base_url, 'orchestrator', config_id)
        return cl._get(url)

    def get_orchestration_details(self, token):
        url_suffix = REGION_SUFFIXES[token['region']]
        orchs = self.get_orchestrations(token[KEY_TOKEN_KEY], url_suffix)

        details = []
        for o in orchs:
            orch_detail = self.get_orch_detail(
                o['id'], token[KEY_TOKEN_KEY], url_suffix)
            last_jobs = self._get_last_orch_job(
                token[KEY_TOKEN_KEY], url_suffix, o['id'])
            orch_detail['last_run'] = last_jobs.get('startTime')
            details.append(orch_detail)
        return details

    def _get_project_tokens(self, master_token, params):
        # get list of projects
        org = self.get_organization(master_token)
        projects = org['projects']
        for proj in projects:
            # try create token
            tok = self.generate_token(self._get_std_token_name(proj['name']), master_token[KEY_MASTER_TOKEN_KEY],
                                      proj['id'],
                                      master_token[KEY_REGION])
            # add token details
            tok[KEY_REGION] = master_token[KEY_REGION]
            tok[KEY_PROJECTID] = proj['id']
            tok[KEY_TOKEN_KEY] = tok['token']
            yield tok

    def _get_n_write_all_in_org(self, writer, master_token, params):
        # get list of projects
        org = self.get_organization(master_token)
        projects = org['projects']
        for proj in projects:
            # try create token
            tok = self.generate_token(self._get_std_token_name(proj['name']), master_token['token'], proj['id'],
                                      master_token[KEY_REGION])
            # add token details
            tok[KEY_REGION] = master_token[KEY_REGION]
            tok[KEY_PROJECTID] = proj['id']
            tok[KEY_TOKEN_KEY] = tok['token']

            logging.info(
                "Collecting TR details for project_id %s in region %r", proj['id'], tok[KEY_REGION])
            self.write_all_tr_details(writer, tok)

            if params.get(KEY_TOKENS):
                logging.info(
                    "Collecting token details for project_id %s in region %r", proj['id'], tok[KEY_REGION])
                self.write_all_token_details(tok)

    def write_all_token_details(self, writer, tok):
        res = self.get_project_tokens(tok)
        for t in res:
            last_event = self._get_token_last_event(tok, t['id'])
            t["last_event_date"] = last_event.get('created')
            t["last_event"] = last_event.get('event')

        writer.write_all(res, object_from_arrays=True,
                         user_values={'project_pk': str(tok[KEY_PROJECTID]) + '|' + tok['region']})

    def _get_token_last_event(self, token, token_id):
        headers = {
            'Content-Type': 'application/json',
            'X-StorageApi-Token': token[KEY_TOKEN_KEY],
        }
        params = dict()
        params['limit'] = 1
        response = requests.get(
            REGIONS[token[KEY_REGION]] +
            '/v2/storage/tokens/' + token_id + '/events',
            headers=headers, params=params)
        try:
            response.raise_for_status()
        except requests.HTTPError as e:
            logging.warning(F'Failed to get token events {token_id}. {e}')
            raise e
        else:
            json_resp = response.json()
            if not json_resp:
                return dict()
            else:
                return json_resp[0]

    def _get_last_orch_job(self, token_key, url_suffix, orch_id):
        headers = {
            'Content-Type': 'application/json',
            'X-StorageApi-Token': token_key,
        }
        params = dict()
        params['limit'] = 1
        response = requests.get(
            'https://syrup' + url_suffix +
            '/orchestrator/orchestrations/' + str(orch_id) + '/jobs',
            headers=headers, params=params)
        try:
            response.raise_for_status()
        except requests.HTTPError as e:
            raise e
        else:
            json_resp = response.json()
            if not json_resp:
                return dict()
            else:
                return json_resp[0]

    def _get_last_config_job(self, token_key, url_suffix, component_type, config_id):
        headers = {
            'Content-Type': 'application/json',
            'X-StorageApi-Token': token_key,
        }
        params = dict()
        params['limit'] = 1
        params['q'] = F'(component:{component_type} OR params.component:{component_type}) AND params.config:{config_id}'
        response = requests.get(
            'https://syrup' + url_suffix + '/queue/jobs',
            headers=headers, params=params)
        try:
            response.raise_for_status()
        except requests.HTTPError as e:
            raise e
        else:
            json_resp = response.json()
            if not json_resp:
                return dict()
            else:
                return json_resp[0]

    def _get_last_job(self, token_key, url_suffix, config_id):
        headers = {
            'Content-Type': 'application/json',
            'X-StorageApi-Token': token_key,
        }
        params = dict()
        params['limit'] = 1
        params['q'] = '+params.config:' + config_id
        response = requests.get(
            'https://syrup' + url_suffix + '/queue/jobs',
            headers=headers, params=params)
        try:
            response.raise_for_status()
        except requests.HTTPError as e:
            raise e
        else:
            json_resp = response.json()
            if not json_resp:
                return dict()
            else:
                return json_resp[0]


"""
        Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug = sys.argv[1]
    else:
        debug = False
    comp = Component(debug)
    comp.run()



================================================
FILE: src/result/__init__.py
================================================
[Empty file]


================================================
FILE: src/result/kbc_result.py
================================================
from kbc.result import KBCTableDef
from kbc.result import ResultWriter

TOKEN_PK = ['id', 'project_pk']

TOKEN_COLS = ["id", "description", "created", "refreshed", "uri", "isMasterToken", "canManageBuckets",
              "canManageTokens", "canReadAllFileUploads", "canPurgeTrash", "isExpired", "isDisabled",
              "dailyCapacity", "kbc_user_id", "project_pk", "last_event", "last_event_date"]

ORCH_PK = ['id', 'project_pk']
ORCH_COLS = ["id", "name", "description", "created", "is_deleted", "active", "project_pk", "last_run"]


class TokensWriter(ResultWriter):
    """
    Overridden constructor method of ResultWriter. It creates extra ResultWriter instance that handles processing of
    the nested object 'deals_stage_history'. That writer is then called from within the write method.
    """

    def __init__(self, out_path, buffer=8192):
        # specify result table
        # table def
        token_table = KBCTableDef(name='tokens', pk=TOKEN_PK, columns=TOKEN_COLS)
        # writer setup
        ResultWriter.__init__(self, result_dir_path=out_path, table_def=token_table, flatten_objects=False,
                              fix_headers=True, exclude_fields=['token'])

        permission_tb = KBCTableDef('token_bucket_permissions', [], ['bucket', 'project_pk'])
        self.permission_writer = ResultWriter(out_path, permission_tb,
                                              flatten_objects=False, buffer_size=buffer)

    """
    Overridden write method that is modified to process the nested object separately using newly created nested writer.
    """

    def write(self, data, file_name=None, user_values=None, object_from_arrays=True, write_header=True):
        # write ext users
        permissions = data.pop('bucketPermissions')
        if permissions:
            bucket_permissions = self.dict_to_array(permissions, user_values['project_pk'])
            self.permission_writer.write_all(bucket_permissions)
            self.results = {**self.results, **self.permission_writer.results}
        # get user id
        if data.get('creatorToken'):
            uid = data['creatorToken']['id']
            data.pop('creatorToken')
        elif data.get('admin'):
            uid = data['admin']['id']
            data.pop('admin')
        else:
            uid = 'N/A'
        user_values['kbc_user_id'] = uid
        super().write(data, user_values=user_values, object_from_arrays=object_from_arrays)

    def dict_to_array(self, dict, id):
        res_list = list()
        for k in dict:
            res_list.append({"bucket": k, "permission": dict[k], "project_pk": id})
        return res_list


class OrchWriter(ResultWriter):
    """
    Overridden constructor method of ResultWriter. It creates extra ResultWriter instance that handles processing of
    the nested object 'deals_stage_history'. That writer is then called from within the write method.
    """

    def __init__(self, out_path, buffer=8192):
        # specify result table
        # table def
        orch_table = KBCTableDef(name='orchestrations', pk=ORCH_PK, columns=ORCH_COLS)
        # writer setup
        ResultWriter.__init__(self, result_dir_path=out_path, table_def=orch_table, flatten_objects=True,
                              fix_headers=True,
                              exclude_fields=['token', 'state', 'currentVersion', 'rows', 'rowsSortOrder',
                                              'notifications'])

        tasks_tb = KBCTableDef('orchestration_tasks', [], ['orchestration_id', 'project_pk', 'id'])
        self.tasks_writer = ResultWriter(out_path, tasks_tb,
                                         flatten_objects=False, buffer_size=buffer)

    """
       Overridden write method that is modified to process the nested object separately using newly created
       nested writer.
       """

    def write(self, data, file_name=None, user_values=None, object_from_arrays=True, write_header=True):
        # write ext users
        tasks = data.get('configuration').get('tasks')
        if tasks:
            for t in tasks:
                t['config_id'] = t.pop('actionParameters', {}).get('config')
                t['orchestration_id'] = data['id']
                self.tasks_writer.write(t, user_values=user_values)
            self.results = {**self.results, **self.tasks_writer.results}

        data.pop('configuration')
        super().write(data, user_values=user_values, object_from_arrays=object_from_arrays)



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
FILE: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


