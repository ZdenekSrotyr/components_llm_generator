Directory structure:
└── kds_consulting_team-kds-team.processor-kb-account-statement-parser/
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    ├── Dockerfile
    ├── flake8.cfg
    ├── src/
    │   ├── component.py
    │   └── kb_parser/
    │       ├── templates/
    │       │   ├── total_balance_header.tabula-template.json
    │       │   ├── report_metadata_header.tabula-template.json
    │       │   ├── account_type_header.tabula-template.json
    │       │   ├── account_entity_header.tabula-template.json
    │       │   ├── columns.tabula-template.json
    │       │   ├── last_page_odd.tabula-template.json
    │       │   └── last_page.tabula-template.json
    │       ├── __init__.py
    │       └── parser.py
    ├── LICENSE.md
    ├── docs/
    │   └── imgs/
    ├── requirements.txt
    ├── bitbucket-pipelines.yml
    ├── component_config/
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── configRowSchema.json
    │   ├── component_long_description.md
    │   ├── sample-config/
    │   │   ├── in/
    │   │   │   ├── tables/
    │   │   │   │   ├── test.csv
    │   │   │   │   └── test.csv.manifest
    │   │   │   ├── state.json
    │   │   │   └── files/
    │   │   │       └── order1.xml
    │   │   ├── out/
    │   │   │   ├── tables/
    │   │   │   │   └── test.csv
    │   │   │   └── files/
    │   │   │       └── order1.xml
    │   │   └── config.json
    │   ├── loggerConfiguration.json
    │   └── logger
    ├── deploy.sh
    ├── docker-compose.yml
    ├── scripts/
    │   ├── build_n_test.sh
    │   ├── update_dev_portal_properties.sh
    │   ├── build_n_run.ps1
    │   └── run_kbc_tests.ps1
    └── README.md

================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: /Dockerfile
================================================
FROM python:3.8.6-buster
ENV PYTHONIOENCODING utf-8

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY requirements.txt /code/requirements.txt
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

# Install Java dependencies needed for Tabula
RUN apt-get update && \
    apt-get install -y openjdk-11-jre-headless && \
    apt-get clean;

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/

# set switch that enables correct JVM memory allocation in containers
ENV JAVA_OPTS="-XX:+UseContainerSupport -Xmx512m"


CMD ["python", "-u", "/code/src/component.py"]


================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /src/component.py
================================================
'''
Template Component main class.

'''
import csv
import hashlib
import logging
import os
import re
import shutil
from dataclasses import asdict
from itertools import groupby
from pathlib import Path
from typing import List

from PyPDF2 import PdfFileMerger, PdfFileReader
from keboola.component.base import ComponentBase
from keboola.component.dao import FileDefinition, TableDefinition
from keboola.component.exceptions import UserException

from kb_parser import parser as statement_parser
from kb_parser.parser import StatementRow, StatementMetadata, ParserError


class Component(ComponentBase):
    """
        Extends base class for general Python components. Initializes the CommonInterface
        and performs configuration validation.

        For easier debugging the data folder is picked up by default from `../data` path,
        relative to working directory.

        If `debug` parameter is present in the `config.json`, the default logger is set to verbose DEBUG mode.
    """

    def __init__(self):
        super().__init__()

        # init table definitions
        self.statements_table: TableDefinition
        self.statement_metadata_table: TableDefinition

    def _init_tables(self):
        statement_columns = ['pk', 'statement_metadata_pk', 'row_nr']
        statement_columns.extend(list(StatementRow.__annotations__.keys()))

        statement_metadata_columns = ['pk']
        statement_metadata_columns.extend(list(StatementMetadata.__annotations__.keys()))

        self.statements_table = self.create_out_table_definition('statements.csv', incremental=True,
                                                                 schema=statement_columns,
                                                                 is_sliced=True,
                                                                 primary_key=['pk'])
        self.statement_metadata_table = self.create_out_table_definition('statements_metadata.csv', incremental=True,
                                                                         schema=statement_metadata_columns,
                                                                         is_sliced=True,
                                                                         primary_key=['pk'])

    def run(self):
        '''
        Main execution code
        '''

        self._init_tables()

        input_files = self.get_input_files_definitions(only_latest_files=False)

        pdf_files = [f for f in input_files if f.full_path.endswith('.pdf')]
        logging.info(f"{len(pdf_files)} PDF files found on the input.")

        # merge files that are split
        pdf_files = self._merge_split_files(pdf_files)

        try:
            for file in pdf_files:
                logging.info(f"Parsing file {file.name}")
                self._parse_to_csv(file)

        except ParserError as e:
            raise UserException(e) from e
        except Exception:
            raise

        # write manifest
        if pdf_files:
            self.write_manifest(self.statements_table)
            self.write_manifest(self.statement_metadata_table)

        logging.info("Parsing finished successfully!")

    def _merge_pdfs(self, paths: List[str], result_path: str):
        """
        Merges pdfs into one.
        Args:
            paths:
            result_path:

        Returns:

        """

        # Call the PdfFileMerger
        merged_object = PdfFileMerger()

        for file in paths:
            merged_object.append(PdfFileReader(file, 'rb'))

        # Write all the files into a file which is named as shown below
        merged_object.write(result_path)

    def _merge_split_files(self, pdf_files) -> List[FileDefinition]:
        """
        Merges statement files and returns File objects.
        Args:
            pdf_files:

        Returns:

        """
        r = re.compile(r'^(.*)\dz\d.pdf')
        file_paths = [f.full_path for f in pdf_files]
        files_to_merge = list(filter(r.match, file_paths))

        # remove these files from the pdf_file list
        normal_files = [f for f in pdf_files if f.full_path not in files_to_merge]

        split_files = self._group_split_files(files_to_merge)
        if len(split_files) > 0:
            logging.info(f'{len(split_files)} files split files received, merging.')

        result_files = []
        for key in split_files:
            result_path = f"{key}.pdf"
            logging.info(f"Merging {len(split_files[key])} parts into {result_path}.")
            file_definition = self._create_file_definition(name=Path(result_path).name, storage_stage='in')
            self._merge_pdfs(split_files[key], file_definition.full_path)

            # remove source files
            self._delete_files(split_files[key])
            result_files.append(file_definition)

        result_files.extend(normal_files)

        return result_files

    def _delete_files(self, paths: List[str]):
        for p in paths:
            os.unlink(p)

    def _group_split_files(self, split_files: List[str]):
        """
        Returns split files grouped by name
        Args:
            split_files:

        Returns:

        """

        def group_key(name: str):
            r = re.compile(r'^.*?(\d+_\d+_ucet_\d+)_\d+z\d+\.pdf')
            group = r.match(name).group(1)
            return group

        def sort_key(name: str):
            r = re.compile(r'^.*_(\d+z\d+)\.pdf')
            group = r.match(name).group(1)
            return group

        files_to_merge = {}
        for k, g in groupby(sorted(split_files, key=group_key), key=group_key):
            files_to_merge[k] = sorted(list(g), key=sort_key)
        return files_to_merge

    def _parse_to_csv(self, pdf_file: FileDefinition):
        """
        Parse PDF statements and store as Sliced csv files.
        Args:
            pdf_file: FileDefinition

        Returns:

        """
        data_path = Path(f"{self.statements_table.full_path}/{pdf_file.name}.csv")
        data_path.parent.mkdir(parents=True, exist_ok=True)
        metadata_path = Path(f"{self.statement_metadata_table.full_path}/{pdf_file.name}.csv")
        metadata_path.parent.mkdir(parents=True, exist_ok=True)

        with open(data_path, 'w+', encoding='utf-8') as statement_out, \
                open(metadata_path, 'w+', encoding='utf-8') as metadata_out:

            data_writer = csv.DictWriter(statement_out, fieldnames=self.statements_table.column_names)
            metadata_writer = csv.DictWriter(metadata_out, fieldnames=self.statement_metadata_table.column_names)

            metadata_pkey = None
            idx = 0
            for data, metadata in statement_parser.parse_full_statement(pdf_file.full_path):
                if not data:
                    continue
                dict_row = asdict(data)

                if not metadata_pkey:
                    metadata_pkey = self._build_statement_metadata_pk(metadata)

                dict_row['pk'] = self._build_statement_row_pk(idx, data, metadata_pkey)
                dict_row['statement_metadata_pk'] = metadata_pkey
                dict_row['row_nr'] = idx

                data_writer.writerow(dict_row)

                idx += 1

            # write metadata
            if metadata:
                metadata_row = asdict(metadata)
                metadata_row['pk'] = metadata_pkey
                metadata_writer.writerow(metadata_row)

            # move in_tables untouched
            self._move_in_tables()

    @staticmethod
    def _build_statement_row_pk(idx: int, data: StatementRow, metadata_pkey: str):
        composed_key = [idx, data.transaction_date, metadata_pkey]
        key_str = '|'.join([str(k) for k in composed_key])
        return hashlib.md5(key_str.encode()).hexdigest()

    @staticmethod
    def _build_statement_metadata_pk(metadata: StatementMetadata):
        composed_key = [metadata.statement_date, metadata.account_number, metadata.statement_number,
                        metadata.statement_type, metadata.currency]
        key_str = '|'.join([str(k) for k in composed_key])
        return hashlib.md5(key_str.encode()).hexdigest()

    def _move_in_tables(self):
        for t in self.get_input_tables_definitions():
            source_path = t.full_path
            t.full_path = t.full_path.replace('in', 'out')
            t.stage = 'out'
            shutil.move(source_path, t.full_path)
            self.write_manifest(t)


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)


================================================
File: /src/kb_parser/templates/total_balance_header.tabula-template.json
================================================
[{"page":1,"extraction_method":"guess","x1":39.43700675964355,"x2":271.59448051452637,"y1":217.64763164520264,"y2":248.89959926605223,"width":232.1574737548828,"height":31.25196762084961}]

================================================
File: /src/kb_parser/templates/report_metadata_header.tabula-template.json
================================================
[{"page":1,"extraction_method":"guess","x1":432.3188854217529,"x2":568.4881729125976,"y1":12.277558708190918,"y2":64.36417140960693,"width":136.1692874908447,"height":52.086612701416016}]

================================================
File: /src/kb_parser/templates/account_type_header.tabula-template.json
================================================
[{
        "page": 1,
        "extraction_method": "guess",
        "x1": 183.04723892211913,
        "x2": 409.9960514068603,
        "y1": 8.557086372375489,
        "y2": 91.89566669464111,
        "width": 226.9488124847412,
        "height": 83.33858032226563
    }
]


================================================
File: /src/kb_parser/templates/account_entity_header.tabula-template.json
================================================
[{"page":1,"extraction_method":"guess","x1":281.2677085876465,"x2":572.9527397155762,"y1":92.6397611618042,"y2":247.41141033172607,"width":291.6850311279297,"height":154.77164916992186}]

================================================
File: /src/kb_parser/templates/columns.tabula-template.json
================================================
[{"page":682,"extraction_method":"guess","x1":42.165322861960156,"x2":222.98027838259003,"y1":144.72249647887364,"y2":193.08863684447422,"width":180.81495552062987,"height":48.366140365600586},{"page":682,"extraction_method":"guess","x1":224.46846731691622,"x2":415.70074537782926,"y1":145.46659094603675,"y2":190.1122589758219,"width":191.23227806091307,"height":44.645668029785156},{"page":682,"extraction_method":"guess","x1":417.1889343121555,"x2":464.06688574342985,"y1":143.23430754454748,"y2":182.67131430419104,"width":46.87795143127441,"height":39.43700675964355},{"page":682,"extraction_method":"guess","x1":465.55507467775607,"x2":566.7519222119357,"y1":144.72249647887364,"y2":177.46265303404942,"width":101.19684753417968,"height":32.74015655517578}]

================================================
File: /src/kb_parser/templates/last_page_odd.tabula-template.json
================================================
[{"page":683,"extraction_method":"guess","x1":42.165322861960156,"x2":571.2164890149143,"y1":249.6591975936113,"y2":741.505640388411,"width":529.0511661529541,"height":491.84644279479977}]

================================================
File: /src/kb_parser/templates/last_page.tabula-template.json
================================================
[{
        "extraction_method": "guess",
        "x1": 41.999668104553194,
        "x2": 566.5862674545288,
        "y1": 101.00338297271729,
        "y2": 711.1608460464478,
        "width": 524.5865993499756,
        "height": 610.1574630737305
    }
]


================================================
File: /src/kb_parser/parser.py
================================================
import json
import logging
import math
import re
import tempfile
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import List, Iterator, Tuple, Union, Callable

import PyPDF2
import tabula

MAX_CHUNK_SIZE = 1000

PANDAS_OPTIONS = {'dtype': str}

# Limit the memory for docker execution / requires JAVA 11
JAVA_OPTIONS = '-Xmx480m -Xms480m'

# DATA_COLUMN_BOUNDARIES = [42.16, 223.0, 416.0, 465.0, 566.75]
DATA_COLUMN_BOUNDARIES = [223.0, 416.0, 465.0, 566.75]


class ParserError(Exception):
    pass


def _get_templates_directory():
    return Path(__file__).parent.joinpath('templates')


class HeaderTemplatePaths(Enum):
    account_type = Path(_get_templates_directory(), 'account_type_header.tabula-template.json').as_posix()
    report_metadata = Path(_get_templates_directory(), 'report_metadata_header.tabula-template.json').as_posix()
    account_entity = Path(_get_templates_directory(), 'account_entity_header.tabula-template.json').as_posix()
    total_balance = Path(_get_templates_directory(), 'total_balance_header.tabula-template.json').as_posix()


class DataTemplatePaths(Enum):
    last_page_even = Path(_get_templates_directory(), 'last_page.tabula-template.json').as_posix()
    last_page_odd = Path(_get_templates_directory(), 'last_page_odd.tabula-template.json').as_posix()


@dataclass
class StatementMetadata:
    account_number: str = ''
    statement_type: str = ''
    iban: str = ''
    account_type: str = ''
    currency: str = ''
    statement_date: str = ''
    statement_number: str = ''
    account_entity: str = ''
    start_balance: float = 0
    end_balance: float = 0


@dataclass
class StatementRow:
    accounting_date: str = ''
    transaction_date: str = ''
    transaction_description: str = ''
    transaction_identification: str = ''
    account_name__card_type: str = ''
    account_number__merchant: str = ''
    vs: str = ''
    ks: str = ''
    ss: str = ''
    transaction_type: str = ''
    amount: float = 0


def _get_table_value_strict(data_row: dict, column_name_key: str, value_key: str, expected_column_name: str,
                            errors_buffer: List[str]):
    """
    Helper function to retrieve and validate Table column value with strict name.
    Args:
        data_row:
        column_name_key:
        value_key:
        expected_column_name:
        errors_buffer:

    Returns:

    """
    value = None
    if data_row.get(column_name_key) == expected_column_name:
        value = data_row[value_key]
    else:
        errors_buffer.append(f"Missing '{expected_column_name}' section.")
    return value


def _validate_row_structure(row: dict, column_count: int, section_name: str) -> List[str]:
    """
    Validates row_data structure (column count) and returns list of column headers (keys)
    Args:
        row:
        column_count:
        section_name:

    Returns: List of column keys

    """
    if len(row) != column_count:
        raise ParserError(f"{section_name} has different amount of columns {len(row)}"
                          f" than expected {column_count}!")

    return list(row.keys())


def _convert_to_numeric(number_str: Union[float, str]):
    formatted = str(number_str).replace(',', '.')
    pattern = re.compile(r'\s+')
    formatted = re.sub(pattern, '', formatted)
    return float(formatted)


def _convert_na_to_empty(string: str):
    return str(string).replace('nan', '')


def _load_single_page_section_from_template(file_path: str, section_name: str,
                                            template_path: Union[HeaderTemplatePaths, DataTemplatePaths],
                                            page_nr='1', stream=False, **kwargs):
    """
    Load data from sepcified templated located on single page
    Args:
        file_path: path to pdf file
        section_name:
        template_path: path to template
        page_nr: number of the page
        stream:

    Returns:

    """
    _id, path = tempfile.mkstemp()
    with open(template_path.value, 'r') as template_in, open(path, 'w') as tmp_out:
        template_json = json.load(template_in)
        template_json[0]['page'] = page_nr
        json.dump(template_json, tmp_out)

    try:
        df = tabula.read_pdf_with_template(file_path, path,
                                           pandas_options=PANDAS_OPTIONS,
                                           java_options=JAVA_OPTIONS,
                                           stream=stream,
                                           pages=page_nr, **kwargs)[0]
        return df.to_dict('records')
    except KeyError:
        raise ParserError(f'Statement {Path(file_path).name} does not contain the {section_name} '
                          f'on expected position!')


def _parse_account_type_metadata(file_path: str, statement_metadata: StatementMetadata):
    """
    Parses account type section and updates StatementMetadata values in-place
    Args:
        file_path:
        statement_metadata (StatementMetadata): Metadata container to be updated

    Returns:

    """
    dict_rows = _load_single_page_section_from_template(file_path, 'Account type section',
                                                        HeaderTemplatePaths.account_type)

    # validate initial
    if len(dict_rows) < 4:
        raise ParserError("Header Account type section is missing some rows!")

    if len(dict_rows[0]) != 2:
        raise ParserError(f"Header Account type section has different amount of columns {len(dict_rows[0])}"
                          f" than expected!")
    parse_errors = []

    column_key = list(dict_rows[0].keys())[0]
    vypis_type_key = list(dict_rows[0].keys())[1]
    statement_metadata.statement_type = vypis_type_key

    # account number
    statement_metadata.account_number = _get_table_value_strict(dict_rows[0], column_key, vypis_type_key,
                                                                'k účtu:', errors_buffer=parse_errors)
    # IBAN
    statement_metadata.iban = _get_table_value_strict(dict_rows[1], column_key, vypis_type_key,
                                                      'IBAN:', errors_buffer=parse_errors)
    # start type
    statement_metadata.account_type = _get_table_value_strict(dict_rows[2], column_key, vypis_type_key,
                                                              'typ:', errors_buffer=parse_errors)
    # currency:
    statement_metadata.currency = _get_table_value_strict(dict_rows[3], column_key, vypis_type_key,
                                                          'měna:', errors_buffer=parse_errors)

    if parse_errors:
        raise ParserError(f"Header Account type section parsing failed with errors: {'; '.join(parse_errors)}")

    return statement_metadata


def _parse_report_metadata(file_path: str, statement_metadata: StatementMetadata):
    """
    Parses report metadata section and updates StatementMetadata values in-place
    Args:
        file_path:
        statement_metadata (StatementMetadata): Metadata container to be updated

    Returns:

    """
    section_name = 'Report Metadata section'
    dict_rows = _load_single_page_section_from_template(file_path, section_name, HeaderTemplatePaths.report_metadata)

    # validate initial
    if len(dict_rows) < 3:
        raise ParserError(f"{section_name} section is missing some rows!")

    if len(dict_rows[0]) != 2:
        raise ParserError(f"{section_name} has different amount of columns {len(dict_rows[0])}"
                          f" than expected!")
    parse_errors = []

    first_column_key = list(dict_rows[0].keys())[0]
    second_column_key = list(dict_rows[0].keys())[1]

    # Datum is first row_data
    statement_metadata.statement_date = second_column_key
    # cislo
    statement_metadata.statement_number = _get_table_value_strict(dict_rows[0], first_column_key, second_column_key,
                                                                  'Číslo výpisu:', errors_buffer=parse_errors)

    if parse_errors:
        raise ParserError(f"{section_name} parsing failed with errors: {'; '.join(parse_errors)}")

    return statement_metadata


def _parse_balance_section_metadata(file_path: str, statement_metadata: StatementMetadata):
    """
    Parses balance section and updates StatementMetadata values in-place
    Args:
        file_path:
        statement_metadata (StatementMetadata): Metadata container to be updated

    Returns:

    """
    section_name = 'Report Balance section'
    dict_rows = _load_single_page_section_from_template(file_path, section_name, HeaderTemplatePaths.total_balance)

    # validate initial
    if len(dict_rows) < 1:
        raise ParserError(f"{section_name} section is missing some rows!")

    if len(dict_rows[0]) != 2:
        raise ParserError(f"{section_name} has different amount of columns {len(dict_rows[0])}"
                          f" than expected!")

    parse_errors = []

    column_name_key = list(dict_rows[0].keys())[0]
    second_column_key = list(dict_rows[0].keys())[1]

    # pocatecni is first row_data key
    statement_metadata.start_balance = _convert_to_numeric(second_column_key)

    # koncovy
    end_balance_str = _get_table_value_strict(dict_rows[0], column_name_key, second_column_key,
                                              'Konečný zůstatek', errors_buffer=parse_errors)
    if end_balance_str:
        statement_metadata.end_balance = _convert_to_numeric(end_balance_str)

    if parse_errors:
        raise ParserError(f"{section_name} parsing failed with errors: {'; '.join(parse_errors)}")


def _parse_entity_section(file_path: str, statement_metadata: StatementMetadata):
    """
    Parses entity metadata section and updates StatementMetadata values in-place
    Args:
        file_path:
        statement_metadata (StatementMetadata): Metadata container to be updated

    Returns:

    """
    section_name = 'Account Entity section'
    dict_rows = _load_single_page_section_from_template(file_path, section_name, HeaderTemplatePaths.account_entity)

    # validate initial
    if len(dict_rows) < 1:
        raise ParserError(f"{section_name} section is missing some rows!")

    if len(dict_rows[0]) > 2:
        raise ParserError(f"{section_name} has different amount of columns {len(dict_rows[0])}"
                          f" than expected!")

    column_name_key = list(dict_rows[0].keys())[0]
    second_column_key = ''

    # sometimes the columns are split
    if len(dict_rows[0]) == 2:
        second_column_key = list(dict_rows[0].keys())[1]

    entity_rows = [f'{column_name_key} {second_column_key}\n']
    for row in dict_rows:
        row = [str(value) for value in row.values() if str(value) != 'nan']
        entity_rows.append(' '.join(row))
    statement_metadata.account_entity = '\n'.join(entity_rows)

    return statement_metadata


def parse_statement_metadata(file_path: str) -> StatementMetadata:
    """
    Parse statement metadata present on the first page.
    Args:
        file_path: path to the PDF statement.

    Returns: StatementMetadata - container with metadata values

    """
    statement_metadata = StatementMetadata()
    _parse_account_type_metadata(file_path, statement_metadata)
    _parse_report_metadata(file_path, statement_metadata)
    _parse_balance_section_metadata(file_path, statement_metadata)
    _parse_entity_section(file_path, statement_metadata)

    return statement_metadata


def _get_pdf_page_count(file_path: str):
    reader = PyPDF2.PdfFileReader(file_path)
    # printing number of pages in pdf file
    return reader.numPages


def _get_range_chunks(max_size, chunk_size):
    for i in range(1, max_size, chunk_size):
        if max_size < chunk_size + i:
            end = max_size
        else:
            end = i + chunk_size - 1
        yield i, end


def _get_full_statement_rows(file_path: str) -> Iterator[Iterator[dict]]:
    max_pages = _get_pdf_page_count(file_path)

    for start_range, end_range in _get_range_chunks(max_pages, MAX_CHUNK_SIZE):
        logging.info(f'Processing pages {start_range}-{end_range}')
        for df in tabula.read_pdf(file_path,
                                  columns=DATA_COLUMN_BOUNDARIES,
                                  stream=True,
                                  pandas_options=PANDAS_OPTIONS,
                                  java_options=JAVA_OPTIONS,
                                  pages=f'{start_range}-{end_range}'):
            yield (row for row in df.to_dict('records'))


def _get_last_page_statement_rows(file_path: str) -> Iterator[Iterator[dict]]:
    max_pages = _get_pdf_page_count(file_path)
    if max_pages % 2 == 0:
        template = DataTemplatePaths.last_page_even
    else:
        template = DataTemplatePaths.last_page_odd

    # Sometimes the last page is not parsed properly so use predefined template
    last_page_records = _load_single_page_section_from_template(file_path, 'last_page', template,
                                                                str(max_pages), stream=True,
                                                                columns=DATA_COLUMN_BOUNDARIES)
    yield (row for row in last_page_records)


def _validate_statement_header_first_row(column_names: List[str], column_number=5):
    first_row_keys_4 = [['Datum Popis transakce', 'Datum Popis transakce Název protiúčtu / Číslo a typ karty'],
                        ['Název protiúčtu / Číslo a typ karty', 'Unnamed: 0', 'Název protiúčtu / Číslo a typ karty VS'],
                        ['VS', 'Název protiúčtu / Číslo a typ karty', 'Unnamed: 0'], ['Připsáno', 'VS']]
    first_row_keys_5 = [['Datum', 'Datum Popis transakce'], ['Popis transakce', 'Unnamed: 0'],
                        ['Název protiúčtu / Číslo a typ karty'], ['VS'], ['Připsáno']]
    if column_number == 5:
        first_row_keys = first_row_keys_5
    elif column_number == 4:
        first_row_keys = first_row_keys_4
    else:
        raise ValueError()

    errors = []
    for idx, names in enumerate(first_row_keys):
        if column_names[idx] not in names:
            errors.append(f"Column '{names}' is expected on {idx}. position. '{column_names[idx]}' found instead"),

    if errors:
        raise ParserError(f"Failed to parse the statement transactions header. Found errors: "
                          f"\nf{'; '.join(errors)}")


def _is_first_row_header(row: dict):
    values = list(row.values())
    dict_keys = list(row.keys())
    return dict_keys[0] == 'POČÁTEČNÍ ZŮSTATEK' or values[0] == 'POČÁTEČNÍ ZŮSTATEK'


def _skip_statement_data_header(statement_page: Iterator[dict], page_nr: int) -> Tuple[Callable, bool]:
    """
    Iterates and validates the statement page header.
    Returns function to modify the records to expected structure
    This can happen when there's some other type of table detected.

    Args:
        statement_page: (Iterator[dict]) Iterator of page rows.

    Returns: merge_columns:bool, convert_function:Callable

    """
    first_row = next(statement_page)
    dict_keys = list(first_row.keys())
    dict_values = list(first_row.values())

    # Some reports have recap page at the end. Skip that from parsing
    if list(first_row.keys())[0] in ['Rekapitulace transakcí na účtu', 'Rozpis poplatků za položky']:
        return _pass, True

    if len(first_row) == 4:
        convert_method = _pass
    elif len(first_row) == 5 and dict_keys[0] == 'Datum Popis transakce' and dict_keys[4] in ['Unnamed: 0'] and \
            dict_values[3] == 'Odepsáno':
        convert_method = _merge_last_two_columns
    elif len(first_row) == 5 and dict_keys[0] == 'Datum Popis transakce' and dict_keys[4] in ['Unnamed: 0']:
        convert_method = _drop_last_column
    elif len(first_row) == 5 and dict_keys[0] == 'Datum Popis transakce' and dict_keys[1] in ['Unnamed: 0']:
        convert_method = _merge_second_two_columns
    elif len(first_row) == 6 and dict_keys[0] == 'Datum' and dict_keys[1] == 'Popis transakce' \
            and dict_keys[2] in ['Unnamed: 0']:
        convert_method = _merge_firsttwo_third_and_fourth_column
    elif len(first_row) == 5 and dict_keys[0] in ['Datum', 'Unnamed: 0']:
        convert_method = _merge_first_two_columns
    elif len(first_row) == 6 and dict_keys[1] in ['Unnamed: 0'] and dict_keys[2] == 'Unnamed: 1':
        convert_method = _drop_second_to_third_column
    else:
        raise ParserError(f"Statement Page Header has different amount of columns [{len(first_row)}] than expected")

    if page_nr == 0:
        # scroll to first row, if first page of the statement
        while not _is_first_row_header(first_row):
            try:
                first_row = next(statement_page)
            except StopIteration:
                logging.error('Invalid header structure')
                raise
    else:
        first_row = convert_method(first_row)
        dict_keys = list(first_row.keys())
        if dict_keys[0] == 'POČÁTEČNÍ ZŮSTATEK':
            first_row_header = list(first_row.values())
        else:
            first_row_header = dict_keys

        _validate_statement_header_first_row(first_row_header, column_number=len(first_row))

    is_last_header_row = False
    skipped_rows = 1
    while not is_last_header_row:
        row = next(statement_page)
        skipped_rows += 1
        if skipped_rows > 5:
            raise ParserError("The Statement Page Header has more rows than expected!")
        if list(row.values())[0] == 'transakce':
            break

    return convert_method, False


def _split_date_from_text(text: str):
    date_part = text.split(' ')[0]
    text = text.split(' ')[1]
    # validate
    datetime.strptime(date_part, "%d.%m.%Y")
    return date_part, text


def _parse_first_statement_row_part(row_data: List[str], statement_row_data: StatementRow):
    first_header = row_data[0]
    try:

        date_part, description = _split_date_from_text(first_header)
        account_name = row_data[1]

        statement_row_data.accounting_date = date_part
        statement_row_data.transaction_description = description
        statement_row_data.account_name__card_type = account_name
        statement_row_data.vs = _convert_na_to_empty(row_data[2])
        amount = _convert_to_numeric(row_data[3])
        statement_row_data.amount = amount

        if math.isnan(amount):
            logging.warning(f"Amount is nan in row: {row_data}")
            statement_row_data.amount = 0

        statement_row_data.transaction_type = 'debit' if amount < 0 else 'credit'

    except Exception as e:
        raise ParserError(f"The first statement row_data part has invalid structure: {first_header}") from e

    return date_part, description


def _parse_second_statement_row_part(row_data: List[str], statement_row_data: StatementRow):
    # try to split date
    try:
        date, identification_text = _split_date_from_text(row_data[0])
    except Exception:
        date = ''
        identification_text = row_data[0]

    statement_row_data.transaction_identification = identification_text
    statement_row_data.transaction_date = date

    statement_row_data.account_number__merchant = row_data[1]
    statement_row_data.ks = _convert_na_to_empty(row_data[2])


def _parse_third_statement_row_part(row_data: List[str], statement_row_data: StatementRow):
    identification_text = _convert_na_to_empty(row_data[0])
    statement_row_data.transaction_identification += f"\n{identification_text}"

    statement_row_data.ss = _convert_na_to_empty(row_data[2])


def _is_end_of_statement_data(row: dict):
    row_values = list(row.values())
    values_string = ' '.join([str(v) for v in row_values])
    end_of_page = 'Pokračování na další straně' in values_string

    return len(row_values) == 0 or row_values[0] == 'KONEČNÝ ZŮSTATEK' or end_of_page


def _is_date_text_split(text):
    contains_date = True
    try:
        _split_date_from_text(text)
    except Exception:
        contains_date = False
    return contains_date


# # Record modifying functions

def _pass(row: dict):
    return row


def _merge_first_two_columns(row: dict):
    # TODO: Consider reversing the process => unmerging two columns as it seems only first page has this issue
    new_dict = {}
    keys = list(row.keys())
    values = list(row.values())
    first_two_merged = [_convert_na_to_empty(v) for v in values[:2]]
    new_dict[' '.join(keys[:2])] = ' '.join(first_two_merged)

    for idx, val in enumerate(values[2:], start=2):
        new_dict[keys[idx]] = val

    return new_dict


def _merge_firsttwo_third_and_fourth_column(row: dict):
    merged = _merge_neighbouring_columns(row, 3)
    return _merge_first_two_columns(merged)


def _merge_second_two_columns(row: dict):
    new_dict = {}
    keys = list(row.keys())
    values = list(row.values())

    for idx, val in enumerate(values):

        if idx == 2:
            new_dict[keys[2 - 1]] = _convert_na_to_empty(new_dict[keys[2 - 1]]) + _convert_na_to_empty(val)
        else:
            new_dict[keys[idx]] = val

    return new_dict


def _merge_neighbouring_columns(row: dict, first_col_index: int):
    new_dict = {}
    keys = list(row.keys())
    values = list(row.values())

    for idx, val in enumerate(values):

        if idx == first_col_index:
            new_dict[keys[first_col_index - 1]] = _convert_na_to_empty(
                new_dict[keys[first_col_index - 1]]) + _convert_na_to_empty(val)
        else:
            new_dict[keys[idx]] = val

    return new_dict


def _drop_last_column(row: dict):
    row.pop(list(row.keys())[-1:])
    return row


def _drop_second_to_third_column(row: dict):
    keys = list(row.keys())
    row.pop(keys[1])
    row.pop(keys[2])
    return row


def _merge_last_two_columns(row: dict):
    new_dict = {}
    keys = list(row.keys())
    values = list(row.values())

    for idx, val in enumerate(values):

        if idx == 4:
            new_dict[keys[4 - 1]] = _convert_na_to_empty(new_dict[keys[4 - 1]]) + _convert_na_to_empty(val)
        else:
            new_dict[keys[idx]] = val

    return new_dict


def _get_next_transformed(page_iterator: Iterator[dict], convert_method: Callable = _pass):
    row = next(page_iterator, {})
    return convert_method(row)


def _parse_next_statement_row(page_iterator: Iterator[dict],
                              convert_method: Callable = _pass,
                              first_row_part=None) -> Tuple[StatementRow, dict]:
    """
    Builds statement data row from multiple row parts.

    Args:
        page_iterator: Iterator of page rows
        convert_method: Flag whether to modify the output structure to match the expected.
                             Sometimes the first two columns are merged, if not (5 columns) this flag should be set
                             to true.
        first_row_part: Next statement row returned from the method

    Returns: (Tuple[StatementRow, dict]) Statement row data and next row in the iterator if present.

    """
    statement_row_data = StatementRow()
    # Sometimes the first two are merged, merge to 4 which is expected
    merge_cols = convert_method

    # if starting from the beginning
    if not first_row_part:
        first_row_part = _get_next_transformed(page_iterator, merge_cols)

    _validate_row_structure(first_row_part, 4, 'Statement Page Data')

    row_part_data = list(first_row_part.values())
    _parse_first_statement_row_part(row_part_data, statement_row_data)

    # There should always be second part
    row_part = _get_next_transformed(page_iterator, merge_cols)
    row_part_data = list(row_part.values())
    _parse_second_statement_row_part(row_part_data, statement_row_data)

    # Third part
    row_part = _get_next_transformed(page_iterator, merge_cols)
    row_part_data = list(row_part.values())
    is_end = _is_end_of_statement_data(row_part) or _is_date_text_split(row_part_data[0])
    if not is_end:
        _parse_third_statement_row_part(row_part_data, statement_row_data)

    # Remaining identification parts
    if not is_end:
        row_part = _get_next_transformed(page_iterator, merge_cols)
        row_part_data = list(row_part.values())
        is_end = (_is_end_of_statement_data(row_part) or _is_date_text_split(row_part_data[0]))

        while not is_end:
            text = _convert_na_to_empty(row_part_data[1])
            statement_row_data.transaction_identification += f"\n{text}"
            row_part = _get_next_transformed(page_iterator, merge_cols)
            row_part_data = list(row_part.values())
            is_end = _is_end_of_statement_data(row_part) or _is_date_text_split(row_part_data[0])

    # if complete end do not return next page
    if _is_end_of_statement_data(row_part):
        next_page = None
    else:
        next_page = row_part

    return statement_row_data, next_page


def _iterate_through_rows(pages_iterator, processing_metadata: dict):
    for page_iterator in pages_iterator:
        processing_metadata['pages_processed'] += 1
        convert_method, skip = _skip_statement_data_header(page_iterator,
                                                           page_nr=processing_metadata['pages_processed'])

        # end reached
        if skip:
            break

        has_next = True
        next_page = None
        while has_next:
            data, next_page = _parse_next_statement_row(page_iterator, convert_method, next_page)
            if not next_page:
                has_next = False

            # validation
            if data.amount < 0:
                processing_metadata['debit_total'] += data.amount
            else:
                processing_metadata['credit_total'] += data.amount

            yield data


def parse_full_statement(file_path: str) -> Tuple[StatementRow, StatementMetadata]:
    statement_metadata = parse_statement_metadata(file_path)
    processing_metadata = dict(
        pages_processed=0,
        # for validation
        debit_total=0,
        credit_total=0)

    iterator = _iterate_through_rows(_get_full_statement_rows(file_path),
                                     processing_metadata=processing_metadata)
    pages = 0
    for i in iterator:
        pages = pages + 1
        yield i, statement_metadata

    total_sum = round(processing_metadata['debit_total'], 2) + round(processing_metadata['credit_total'], 2)
    total_sum_check = statement_metadata.end_balance - statement_metadata.start_balance
    if round(total_sum, 2) != round(total_sum_check, 2):
        # Possibly the last page parsing failes, retry with template
        logging.warning('The end sum does not match, trying to reprocess last page from template.')
        processing_metadata['pages_processed'] -= 1
        iterator = _iterate_through_rows(_get_last_page_statement_rows(file_path),
                                         processing_metadata=processing_metadata)
        for i in iterator:
            pages = pages + 1
            yield i, statement_metadata

    total_sum = round(processing_metadata['debit_total'], 2) + round(processing_metadata['credit_total'], 2)
    total_sum_check = statement_metadata.end_balance - statement_metadata.start_balance
    if round(total_sum, 2) != round(total_sum_check, 2):
        raise ParserError(
            f"Parsed result ended with inconsistent data. The transaction sum from totals {total_sum_check} "
            f"is not equal to sum of individual transactions {total_sum}")

    if pages == 0:
        yield None, statement_metadata


================================================
File: /LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /requirements.txt
================================================
keboola.component==1.6.7
tabula-py==2.9.3
mock==4.0.3
freezegun==1.1.0
PyPDF2==1.26.0


================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG

  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /component_config/component_short_description.md
================================================
Komercni Banka PDF Account Statement Parser short description

================================================
File: /component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "extractor configuration",
  "required": [
    "print_hello"
  ],
  "properties": {
    "print_hello": {
      "type": "boolean",
      "title": "Print Hello"
    },
    "debug": {
      "type": "boolean",
      "title": "Debug"
    }
  }
}


================================================
File: /component_config/configuration_description.md
================================================
Configuration description.

================================================
File: /component_config/configRowSchema.json
================================================
{}

================================================
File: /component_config/component_long_description.md
================================================
Komercni Banka PDF Account Statement Parser long description

================================================
File: /component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: /component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}


================================================
File: /component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}

================================================
File: /component_config/logger
================================================
gelf

================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    mem_limit: 512m
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
fi

echo "Updating logger settings"

value=`cat component_config/logger`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} logger --value="$value"
else
    echo "logger type is empty!"
fi

echo "Updating logger configuration"
value=`cat component_config/loggerConfiguration.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} loggerConfiguration --value="$value"
else
    echo "loggerConfiguration is empty!"
fi

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /README.md
================================================
# Komercni Banka PDF Account Statement Parser


Takes all `.pdf` files in `in/files/` and converts the Komercni Banka Statements (in Czech language) to CSV files stored in `out/tables/`.

The output tables are stored as `sliced tables` => each file is represented inside folders `out/tables/statments.csv` and
 `out/tables/statements_metadata.csv` as header-less CSVs. The column names are included 
 in the respective manifest files.


**Table of contents:**  
  
[TOC]

## Functional notes

CZE Komercni Banka statements are converted into two files:

- `statements.csv` containing the statement rows
- `statements_metadata.csv` containing details about particular statements

The data is outputted `incrementally`.


## Output Structure

### **statements**

|pk                              |statement_metadata_pk           |row_nr|accounting_date|transaction_date|transaction_description|transaction_identification                    |account_name__card_type |account_number__merchant|vs      |ks  |ss       |transaction_type|amount   |
|--------------------------------|--------------------------------|------|---------------|----------------|-----------------------|----------------------------------------------|------------------------|------------------------|--------|----|---------|----------------|---------|
|8a8723bbec95374a367de09dc7d2d5cf|0b13871a680ed5bd7f99ecbe61dcccd3|0     |10.08.2021     |                |PŘÍCHOZÍ               |OI0001SOMEID0 01 001-0012345 1602 12345 12345 |DOE JON                 |43-12345678/0100        |        |    |         |credit          |1666.0   |

### **statements_metadata**

|pk                              |account_number                  |statement_type|iban      |account_type|currency |statement_date                                |statement_number        |account_entity     |start_balance|end_balance|
|--------------------------------|--------------------------------|--------------|----------|------------|---------|----------------------------------------------|------------------------|-------------------|-------------|-----------|
|0b13871a680ed5bd7f99ecbe61dcccd3|123-123123123123/0100           |VÝPIS PERIODICKÝ|CZ123123123123|MůjÚčet Plus|CZK      |07.09.2021                                    |9                       |DN12334_2421-12345-2 ID: 1020947192  DOE JOHN Somewhere 123 / 56 170 00  PRAHA|1000.0       |2666.0     |


## Configuration


### Sample configuration

```json
{
    "definition": {
        "component": "kds-team.processor-kb-account-statement-parser"
    }
}
```
 
# Example Use Case

Most commonly the processor is useful in combination with the [IMAP Email Content Extractor](https://help.keboola.com/components/extractors/communication/email-imap/)

![email content](docs/imgs/email_content.png)

Just include following processor configuration:

```json
{
  "before": [],
  "after": [
    {
      "definition": {
        "component": "kds-team.processor-kb-account-statement-parser"
      }
    }
  ]
}
```
# Development
 
This example contains runnable container with simple unittest. For local testing it is useful to include `data` folder in the root
and use docker-compose commands to run the container or execute tests. 

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path:
```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone https://bitbucket.org:kds_consulting_team/kds-team.processor-deduplicate-headers.git my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

# Testing

The preset pipeline scripts contain sections allowing pushing testing image into the ECR repository and automatic 
testing in a dedicated project. These sections are by default commented out. 

# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/) 

Development
-----------

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to
your custom path in the `docker-compose.yml` file:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Clone this repository, init the workspace and run the component with following
command:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
git clone git@bitbucket.org:kds_consulting_team/kds-team.processsor-kb-account-statement-parser.git kds-team.processsor-kb-account-statement-parser
cd kds-team.processsor-kb-account-statement-parser
docker-compose build
docker-compose run --rm dev
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Run the test suite and lint check using this command:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
docker-compose run --rm test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Integration
===========

For information about deployment and integration with KBC, please refer to the
[deployment section of developers
documentation](https://developers.keboola.com/extend/component/deployment/)


