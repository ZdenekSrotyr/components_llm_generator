Directory structure:
└── kds_consulting_team-kds-team.ex-salesforce-v2/
    ├── README.md
    ├── bitbucket-pipelines.yml
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── Pipfile
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run.bat
    │   ├── run_kbc_tests.ps1
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── component.py
    │   └── salesforce/
    │       ├── __init__.py
    │       ├── client.py
    │       └── soql_query.py
    └── tests/
        ├── __init__.py
        └── test_component.py

================================================
FILE: README.md
================================================
# Salesforce Extractor

The component exports data from Salesforce based on a SOQL query or an object you provide 
and saves it into the out/tables directory.

**Table of contents:**  
  
[TOC]
# Prerequisites 

## Choose an authorization method

In order to authorize the component you can select one of the two following methods of authorization:
* With a username, password, and **Security Token** 
* With a username, password, and **Connected App** 
* With a Consumer Key and a Consumer Secret using **OAuth 2.0 Client Credentials Flow**

In the below sections we describe how to get a security token, and how to set up a connected app.

### How to obtain a Security Token

If you do not have your Security Token, you will have to reset your current one following the steps outlined in the
[Salesforce Documentation](https://help.salesforce.com/s/articleView?id=sf.user_security_token.htm&type=5).

### How to set up a Connected App

In the Salesforce Setup menu navigate to Platform Tools > Apps > App Manager.

On the top left you should see a button labeled *New Connected App* and click on it.

In New Connected App fill in :

* *Connected App Name* : name the app distinctively, e.g. Keboola Connection App
* *API Name* :  will be automatically generated from the App name
* *Contact Email* : your email

Next in the API(Enable OAuth Settings) check the *Enable OAuth Settings* box. More options should appear. Enable the 
*Enable for Device Flow* box and the *Callback URL* should be automatically set. Next select the following scopes in the 
'Selected OAuth Scopes' section :

*  Manage user data via APIs (api)
*  Access unique user identifiers (openid)
*  Perform requests at any time (refresh_token, offline_access)

Once filled in, click the *save* button on the top of the form. Now your App should be created, click the shown *Continue*
button to advance.

Next find you newly created App in the  Platform Tools > Apps > App Manager. Once you see your app, in the row that contains the
right App Name click on the downward facing arrow on the right and click *Manage*, you should now be able to see the
Connection App Policies. On the top you should see an *Edit Policies* button, click it.

In the *OAuth Policies* find the *IP Relaxation* and select the  *Relax IP restrictions*.
Leave everything else the same and click *save*

Now finally to get the *Consumer Key* and *Consumer Secret* go back to Platform Tools > Apps > App Manager, find the 
App and in the row that contains the right App Name click on the downward facing arrow on the right and click *View*.
In the *API (Enable OAuth Settings)* > *Consumer Key and Secret* you should see a button *Manage Consumer Details*, click on it.

You might be prompted to fill in a Verification Code. Fill it in.

Now you should see the Consumer details. Save the Key and Secret to a password manager or other secure space.

### Enabling Client Credentials Flow

To enable client credentials flow, you need to create a Connected App in Salesforce. This process is described in **How to set up a Connected App** section above.
The only difference will be checking **Enable Client Credentials Flow** for the Connected App.

You also need to fill in the **domain** field for this type of Authorization.

# Configuration

## Authorization

- **Login Method** (login_method) - Select either "security_token" or "connected_app", Default : "security_token"
- **Username** (username) - (REQ) your username, when exporting data from sandbox don't forget to add .sandboxname at the end
- **Password** (#password) - (REQ) your password
- **Sandbox** (sandbox) - (REQ) true when you want to export data from sandbox

If "security_token" login method is selected:
- **Security Token** (#security_token) - (REQ) your security token, don't forget it is different for sandbox

If "connected_app" login method:
- **Consumer Key** (#consumer_key) - (REQ) The Consumer Key of your Connected App
- **Consumer Secret** (#consumer_secret) - (REQ) The Consumer Secret of your Connected App
- 
## Row configuration
 - Query type (query_type_selector) - [REQ] Either "Object" or  "Custom SOQL"
 - Get deleted records (is_deleted) - [OPT] Fetch records that have been deleted
 - API version (api_version) - [OPT] Specify the version of API you want to extract data from

### Fetching whole Objects (with Native Data Types support)

- **Object** - Salesforce object identifier, eg. Account.

### Fetching using SOQL queries (all columns are typed as string)

- **SOQL query** - Salesforce SOQL query, eg. SELECT Id, FirstName, LastName FROM Contact

## Load type
If set to Incremental update, the result tables will be updated based on primary key. 
Full load overwrites the destination table each time. 
It can override the table name with the parameter **output_table_name** and the bucket with the parameter **bucket_name**.

### Incremental fetching 

Incremental fetching allows the fetching of only the records that have been modified since the previous run of
the component. This is done by specifying an incremental field in the object that contains data on when it wast last modified.


**Example: With Security Token**

```json
{
  "parameters": {
    "username": "user@keboola.com",
    "#password": "password",
    "#security_token": "token_here",
    "sandbox": false,
    "api_version" : "52.0",
    "object": "Contact",
    "soql_query": "select Id, FirstName,LastName,isdeleted,lastmodifieddate from Contact",
    "is_deleted": false,
    "loading_options": {
      "incremental": true,
      "pkey": [
        "Id"
      ],
      "incremental_field": "lastmodifieddate",
      "incremental_fetch": true
    }
  }
}
```





================================================
FILE: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG

  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
FILE: change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "asigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "asigning travis tag"
	export TAG="$TRAVIS_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.13-slim

RUN pip install flake8

COPY requirements.txt /code/requirements.txt
RUN pip install -r /code/requirements.txt

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY requirements.txt /code/requirements.txt
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

WORKDIR /code/

CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: Pipfile
================================================
[[source]]
url = "https://pypi.org/simple"
verify_ssl = true
name = "pypi"

[packages]
"keboola.component" = "*"
"keboola.http-client" = "*"
"keboola.utils" = "*"
backoff = "*"
freezegun = "*"
mock = "*"
retry = "*"
salesforce-bulk = "==2.2.0"

[dev-packages]

[requires]
python_version = "3.13"
python_full_version = "3.13.1"



================================================
FILE: requirements.txt
================================================
backoff==2.2.1; python_version >= '3.7' and python_version < '4.0'
freezegun==1.5.1; python_version >= '3.7'
keboola.component==1.6.10; python_version >= '3.7'
keboola.http-client==1.0.1; python_version >= '3.7'
keboola.utils==1.1.0; python_version >= '3.7'
mock==5.1.0; python_version >= '3.6'
retry==0.9.2
salesforce-bulk==2.2.0



================================================
FILE: component_config/component_long_description.md
================================================
This component allows you to download data with a SOQL query or directly download a whole object.


================================================
FILE: component_config/component_short_description.md
================================================
A customer relationship management service (CRM) 


================================================
FILE: component_config/configRowSchema.json
================================================
{
  "title": "Query Configuration",
  "type": "object",
  "required": [
    "query_type_selector"
  ],
  "properties": {
    "query_type_selector": {
      "title": "Query type",
      "type": "string",
      "enum": [
        "Object",
        "Custom SOQL"
      ],
      "default": "Object",
      "propertyOrder": 1
    },
    "object": {
      "title": "Object Name",
      "type": "string",
      "options": {
        "dependencies": {
          "query_type_selector": "Object"
        },
        "async": {
          "label": "Re-load Objects",
          "action": "loadObjects"
        }
      },
      "items": {
        "enum": [
        ],
        "type": "string"
      },
      "enum": [
      ],
      "format": "select",
      "description": "Salesforce object identifier, eg. Contact",
      "propertyOrder": 2
    },
    "fields": {
      "title": "Fields (optional)",
      "type": "array",
      "items": {
        "enum": [
        ],
        "type": "string"
      },
      "options": {
        "dependencies": {
          "query_type_selector": "Object"
        },
        "async": {
          "label": "Load Fields",
          "action": "loadFields"
        }
      },
      "enum": [
      ],
      "format": "select",
      "description": "Salesforce fields to fetch. If left empty, all fields will be downloaded.",
      "uniqueItems": true,
      "propertyOrder": 3
    },
    "soql_query": {
      "title": "SOQL Query",
      "type": "string",
      "format": "textarea",
      "options": {
        "dependencies": {
          "query_type_selector": "Custom SOQL"
        }
      },
      "description": "Specify the SOQL query, eg. SELECT Id, FirstName, LastName FROM Contact. You can easily develop your SOQL code in the <a href='https://workbench.developerforce.com/login.php?startUrl=%2Fquery.php'>Developer Workbench</a>",
      "propertyOrder": 4
    },
    "validation_button": {
      "type": "button",
      "format": "sync-action",
      "propertyOrder": 5,
      "options": {
        "async": {
          "label": "Test Query",
          "action": "testQuery"
        },
        "dependencies": {
          "query_type_selector": "Custom SOQL"
        }
      }
    },
    "is_deleted": {
      "title": "Get deleted records",
      "type": "boolean",
      "format": "checkbox",
      "default": false,
      "description": "Fetch records that have been deleted",
      "propertyOrder": 6
    },
    "loading_options": {
      "type": "object",
      "title": "Loading Options",
      "propertyOrder": 100,
      "required": [
        "incremental"
      ],
      "properties": {
        "incremental_field": {
          "type": "string",
          "title": "Incremental Field",
          "description": "Salesforce object field to use for incremental fetching, eg. LastModifiedDate for most Salesforce objects or CreatedDate for Salesforce History objects",
          "propertyOrder": 300,
          "options": {
            "dependencies": {
              "incremental": 1,
              "incremental_fetch": true
            },
            "async": {
              "label": "Re-load Fields",
              "action": "loadPossibleIncrementalField"
            }
          },
          "enum": [
          ],
          "format": "select"
        },
        "pkey": {
          "type": "array",
          "items": {
            "enum": [
            ],
            "type": "string"
          },
          "default": [
            "Id"
          ],
          "format": "select",
          "title": "Primary key",
          "options": {
            "async": {
              "label": "Re-load Fields",
              "action": "loadPossiblePrimaryKeys"
            }
          },
          "uniqueItems": true,
          "propertyOrder": 5000
        },
        "incremental_fetch": {
          "title": "Incremental fetch",
          "type": "boolean",
          "format": "checkbox",
          "default": false,
          "description": "Fetch records that have been updated since the last run of the component with a specified field",
          "propertyOrder": 250,
          "options": {
            "dependencies": {
              "incremental": 1
            }
          }
        },
        "incremental": {
          "type": "integer",
          "enum": [
            0,
            1
          ],
          "options": {
            "enum_titles": [
              "Full Load",
              "Incremental Update"
            ]
          },
          "default": 0,
          "title": "Load type",
          "description": "If set to Incremental update, the result tables will be updated based on primary key and new records will be fetched. Full load overwrites the destination table each time.",
          "propertyOrder": 200
        },
        "output_table_name": {
            "type": "string",
            "title": "Storage Table Name",
            "description": "Override the default name of the table in Storage",
            "propertyOrder": 20
          }
      }
    }
  }
}


================================================
FILE: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Salesforce Credentials",
  "format": "table",
  "$schema": "http://json-schema.org/draft-04/schema#",
  "required": [
    "username",
    "#password",
    "api_version"
  ],
  "properties": {
    "login_method": {
      "title": "Login Method",
      "type": "string",
      "enum": [
        "security_token",
        "connected_app",
        "connected_app_oauth_cc"
      ],
      "options": {
        "enum_titles": [
          "Security Token with Username and Password",
          "Connected App with Username and Password",
          "Connected App OAuth Client 2.0 Client Credentials"
        ]
      },
      "default": "security_token",
      "description": "Specify the login method you wish to use",
      "propertyOrder": 1
    },
    "username": {
      "type": "string",
      "title": "Login Name",
      "default": "",
      "options": {
        "dependencies": {
          "login_method": [
            "security_token",
            "connected_app"
          ]
        }
      },
      "minLength": 1,
      "description": "Login name for Salesforce",
      "propertyOrder": 10
    },
    "#password": {
      "type": "string",
      "title": "Password",
      "format": "password",
      "default": "",
      "options": {
        "dependencies": {
          "login_method": [
            "security_token",
            "connected_app"
          ]
        }
      },
      "minLength": 1,
      "description": "Salesforce password",
      "propertyOrder": 20
    },
    "#security_token": {
      "type": "string",
      "title": "Security token",
      "format": "password",
      "default": "",
      "options": {
        "dependencies": {
          "login_method": "security_token"
        }
      },
      "description": "Salesforce security token",
      "propertyOrder": 30
    },
    "domain": {
      "type": "string",
      "title": "Domain",
      "default": "",
      "options": {
        "dependencies": {
          "login_method": "connected_app_oauth_cc"
        }
      },
      "description": "Your Salesforce Domain. For example: https://keboola-dev-ed.my.salesforce.com",
      "propertyOrder": 30
    },
    "#consumer_key": {
      "type": "string",
      "title": "Consumer Key",
      "format": "password",
      "options": {
        "dependencies": {
          "login_method": [
            "connected_app",
            "connected_app_oauth_cc"
          ]
        }
      },
      "description": "Salesforce Connected App Consumer Key",
      "propertyOrder": 33
    },
    "#consumer_secret": {
      "type": "string",
      "title": "Consumer Secret",
      "format": "password",
      "options": {
        "dependencies": {
          "login_method": [
            "connected_app",
            "connected_app_oauth_cc"
          ]
        }
      },
      "description": "Salesforce Connected App Consumer Secret",
      "propertyOrder": 36
    },
    "sandbox": {
      "type": "boolean",
      "title": "Sandbox",
      "format": "checkbox",
      "options": {
        "dependencies": {
          "login_method": [
            "security_token",
            "connected_app"
          ]
        }
      },
      "description": "Download records from sandbox instead of the production environment.",
      "propertyOrder": 40
    },
    "api_version": {
      "title": "API version",
      "type": "string",
      "enum": [
        "52.0",
        "53.0",
        "54.0",
        "55.0",
        "56.0",
        "57.0",
        "58.0",
        "59.0",
        "60.0",
        "61.0",
        "62.0"
      ],
      "default": "62.0",
      "description": "Specify the version of API you want to extract data from",
      "propertyOrder": 50
    },
    "proxy": {
      "type": "object",
      "title": "Proxy Settings",
      "format": "grid-strict",
      "description": "Proxy address will be constructed in (username:password@)your.proxy.server.com(:port) format.",
      "propertyOrder": 60,
      "properties": {
        "use_proxy": {
          "title": "Use Proxy",
          "type": "boolean",
          "format": "checkbox",
          "default": false,
          "options": {
            "grid_columns": 6,
            "grid_break": true
          },
          "propertyOrder": 1
        },
        "proxy_server": {
          "type": "string",
          "title": "HTTPS Proxy Server Address",
          "options": {
            "grid_columns": 8,
            "dependencies": {
              "use_proxy": true
            }
          },
          "propertyOrder": 2
        },
        "proxy_port": {
          "type": "string",
          "title": "HTTPS Proxy Server Port",
          "options": {
            "grid_columns": 4,
            "dependencies": {
              "use_proxy": true
            }
          },
          "propertyOrder": 3
        },
        "basic_auth": {
          "title": "Basic Authentication",
          "type": "boolean",
          "format": "checkbox",
          "default": false,
          "options": {
            "grid_columns": 6,
            "grid_break": true,
            "dependencies": {
              "use_proxy": true
            }
          },
          "propertyOrder": 6
        },
        "username": {
          "type": "string",
          "title": "HTTPS Proxy Server Username",
          "options": {
            "dependencies": {
              "use_proxy": true,
              "basic_auth": true
            }
          },
          "propertyOrder": 10
        },
        "#password": {
          "type": "string",
          "title": "HTTPS Proxy Server Password",
          "format": "password",
          "options": {
            "dependencies": {
              "use_proxy": true,
              "basic_auth": true
            }
          },
          "propertyOrder": 15
        }
      }
    },
    "test_connection": {
      "type": "button",
      "format": "test-connection",
      "propertyOrder": 70
    }
  }
}


================================================
FILE: component_config/configuration_description.md
================================================
[Empty file]


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}



================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
fi

echo "Updating logger settings"

value=`cat component_config/logger`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} logger --value="$value"
else
    echo "logger type is empty!"
fi

echo "Updating logger configuration"
value=`cat component_config/loggerConfiguration.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} loggerConfiguration --value="$value"
else
    echo "loggerConfiguration is empty!"
fi


================================================
FILE: src/component.py
================================================
import csv
import logging
import os
import shutil
from collections import OrderedDict
from datetime import datetime, timezone
from enum import Enum
from os import mkdir, path

from keboola.component.base import ComponentBase, sync_action
from keboola.component.dao import SupportedDataTypes, BaseType, ColumnDefinition
from keboola.component.exceptions import UserException
from keboola.component.sync_actions import MessageType, SelectElement, ValidationResult
from keboola.utils.header_normalizer import NormalizerStrategy, get_normalizer
from retry import retry
from simple_salesforce.exceptions import SalesforceAuthenticationFailed, SalesforceError, SalesforceResourceNotFound

from salesforce.client import DEFAULT_API_VERSION, SalesforceClient, SalesforceClientException
from salesforce.soql_query import SoqlQuery

KEY_LOGIN_METHOD = "login_method"
KEY_CONSUMER_KEY = "#consumer_key"
KEY_CONSUMER_SECRET = "#consumer_secret"
KEY_DOMAIN = "domain"
KEY_USERNAME = "username"
KEY_PASSWORD = "#password"
KEY_SECURITY_TOKEN = "#security_token"
KEY_SANDBOX = "sandbox"
KEY_API_VERSION = "api_version"
KEY_OBJECT = "object"
KEY_QUERY_TYPE = "query_type_selector"
KEY_SOQL_QUERY = "soql_query"
KEY_IS_DELETED = "is_deleted"
KEY_FIELDS = "fields"

KEY_BUCKET_NAME = "bucket_name"
KEY_OUTPUT_TABLE_NAME = "output_table_name"

KEY_LOADING_OPTIONS = "loading_options"
KEY_LOADING_OPTIONS_INCREMENTAL = "incremental"
KEY_LOADING_OPTIONS_INCREMENTAL_FIELD = "incremental_field"
KEY_LOADING_OPTIONS_INCREMENTAL_FETCH = "incremental_fetch"
KEY_LOADING_OPTIONS_PKEY = "pkey"

# Proxy
KEY_PROXY = "proxy"
KEY_USE_PROXY = "use_proxy"
KEY_PROXY_SERVER = "proxy_server"
KEY_PROXY_PORT = "proxy_port"
KEY_PROXY_USERNAME = "username"
KEY_PROXY_PASSWORD = "#password"
KEY_USE_HTTP_PROXY_AS_HTTPS = "use_http_proxy_as_https"

RECORDS_NOT_FOUND = ["Records not found for this query"]

# list of mandatory parameters => if some is missing,
# component will fail with readable message on initialization.
REQUIRED_PARAMETERS = [[KEY_SOQL_QUERY, KEY_OBJECT]]
REQUIRED_IMAGE_PARS = []

DEFAULT_LOGIN_METHOD = "security_token"


class LoginType(str, Enum):
    SECURITY_TOKEN_LOGIN = "security_token"
    CONNECTED_APP_LOGIN = "connected_app"
    CONNECTED_APP_OAUTH_CC = "connected_app_oauth_cc"

    @classmethod
    def list(cls):
        return list(map(lambda c: c.value, cls))


def ordereddict_to_dict(value):
    if isinstance(value, OrderedDict):
        return {k: ordereddict_to_dict(v) for k, v in value.items()}
    elif isinstance(value, list):
        return [ordereddict_to_dict(item) for item in value]
    elif isinstance(value, dict):
        return {k: ordereddict_to_dict(v) for k, v in value.items()}
    else:
        return value


class Component(ComponentBase):
    def __init__(self):
        super().__init__()

    def run(self):
        self.validate_configuration_parameters(REQUIRED_PARAMETERS)
        self.validate_image_parameters(REQUIRED_IMAGE_PARS)

        start_run_time = str(datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S.000Z"))

        params = self.configuration.parameters
        loading_options = params.get(KEY_LOADING_OPTIONS, {})

        bucket_name = params.get(KEY_BUCKET_NAME, self.get_bucket_name())
        bucket_name = f"in.c-{bucket_name}"

        state_file = self.get_state_file()

        last_run = state_file.get("last_run")
        if not last_run:
            last_run = str(datetime(2000, 1, 1).strftime("%Y-%m-%dT%H:%M:%S.000Z"))

        prev_output_columns = state_file.get("prev_output_columns")

        pkey = loading_options.get(KEY_LOADING_OPTIONS_PKEY, [])
        incremental = bool(loading_options.get(KEY_LOADING_OPTIONS_INCREMENTAL))

        self.validate_incremental_settings(incremental, pkey)

        salesforce_client = self.get_salesforce_client(params)
        soql_query = self.build_soql_query(salesforce_client, params, last_run)
        self.validate_soql_query(soql_query, pkey)
        logging.info(f"Primary key : {pkey} set")

        table_name = loading_options.get(KEY_OUTPUT_TABLE_NAME, False) or soql_query.sf_object
        table = self.create_out_table_definition(
            table_name, incremental=incremental, is_sliced=True, destination=f"{bucket_name}.{table_name}"
        )
        self.create_sliced_directory(table.full_path)

        self._test_query(salesforce_client, soql_query, True)

        results = salesforce_client.download(soql_query, table.full_path)
        logging.info(f"Downloaded {len(results)} files")
        total_records = sum(result.get("number_of_records", 0) for result in results)
        logging.debug([result for result in results])
        logging.info(f"Downloaded {total_records} records in total")

        # remove headers and get columns
        output_columns = self._fix_header_from_csv(results)
        output_columns = self.normalize_column_names(output_columns)

        if not output_columns:
            if prev_output_columns:
                output_columns = prev_output_columns
            elif params.get(KEY_QUERY_TYPE) == "Object":
                output_columns = soql_query.sf_object_fields

        if output_columns:
            query_type = params.get(KEY_QUERY_TYPE)

            table.schema = self._get_schema(salesforce_client, query_type, soql_query.sf_object, output_columns, pkey)

            self.write_manifest(table)
            self.write_state_file({"last_run": start_run_time, "prev_output_columns": output_columns})
        else:
            shutil.rmtree(table.full_path)

    @staticmethod
    def _fix_header_from_csv(results: list[dict]) -> list[str]:
        expected_header = None
        for result in results:
            result_file_path = result.get("file")
            temp_file_path = f"{result_file_path}.tmp"
            with (
                open(result_file_path, "r", encoding="utf-8") as infile,
                open(temp_file_path, "w", newline="", encoding="utf-8") as outfile,
            ):
                reader = csv.reader(infile)
                writer = csv.writer(outfile)
                # check if header is same as in other files
                actual_header = next(reader)  # Also skip the header
                if expected_header:
                    if actual_header != expected_header:
                        raise UserException(
                            f"Header in file {result_file_path} is different from expected. "
                            f"Expected: {expected_header}, Actual: {actual_header}"
                        )
                else:
                    expected_header = actual_header
                for row in reader:
                    writer.writerow(row)
            os.replace(temp_file_path, result_file_path)
        return expected_header

    def set_proxy(self) -> None:
        """Sets proxy if defined"""
        proxy_config = self.configuration.parameters.get(KEY_PROXY, {})
        if proxy_config.get(KEY_USE_PROXY):
            self._set_proxy(proxy_config)

    def _set_proxy(self, proxy_config: dict) -> None:
        """
        Sets proxy using environmental variables.
        Also, a special case when http proxy is used for https is handled by using KEY_USE_HTTP_PROXY_AS_HTTPS.
        os.environ['HTTPS_PROXY'] = (username:password@)your.proxy.server.com(:port)
        """
        proxy_server = proxy_config.get(KEY_PROXY_SERVER)
        proxy_port = str(proxy_config.get(KEY_PROXY_PORT))
        proxy_username = proxy_config.get(KEY_PROXY_USERNAME)
        proxy_password = proxy_config.get(KEY_PROXY_PASSWORD)
        use_http_proxy_as_https = proxy_config.get(
            KEY_USE_HTTP_PROXY_AS_HTTPS
        ) or self.configuration.image_parameters.get(KEY_USE_HTTP_PROXY_AS_HTTPS)

        if not proxy_server:
            raise UserException("You have selected use_proxy parameter, but you have not specified proxy server.")
        if not proxy_port:
            raise UserException("You have selected use_proxy parameter, but you have not specified proxy port.")

        _proxy_credentials = f"{proxy_username}:{proxy_password}@" if proxy_username and proxy_password else ""
        _proxy_server = f"{_proxy_credentials}{proxy_server}:{proxy_port}"

        if use_http_proxy_as_https:
            # This is a case of http proxy which also supports https.
            _proxy_server = f"http://{_proxy_server}"
        else:
            _proxy_server = f"https://{_proxy_server}"

        os.environ["HTTPS_PROXY"] = _proxy_server

        logging.info("Component will use proxy server.")

    @staticmethod
    def _test_query(salesforce_client, soql_query, add_limit: bool = False):
        try:
            result = salesforce_client.test_query(soql_query=soql_query, add_limit=add_limit)
            return result
        except SalesforceClientException as e:
            raise UserException(e) from e

    @staticmethod
    def get_description(salesforce_client, sf_object):
        try:
            return salesforce_client.describe_object_w_complete_metadata(sf_object)
        except SalesforceClientException as salesforce_error:
            logging.error(f"Cannot fetch metadata for object {sf_object}: {salesforce_error}")
            return None

    def _add_columns_to_table_metadata(self, tm, description, output_columns):
        for item in description["fields"]:
            if item.get("name", "") in output_columns:
                column_name = str(item["name"])
                column_type = str(item["type"])
                nullable = item["nillable"]
                default = item["defaultValue"]
                label = item["label"]

                tm.add_column_data_type(
                    column=column_name,
                    data_type=self.convert_to_kbc_basetype(column_type),
                    source_data_type=column_type,
                    nullable=nullable,
                    default=default,
                )

                # The following is disabled since it caused exceeded metadata size
                # tm.add_column_metadata(column_name, "source_metadata", json.dumps(item))

                tm.add_column_descriptions({column_name: label})

    @staticmethod
    def add_table_metadata(tm, description):
        def recursive_flatten(prefix, nested_value):
            if isinstance(nested_value, dict):
                for k, v in nested_value.items():
                    recursive_flatten(f"{prefix}_{k}", v)
            elif isinstance(nested_value, list):
                for i, v in enumerate(nested_value):
                    recursive_flatten(f"{prefix}_{i}", v)
            else:
                tm.add_table_metadata(prefix, str(nested_value))

        table_md = {str(k): v for k, v in description.items() if k != "fields"}
        for key, value in table_md.items():
            value = ordereddict_to_dict(value)
            recursive_flatten(key, value)

    def _get_schema(self, salesforce_client, query_type, sf_object, output_columns, pkey):
        if query_type == "Object":
            fields_all = self.get_description(salesforce_client, sf_object).get("fields")
            fields = {field["name"]: field for field in fields_all if field["name"] in output_columns}
        else:
            fields = []

        schema = OrderedDict()
        for col in output_columns:
            if col in fields:
                field = fields[col]
                schema[col] = ColumnDefinition(
                    data_types=BaseType(dtype=self.convert_to_kbc_basetype(field.get("type"))),
                    nullable=field.get("nillable"),
                    description=field.get("label"),
                    primary_key=col in pkey,
                )
            else:
                schema[col] = ColumnDefinition(
                    data_types=BaseType(dtype=SupportedDataTypes.STRING), primary_key=col in pkey
                )

        return schema

    @staticmethod
    def convert_to_kbc_basetype(source_type: str) -> SupportedDataTypes:
        dtypes_mapping = {
            "id": SupportedDataTypes.STRING,
            "boolean": SupportedDataTypes.BOOLEAN,
            "reference": SupportedDataTypes.STRING,
            "string": SupportedDataTypes.STRING,
            "picklist": SupportedDataTypes.STRING,
            "textarea": SupportedDataTypes.STRING,
            "double": SupportedDataTypes.FLOAT,
            "phone": SupportedDataTypes.STRING,
            "email": SupportedDataTypes.STRING,
            "date": SupportedDataTypes.DATE,
            "datetime": SupportedDataTypes.TIMESTAMP,
            "url": SupportedDataTypes.STRING,
            "int": SupportedDataTypes.INTEGER,
            "currency": SupportedDataTypes.STRING,
            "multipicklist": SupportedDataTypes.STRING,
        }

        if source_type in dtypes_mapping:
            return SupportedDataTypes[dtypes_mapping[source_type]].value
        else:
            logging.warning(f"Unknown source type: {source_type}. Casting it to STRING.")
            return SupportedDataTypes.STRING

    @staticmethod
    def validate_incremental_settings(incremental: bool, pkey: list[str]) -> None:
        if incremental and not pkey:
            raise UserException(
                "Incremental load is set but no private key. Specify a private key in the configuration parameters"
            )

    @staticmethod
    def validate_soql_query(soql_query: SoqlQuery, pkey: list[str]) -> None:
        logging.info("Validating SOQL query")
        missing_keys = soql_query.check_pkey_in_query(pkey)
        if missing_keys:
            raise UserException(
                f"Private Keys {missing_keys} not in query, Add to SOQL query or check that it exists"
                f" in the Salesforce object."
            )

    def get_salesforce_client(self, params: dict) -> SalesforceClient:
        self.set_proxy()
        try:
            logging.info("Logging in to Salesforce")
            return self._login_to_salesforce(params)
        except SalesforceAuthenticationFailed as e:
            raise UserException(f"Authentication Failed : recheck your authorization parameters : {e}") from e

    @retry(SalesforceAuthenticationFailed, tries=3, delay=5)
    def _login_to_salesforce(self, params: dict) -> SalesforceClient:
        login_method = self._get_login_method()

        if login_method == LoginType.SECURITY_TOKEN_LOGIN:
            if not params.get(KEY_SECURITY_TOKEN):
                raise UserException(
                    "Missing Required Parameter: Security Token. It is required when using Security Token Login"
                )

            if not params.get(KEY_USERNAME) or not params.get(KEY_PASSWORD):
                raise UserException(
                    "Missing Required Parameter: Both username and password are required for Security Token Login."
                )

            return SalesforceClient.from_security_token(
                username=params.get(KEY_USERNAME),
                password=params.get(KEY_PASSWORD),
                security_token=params.get(KEY_SECURITY_TOKEN),
                sandbox=params.get(KEY_SANDBOX),
                api_version=params.get(KEY_API_VERSION, DEFAULT_API_VERSION),
            )

        elif login_method == LoginType.CONNECTED_APP_LOGIN:
            if not params.get(KEY_CONSUMER_KEY) or not params.get(KEY_CONSUMER_SECRET):
                raise UserException(
                    "Missing Required Parameter: At least one of Consumer Key and Consumer Secret "
                    "are missing. They are both required when using Connected App Login"
                )

            if not params.get(KEY_USERNAME) or not params.get(KEY_PASSWORD):
                raise UserException(
                    "Missing Required Parameter: Both username and password are required for Connected App Login."
                )

            return SalesforceClient.from_connected_app(
                username=params.get(KEY_USERNAME),
                password=params.get(KEY_PASSWORD),
                consumer_key=params.get(KEY_CONSUMER_KEY),
                consumer_secret=params.get(KEY_CONSUMER_SECRET),
                sandbox=params.get(KEY_SANDBOX),
                api_version=params.get(KEY_API_VERSION, DEFAULT_API_VERSION),
            )

        elif login_method == LoginType.CONNECTED_APP_OAUTH_CC:
            if not params.get(KEY_CONSUMER_KEY) or not params.get(KEY_CONSUMER_SECRET):
                raise UserException(
                    "Missing Required Parameter: At least one of Consumer Key and Consumer Secret "
                    "are missing. They are both required when using Connected App Login"
                )

            if not (domain := params.get(KEY_DOMAIN)):
                raise UserException("Parameter 'domain' is needed for Client Credentials Flow. ")
            domain = self.process_salesforce_domain(domain)

            return SalesforceClient.from_connected_app_oauth_cc(
                consumer_key=params[KEY_CONSUMER_KEY],
                consumer_secret=params[KEY_CONSUMER_SECRET],
                api_version=params.get(KEY_API_VERSION, DEFAULT_API_VERSION),
                domain=domain,
            )

    def _get_login_method(self) -> LoginType:
        login_type_name = self.configuration.parameters.get(KEY_LOGIN_METHOD, DEFAULT_LOGIN_METHOD)
        try:
            return LoginType(login_type_name)
        except ValueError as val_err:
            raise UserException(
                f"'{login_type_name}' is not a valid Login Type. Enter one of : {LoginType.list()}"
            ) from val_err

    @staticmethod
    def process_salesforce_domain(url):
        if url.startswith("http://"):
            url = url[len("http://"):]
        if url.startswith("https://"):
            url = url[len("https://"):]
        if url.endswith(".salesforce.com"):
            url = url[: -len(".salesforce.com")]

        logging.debug(f"The component will use {url} for Client Credentials Flow type authentication.")

        return url

    @staticmethod
    def create_sliced_directory(table_path: str) -> None:
        logging.info("Creating sliced directory")
        if not path.isdir(table_path):
            mkdir(table_path)

    def build_soql_query(self, salesforce_client: SalesforceClient, params: dict, last_run: str = "") -> SoqlQuery:
        try:
            logging.info("Building SOQL query")
            return self._build_soql_query(salesforce_client, params, last_run)
        except (ValueError, TypeError) as query_error:
            raise UserException(query_error) from query_error

    @staticmethod
    def _build_soql_query(salesforce_client: SalesforceClient, params: dict, last_run: str) -> SoqlQuery:
        loading_options = params.get(KEY_LOADING_OPTIONS, {})
        salesforce_object = params.get(KEY_OBJECT)
        soql_query_string = params.get(KEY_SOQL_QUERY)
        incremental = loading_options.get(KEY_LOADING_OPTIONS_INCREMENTAL, False)
        incremental_field = loading_options.get(KEY_LOADING_OPTIONS_INCREMENTAL_FIELD)
        incremental_fetch = loading_options.get(KEY_LOADING_OPTIONS_INCREMENTAL_FETCH)
        is_deleted = params.get(KEY_IS_DELETED, False)
        query_type = params.get(KEY_QUERY_TYPE)
        fields = params.get(KEY_FIELDS, None)

        if query_type == "Custom SOQL":
            try:
                soql_query = salesforce_client.build_query_from_string(soql_query_string)
            except SalesforceResourceNotFound as salesforce_error:
                raise UserException(f"Custom SOQL could not be built : {salesforce_error}") from salesforce_error
            except SalesforceClientException as salesforce_error:
                raise UserException(
                    f"Cannot get Salesforce object description, error: {salesforce_error}"
                ) from salesforce_error

        elif query_type == "Object":
            try:
                soql_query = salesforce_client.build_soql_query_from_object_name(salesforce_object, fields)
                logging.info(f"Downloading salesforce object: {salesforce_object}.")

            except SalesforceResourceNotFound as salesforce_error:
                raise UserException(
                    f"Object type {salesforce_object} does not exist in Salesforce, enter a valid object"
                ) from salesforce_error
            except SalesforceClientException as salesforce_error:
                error_message = str(salesforce_error)
                if "INVALID_OPERATION_WITH_EXPIRED_PASSWORD" in error_message:
                    custom_message = (
                        "Your password has expired. Please reset your password or contact your "
                        "Salesforce Admin to reset the password and use the new one. You can also set "
                        "your Salesforce user for a password that never expires in the Password "
                        "Policies: https://help.salesforce.com/s/articleView?id=sf.admin_password.htm"
                        "&type=5. Please note that when the password is changed, "
                        "a new security token is generated."
                    )
                else:
                    custom_message = error_message
                raise UserException(custom_message) from salesforce_error

        else:
            raise UserException(f"Either {KEY_SOQL_QUERY} or {KEY_OBJECT} parameters must be specified.")

        if incremental and incremental_fetch and incremental_field and last_run:
            soql_query.set_query_to_incremental(incremental_field, last_run)
        elif incremental and incremental_fetch and not incremental_field:
            raise UserException(
                "Incremental field is not specified, if you want to use incremental fetching, it must specified."
            )

        soql_query.set_deleted_option_in_query(is_deleted)

        return soql_query

    @staticmethod
    def normalize_column_names(output_columns: list[str]) -> list[str]:
        header_normalizer = get_normalizer(strategy=NormalizerStrategy.DEFAULT, forbidden_sub="_")
        return header_normalizer.normalize_header(output_columns)

    def get_bucket_name(self) -> str:
        config_id = self.environment_variables.config_id
        if not config_id:
            config_id = "000000000"
        bucket_name = f"kds-team-ex-salesforce-v2-{config_id}"
        return bucket_name

    @sync_action("testConnection")
    def test_connection(self):
        """
        Tries to log into Salesforce, raises user exception if login params ar incorrect

        """
        params = self.configuration.parameters
        self.get_salesforce_client(params)

    @staticmethod
    def create_markdown_table(data):
        if not data:
            return ""
        headers = list(data[0].keys())
        table = "| " + " | ".join(headers) + " |\n"
        table += "| " + " | ".join(["---"] * len(headers)) + " |\n"
        for row in data:
            row_values = [str(row[header]) for header in headers]
            table += "| " + " | ".join(row_values) + " |\n"
        return table

    @staticmethod
    def parse_result(data: OrderedDict) -> dict:
        return dict((k, v) for k, v in data.items() if k != "attributes")

    @sync_action("testQuery")
    def test_query(self):
        params = self.configuration.parameters
        salesforce_client = self.get_salesforce_client(params)
        soql_query_string = params.get(KEY_SOQL_QUERY)
        soql_query = salesforce_client.build_query_from_string(soql_query_string)
        self.validate_soql_query(soql_query, [])
        data = []
        try:
            result = self._test_query(salesforce_client, soql_query, False)
            if not result:
                return ValidationResult("Query returned no results", MessageType.WARNING)
            for _, result in enumerate(result.get("records", [])):
                data.append(self.parse_result(result))
            markdown = self.create_markdown_table(data)
            return ValidationResult(markdown, "table")
        except UserException as e:
            return ValidationResult(f"Query Failed: {e}", MessageType.WARNING)

    @sync_action("loadObjects")
    def load_possible_objects(self) -> list[SelectElement]:
        """
        Finds all possible objects in Salesforce that can be fetched by the Bulk API

        Returns: a List of dictionaries containing 'name' and 'value' of the SF object, where 'name' is the Label name/
        readable name of the object, and 'value' is the name of the object you can use to query the object

        """
        params = self.configuration.parameters
        salesforce_client = self.get_salesforce_client(params)
        return [SelectElement(**c) for c in salesforce_client.get_bulk_fetchable_objects()]

    @sync_action("loadFields")
    def load_fields(self) -> list[SelectElement]:
        """Returns fields available for selected object."""
        params = self.configuration.parameters
        object_name = params.get("object")
        salesforce_client = self.get_salesforce_client(params)
        descriptions = salesforce_client.describe_object_w_metadata(object_name)
        return [SelectElement(label=f"{field[0]} ({field[1]})", value=field[0]) for field in descriptions]

    @sync_action("loadPossibleIncrementalField")
    def load_possible_incremental_field(self) -> list[SelectElement]:
        """
        Gets all possible fields of a Salesforce object. It determines the name of the SF object either from the input
        object name or from the SOQL query. This data is used to select an incremental field

        Returns: a List of dictionaries containing 'name' and 'value' of each field of the SF object,
        where 'name' is the Label name/ readable name of the field, and 'value' is the exact name of the field that can
        be used in an SOQL query

        """
        params = self.configuration.parameters
        if params.get(KEY_QUERY_TYPE) == "Custom SOQL":
            object_name = self._get_object_name_from_custom_query()
        else:
            object_name = params.get(KEY_OBJECT)
        return self._get_object_fields_names_and_values(object_name)

    @sync_action("loadPossiblePrimaryKeys")
    def load_possible_primary_keys(self) -> list[SelectElement]:
        """
        Gets all possible primary keys of the data returned of a saleforce object. If the exact object is specified,
        each field is returned, if a query is specified, it is run with LIMIT 1 and the returned data is analyzed to
        determine the fieldnames of the final table.

        Returns: a List of dictionaries containing 'name' and 'value' of each field of the SF object or each field that
        is returned by a custom SOQL query. 'name' is the Label name/ readable name of the field,
        and 'value' is the name of the field in storage

        """
        params = self.configuration.parameters
        if params.get(KEY_QUERY_TYPE) == "Custom SOQL":
            return self._get_object_fields_from_query()
        elif params.get(KEY_QUERY_TYPE) == "Object":
            object_name = params.get(KEY_OBJECT)
            return self._get_object_fields_names_and_normalized_values(object_name)
        else:
            raise UserException(f"Invalid {KEY_QUERY_TYPE}")

    def _get_object_name_from_custom_query(self) -> str:
        params = self.configuration.parameters
        salesforce_client = self.get_salesforce_client(params)
        query = self.build_soql_query(salesforce_client, params)
        return query.sf_object

    def _get_object_fields_names_and_normalized_values(self, object_name: str) -> list[SelectElement]:
        """
        Return object fields for sync action

        Args:
            object_name:

        Returns:

        """
        columns = self._get_fields_of_object_by_name(object_name)
        column_values = self.normalize_column_names(columns)
        return [SelectElement(label=column, value=column_values[i]) for i, column in enumerate(columns)]

    def _get_object_fields_names_and_values(self, object_name: str) -> list[SelectElement]:
        columns = self._get_fields_of_object_by_name(object_name)
        return [SelectElement(label=column, value=column) for column in columns]

    def _get_fields_of_object_by_name(self, object_name: str) -> list[str]:
        params = self.configuration.parameters
        salesforce_client = self.get_salesforce_client(params)
        return salesforce_client.describe_object(object_name)

    def _get_object_fields_from_query(self) -> list[SelectElement]:
        """
        Return object fields for sync action
        Returns:

        """
        result = self._get_first_result_from_custom_soql()

        if not result:
            raise UserException(
                "Failed to determine fields from SOQL query, "
                "make sure the SOQL query is valid and that it returns data."
            )

        columns = list(result.keys())
        if "attributes" in columns:
            columns.remove("attributes")
        column_values = self.normalize_column_names(columns)

        return [SelectElement(label=column, value=column_values[i]) for i, column in enumerate(columns)]

    def _get_first_result_from_custom_soql(self) -> dict:
        params = self.configuration.parameters
        salesforce_client = self.get_salesforce_client(params)
        query = params.get(KEY_SOQL_QUERY)
        if " limit " not in query.lower():
            query = f"{query} LIMIT 1"
        else:
            # LIMIT statement should always come at the end
            limit_location = query.lower().find(" limit ")
            query = f"{query[:limit_location]} LIMIT 1"
        try:
            result = salesforce_client.simple_client.query(query)
        except SalesforceError as e:
            raise UserException("Failed to determine fields from SOQL query, make sure the SOQL query is valid") from e

        return result.get("records")[0] if result.get("totalSize") == 1 else None


if __name__ == "__main__":
    try:
        comp = Component()
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)



================================================
FILE: src/salesforce/__init__.py
================================================
[Empty file]


================================================
FILE: src/salesforce/client.py
================================================
import copy
import logging
import os
from collections import OrderedDict
from typing import Any, Iterator
from urllib.parse import urlparse

import backoff
from keboola.http_client import HttpClient
from simple_salesforce.api import Salesforce, SFType
from simple_salesforce.bulk2 import ColumnDelimiter, LineEnding, Operation, QueryResult, SFBulk2Type
from simple_salesforce.exceptions import SalesforceBulkV2LoadError, SalesforceExpiredSession, SalesforceMalformedRequest

from .soql_query import SoqlQuery

NON_SUPPORTED_BULK_FIELD_TYPES = ["address", "location", "base64"]

# Some objects are not supported by bulk and there is no exact way to determine them, they must be set like this
# https://help.salesforce.com/s/articleView?id=000383508&type=1
OBJECTS_NOT_SUPPORTED_BY_BULK = ["AccountFeed", "AssetFeed", "AccountHistory", "AcceptedEventRelation",
                                 "DeclinedEventRelation", "AggregateResult", "AttachedContentDocument", "CaseStatus",
                                 "CaseTeamMember", "CaseTeamRole", "CaseTeamTemplate", "CaseTeamTemplateMember",
                                 "CaseTeamTemplateRecord", "CombinedAttachment", "ContentFolderItem", "ContractStatus",
                                 "EventWhoRelation", "FolderedContentDocument", "KnowledgeArticleViewStat",
                                 "KnowledgeArticleVoteStat", "LookedUpFromActivity", "Name", "NoteAndAttachment",
                                 "OpenActivity", "OwnedContentDocument", "PartnerRole", "RecentlyViewed",
                                 "ServiceAppointmentStatus", "SolutionStatus", "TaskPriority", "TaskStatus",
                                 "TaskWhoRelation", "UserRecordAccess", "WorkOrderLineItemStatus", "WorkOrderStatus"]

DEFAULT_QUERY_PAGE_SIZE = 50000

# default as previous versions of this component ex-salesforce-v2 had 40.0
DEFAULT_API_VERSION = "52.0"
MAX_RETRIES = 3


class SalesforceClientException(Exception):
    pass


class SalesforceBulk2(SFBulk2Type):
    def __init__(self, sf_client, object_name: str):
        super().__init__(object_name, sf_client.bulk2_url, sf_client.headers, sf_client.session)

    def download(self,
                 query: str,
                 path: str,
                 max_records: int = DEFAULT_QUERY_PAGE_SIZE,
                 column_delimiter: ColumnDelimiter = ColumnDelimiter.COMMA,
                 line_ending: LineEnding = LineEnding.LF,
                 wait: int = 5, ) -> list[QueryResult]:

        if not os.path.exists(path):
            raise SalesforceBulkV2LoadError(f"Path does not exist: {path}")

        res = self._client.create_job(Operation.query_all, query, column_delimiter, line_ending)
        job_id = res["id"]
        self._client.wait_for_job(job_id, True, wait)

        results = []
        locator = "INIT"
        while locator:
            if locator == "INIT":
                locator = ""
            result = self._client.download_job_data(path, job_id, locator, max_records)
            locator = result["locator"]
            results.append(result)
        return results


class SalesforceClient(HttpClient):
    def __init__(self, simple_client: Salesforce, api_version: str,
                 consumer_key: str = None, consumer_secret: str = None) -> None:
        # Initialize the client with from_connected_app or from_security_token, this creates a login with the
        # simple salesforce client. The simple_client sessionId is a Bearer token that is result of the login.
        super().__init__('NONE', max_retries=MAX_RETRIES)
        self._consumer_key = consumer_key
        self._consumer_secret = consumer_secret
        self.simple_client = simple_client
        self.api_version = api_version
        self.host = urlparse(self.simple_client.base_url).hostname
        self.sessionId = self.simple_client.session_id

    @classmethod
    def from_connected_app(cls, username: str, password: str, consumer_key: str, consumer_secret: str, sandbox: str,
                           api_version: str = DEFAULT_API_VERSION, domain: str = None):
        domain = 'test' if sandbox else domain

        simple_client = Salesforce(username=username, password=password, consumer_secret=consumer_secret,
                                   consumer_key=consumer_key,
                                   domain=domain, version=api_version)

        return cls(simple_client=simple_client, api_version=api_version)

    @classmethod
    def from_security_token(cls, username: str, password: str, security_token: str, sandbox: str, api_version: str,
                            domain: str = None):

        domain = 'test' if sandbox else domain
        simple_client = Salesforce(username=username, password=password, security_token=security_token,
                                   domain=domain, version=api_version)

        return cls(simple_client=simple_client, api_version=api_version)

    @classmethod
    def from_connected_app_oauth_cc(cls, consumer_key: str, consumer_secret: str, domain: str, api_version: str):

        simple_client = Salesforce(consumer_key=consumer_key, consumer_secret=consumer_secret, domain=domain,
                                   version=api_version)

        return cls(simple_client=simple_client, api_version=api_version)

    @backoff.on_exception(backoff.expo, SalesforceClientException, max_tries=3)
    def describe_object(self, sf_object: str) -> list[str]:
        salesforce_type = SFType(sf_object, self.sessionId, self.host, sf_version=self.api_version)

        try:
            object_desc = salesforce_type.describe()
        except ConnectionError as e:
            raise SalesforceClientException(f"Cannot get SalesForce object description, error: {e}.") from e

        return [field['name'] for field in object_desc['fields'] if self.is_bulk_supported_field(field)]

    @backoff.on_exception(backoff.expo, SalesforceClientException, max_tries=3)
    def describe_object_w_metadata(self, sf_object: str) -> list[tuple[str, str]]:
        salesforce_type = SFType(sf_object, self.sessionId, self.host, sf_version=self.api_version)

        try:
            object_desc = salesforce_type.describe()
        except ConnectionError as e:
            raise SalesforceClientException(f"Cannot get SalesForce object description, error: {e}.") from e

        return [(field['name'], field['type']) for field in object_desc['fields']
                if self.is_bulk_supported_field(field)]

    @backoff.on_exception(backoff.expo, SalesforceClientException, max_tries=3)
    def describe_object_w_complete_metadata(self, sf_object: str) -> dict[str, Any]:
        salesforce_type = SFType(sf_object, self.sessionId, self.host, sf_version=self.api_version)

        try:
            object_desc = salesforce_type.describe()
        except ConnectionError as e:
            raise SalesforceClientException(f"Cannot get SalesForce object description, error: {e}.") from e

        return object_desc

    @staticmethod
    def is_bulk_supported_field(field: OrderedDict) -> bool:
        return field["type"] not in NON_SUPPORTED_BULK_FIELD_TYPES

    def download(self, soql_query: SoqlQuery, path: str, fail_on_error: bool = False,
                 query_page_size: int = DEFAULT_QUERY_PAGE_SIZE) -> list[QueryResult]:
        try:
            bulk2 = SalesforceBulk2(self.simple_client, soql_query.sf_object)

            logging.info(f"Running SOQL : {soql_query.query}")
            query_results = bulk2.download(soql_query.query, path, max_records=query_page_size)
            logging.info("SOQL ran successfully")

            return query_results
        except SalesforceBulkV2LoadError as e:
            if fail_on_error:
                raise SalesforceClientException(e)
            logging.exception(e)

    def test_query(self, soql_query: SoqlQuery, add_limit: bool = False) -> Iterator:
        """Test query has been implemented to prevent long timeouts of batched queries."""
        test_query = copy.deepcopy(soql_query)
        if add_limit:
            test_query.add_limit()
        try:
            logging.info("Running test SOQL.")
            result = self.simple_client.query(test_query.query)
        except (SalesforceMalformedRequest, SalesforceClientException):
            raise SalesforceClientException(f"Test Query {test_query.query} failed, please re-check the query.")

        logging.info("Test query has been successful.")
        return result

    def build_query_from_string(self, soql_query_string: str) -> SoqlQuery:
        try:
            soql_query = SoqlQuery.build_from_query_string(soql_query_string, self.describe_object)
        except SalesforceExpiredSession as expired_error:
            raise SalesforceClientException(expired_error) from expired_error
        return soql_query

    def build_soql_query_from_object_name(self, sf_object: str, fields: list = None) -> SoqlQuery:
        sf_object = sf_object.strip()
        try:
            soql_query = SoqlQuery.build_from_object(sf_object, self.describe_object, fields=fields)
        except SalesforceExpiredSession as expired_error:
            raise SalesforceClientException(expired_error) from expired_error
        except ValueError as e:
            raise SalesforceClientException(e) from e
        return soql_query

    def get_bulk_fetchable_objects(self):
        all_s_objects = self.simple_client.describe()["sobjects"]
        to_fetch = []
        # Only objects with the 'queryable' set to True and ones that are not in the OBJECTS_NOT_SUPPORTED_BY_BULK are
        # queryable by the Bulk API. This list might not be exact, and some edge-cases might have to be addressed.
        for sf_object in all_s_objects:
            if sf_object.get("queryable") and sf_object.get("name") not in OBJECTS_NOT_SUPPORTED_BY_BULK:
                to_fetch.append({"label": sf_object.get("label"), "value": sf_object.get("name")})
        return to_fetch



================================================
FILE: src/salesforce/soql_query.py
================================================
import logging
import re
from enum import Enum
from typing import Callable


class QueryType(Enum):
    GET = "get"


class SoqlQuery:
    def __init__(self, query: str, sf_object: str, sf_object_fields: list[str], query_type="get"):
        self.sf_object_fields = sf_object_fields
        self.sf_object = sf_object
        self.query = query
        self.query_type = QueryType(query_type)
        self.check_query(self.query)

    def __str__(self) -> str:
        return self.query

    @classmethod
    def build_from_object(cls, sf_object: str, describe_object_method: Callable, query_type="get",
                          fields: list = None) -> 'SoqlQuery':
        sf_object_fields = describe_object_method(sf_object)
        if fields:
            invalid_fields = [field for field in fields if field not in sf_object_fields]
            if invalid_fields:
                raise ValueError(
                    f"The following field(s) are not available for the '{sf_object}' "
                    f"object: {', '.join(invalid_fields)}")
            sf_object_fields = [field for field in fields if field in sf_object_fields]
        query = cls._construct_soql_from_fields(sf_object, sf_object_fields)
        return SoqlQuery(query, sf_object, sf_object_fields, query_type)

    @classmethod
    def build_from_query_string(cls, query_string: str,
                                describe_object_method: Callable, query_type="get") -> 'SoqlQuery':
        SoqlQuery.check_query(query_string)
        sf_object = cls._get_object_from_query(query_string)
        sf_object_fields = describe_object_method(sf_object)
        return SoqlQuery(query_string, sf_object, sf_object_fields, query_type)

    @staticmethod
    def _list_to_lower(str_list: list[str]) -> list[str]:
        return [x.lower() for x in str_list]

    @staticmethod
    def _construct_soql_from_fields(sf_object: str, sf_object_fields: list[str]) -> str:
        soql_query = f"SELECT {','.join(sf_object_fields)} FROM {sf_object}"
        return soql_query

    @staticmethod
    def _get_object_from_query(query):
        # remove strings within brackets
        query_no_brackets = re.sub("\\(.*?\\)", "", query)
        # list words in query
        word_list = query_no_brackets.lower().split()
        # Only 1 from should exist
        from_index = word_list.index("from")
        #  object name is 1 word after the "from"
        object_name = word_list[from_index + 1]
        # remove non alphanumeric from objectname
        object_name = re.sub(r'\W+', '', object_name)
        return object_name

    @staticmethod
    def check_query(query: str) -> None:
        if not isinstance(query, str):
            raise ValueError("SOQL query must be a single string")
        query_words = query.lower().split()
        if "select" not in query_words:
            raise ValueError("SOQL query must contain SELECT")
        if "from" not in query_words:
            raise ValueError("SOQL query must contain FROM")
        if "offset" in query_words:
            raise ValueError("SOQL bulk queries do not support OFFSET clauses")
        if "typeof" in query_words:
            raise ValueError("SOQL bulk queries do not support TYPEOF clauses")

    def set_query_to_incremental(self, incremental_field: str, continue_from_value: str) -> None:
        if incremental_field.lower() in self._list_to_lower(self.sf_object_fields):
            incremental_string = f" WHERE {incremental_field} >= {continue_from_value}"
        else:
            raise ValueError(f"Field {incremental_field} is not present in the {self.sf_object} object ")

        self.query = self._add_to_where_clause(self.query, incremental_string)

    def set_deleted_option_in_query(self, deleted: bool) -> None:
        if not deleted and "isdeleted" in self._list_to_lower(self.sf_object_fields):
            is_deleted_string = " WHERE IsDeleted = false "
            self.query = self._add_to_where_clause(self.query, is_deleted_string)
        elif deleted and "isdeleted" not in self._list_to_lower(self.sf_object_fields):
            logging.warning(f"Waring: IsDeleted is not a field in the {self.sf_object} object, cannot fetch deleted "
                            f"records")

    @staticmethod
    def _add_to_where_clause(soql: str, new_where_string: str) -> str:
        and_string = " and "

        match = re.search(r'\bwhere\b', soql, re.IGNORECASE)

        if match:
            where_location_start = match.start()
            where_location_end = match.end()
            before_where = soql[:where_location_start]
            after_where = soql[where_location_end:]
            new_query = "".join([before_where, new_where_string, and_string, after_where])
        else:
            new_query = "".join([soql, new_where_string])
        return new_query

    def check_pkey_in_query(self, pkeys: list[str]) -> list:
        missing_keys = []
        # split a string by space, comma, and period characters
        query_words = re.split("\\s|(?<!\\d)[,.](?!\\d)", self.query.lower())
        for pkey in pkeys:
            if pkey.lower() not in query_words:
                missing_keys.append(pkey)
        return missing_keys

    def add_limit(self, limit: int = 1) -> None:
        """
        This method adds LIMIT 10 clause to the SOQL query.
        """
        if "limit" not in self.query.lower():
            self.query = self.query + f" LIMIT {limit}"
        else:
            logging.warning("The SOQL query already contains a LIMIT clause. Ignoring add_limit request.")



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
FILE: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


