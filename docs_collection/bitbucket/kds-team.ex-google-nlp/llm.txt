Directory structure:
└── kds_consulting_team-kds-team.ex-google-nlp/
    ├── README.md
    ├── bitbucket-pipelines.yml
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── example/
    │   └── sample-response.json
    ├── scripts/
    │   ├── build_n_test.sh
    │   ├── run.sh
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── client.py
    │   ├── component.py
    │   └── result.py
    └── tests/
        ├── __init__.py
        └── test_component.py

================================================
FILE: README.md
================================================
# Google NLP

## Overview

The Google Natual Language API allows users to utilize powerful pre-trained NLP models to understand various language features such as sentiment, entities and mentions, content and syntax. The component utilizes the API to send text features and retrieves desired analysis.

The Cloud Natural Language API is a paid service maintained by Google and is subject to [Google Cloud's Terms & Conditions](https://cloud.google.com/terms/). 

The information about pricing can be found on [Google's support forums](https://cloud.google.com/natural-language/pricing). The API currently supports [these methods and languages](https://cloud.google.com/natural-language/docs/languages).

#### Useful links

If you are new to the natural language processing or are unsure about the meaning of outputted values, the following guides might clear up some of the confusion:

- [Natual Language API Basics](https://cloud.google.com/natural-language/docs/basics)
- [Morphology & Dependency Trees](https://cloud.google.com/natural-language/docs/morphology)
- [HTTP status and error codes for JSON](https://cloud.google.com/storage/docs/json_api/v1/status-codes)

## Requirements

The component requires valid Google Cloud API token with Natural Language API allowed. The API token is subject to [limits](https://cloud.google.com/natural-language/quotas) thus it is important to set the correct daily limits. To overcome the 100 second limits, the application uses exponential backoff with 10 retries. In case, the retries are unsuccessful (i.e. daily limit is reached), the component fails.

## Supported languages & methods

The component supports all of the [languages supported by the API](https://cloud.google.com/natural-language/docs/languages). If the (detected) language isn't supported, the request will be cancelled, the document is skipped and no output is produced for it. All of the language errors are outputted to `errors` table.

The component currently supports following methods:

- `analyzeEntities` - entity analysis,
- `analyzeEntitySentiment` - entity sentiment analysis,
- `analyzeSentiment` - sentiment analysis,
- `analyzeSyntax` - syntactic analysis,
- `classifyText` - content classification.

Note: The component uses API v1. The list of all methods supported by the version is listed in [API reference](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents). Mind that, `annotateText` method is a wrapper for all other methods listed above and does not provide any new information. It is used in the component to bundle [all feature requests into a single API call](https://cloud.google.com/natural-language/docs/basics/#text-annotations).

If a new method is available and you'd like to see it, contact us at [support@keboola.com](mailto:support@keboola.com) or via support ticket.

---

#### Entity analysis (`analyzeEntities`)

The method inspects the document for known entities and returns [information about the entities](https://cloud.google.com/natural-language/docs/basics/#entity_analysis), such as their type, salience and mentions in the text. Moreover, if any metadata (e.g. address, phone number, famous person) is identified, the information on the subject is returned as well (e.g. full address, country code for the phone number, wikipedia article about the person). For more information about metadata, see the [`Entity` type documentation](https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity#type).

For each entity, a list of mentions is returned. The mentions list is always of length at least 1, i.e. each entity has at least one mention.

###### reference: [analyzeEntities](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeEntities), [entity](https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity)

---

#### Entity sentiment analysis (`analyzeEntitySentiment`)

The method returns the same analysis type as `analyzeEntities` method but adds sentiment analysis for each entity and mention in the text, thus [combining both entity and sentiment analysis](https://cloud.google.com/natural-language/docs/basics/#entity-sentiment-analysis). Note that, if both `analyzeEntities` and `analyzeEntitySentiment` methods are used, you will not be charged for both methods; only `analyzeEntitySentiment` will be utilized and billed.

###### reference: [analyzeEntitySentiment](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeEntitySentiment), [entity](https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity)

---

#### Sentiment analysis (`analyzeSentiment`)

The `analyzeSentiment` method inspects the document for [emotional opinion present within the text](https://cloud.google.com/natural-language/docs/basics/#sentiment_analysis) and magnitude of the opinion. The result is the overall attitude of the document (positive, neutral or negative) and of each sentence present within the document. Score and magnitude of the document and sentences can be interpreted according to [this guide](https://cloud.google.com/natural-language/docs/basics/#interpreting_sentiment_analysis_values).

###### reference: [analyzeSentiment](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeSentiment), [sentiment](https://cloud.google.com/natural-language/docs/reference/rest/v1/Sentiment)

---

#### Syntactic analysis (`analyzeSyntax`)

The syntactic analysis breaks up the documents into [sentences and extracts tokens (words)](https://cloud.google.com/natural-language/docs/basics/#syntactic_analysis) from the document. For each token, information about lemma, [part of the speech](https://cloud.google.com/natural-language/docs/reference/rest/v1/Token/#PartOfSpeech) and [depencency index](https://cloud.google.com/natural-language/docs/reference/rest/v1/Token/#DependencyEdge) is added using Cloud Natural Language API. For more information about how to interpret the values, refer to [Syntacting analysis](https://cloud.google.com/natural-language/docs/basics/#syntactic_analysis) guide and [Morphology & Dependency Trees](https://cloud.google.com/natural-language/docs/morphology) guide.

###### reference: [analyzeSyntax](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeSyntax), [token](https://cloud.google.com/natural-language/docs/reference/rest/v1/Token)

---

#### Content classification (`classifyText`)

The `classifyText` method analyzes the document and returns a list of [categories that apply to the text](https://cloud.google.com/natural-language/docs/basics/#content-classification) found in the document. For each category, a confidence level is provided as well as the name of the category. A complete list of all available categories can be found in [Categories section](https://cloud.google.com/natural-language/docs/categories) of API documentation. 

For successful content classification, the document needs to be of certain length (~ 20 words) and must not be too abstract. If the two conditions are not met, no categories are returned.

###### reference: [classifyText](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/classifyText), [ClassificationCategory](https://cloud.google.com/natural-language/docs/reference/rest/v1/ClassificationCategory)

---

## Input and Output

The sample of the configuration, including input & output tables, can be found in the [component's repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/component_config/sample-config/). In general, an input table and 3 parameters are required to configure the component.

### Parameters

All of the parameters listed in the section are required. If any of the parameters are not provided or an invalid value is provided, the component will fail. The API key is verified against Cloud API. The sample of the configuration file can be found [here](https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/component_config/sample-config/config.json).

#### API Key (`#API_key`)

The API key can be obtained in the credentials section of the [Google Cloud Console](https://console.cloud.google.com/apis/credentials). The API key must have Cloud Natural Language API allowed, otherwise, requests will not be authorized. The [limits](https://cloud.google.com/natural-language/quotas) for the token must be specified according to your needs.

#### Analysis Type (`analysis_type`)

A list of methods to be used in the analysis. Supported methods are:

- `analyzeEntities`
- `analyzeEntitySentiment`
- `analyzeSentiment`
- `analyzeSyntax`
- `classifyText`

#### Input Type (`input_type`)

A string represing the type of text inputted into `text` column in the input table. Must be one of the two:

- `PLAIN_TEXT` - a plain text; consumes less characters,
- `HTML` - a html text; consumes more characters as all html tags counted in as well.

### Input table

The input table must contain two required columns `id` and `text` and might contain an optional column `sourceLanguage`. All extra columns are ignored; an exception is raised if any of the required columns is missing. The sample of the table can be found in the [repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/component_config/sample-config/in/tables/test.csv).

The column descriptions are:

- `id` (required) - ID of a document; make sure the ID is unique as the output data is loaded incrementally,
- `text` (required) - document to be analyzed; might contain html tags if `input_type=HTML` is specified
- `sourceLanguage` (optional) - an [ISO-639-1 language identifier](https://cloud.google.com/translate/docs/languages) of the source language; see section *Supported languages & methods*; if left empty, the API automatically detects the language.

The input table, therefore, might take the following form:

| id 	| text 	| sourceLanguage 	| otherColumn 	|
|----	|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|----------------	|-------------	|
| 1 	| Keboola brings data engineering and data analytics together on one single platform that anyone can use. One click managed infrastructure, data hub, user provisioning, process automation - all rolled into one platform. Call us at +420 739 632 821 or visit us at Křižíkova 488/115 Prague 8 186 00 	| en 	| foo 	|
| 2 	| Google, headquartered in Mountain View (1600 Amphitheatre Pkwy,  Mountain View, CA 940430), unveiled the new Android phone for  $799 at the Consumer Electronic Show. Sundar Pichai said in his  keynote that users love their new Android phones. 	|  	| bar 	|
| 3 	| This text is too short to use classifyContent method. 	|  	| foobar 	|

### Data splitting

The output of the API is, usually, a quite complicated object, that needs to be parsed and split into multiple tables. For example, the full response body, with all methods used, for the sentence

```
Keboola brings data engineering and data analytics together on one single platform that anyone can use. One click managed infrastructure, data hub, user provisioning, process automation - all rolled into one platform. Call us at +420 739 632 821 or visit us at Křižíkova 488/115 Prague 8 186 00
```

is over 2000 lines long; it can be viewed in [sample examples](https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/example/sample-response.json). If all methods are used, the result is split into 7 tables:

- `documents`,
- `errors`,
- `sentences`,
- `entities`,
- `mentions`,
- `tokens`,
- `categories`.

All of the tables, except `errors` are loaded incrementally and contain a unique identifier, which is based mainly on document's `id`.

#### Primary keys and relationships

In the following sections, each output table will be discussed, its PK creation process explained and relationship to other tables touched on.

##### `documents`

Contains information about documents, their language and if applicable, sentiment values of the whole document. The table is considered to be a root table, i.e. all other tables are referencing to this table. 

As the primary key, the provided identifier of each document is used. If the NLP analysis fails, for whatever reason, the failed document is not recorded in the table; instead it appears in the `errors` table. If method `analyzeSentiment` is not used, the columns containing information about sentiment values are left empty.

###### Result of: `analyzeEntities`, `analyzeEntitySentiment`, `analyzeSentiment`, `analyzeSyntax`, `classifyText`

##### `errors`

Contains information about warnings and errors sustained during the run of the component. All messages are referencing to `documents` table via column `documentId`.

All of the records contain category of the error, possible value are:

- `categoryError` - error when no category could be identified,
- `nlpError` - error related to Cloud Natural Language API,
- `emptyDocumentError` - error related to document being empty.

If `categoryError` occurs, the process for the document will be retried without the `classifyText` method, unless there are no other methods to be processed. The `nlpError` marks language errors, i.e. unsupported languages for some or all entities, no retry is tried. `emptyDocumentError` causes the process to skip over the document. Additionally, each row contains the message returned by the API for better understanding.

The table is **not** loaded incrementally.

###### Result of: `analyzeEntities`, `analyzeEntitySentiment`, `analyzeSentiment`, `analyzeSyntax`, `classifyText`

##### `sentences`

A child table to `documents`; each child is referenced back via `documentId` column and has a unique identifier, which is created as

```
sentenceId = md5(documentId + '|' + sentence.content + '|' + sentence.beginOffset)
```

where `md5()` is a hashing function, `sentence.content` is a textual representation of the sentence (API output) and `sentence.beginOffset` is a character offset to the start of the sentence (API output).

The table contains information about sentences present in the document. If `analyzeSentiment` is used, the table also contains sentiment value for each of the sentences. In addition to standard API output, column `index` is added manually and marks position of sentence in the document. The indexing columns starts at 0.

###### Result of: `analyzeSentiment`, `analyzeSyntax`

##### `entities`

The table `entities` contains information about proper entities present in the document. Each entity is a child of a document (referenced via `documentId`) and has a unique `entityId` created as

```
entityId = md5(`documentId` + '|' + entity.name)
```

where `entity.name` is the textual representation of the entity. If `analyzeEntitySentiment` method is used, also contains information about sentiment values, otherwise the columns are left empty. 

The table is a parent table to `mentions`.

###### Result of: `analyzeEntities`, `analyzeEntitySentiment`

##### `mentions`

A child table to `entities`, related via `entityId` column. Each entity has one or more mentions, words, which are referencing said entity. Textual representations of mentions for an entity might be duplicate if the same word is used to reference back to the entity, though they are different mentions. 

The column `mentionId`, a primary key, is created as a combination of

```
mentionId = md5(entityId + '|' + mention.content + '|' + mention.beginOffset)
```

where `entityId` is a parent entity, `mention.content` is a word representation of the mention (API output) and `mention.beginOffset` is a character offset of the mention. 

The effect of using `analyzeEntitySentiment` is the same as for `entities` table.

###### Result of: `analyzeEntities`, `analyzeEntitySentiment`

##### `tokens`

A child table to `documents` (foreign key `documentId`), which captures information about tokens in a document. The unique identifier is created by

```
tokenId = md5(documentId + '|' + token.content + '|' + token.beginOffset + '|' + index)
```

where `token.content` is a token name (word), `token.beginOffset` is the character offset of the token in the document and `index` is a tokens position in the document. The index starts at 0 and is useful for creating dependency trees with `token.dependencyEdge` parameters.

###### Result of: `analyzeSyntax`

##### `categories`

Referencing back to `documents` via `documentId` column, the `categories` table contains information about catefories identified using `classifyText` method. The `categoryDocumentId` is a concatenation of category's name and `documentId`, i.e.

```
categoryDocumentId = md5(documentId + '|' + category.name)
```

where `category.name` is the name of the identified category (API output). Each document can have 0 or more categories.

###### Result of: `classifyText`

#### Column descriptions

Due to high number of columns and tables returned by the component, the column descriptions will not be a part of this documentation. However, a great in-depth description is available in [Natual Language API Basics](https://cloud.google.com/natural-language/docs/basics) guide.

## Development

For development purposed the following `docker-compose` commands should be used:

```
docker-compose build dev
docker-compose run --rm dev
```

or 

```
docker-compose up
```


================================================
FILE: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG


  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
            - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:$TEST_TAG
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
#            - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP <config_id> test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
FILE: change_log.md
================================================
**0.3.3**
Pushed the component to ECR, tested out.
Small changes to documentation.

**0.3.2**
Fixed wrong links in documentation.
Added row logging after 500 requests.

**0.3.1**
Removed feature, which would write documents to `documents` table event when error occured. Fine-tuned category classification and added skip mechanism for category to prevent duplicate messages in `errors` table.
Added error category to `errors` to be able to better identify source of errors.
Added sample configuration to the component, as well as descriptions and README.
Small changes to overall code structure.

**0.3.0**
Completely reworked source code that now utilizes KBC library.
All of the requests are sent as one batch request with features and all is returned as a single call. Added retry mechanism for 403 errors (`rateLimitExceeded` or `dailyLimitExceeded`) and added retry for 400 error when using `classifyText` endpoint.


================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command: 
      - /bin/sh
      - /code/scripts/run.sh
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.11-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: requirements.txt
================================================
bs4
logging_gelf
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.4.zip#egg=kbc
keboola.component
keboola.http-client



================================================
FILE: component_config/component_long_description.md
================================================
# Google NLP

Full documentation is available [here](https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/README.md)!

## Overview

The Google Natual Language API allows users to utilize powerful pre-trained NLP models to understand various language features such as sentiment, entities and mentions, content and syntax. The component utilizes the API to send text features and retrieves desired analysis.

The Cloud Natural Language API is a paid service maintained by Google and is subject to [Google Cloud's Terms & Conditions](https://cloud.google.com/terms/). 

The information about pricing can be found on [Google's support forums](https://cloud.google.com/natural-language/pricing). The API currently supports [these methods and languages](https://cloud.google.com/natural-language/docs/languages).

#### Useful links

If you are new to the natural language processing or are unsure about the meaning of outputted values, the following guides might clear up some of the confusion:

- [Natual Language API Basics](https://cloud.google.com/natural-language/docs/basics)
- [Morphology & Dependency Trees](https://cloud.google.com/natural-language/docs/morphology)
- [HTTP status and error codes for JSON](https://cloud.google.com/storage/docs/json_api/v1/status-codes)

## Requirements

The component requires valid Google Cloud API token with Natural Language API allowed. The API token is subject to [limits](https://cloud.google.com/natural-language/quotas) thus it is important to set the correct daily limits. To overcome the 100 second limits, the application uses exponential backoff with 10 retries. In case, the retries are unsuccessful (i.e. daily limit is reached), the component fails.

## Supported languages & methods

The component supports all of the [languages supported by the API](https://cloud.google.com/natural-language/docs/languages). If the (detected) language isn't supported, the request will be cancelled, the document is skipped and no output is produced for it. All of the language errors are outputted to `errors` table.

The component currently supports following methods:

- `analyzeEntities` - entity analysis,
- `analyzeEntitySentiment` - entity sentiment analysis,
- `analyzeSentiment` - sentiment analysis,
- `analyzeSyntax` - syntactic analysis,
- `classifyText` - content classification.

Note: The component uses API v1. The list of all methods supported by the version is listed in [API reference](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents). Mind that, `annotateText` method is a wrapper for all other methods listed above and does not provide any new information. It is used in the component to bundle [all feature requests into a single API call](https://cloud.google.com/natural-language/docs/basics/#text-annotations).

If a new method is available and you'd like to see it, contact us at [support@keboola.com](mailto:support@keboola.com) or via support ticket.

---

#### Entity analysis (`analyzeEntities`)

The method inspects the document for known entities and returns [information about the entities](https://cloud.google.com/natural-language/docs/basics/#entity_analysis), such as their type, salience and mentions in the text. Moreover, if any metadata (e.g. address, phone number, famous person) is identified, the information on the subject is returned as well (e.g. full address, country code for the phone number, wikipedia article about the person). For more information about metadata, see the [`Entity` type documentation](https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity#type).

For each entity, a list of mentions is returned. The mentions list is always of length at least 1, i.e. each entity has at least one mention.

###### reference: [analyzeEntities](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeEntities), [entity](https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity)

---

#### Entity sentiment analysis (`analyzeEntitySentiment`)

The method returns the same analysis type as `analyzeEntities` method but adds sentiment analysis for each entity and mention in the text, thus [combining both entity and sentiment analysis](https://cloud.google.com/natural-language/docs/basics/#entity-sentiment-analysis). Note that, if both `analyzeEntities` and `analyzeEntitySentiment` methods are used, you will not be charged for both methods; only `analyzeEntitySentiment` will be utilized and billed.

###### reference: [analyzeEntitySentiment](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeEntitySentiment), [entity](https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity)

---

#### Sentiment analysis (`analyzeSentiment`)

The `analyzeSentiment` method inspects the document for [emotional opinion present within the text](https://cloud.google.com/natural-language/docs/basics/#sentiment_analysis) and magnitude of the opinion. The result is the overall attitude of the document (positive, neutral or negative) and of each sentence present within the document. Score and magnitude of the document and sentences can be interpreted according to [this guide](https://cloud.google.com/natural-language/docs/basics/#interpreting_sentiment_analysis_values).

###### reference: [analyzeSentiment](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeSentiment), [sentiment](https://cloud.google.com/natural-language/docs/reference/rest/v1/Sentiment)

---

#### Syntactic analysis (`analyzeSyntax`)

The syntactic analysis breaks up the documents into [sentences and extracts tokens (words)](https://cloud.google.com/natural-language/docs/basics/#syntactic_analysis) from the document. For each token, information about lemma, [part of the speech](https://cloud.google.com/natural-language/docs/reference/rest/v1/Token/#PartOfSpeech) and [depencency index](https://cloud.google.com/natural-language/docs/reference/rest/v1/Token/#DependencyEdge) is added using Cloud Natural Language API. For more information about how to interpret the values, refer to [Syntacting analysis](https://cloud.google.com/natural-language/docs/basics/#syntactic_analysis) guide and [Morphology & Dependency Trees](https://cloud.google.com/natural-language/docs/morphology) guide.

###### reference: [analyzeSyntax](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/analyzeSyntax), [token](https://cloud.google.com/natural-language/docs/reference/rest/v1/Token)

---

#### Content classification (`classifyText`)

The `classifyText` method analyzes the document and returns a list of [categories that apply to the text](https://cloud.google.com/natural-language/docs/basics/#content-classification) found in the document. For each category, a confidence level is provided as well as the name of the category. A complete list of all available categories can be found in [Categories section](https://cloud.google.com/natural-language/docs/categories) of API documentation. 

For successful content classification, the document needs to be of certain length (~ 20 words) and must not be too abstract. If the two conditions are not met, no categories are returned.

###### reference: [classifyText](https://cloud.google.com/natural-language/docs/reference/rest/v1/documents/classifyText), [ClassificationCategory](https://cloud.google.com/natural-language/docs/reference/rest/v1/ClassificationCategory)


================================================
FILE: component_config/component_short_description.md
================================================
Google Cloud Natural Language API allows to extract text features such as sentiment, entities and mentions, sentences, syntax and category classification, using pre-trained machine learning models.


================================================
FILE: component_config/configRowSchema.json
================================================
{}


================================================
FILE: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Configuration",
  "required": [
    "#API_key",
    "analysis_type",
    "input_type"
  ],
  "properties": {
    "#API_key": {
      "type": "string",
      "title": "Google API Key",
      "format": "password",
      "minLength": 4,
      "default": "",
      "description": "For API key, please visit <a href='https://console.cloud.google.com/apis/credentials'>Google Cloud Console</a><ol><li>Create a new API key by clicking: Create credentials -> API Keys</li><li>Use your existing API key and add Natural Language API to API restrictions</li></ol><br>Please, refer to GCP <a href='https://cloud.google.com/natural-language/pricing'>pricing guide</a> for detailed information about the costs of API requests.",
      "propertyOrder": 100
    },
    "analysis_type": {
      "type": "array",
      "title": "Analysis Type",
      "description": "Choose the methods for NLP analysis.</br>Refer to <a href='https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/README.md'>component documentation</a> for more information.",
      "items": {
        "type": "string",
        "enum": [
          "extractEntitySentiment",
          "extractEntities",
          "extractDocumentSentiment",
          "extractSyntax",
          "classifyText"
        ],
        "options": {
          "enum_titles": [
            "Entities Sentiment",
            "Entities",
            "Document Sentiment",
            "Syntax",
            "Categories"
          ]
        }
      },
      "uniqueItems": true,
      "propertyOrder": 200,
      "format": "checkbox"
    },
    "input_type": {
      "type": "string",
      "title": "Input Type",
      "description": "Choose, whether text input is a plain text or html.",
      "enum": [
        "PLAIN_TEXT",
        "HTML"
      ],
      "uniqueItems": true,
      "propertyOrder": 300,
      "default": "PLAIN_TEXT"
    }
  }
}


================================================
FILE: component_config/configuration_description.md
================================================
The sample of the configuration, including input & output tables, can be found in the [component's repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/component_config/sample-config/). In general, an input table and 3 parameters are required to configure the component.

All of the parameters listed in the section are required. If any of the parameters are not provided or an invalid value is provided, the component will fail. The API key is verified against Cloud API. The sample of the configuration file can be found [here](https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/component_config/sample-config/config.json).

#### API Key (`#API_key`)

The API key can be obtained in the credentials section of the [Google Cloud Console](https://console.cloud.google.com/apis/credentials). The API key must have Cloud Natural Language API allowed, otherwise, requests will not be authorized. The [limits](https://cloud.google.com/natural-language/quotas) for the token must be specified according to your needs.

#### Analysis Type (`analysis_type`)

A list of methods to be used in the analysis. Supported methods are:

- `analyzeEntities`
- `analyzeEntitySentiment`
- `analyzeSentiment`
- `analyzeSyntax`
- `classifyText`

#### Input Type (`input_type`)

A string represing the type of text inputted into `text` column in the input table. Must be one of the two:

- `PLAIN_TEXT` - a plain text; consumes less characters,
- `HTML` - a html text; consumes more characters as all html tags counted in as well.

### Input table

The input table must contain two required columns `id` and `text` and might contain an optional column `sourceLanguage`. All extra columns are ignored; an exception is raised if any of the required columns is missing. The sample of the table can be found in the [repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-google-nlp/src/master/component_config/sample-config/in/tables/test.csv).

The column descriptions are:

- `id` (required) - ID of a document; make sure the ID is unique as the output data is loaded incrementally,
- `text` (required) - document to be analyzed; might contain html tags if `input_type=HTML` is specified
- `sourceLanguage` (optional) - an [ISO-639-1 language identifier](https://cloud.google.com/translate/docs/languages) of the source language; see section *Supported languages & methods*; if left empty, the API automatically detects the language.

The input table, therefore, might take the following form:

| id 	| text 	| sourceLanguage 	| otherColumn 	|
|----	|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	|----------------	|-------------	|
| 1 	| Keboola brings data engineering and data analytics together on one single platform that anyone can use. One click managed infrastructure, data hub, user provisioning, process automation - all rolled into one platform. Call us at +420 739 632 821 or visit us at Křižíkova 488/115 Prague 8 186 00 	| en 	| foo 	|
| 2 	| Google, headquartered in Mountain View (1600 Amphitheatre Pkwy,  Mountain View, CA 940430), unveiled the new Android phone for  $799 at the Consumer Electronic Show. Sundar Pichai said in his  keynote that users love their new Android phones. 	|  	| bar 	|
| 3 	| This text is too short to use classifyContent method. 	|  	| foobar 	|


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#API_key": "AIzaXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX",
    "analysis_type": ["classifyText", "extractEntitySentiment", "extractDocumentSentiment", "extractSyntax"],
    "input_type": "PLAIN_TEXT"
  },
  "image_parameters": {}
}



================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"id","text","sourceLanguage"
"VERIFY_ALL_METHODS_SUCCESS","Pavol Demitra was a Slovak pebrofessional ice hockey player. He played nineteen seasons of professional hockey, for teams in the Czechoslovak First Ice Hockey League, National Hockey League, Slovak Extraliga, and Kontinental Hockey League. Pavol Demitra scored 211 goals in NHL in his carrer. He died in 2011 in a plane crash near Jaroslavl. Pavol Demitra has died aged 39. The hocker player was considered one of the best in the country.","en"
"FAIL_CATEGORY_RETRY_SUCCESS","Pavol Demitra was a hockey player.","en"
"FAIL_LANGUAGE","Pavol Demitra bol slovensky hokejovy hrac.","sk"
"AUTO_DETECT_LANGUAGE_SUCCESS","Pavol Demitra was a hockey player.",""
"AUTO_DETECT_LANGUAGE_FAIL","Pavol Demitra bol slovensky hokejovy hrac.",""
"EMPTY_STRING","","en"
"AUTO_DETECT_FAIL_LANGUAGE_FR","Je n'ai pas fait mon devoir.",""
"FAIL_LANGUAGE_FR","Je n'ai pas fait mon devoir.","fr"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "id",
        "text",
        "sourceLanguage"
    ],
    "metadata": [],
    "column_metadata": {}
}


================================================
FILE: example/sample-response.json
================================================
{
  "sentences": [
    {
      "text": {
        "content": "Keboola brings data engineering and data analytics together on one single platform that anyone can use.",
        "beginOffset": 0
      },
      "sentiment": {
        "magnitude": 0.4,
        "score": 0.4
      }
    },
    {
      "text": {
        "content": "One click managed infrastructure, data hub, user provisioning, process automation - all rolled into one platform.",
        "beginOffset": 104
      },
      "sentiment": {
        "magnitude": 0.4,
        "score": 0.4
      }
    },
    {
      "text": {
        "content": "Call us at +420 739 632 821 or visit us at Křižíkova 488/115 Prague 8 186 00",
        "beginOffset": 218
      },
      "sentiment": {
        "magnitude": 0.2,
        "score": 0.2
      }
    }
  ],
  "tokens": [
    {
      "text": {
        "content": "Keboola",
        "beginOffset": 0
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 1,
        "label": "NSUBJ"
      },
      "lemma": "Keboola"
    },
    {
      "text": {
        "content": "brings",
        "beginOffset": 8
      },
      "partOfSpeech": {
        "tag": "VERB",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "INDICATIVE",
        "number": "SINGULAR",
        "person": "THIRD",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "PRESENT",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 1,
        "label": "ROOT"
      },
      "lemma": "bring"
    },
    {
      "text": {
        "content": "data",
        "beginOffset": 15
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 3,
        "label": "NN"
      },
      "lemma": "data"
    },
    {
      "text": {
        "content": "engineering",
        "beginOffset": 20
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 1,
        "label": "DOBJ"
      },
      "lemma": "engineering"
    },
    {
      "text": {
        "content": "and",
        "beginOffset": 32
      },
      "partOfSpeech": {
        "tag": "CONJ",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 3,
        "label": "CC"
      },
      "lemma": "and"
    },
    {
      "text": {
        "content": "data",
        "beginOffset": 36
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 6,
        "label": "NN"
      },
      "lemma": "data"
    },
    {
      "text": {
        "content": "analytics",
        "beginOffset": 41
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 3,
        "label": "CONJ"
      },
      "lemma": "analytic"
    },
    {
      "text": {
        "content": "together",
        "beginOffset": 51
      },
      "partOfSpeech": {
        "tag": "ADV",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 1,
        "label": "ADVMOD"
      },
      "lemma": "together"
    },
    {
      "text": {
        "content": "on",
        "beginOffset": 60
      },
      "partOfSpeech": {
        "tag": "ADP",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 1,
        "label": "PREP"
      },
      "lemma": "on"
    },
    {
      "text": {
        "content": "one",
        "beginOffset": 63
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 11,
        "label": "NUM"
      },
      "lemma": "one"
    },
    {
      "text": {
        "content": "single",
        "beginOffset": 67
      },
      "partOfSpeech": {
        "tag": "ADJ",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 11,
        "label": "AMOD"
      },
      "lemma": "single"
    },
    {
      "text": {
        "content": "platform",
        "beginOffset": 74
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 8,
        "label": "POBJ"
      },
      "lemma": "platform"
    },
    {
      "text": {
        "content": "that",
        "beginOffset": 83
      },
      "partOfSpeech": {
        "tag": "DET",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 15,
        "label": "DOBJ"
      },
      "lemma": "that"
    },
    {
      "text": {
        "content": "anyone",
        "beginOffset": 88
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 15,
        "label": "NSUBJ"
      },
      "lemma": "anyone"
    },
    {
      "text": {
        "content": "can",
        "beginOffset": 95
      },
      "partOfSpeech": {
        "tag": "VERB",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 15,
        "label": "AUX"
      },
      "lemma": "can"
    },
    {
      "text": {
        "content": "use",
        "beginOffset": 99
      },
      "partOfSpeech": {
        "tag": "VERB",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 11,
        "label": "RCMOD"
      },
      "lemma": "use"
    },
    {
      "text": {
        "content": ".",
        "beginOffset": 102
      },
      "partOfSpeech": {
        "tag": "PUNCT",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 1,
        "label": "P"
      },
      "lemma": "."
    },
    {
      "text": {
        "content": "One",
        "beginOffset": 104
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 18,
        "label": "NUM"
      },
      "lemma": "One"
    },
    {
      "text": {
        "content": "click",
        "beginOffset": 108
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "IMPERATIVE",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 19,
        "label": "NSUBJ"
      },
      "lemma": "click"
    },
    {
      "text": {
        "content": "managed",
        "beginOffset": 114
      },
      "partOfSpeech": {
        "tag": "VERB",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "INDICATIVE",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "PAST",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 19,
        "label": "ROOT"
      },
      "lemma": "manage"
    },
    {
      "text": {
        "content": "infrastructure",
        "beginOffset": 122
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 32,
        "label": "NSUBJ"
      },
      "lemma": "infrastructure"
    },
    {
      "text": {
        "content": ",",
        "beginOffset": 136
      },
      "partOfSpeech": {
        "tag": "PUNCT",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 20,
        "label": "P"
      },
      "lemma": ","
    },
    {
      "text": {
        "content": "data",
        "beginOffset": 138
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 23,
        "label": "NN"
      },
      "lemma": "data"
    },
    {
      "text": {
        "content": "hub",
        "beginOffset": 143
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 20,
        "label": "CONJ"
      },
      "lemma": "hub"
    },
    {
      "text": {
        "content": ",",
        "beginOffset": 146
      },
      "partOfSpeech": {
        "tag": "PUNCT",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 20,
        "label": "P"
      },
      "lemma": ","
    },
    {
      "text": {
        "content": "user",
        "beginOffset": 148
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 26,
        "label": "NN"
      },
      "lemma": "user"
    },
    {
      "text": {
        "content": "provisioning",
        "beginOffset": 153
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 20,
        "label": "APPOS"
      },
      "lemma": "provisioning"
    },
    {
      "text": {
        "content": ",",
        "beginOffset": 165
      },
      "partOfSpeech": {
        "tag": "PUNCT",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 20,
        "label": "P"
      },
      "lemma": ","
    },
    {
      "text": {
        "content": "process",
        "beginOffset": 167
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 29,
        "label": "NN"
      },
      "lemma": "process"
    },
    {
      "text": {
        "content": "automation",
        "beginOffset": 175
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 20,
        "label": "APPOS"
      },
      "lemma": "automation"
    },
    {
      "text": {
        "content": "-",
        "beginOffset": 186
      },
      "partOfSpeech": {
        "tag": "PUNCT",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 20,
        "label": "P"
      },
      "lemma": "-"
    },
    {
      "text": {
        "content": "all",
        "beginOffset": 188
      },
      "partOfSpeech": {
        "tag": "DET",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 20,
        "label": "DEP"
      },
      "lemma": "all"
    },
    {
      "text": {
        "content": "rolled",
        "beginOffset": 192
      },
      "partOfSpeech": {
        "tag": "VERB",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "INDICATIVE",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "PAST",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 19,
        "label": "CCOMP"
      },
      "lemma": "roll"
    },
    {
      "text": {
        "content": "into",
        "beginOffset": 199
      },
      "partOfSpeech": {
        "tag": "ADP",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 32,
        "label": "PREP"
      },
      "lemma": "into"
    },
    {
      "text": {
        "content": "one",
        "beginOffset": 204
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 35,
        "label": "NUM"
      },
      "lemma": "one"
    },
    {
      "text": {
        "content": "platform",
        "beginOffset": 208
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 33,
        "label": "POBJ"
      },
      "lemma": "platform"
    },
    {
      "text": {
        "content": ".",
        "beginOffset": 216
      },
      "partOfSpeech": {
        "tag": "PUNCT",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 19,
        "label": "P"
      },
      "lemma": "."
    },
    {
      "text": {
        "content": "Call",
        "beginOffset": 218
      },
      "partOfSpeech": {
        "tag": "VERB",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "IMPERATIVE",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 37,
        "label": "ROOT"
      },
      "lemma": "Call"
    },
    {
      "text": {
        "content": "us",
        "beginOffset": 223
      },
      "partOfSpeech": {
        "tag": "PRON",
        "aspect": "ASPECT_UNKNOWN",
        "case": "ACCUSATIVE",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "PLURAL",
        "person": "FIRST",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 37,
        "label": "DOBJ"
      },
      "lemma": "us"
    },
    {
      "text": {
        "content": "at",
        "beginOffset": 226
      },
      "partOfSpeech": {
        "tag": "ADP",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 37,
        "label": "PREP"
      },
      "lemma": "at"
    },
    {
      "text": {
        "content": "+",
        "beginOffset": 229
      },
      "partOfSpeech": {
        "tag": "X",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 39,
        "label": "P"
      },
      "lemma": "+"
    },
    {
      "text": {
        "content": "420",
        "beginOffset": 230
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 43,
        "label": "NUMBER"
      },
      "lemma": "420"
    },
    {
      "text": {
        "content": "739",
        "beginOffset": 234
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 43,
        "label": "NUMBER"
      },
      "lemma": "739"
    },
    {
      "text": {
        "content": "632",
        "beginOffset": 238
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 44,
        "label": "NUM"
      },
      "lemma": "632"
    },
    {
      "text": {
        "content": "821",
        "beginOffset": 242
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 39,
        "label": "POBJ"
      },
      "lemma": "821"
    },
    {
      "text": {
        "content": "or",
        "beginOffset": 246
      },
      "partOfSpeech": {
        "tag": "CONJ",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 37,
        "label": "CC"
      },
      "lemma": "or"
    },
    {
      "text": {
        "content": "visit",
        "beginOffset": 249
      },
      "partOfSpeech": {
        "tag": "VERB",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 37,
        "label": "CONJ"
      },
      "lemma": "visit"
    },
    {
      "text": {
        "content": "us",
        "beginOffset": 255
      },
      "partOfSpeech": {
        "tag": "PRON",
        "aspect": "ASPECT_UNKNOWN",
        "case": "ACCUSATIVE",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "PLURAL",
        "person": "FIRST",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 46,
        "label": "DOBJ"
      },
      "lemma": "us"
    },
    {
      "text": {
        "content": "at",
        "beginOffset": 258
      },
      "partOfSpeech": {
        "tag": "ADP",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 46,
        "label": "PREP"
      },
      "lemma": "at"
    },
    {
      "text": {
        "content": "Křižíkova",
        "beginOffset": 261
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 48,
        "label": "POBJ"
      },
      "lemma": "Křižíkova"
    },
    {
      "text": {
        "content": "488",
        "beginOffset": 274
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 53,
        "label": "NUM"
      },
      "lemma": "488"
    },
    {
      "text": {
        "content": "/",
        "beginOffset": 277
      },
      "partOfSpeech": {
        "tag": "X",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 53,
        "label": "P"
      },
      "lemma": "/"
    },
    {
      "text": {
        "content": "115",
        "beginOffset": 278
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 53,
        "label": "NUM"
      },
      "lemma": "115"
    },
    {
      "text": {
        "content": "Prague",
        "beginOffset": 282
      },
      "partOfSpeech": {
        "tag": "NOUN",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "SINGULAR",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 37,
        "label": "DOBJ"
      },
      "lemma": "Prague"
    },
    {
      "text": {
        "content": "8",
        "beginOffset": 289
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 53,
        "label": "NUM"
      },
      "lemma": "8"
    },
    {
      "text": {
        "content": "186",
        "beginOffset": 291
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 56,
        "label": "NUM"
      },
      "lemma": "186"
    },
    {
      "text": {
        "content": "00",
        "beginOffset": 295
      },
      "partOfSpeech": {
        "tag": "NUM",
        "aspect": "ASPECT_UNKNOWN",
        "case": "CASE_UNKNOWN",
        "form": "FORM_UNKNOWN",
        "gender": "GENDER_UNKNOWN",
        "mood": "MOOD_UNKNOWN",
        "number": "NUMBER_UNKNOWN",
        "person": "PERSON_UNKNOWN",
        "proper": "PROPER_UNKNOWN",
        "reciprocity": "RECIPROCITY_UNKNOWN",
        "tense": "TENSE_UNKNOWN",
        "voice": "VOICE_UNKNOWN"
      },
      "dependencyEdge": {
        "headTokenIndex": 53,
        "label": "DEP"
      },
      "lemma": "00"
    }
  ],
  "entities": [
    {
      "name": "platform",
      "type": "OTHER",
      "metadata": {},
      "salience": 0.22810584,
      "mentions": [
        {
          "text": {
            "content": "platform",
            "beginOffset": 74
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.2,
            "score": 0.2
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.5,
        "score": 0.2
      }
    },
    {
      "name": "data analytics",
      "type": "OTHER",
      "metadata": {},
      "salience": 0.20965189,
      "mentions": [
        {
          "text": {
            "content": "data analytics",
            "beginOffset": 36
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.4,
            "score": 0.4
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.4,
        "score": 0.4
      }
    },
    {
      "name": "data engineering",
      "type": "OTHER",
      "metadata": {},
      "salience": 0.18361077,
      "mentions": [
        {
          "text": {
            "content": "data engineering",
            "beginOffset": 15
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "Keboola",
      "type": "PERSON",
      "metadata": {},
      "salience": 0.1373092,
      "mentions": [
        {
          "text": {
            "content": "Keboola",
            "beginOffset": 0
          },
          "type": "PROPER",
          "sentiment": {
            "magnitude": 0.1,
            "score": 0.1
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.1,
        "score": 0.1
      }
    },
    {
      "name": "anyone",
      "type": "PERSON",
      "metadata": {},
      "salience": 0.06476169,
      "mentions": [
        {
          "text": {
            "content": "anyone",
            "beginOffset": 88
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.2,
            "score": 0.2
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.2,
        "score": 0.2
      }
    },
    {
      "name": "platform",
      "type": "OTHER",
      "metadata": {},
      "salience": 0.029939458,
      "mentions": [
        {
          "text": {
            "content": "platform",
            "beginOffset": 208
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.1,
            "score": 0.1
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.1,
        "score": 0.1
      }
    },
    {
      "name": "data hub",
      "type": "ORGANIZATION",
      "metadata": {},
      "salience": 0.028680414,
      "mentions": [
        {
          "text": {
            "content": "data hub",
            "beginOffset": 138
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.2,
            "score": 0.2
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.2,
        "score": 0.2
      }
    },
    {
      "name": "user provisioning",
      "type": "OTHER",
      "metadata": {},
      "salience": 0.028680414,
      "mentions": [
        {
          "text": {
            "content": "user provisioning",
            "beginOffset": 148
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.3,
            "score": 0.3
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.3,
        "score": 0.3
      }
    },
    {
      "name": "infrastructure",
      "type": "OTHER",
      "metadata": {},
      "salience": 0.028334498,
      "mentions": [
        {
          "text": {
            "content": "infrastructure",
            "beginOffset": 122
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.6,
            "score": 0.6
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.6,
        "score": 0.6
      }
    },
    {
      "name": "process automation",
      "type": "OTHER",
      "metadata": {},
      "salience": 0.026343675,
      "mentions": [
        {
          "text": {
            "content": "process automation",
            "beginOffset": 167
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.3,
            "score": 0.3
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.3,
        "score": 0.3
      }
    },
    {
      "name": "all",
      "type": "PERSON",
      "metadata": {},
      "salience": 0.026343675,
      "mentions": [
        {
          "text": {
            "content": "all",
            "beginOffset": 188
          },
          "type": "COMMON",
          "sentiment": {
            "magnitude": 0.1,
            "score": 0.1
          }
        }
      ],
      "sentiment": {
        "magnitude": 0.1,
        "score": 0.1
      }
    },
    {
      "name": "Křižíkova 488",
      "type": "OTHER",
      "metadata": {},
      "salience": 0.008238481,
      "mentions": [
        {
          "text": {
            "content": "Křižíkova 488",
            "beginOffset": 261
          },
          "type": "PROPER",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "+420 739 632 821",
      "type": "PHONE_NUMBER",
      "metadata": {
        "number": "739632821"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "+420 739 632 821",
            "beginOffset": 229
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "Křižíkova 488/115 Prague 8 186 00",
      "type": "ADDRESS",
      "metadata": {
        "locality": "Praha",
        "broad_region": "Hlavní město Praha",
        "street_name": "Křižíkova",
        "narrow_region": "Hlavní město Praha",
        "street_number": "488/115",
        "country": "CZ",
        "postal_code": "186 00",
        "sublocality": "Praha 8"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "Křižíkova 488/115 Prague 8 186 00",
            "beginOffset": 261
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "+420 739 632 821",
      "type": "NUMBER",
      "metadata": {
        "value": "420739632821"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "+420 739 632 821",
            "beginOffset": 229
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "488/115",
      "type": "NUMBER",
      "metadata": {
        "value": "4.243478"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "488/115",
            "beginOffset": 274
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "00",
      "type": "NUMBER",
      "metadata": {
        "value": "0"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "00",
            "beginOffset": 295
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "8",
      "type": "NUMBER",
      "metadata": {
        "value": "8"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "8",
            "beginOffset": 289
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "One",
      "type": "NUMBER",
      "metadata": {
        "value": "1"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "One",
            "beginOffset": 104
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "one",
      "type": "NUMBER",
      "metadata": {
        "value": "1"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "one",
            "beginOffset": 204
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "one",
      "type": "NUMBER",
      "metadata": {
        "value": "1"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "one",
            "beginOffset": 63
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    },
    {
      "name": "186",
      "type": "NUMBER",
      "metadata": {
        "value": "186"
      },
      "salience": 0,
      "mentions": [
        {
          "text": {
            "content": "186",
            "beginOffset": 291
          },
          "type": "TYPE_UNKNOWN",
          "sentiment": {
            "magnitude": 0,
            "score": 0
          }
        }
      ],
      "sentiment": {
        "magnitude": 0,
        "score": 0
      }
    }
  ],
  "documentSentiment": {
    "magnitude": 1.1,
    "score": 0.3
  },
  "language": "en",
  "categories": [
    {
      "name": "/Computers & Electronics/Enterprise Technology/Data Management",
      "confidence": 0.8
    },
    {
      "name": "/Business & Industrial",
      "confidence": 0.79
    },
    {
      "name": "/Internet & Telecom/Web Services",
      "confidence": 0.64
    }
  ]
}


================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run.sh
================================================
#!/bin/sh
set -e

TABLES_PATH=$KBC_DATADIR/out/tables

if [ "$(ls -A $TABLES_PATH)" ]; then
    TABLES_PATH=$TABLES_PATH/*
     rm -r $TABLES_PATH
fi

python /code/src/component.py


================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
fi

echo "Updating logger settings"

value=`cat component_config/logger`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} logger --value="$value"
else
    echo "logger type is empty!"
fi

echo "Updating logger configuration"
value=`cat component_config/loggerConfiguration.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} loggerConfiguration --value="$value"
else
    echo "loggerConfiguration is empty!"
fi


================================================
FILE: src/client.py
================================================
from bs4 import BeautifulSoup
import json
import logging
import requests
import sys
from keboola.http_client import HttpClient

BASE_URL = 'https://language.googleapis.com/v1/documents:annotateText'


class GoogleNLPClientException(Exception):
    pass


class GoogleNLPClient(HttpClient):

    def __init__(self, token):

        _def_params = {'key': token}
        _def_header = {"Content-Type": "application/json",
                       "Accept": "application/json"}
        self.token = token

        super().__init__(base_url=BASE_URL, max_retries=10,
                         backoff_factor=0.3, default_params=_def_params,
                         status_forcelist=(500, 502), default_http_header=_def_header)

        self._check_token()

    def _check_token(self):

        _body = self._create_body('', '', {})

        # Will produce a 400 error due to invalid payload.
        # Depending on message, the token can be verified
        _rsp = self.post_raw(data=_body)
        _sc = _rsp.status_code
        _msg = _rsp.json()['error'].get('message')

        if 'API key not valid' in _msg:
            logging.error("Please check the API token.")
            logging.error("The API token could not be verified. The response received was %s: %s" % (_sc, _msg))
            sys.exit(1)
        else:
            logging.info("Verified API token.")

    def _get_supported_languages(self):

        # No API call to obtain supported languages in Google NLP.
        # Will perform a scrape once in a while to ensure languages are supported.

        _map = {'content_classification': 'classifyText',
                'syntactic_analysis': 'analyzeSyntax',
                'entity_analysis': 'analyzeEntities',
                'sentiment_analysis': 'analyzeSentiment',
                'entity_sentiment_analysis': 'analyzeEntitySentiment'}

        _page = self.get_raw(endpoint_path='https://cloud.google.com/natural-language/docs/languages',
                             is_absolute_path=True)
        soup = BeautifulSoup(_page.text, "html.parser")

        table_headers = soup.findAll('h2')
        table_contents = soup.findAll('table')

        if len(table_contents) != len(table_headers):
            logging.info("Skipping obtaining languages due to not matching website inputs.")
            return

        supported_languages = {}
        try:
            for t in range(len(table_headers)):
                _name = table_headers[t]['id']
                _name_mapped = _map.get(_name)

                _table = table_contents[t].select('tbody > tr > td > code')

                supported_languages[_name_mapped] = [lang.text for lang in _table]

            return supported_languages

        except (KeyError, AttributeError) as e:

            logging.warning("Could not obtain languages.")
            logging.warning(e)

    def _create_body(self, content, language, features, inputType='PLAIN_TEXT') -> str:

        if language is None:
            language = ''

        _template = {"document": {"type": inputType,
                                  "content": content,
                                  "language": language},
                     "encodingType": "UTF8",
                     "features": features}

        logging.debug(f"Body template: {_template}")

        return json.dumps(_template)

    def analyze_text(self, content, language, features, inputType='PLAIN_TEXT'):

        _body = self._create_body(content, language, features, inputType)
        logging.debug(f"Body: {_body}")

        try:
            _rsp = self.post_raw(data=_body)
            return _rsp

        except requests.exceptions.RetryError as e:
            raise GoogleNLPClientException(f"There was a problem calling documents:annotateText endpoint. "
                                           f"Retry 10x failed. Reason: {e} "
                                           f"Following features were used: {str(features)} "
                                           f"The issue might be caused by daily limits reached. "
                                           f"Please, raise the limits if necessary.") from e



================================================
FILE: src/component.py
================================================
import csv
from hashlib import md5
import json
import logging
import sys
from client import GoogleNLPClient, GoogleNLPClientException
from result import resultWriter
from keboola.component.base import ComponentBase
from keboola.component.exceptions import UserException


API_KEY = '#API_key'
ANALYSIS_TYPE_KEY = 'analysis_type'
INPUT_TYPE_KEY = 'input_type'

SUPPORTED_ANALYSIS = ['extractEntities', 'extractEntitySentiment', 'classifyText',
                      'extractDocumentSentiment', 'extractSyntax']

SUPPORTED_INPUT = ['PLAIN_TEXT', 'HTML']
MANDATORY_PARS = [API_KEY, ANALYSIS_TYPE_KEY, INPUT_TYPE_KEY]


class Component(ComponentBase):

    def __init__(self):
        super().__init__()
        self.validate_configuration_parameters(MANDATORY_PARS)

        # Parameter fetching
        self.paramToken = self.configuration.parameters[API_KEY]
        self.paramAnalysisType = self.configuration.parameters[ANALYSIS_TYPE_KEY]
        self.paramInputType = self.configuration.parameters[INPUT_TYPE_KEY]

        # Check inputs and create necessary variables for making requests
        self._check_input_tables()
        self._check_parameter_values()
        self._create_request_features()
        self._identify_sentiment()

        self.client = GoogleNLPClient(token=self.paramToken)
        self.writer = resultWriter(
            methodList=self.paramAnalysisType, dataPath=self.tables_out_path)

    def run(self):

        _path = self.input_table.full_path
        logging.info(f"Processing data from table {self.input_table.name}")

        with open(_path) as fileInput:
            _reader = csv.DictReader(fileInput)
            logging.debug(f"Defined columns: {self.input_table.columns}, found columns: {_reader.fieldnames}")

            for row in _reader:

                try:
                    self.process_document(documentDict=row, retry=True)
                except GoogleNLPClientException as e:
                    raise e

                if _reader.line_num % 250 == 0:
                    logging.info("Made %s call to API so far." % _reader.line_num)

    def _create_request_features(self):

        _template = {}

        for _analysis in self.paramAnalysisType:
            _template[_analysis] = True

        self.requestFeatures = _template

    def _identify_sentiment(self):

        _mappingSentiment = {'extractDocumentSentiment': ['documents', 'sentences'],
                             'extractEntitySentiment': ['entities', 'mentions']}

        _includedSentiment = []

        for key in _mappingSentiment:
            if key in self.paramAnalysisType:
                _includedSentiment += _mappingSentiment[key]
            else:
                pass

        logging.debug(f"Sentiment is present in the following tables: {_includedSentiment}")
        self.resultSentimentTables = _includedSentiment

    def _check_input_tables(self):

        _input_tables = self.get_input_tables_definitions()

        logging.debug("Input tables:")
        logging.debug(_input_tables)

        if len(_input_tables) == 0:

            logging.error("No input table was provided. Please provide an input table, with mandatory columns \"id\"," +
                          " \"text\" and optional column \"sourceLanguage\". See documentation for more information.")
            sys.exit(1)

        else:

            _input = _input_tables[0]
            _path = _input.full_path
            _mnfst_path = _path + '.manifest'

            with open(_mnfst_path) as _mnfst_file:
                _mnfst = json.load(_mnfst_file)
                _columns = _mnfst['columns']

                if 'id' in _columns and 'text' in _columns:
                    pass
                else:
                    logging.error("Missing required column \"id\" or \"text\" in table %s." % _input.name)
                    logging.error("Please, make sure all of the required columns are inputted.")
                    sys.exit(1)

        self.input_table = _input

    def _check_parameter_values(self):

        _setAnalysis = list(set(self.paramAnalysisType) -
                            set(SUPPORTED_ANALYSIS))

        if len(_setAnalysis) != 0:

            logging.error("Unsupported analysis type: %s. Supported types are %s." % (
                _setAnalysis, SUPPORTED_ANALYSIS))

            sys.exit(1)

        if self.paramInputType not in SUPPORTED_INPUT:

            logging.error("Unsupported input type %s. Input type must be one of %s." % (
                self.paramInputType, SUPPORTED_INPUT))

            sys.exit(1)

    def process_document(self, documentDict: dict, retry):

        logging.debug(f"Processing document: {documentDict}")
        documentId = documentDict['id']
        documentText = documentDict['text']
        documentLanguage = documentDict.get('sourceLanguage')
        skipCategories = False

        if documentText.strip() == '':

            _message = "The document %s is empty and was skipped." % documentId

            logging.warning(_message)

            self.writer.writerErrors.writerow({'documentId': documentId,
                                               'category': 'emptyDocumentError',
                                               'severity': 'WARNING',
                                               'message': _message})

            return

        _features = self.requestFeatures
        if retry is False:

            _features['classifyText'] = False
            skipCategories = True

        _nlpResponse = self.client.analyze_text(content=documentText, features=_features,
                                                language=documentLanguage, inputType=self.paramInputType)

        _sc = _nlpResponse.status_code
        _js = _nlpResponse.json()

        logging.debug(f"Received response and status code: {_sc} {_js}")

        if _sc == 200:
            logging.debug(f"Received response: {_js}")
            self.split_and_write_data(documentId, _js, skipCategories)
            # write results

        elif _sc == 400:

            _message = _js['error']['message']

            if (retry is True and 'Invalid text content: too few tokens' in _message
                    and 'classifyText' in self.paramAnalysisType):

                logging.warning(
                    "Could not use method classifyText for document %s." % documentId)

                if len(self.paramAnalysisType) > 1:

                    _additionalMessage = 'Retrying without classifyText method.'
                    _message = ' '.join([_message, _additionalMessage])

                    self.writer.writerErrors.writerow({'documentId': documentId,
                                                       'category': 'categoryError',
                                                       'severity': 'WARNING',
                                                       'message': _message})

                    logging.info(
                        "Retrying request for document %s without classifyText method." % documentId)

                    self.process_document(documentDict, retry=False)

                    return

                elif len(self.paramAnalysisType) == 1:

                    '''

                    self.writer.writerDocuments.writerow({'documentId': documentId,
                                                          'language': documentLanguage,
                                                          'sentimentMagnitude': '',
                                                          'sentimentScore': ''})

                    '''

                    _additionalMessage = 'Request could not be retried because no other method was specified.'
                    _message = ' '.join([_message, _additionalMessage])

                    logging.warning(_additionalMessage)

                    self.writer.writerErrors.writerow({'documentId': documentId,
                                                       'category': 'categoryError',
                                                       'severity': 'ERROR',
                                                       'message': _message})

                    return

            else:

                _additionalMessage = "Document %s could not be processed. Received:" % documentId
                _logMessage = ' '.join([_additionalMessage, _message])

                logging.warning(_logMessage)

                self.writer.writerErrors.writerow({'documentId': documentId,
                                                   'category': 'nlpError',
                                                   'severity': 'ERROR',
                                                   'message': _message})

                return

        elif _sc > 400:
            _message = _js.get('error', {}).get('message', "")
            raise UserException(f"Received status code {str(_sc)} and message: {_message}")

    @staticmethod
    def _hash_string(hashList, delim='|'):

        _toHash = '|'.join([str(i) for i in hashList])

        return md5(_toHash.encode()).hexdigest()

    def write_documents(self, documentId, nlpResult):

        if 'documents' in self.resultSentimentTables:

            docSentiment = nlpResult['documentSentiment']['score']
            docMagnitude = nlpResult['documentSentiment']['magnitude']

        else:

            docSentiment, docMagnitude = '', ''

        language = nlpResult['language']

        _writeRowDocuments = {'documentId': documentId,
                              'language': language,
                              'sentimentScore': docSentiment,
                              'sentimentMagnitude': docMagnitude}

        self.writer.writerDocuments.writerow(_writeRowDocuments)

    def write_sentences(self, documentId, nlpResult):

        nlpSentences = nlpResult['sentences']

        idx = -1

        for sentence in nlpSentences:

            idx += 1
            textContent = sentence['text']['content']
            textOffset = sentence['text']['beginOffset']

            if 'sentences' in self.resultSentimentTables:

                senSentiment = sentence['sentiment']['score']
                senMagnitude = sentence['sentiment']['magnitude']

            else:

                senSentiment, senMagnitude = '', ''

            sentenceId = self._hash_string([documentId,
                                            textContent,
                                            textOffset])

            _writerRowSentences = {'sentenceId': sentenceId,
                                   'documentId': documentId,
                                   'index': idx,
                                   'textContent': textContent,
                                   'textOffset': textOffset,
                                   'sentimentScore': senSentiment,
                                   'sentimentMagnitude': senMagnitude}

            self.writer.writerSentences.writerow(_writerRowSentences)

    def write_categories(self, documentId, nlpResult):

        nlpCategories = nlpResult['categories']

        if len(nlpCategories) != 0:

            for category in nlpCategories:

                categoryName = category['name']
                confidence = category['confidence']

                categoryDocumentId = self._hash_string([documentId,
                                                        categoryName])

                _writerRowCategories = {'categoryDocumentId': categoryDocumentId,
                                        'documentId': documentId,
                                        'categoryName': categoryName,
                                        'confidence': confidence}

                self.writer.writerCategories.writerow(_writerRowCategories)

        else:

            _message = "No category detected for document %s." % documentId

            logging.warning(_message)

            self.writer.writerErrors.writerow({'documentId': documentId,
                                               'category': 'categoryError',
                                               'severity': 'WARNING',
                                               'message': _message})

    def write_entities(self, documentId, nlpResult):

        nlpEntities = nlpResult['entities']

        for entity in nlpEntities:

            if 'entities' in self.resultSentimentTables:

                entSentiment = entity['sentiment']['score']
                entMagnitude = entity['sentiment']['magnitude']

            else:

                entSentiment, entMagnitude = '', ''

            name = entity['name']
            entType = entity['type']
            salience = entity['salience']
            metadata = json.dumps(
                entity['metadata']) if entity['metadata'] != {} else ''

            entityId = self._hash_string([documentId, name])

            _writerRowEntities = {'entityId': entityId,
                                  'documentId': documentId,
                                  'name': name,
                                  'type': entType,
                                  'salience': salience,
                                  'metadata': metadata,
                                  'sentimentScore': entSentiment,
                                  'sentimentMagnitude': entMagnitude}

            self.writer.writerEntities.writerow(_writerRowEntities)
            self.write_mentions(entityId, entity)

    def write_mentions(self, entityId, nlpEntity):

        nlpMentions = nlpEntity['mentions']

        for mention in nlpMentions:

            if 'mentions' in self.resultSentimentTables:

                mentSentiment = mention['sentiment']['score']
                mentMagnitude = mention['sentiment']['magnitude']

            else:

                mentSentiment, mentMagnitude = '', ''

            textContent = mention['text']['content']
            textOffset = mention['text']['beginOffset']
            mentType = mention['type']

            mentionId = self._hash_string([entityId, textContent, textOffset])

            _writerRowMentions = {'mentionId': mentionId,
                                  'entityId': entityId,
                                  'textContent': textContent,
                                  'textOffset': textOffset,
                                  'type': mentType,
                                  'sentimentScore': mentSentiment,
                                  'sentimentMagnitude': mentMagnitude}

            self.writer.writerMentions.writerow(_writerRowMentions)

    @staticmethod
    def flatten_json(js, out={}, name='', delim='_'):
        if type(js) is dict:
            for a in js:
                Component.flatten_json(js[a], out, name + a + delim)
        else:
            out[name[:-1]] = js

        return out

    def write_tokens(self, documentId, nlpResult):

        nlpTokens = nlpResult['tokens']

        idx = -1

        for token in nlpTokens:

            idx += 1

            textContent = token['text']['content']
            textOffset = token['text']['beginOffset']

            _flatToken = self.flatten_json(js=token)

            tokenId = self._hash_string([documentId,
                                         textContent,
                                         textOffset,
                                         idx])

            _writerRowTokens = {**{'tokenId': tokenId,
                                   'documentId': documentId,
                                   'textContent': textContent,
                                   'textOffset': textOffset,
                                   'index': idx},
                                **_flatToken}

            self.writer.writerTokens.writerow(_writerRowTokens)

    def split_and_write_data(self, documentId, nlpResult, skipCategories=False):

        for table in self.writer.resultTableNames:

            # Mentions are automatically created with entities and are its child
            # Errors are logged separately
            if table in ['mentions', 'errors']:
                continue

            elif skipCategories is True:
                continue

            else:
                f = eval('self.write_' + table)
                f(documentId, nlpResult)


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)



================================================
FILE: src/result.py
================================================
import csv
import json
import logging
import os
from kbc.result import KBCResult, KBCTableDef

# First column used as ID
FIELDS_ENTITIES = ['entityId', 'documentId', 'name', 'type',
                   'salience', 'metadata', 'sentimentMagnitude',
                   'sentimentScore']

FIELDS_MENTIONS = ['mentionId', 'entityId', 'textContent', 'textOffset',
                   'type', 'sentimentMagnitude', 'sentimentScore']

FIELDS_DOCUMENTS = ['documentId', 'language', 'sentimentMagnitude', 'sentimentScore']

FIELDS_SENTENCES = ['sentenceId', 'documentId', 'index', 'textContent', 'textOffset',
                    'sentimentMagnitude', 'sentimentScore']

FIELDS_CATEGORIES = ['categoryDocumentId', 'documentId', 'categoryName', 'confidence']

FIELDS_TOKENS = ['tokenId', 'documentId', 'textContent', 'textOffset', 'lemma', 'index',
                 'partOfSpeech_tag', 'partOfSpeech_aspect', 'partOfSpeech_case',
                 'partOfSpeech_form', 'partOfSpeech_gender', 'partOfSpeech_mood',
                 'partOfSpeech_number', 'partOfSpeech_person', 'partOfSpeech_proper',
                 'partOfSpeech_reciprocity', 'partOfSpeech_tense', 'partOfSpeech_voice',
                 'dependencyEdge_headTokenIndex', 'dependencyEdge_label']

FIELDS_ERRORS = ['documentId', 'category', 'severity', 'message']


class resultWriter:

    def __init__(self, methodList, dataPath):

        self.paramMethods = methodList
        self.paramDataPath = dataPath

        self.create_writers()
        self.create_manifests()

    def _create_table_definition(self, tableName, tableColumns):

        _pk = tableColumns[0]
        _fileName = tableName + '.csv'
        _full_path = os.path.join(self.paramDataPath, _fileName)

        _tbl_def = KBCTableDef(name=tableName, columns=tableColumns, pk=[_pk])
        _result_def = KBCResult(file_name=_fileName, full_path=_full_path, table_def=_tbl_def)

        return _result_def

    @staticmethod
    def _create_csv_writer(tableDefinition):

        _writer = csv.DictWriter(open(tableDefinition.full_path, 'w'),
                                 fieldnames=tableDefinition.table_def.columns,
                                 restval='', extrasaction='ignore',
                                 quotechar='"', quoting=csv.QUOTE_ALL)

        return _writer

    def create_writers(self):

        _resultTableMap = {'classifyText': ['documents', 'categories', 'errors'],
                           'extractDocumentSentiment': ['documents', 'sentences', 'errors'],
                           'extractEntities': ['documents', 'entities', 'mentions', 'errors'],
                           'extractEntitySentiment': ['documents', 'entities', 'mentions', 'errors'],
                           'extractSyntax': ['documents', 'sentences', 'tokens', 'errors']}

        _resultsTableColumn = {'documents': FIELDS_DOCUMENTS,
                               'sentences': FIELDS_SENTENCES,
                               'categories': FIELDS_CATEGORIES,
                               'tokens': FIELDS_TOKENS,
                               'entities': FIELDS_ENTITIES,
                               'mentions': FIELDS_MENTIONS,
                               'errors': FIELDS_ERRORS}

        _createdTables = []
        _createdTablesDef = []

        for method in self.paramMethods:

            _tables = _resultTableMap[method]

            for t in _tables:

                if t not in _createdTables:

                    logging.debug("Creating writer for %s." % t)

                    _tableDef = self._create_table_definition(t, _resultsTableColumn[t])
                    _writer = self._create_csv_writer(_tableDef)

                    if t == 'documents':

                        self.writerDocuments = _writer
                        self.writerDocuments.writeheader()

                    elif t == 'sentences':

                        self.writerSentences = _writer
                        self.writerSentences.writeheader()

                    elif t == 'categories':

                        self.writerCategories = _writer
                        self.writerCategories.writeheader()

                    elif t == 'tokens':

                        self.writerTokens = _writer
                        self.writerTokens.writeheader()

                    elif t == 'entities':

                        self.writerEntities = _writer
                        self.writerEntities.writeheader()

                    elif t == 'mentions':

                        self.writerMentions = _writer
                        self.writerMentions.writeheader()

                    elif t == 'errors':

                        self.writerErrors = _writer
                        self.writerErrors.writeheader()

                    _writer = None
                    _createdTables += [t]
                    _createdTablesDef += [_tableDef]

                else:

                    continue

        self.resultTableDefinitions = _createdTablesDef
        self.resultTableNames = _createdTables

    @staticmethod
    def _create_manifest_template(pk=[], incremental=True):

        return {'primary_key': pk, 'incremental': incremental}

    def create_manifests(self):

        for tableDef in self.resultTableDefinitions:

            if tableDef.table_def.name == 'errors':

                _manifest = self._create_manifest_template(pk=[], incremental=False)

            else:

                _manifest = self._create_manifest_template(pk=tableDef.table_def.pk)

            _path = tableDef.full_path + '.manifest'
            with open(_path, 'w') as file:

                json.dump(_manifest, file)



================================================
FILE: tests/__init__.py
================================================



================================================
FILE: tests/test_component.py
================================================


