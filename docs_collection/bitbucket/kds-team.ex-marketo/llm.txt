Directory structure:
└── kds_consulting_team-kds-team.ex-marketo/
    ├── flake8.cfg
    ├── deploy.sh
    ├── docker-compose.yml
    ├── change_log.md
    ├── src/
    │   ├── functions.py
    │   └── main.py
    ├── bitbucket-pipelines.yml
    ├── tests/
    │   ├── test_component.py
    │   └── __init__.py
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── configuration_description.md
    │   ├── component_short_description.md
    │   ├── stack_parameters.json
    │   ├── configSchema.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   ├── tables/
    │       │   │   ├── test.csv
    │       │   │   └── test.csv.manifest
    │       │   └── files/
    │       │       └── order1.xml
    │       └── out/
    │           ├── tables/
    │           │   └── test.csv
    │           └── files/
    │               └── order1.xml
    ├── scripts/
    │   ├── run_kbc_tests.ps1
    │   ├── run.bat
    │   └── build_n_run.ps1
    ├── requirements.txt
    ├── Dockerfile
    ├── LICENSE.md
    └── README.md

================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi

================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data

================================================
File: /change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog


================================================
File: /src/functions.py
================================================
import csv
import pandas as pd
from keboola import docker
# from marketorestpython.client import MarketoClient
import json
import logging

cfg = docker.Config('/data/')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt="%Y-%m-%d %H:%M:%S")


def get_tables(in_tables):
    """
    Evaluate input and output table names.
    Only taking the first one into consideration!
    """

    # input file
    table = in_tables[0]
    in_name = table["full_path"]
    in_destination = table["destination"]
    logging.info("Data table: " + str(in_name))
    logging.info("Input table source: " + str(in_destination))

    return in_name


def get_output_tables(out_tables):
    """
    Evaluate output table names.
    Only taking the first one into consideration!
    """

    # input file
    table = out_tables[0]
    in_name = table["full_path"]
    in_destination = table["source"]
    logging.info("Data table: " + str(in_name))
    logging.info("Input table source: " + str(in_destination))

    return in_name


def output_file(output_model, file_out="data.json"):
    """
    Save output data as CSV (pandas)
    """

    # with open(file_out, "w", encoding="utf-8") as csvfile:
    with open(file_out, 'w') as jsonfile:
        json.dump(output_model, jsonfile)

    jsonfile.close()


def extract_leads_by_ids(output_file, source_file, mc_object,
                         fields=['id', 'firstName', 'lastName', 'email',
                                 'updatedAt', 'createdAt', 'Do Not Call Reason']):
    """
    Extracts leads by lead_id.
    The input file needs to contain a column with
    """

    with open(source_file, mode='rt', encoding='utf-8') as in_file,\
            open(output_file, mode='w', encoding='utf-8') as out_file:

        leads = []
        lazy_lines = (line for line in in_file)
        reader = csv.DictReader(lazy_lines, lineterminator='\n')

        for lead_record in reader:
            lead_detail = mc_object.execute(method='get_lead_by_id',
                                            id=lead_record["lead_id"])
            if len(lead_detail) > 0:
                leads.append(lead_detail[0])

        keys = (fields)
        dict_writer = csv.DictWriter(out_file, keys)
        dict_writer.writeheader()
        dict_writer.writerows(leads)

        cfg.write_table_manifest(file_name=output_file,
                                 destination='',
                                 primary_key=['id'],
                                 incremental=True)


def extract_leads_by_filter(output_file,
                            source_file,
                            mc_object,
                            filter_on,
                            filter_values_column,
                            fields=['id', 'firstName', 'lastName', 'email',
                                    'updatedAt', 'createdAt', 'Do Not Call Reason']):
    """
    Extracts leads based on filters
    source_file -  needs to contain a column with the values to input
    to the filter
    filter_values_column - the column in the source file that contains
    the values to input to the filter
    filter_on- specifies the field in API (e.g. "email")
    fields - the fields in API that will be returned
    """

    with open(source_file, mode='rt', encoding='utf-8') as in_file,\
            open(output_file, mode='w', encoding='utf-8') as out_file:

        leads = []
        lazy_lines = (line for line in in_file)
        reader = csv.DictReader(lazy_lines, lineterminator='\n')

        filter_values_list = []
        for lead_record in reader:
            filter_values_list.append(lead_record[filter_values_column])

        leads = mc_object.execute(method='get_multiple_leads_by_filter_type',
                                  filterType=filter_on,
                                  filterValues=filter_values_list,
                                  fields=fields,
                                  batchSize=None)

        if len(leads) > 0:
            print('%i leads extracted', len(leads))
        else:
            print('No leads match the criteria!')

        keys = (fields)
        dict_writer = csv.DictWriter(out_file, keys)
        dict_writer.writeheader()
        dict_writer.writerows(leads)

        cfg.write_table_manifest(file_name=output_file,
                                 destination='',
                                 primary_key=['id'],
                                 incremental=True)


def get_companies(output_file,
                  source_file,
                  mc_object,
                  filter_on,
                  filter_values_column,
                  fields=['id', 'firstName', 'lastName', 'email', 'updatedAt',
                          'createdAt', 'Do Not Call Reason']):
    """
    filterType can be: externalCompanyId, id, externalSalesPersonId, company

    Extracts companies based on filters
    source_file -  needs to contain a column with the values to input
    to the filter
    filter_values_column - the column in the source file that contains
    the values to input to the filter
    filter_on- specifies the field in API (e.g. "company")
    fields - the fields in the API that will be returned
    """

    with open(source_file, mode='rt', encoding='utf-8') as in_file,\
            open(output_file, mode='w', encoding='utf-8') as out_file:

        companies = []
        lazy_lines = (line for line in in_file)
        reader = csv.DictReader(lazy_lines, lineterminator='\n')

        filter_values_list = []
        for company_record in reader:
            filter_values_list.append(company_record[filter_values_column])

        companies = mc_object.execute(method='get_companies',
                                      filterType=filter_on,
                                      filterValues=filter_values_list,
                                      fields=fields,
                                      batchSize=None)

        if len(companies) > 0:
            print('%i companies extracted', len(companies))
        else:
            print('No companies match the criteria!')

        keys = (fields)
        dict_writer = csv.DictWriter(out_file, keys)
        dict_writer.writeheader()
        dict_writer.writerows(companies)

        cfg.write_table_manifest(file_name=output_file,
                                 destination='',
                                 primary_key=['id'],
                                 incremental=True)


def get_lead_activities(output_file,
                        source_file,
                        mc_object,
                        since_date,
                        until_date):
    """
    source file: has to contain columns 'activity_type_ids' and 'lead_ids'. These
    must contain the values corresponding for the query.
    output file: will contain columns based on the fields in extracted responses
    - it can definitely happen that different runs will produce different number of columns!!
    since_date
    until_date
    """

    activity_type_ids = list(pd.read_csv(source_file,
                                         skipinitialspace=True,
                                         usecols=['activity_type_ids']).iloc[:, 0])
    lead_ids = list(pd.read_csv(source_file,
                                skipinitialspace=True,
                                usecols=['lead_ids']).iloc[:, 0])

    lead_ids = [str(int(i)) for i in lead_ids if str(i) != 'nan']
    activity_type_ids = [str(int(i))
                         for i in activity_type_ids if str(i) != 'nan']

    results = mc_object.execute(method='get_lead_activities',
                                activityTypeIds=activity_type_ids,
                                nextPageToken=None,
                                sinceDatetime=since_date,
                                untilDatetime=until_date,
                                batchSize=None,
                                listId=None,
                                leadIds=lead_ids)

    if len(results) == 0:
        print('No results!')
        return

    unique_keys = []
    for i in results:
        try:
            for j in i['attributes']:
                i[j['name']] = j['value']

            i.pop('attributes', None)

            unique_keys.extend(list(i.keys()))
        except KeyError:
            pass

    unique_keys = list(set(unique_keys))

    with open(output_file, mode='w', encoding='utf-8') as out_file:

        keys = (unique_keys)
        dict_writer = csv.DictWriter(out_file, keys)
        dict_writer.writeheader()
        dict_writer.writerows(results)

    cfg.write_table_manifest(file_name=output_file,
                             destination='',
                             primary_key=['id'],
                             incremental=True)


def get_lead_changes(output_file,
                     fields,
                     since_date,
                     until_date,
                     mc_object):
    """
    this function takes very long to run
    output file: will contain columns based on the fields in extracted responses
     - it can definitely happen that different runs will produce different number of columns!!
    since_date
    until_date
    fields: list of field names to return changes for, field names can be
     retrieved with the Describe Lead API
    """
    logging.info('function get_lead_changes started')
    results = mc_object.execute(method='get_lead_changes',
                                fields=fields,
                                nextPageToken=None,
                                sinceDatetime=since_date,
                                untilDatetime=until_date,
                                batchSize=None,
                                listId=None)
    logging.info(str(len(results)) + ' results fetched')
    if len(results) == 0:
        print('No results!')
        return

    unique_keys = []
    for i in results:

        try:
            for j in i['attributes']:
                i[j['name']] = j['value']

            i.pop('attributes', None)

        except KeyError:
            pass

        try:
            for l in range(len(i['fields'])):  # noqa
                i['name'] = i['fields'][l]['name']
                i[("newValue_{}").format(i['name'])] = i['fields'][l]['newValue']
                i['oldValue' + '_' + i['name']] = i['fields'][l]['oldValue']

            i.pop('fields', None)

        except KeyError:
            pass

        except TypeError:
            pass

        unique_keys.extend(list(i.keys()))

    unique_keys = list(set(unique_keys))

    with open(output_file, mode='w', encoding='utf-8') as out_file:

        fieldnames = ['leadId', 'activityDate', 'activityTypeId']
        results_trimmed = [0] * len(results)
        for i in range(len(results)):
            results_trimmed[i] = {
                your_key: results[i][your_key] for your_key in fieldnames}

        dict_writer = csv.DictWriter(out_file, fieldnames)
        dict_writer.writeheader()
        dict_writer.writerows(results_trimmed)

    cfg.write_table_manifest(file_name=output_file,
                             destination='',
                             primary_key=['leadId'],
                             incremental=True)


def get_deleted_leads(output_file,
                      since_date,
                      mc_object):
    """
    output file: will contain first and last name, Marketo ID and time of deletion,
    but no additional Lead attributes
    since_date
    """

    results = mc_object.execute(method='get_deleted_leads',
                                nextPageToken=None,
                                sinceDatetime=since_date,
                                batchSize=None)

    if len(results) == 0:
        print('No results!')
        return

    with open(output_file, mode='w', encoding='utf-8') as out_file:

        keys = ['id',
                'marketoGUID',
                'leadId',
                'activityDate',
                'activityTypeId',
                'campaignId',
                'primaryAttributeValueId',
                'primaryAttributeValue',
                'attributes']
        dict_writer = csv.DictWriter(out_file, keys)
        dict_writer.writeheader()
        dict_writer.writerows(results)

    cfg.write_table_manifest(file_name=output_file,
                             destination='',
                             primary_key=['id'],
                             incremental=True)


def get_opportunities(output_file,
                      source_file,
                      mc_object,
                      filter_on,
                      filter_values_column,
                      fields=['id', 'firstName', 'lastName', 'email',
                              'updatedAt', 'createdAt', 'Do Not Call Reason']):
    """
    Returns opportunities based on a filter and set of values.
    source_file -  needs to contain a column with the values to input
    to the filter
    output_file -
    filter_values_column - the column in the source file that contains
    the values to input to the filter
    filter_on- specifies the field in API (e.g. "email")
    fields - the fields in the API that will be returned
    """

    with open(source_file, mode='rt', encoding='utf-8') as in_file,\
            open(output_file, mode='w', encoding='utf-8') as out_file:

        leads = []
        lazy_lines = (line for line in in_file)
        reader = csv.DictReader(lazy_lines, lineterminator='\n')

        filter_values_list = []
        for lead_record in reader:
            filter_values_list.append(lead_record[filter_values_column])

        leads = mc_object.execute(method='get_opportunities',
                                  filterType=filter_on,
                                  filterValues=filter_values_list,
                                  fields=fields,
                                  batchSize=None)

        if len(leads) > 0:
            print('%i leads extracted', len(leads))
        else:
            print('No leads match the criteria!')

        keys = (fields)
        dict_writer = csv.DictWriter(out_file, keys)
        dict_writer.writeheader()
        dict_writer.writerows(leads)

    cfg.write_table_manifest(file_name=output_file,
                             destination='',
                             primary_key=['id'],
                             incremental=True)


def get_campaigns(output_file,
                  mc_object,
                  filter_values_column,
                  source_file='blankblank.csv'):
    '''
    extract all the campaigns, if id argument is left blank, all campaigns are
    retrieved
    '''

    try:
        with open(source_file, mode='rt', encoding='utf-8') as in_file,\
                open(output_file, mode='w', encoding='utf-8') as out_file:

            logging.info('Extracting specific campaigns based on id.')

            lazy_lines = (line for line in in_file)
            reader = csv.DictReader(lazy_lines, lineterminator='\n')

            id_values_list = []
            for record in reader:
                id_values_list.append(record[filter_values_column])

            results = mc_object.execute(method='get_multiple_campaigns',
                                        id=id_values_list
                                        )

            if len(results) > 0:
                logging.info('%i campaigns extracted', len(results))
            else:
                logging.info('No campaigns match the criteria!')

            keys = ['id', 'name', 'description', 'type', 'programName',
                    'programId', 'workspaceName', 'createdAt', 'updatedAt', 'active']
            dict_writer = csv.DictWriter(out_file, keys)
            dict_writer.writeheader()
            dict_writer.writerows(results)

        cfg.write_table_manifest(file_name=output_file,
                                 destination='',
                                 primary_key=['id'],
                                 incremental=True)

    except FileNotFoundError:
        results = mc_object.execute(method='get_multiple_campaigns')
        logging.info('No input file specified, extracting all campaigns.')

        if len(results) > 0:
            print('%i campaigns extracted', len(results))
        else:
            print('No campaigns match the criteria!')

        keys = ['id', 'name', 'description', 'type', 'programName',
                'programId', 'workspaceName', 'createdAt', 'updatedAt', 'active']

        with open(output_file, mode='w', encoding='utf-8') as out_file:
            dict_writer = csv.DictWriter(out_file, keys)
            dict_writer.writeheader()
            dict_writer.writerows(results)

        cfg.write_table_manifest(file_name=output_file,
                                 destination='',
                                 primary_key=['id'],
                                 incremental=True)


def get_activity_types(output_file,
                       mc_object):
    """
    Returns a list of available activity types in the target instance,
    along with associated metadata of each type
    """
    activity_types = mc_object.execute(method='get_activity_types')

    if len(activity_types) > 0:
        print('%i activities found', len(activity_types))
    else:
        print('No activities found!')

    with open(output_file, mode='w', encoding='utf-8') as out_file:
        keys = ['attributes', 'description', 'id', 'name', 'primaryAttribute']
        dict_writer = csv.DictWriter(out_file, keys)
        dict_writer.writeheader()
        dict_writer.writerows(activity_types)

    cfg.write_table_manifest(file_name=output_file,
                             destination='',
                             primary_key=['id'],
                             incremental=True)


================================================
File: /src/main.py
================================================
import sys
import os
import logging
from keboola import docker
from marketorestpython.client import MarketoClient
from datetime import datetime, timedelta
import functions as fces
import logging_gelf.handlers
import logging_gelf.formatters  # noqa
"__author__ = 'Radim Kasparek kasrad'"
"__credits__ = 'Keboola Drak"
"__component__ = 'Marketo Extractor'"

"""
Python 3 environment
"""


sys.tracebacklimit = None

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt="%Y-%m-%d %H:%M:%S")

logger = logging.getLogger()
logging_gelf_handler = logging_gelf.handlers.GELFTCPSocketHandler(
    host=os.getenv('KBC_LOGGER_ADDR'),
    port=int(os.getenv('KBC_LOGGER_PORT'))
)
logging_gelf_handler.setFormatter(
    logging_gelf.formatters.GELFFormatter(null_character=True))
logger.addHandler(logging_gelf_handler)

# removes the initial stdout logging
logger.removeHandler(logger.handlers[0])

# Disabling list of libraries you want to output in the logger
disable_libraries = [
    'matplotlib'
]
for library in disable_libraries:
    logging.getLogger(library).disabled = True


COMPONENT_VERSION = '0.0.13'
logging.info(f'Version: {COMPONENT_VERSION}')

# Destination to fetch and output files and tables
DEFAULT_TABLE_INPUT = "/data/in/tables/"
DEFAULT_FILE_INPUT = "/data/in/files/"

DEFAULT_FILE_DESTINATION = "/data/out/files/"
DEFAULT_TABLE_DESTINATION = "/data/out/tables/"

# Access the supplied rules
cfg = docker.Config('/data/')
params = cfg.get_parameters()

if params == {}:
    logging.error('Empty configuration. Please configure your component.')
    sys.exit(1)

# enter Client ID from Admin > LaunchPoint > View Details
# fill in Munchkin ID, typical format 000-AAA-000
# enter Client ID and Secret from Admin > LaunchPoint > View Details

client_id = cfg.get_parameters()["#client_id"]
munchkin_id = cfg.get_parameters()["#munchkin_id"]
client_secret = cfg.get_parameters()["#client_secret"]
method = cfg.get_parameters()["method"]
desired_fields_tmp = cfg.get_parameters()["desired_fields"]
since_date = cfg.get_parameters()["since_date"]  # YYYY-MM-DD
until_date = cfg.get_parameters()["until_date"]  # YYYY-MM-DD
filter_column = cfg.get_parameters()["filter_column"]
filter_field = cfg.get_parameters()["filter_field"]
dayspan = cfg.get_parameters()["dayspan"]
desired_fields = [i.strip() for i in desired_fields_tmp.split(",")]

logging.info("Desired fields: %s" %
             str(desired_fields)) if desired_fields_tmp else ''
logging.info("Since date: %s" % since_date) if since_date else ''
logging.info("Until date: %s" % until_date) if until_date else ''
logging.info("Filter column: %s" % filter_column) if filter_column else ''
logging.info("Filter field: %s" % filter_field) if filter_field else ''
logging.info("Dayspan: %s" % dayspan) if dayspan else dayspan

# Get proper list of tables
in_tables = cfg.get_input_tables()
out_tables = cfg.get_expected_output_tables()
out_files = cfg.get_expected_output_files()
logging.info("IN tables mapped: " + str(in_tables))
logging.info(filter_column)

# Preset data parameters if not specified
if since_date != '' and dayspan != '':
    logging.error("Please add either since_date or dayspan, not both.")
    sys.exit(1)
elif since_date == '' and dayspan != '':
    since_date = str((datetime.utcnow() - timedelta(days=int(dayspan)))
                     .date())
    until_date = str(datetime.utcnow().date())


# main


def main():
    """
    Main execution script.
    """
    mc = MarketoClient(munchkin_id, client_id, client_secret)
    logging.info('Reading Marketo')
    if method == 'extract_leads_by_ids':
        fces.extract_leads_by_ids(output_file=DEFAULT_TABLE_DESTINATION + 'leads_by_ids.csv',
                                  source_file=DEFAULT_TABLE_INPUT +
                                  in_tables[0]['destination'],
                                  fields=desired_fields,
                                  mc_object=mc)

    elif method == 'extract_leads_by_filter':
        fces.extract_leads_by_filter(output_file=DEFAULT_TABLE_DESTINATION + 'leads_by_filter.csv',
                                     source_file=DEFAULT_TABLE_INPUT +
                                     in_tables[0]['destination'],
                                     filter_on=filter_field,
                                     filter_values_column=filter_column,
                                     fields=desired_fields,
                                     mc_object=mc)

    elif method == 'get_deleted_leads':
        fces.get_deleted_leads(output_file=DEFAULT_TABLE_DESTINATION + 'deleted_leads.csv',
                               since_date=since_date,
                               mc_object=mc)

    elif method == 'get_lead_changes':
        fces.get_lead_changes(output_file=DEFAULT_TABLE_DESTINATION + 'lead_changes.csv',
                              fields=desired_fields,
                              since_date=since_date,
                              until_date=until_date,
                              mc_object=mc)

    elif method == 'get_lead_activities':
        fces.get_lead_activities(output_file=DEFAULT_TABLE_DESTINATION + 'lead_activites.csv',
                                 source_file=DEFAULT_TABLE_INPUT +
                                 in_tables[0]['destination'],
                                 since_date=since_date,
                                 until_date=until_date,
                                 mc_object=mc)

    elif method == 'get_companies' and len(in_tables):
        fces.get_companies(output_file=DEFAULT_TABLE_DESTINATION + 'companies.csv',
                           source_file=DEFAULT_TABLE_INPUT +
                           in_tables[0]['destination'],
                           filter_on=filter_field,
                           filter_values_column=filter_column,
                           fields=desired_fields,
                           mc_object=mc)

    elif method == 'get_campaigns' and len(in_tables) != 0:
        fces.get_campaigns(
            output_file=DEFAULT_TABLE_DESTINATION + 'campaigns.csv',
            source_file=DEFAULT_TABLE_INPUT +
            in_tables[0]['destination'],
            mc_object=mc,
            filter_values_column=filter_column)

    elif method == 'get_campaigns' and len(in_tables) == 0:
        fces.get_campaigns(
            output_file=DEFAULT_TABLE_DESTINATION + 'campaigns.csv',
            mc_object=mc,
            filter_values_column=filter_column)

    elif method == 'get_activity_types':
        fces.get_activity_types(output_file=DEFAULT_TABLE_DESTINATION + 'activity_types.csv',
                                mc_object=mc)


def validate_user_parameters():
    # 1 - Component cannot accepy more than one table as input table
    if len(in_tables) > 1:
        logging.error('Please do not use more than one table as input table.')
        sys.exit(1)

    # 2 - ensure there is a configuration
    if params == {}:
        logging.error(
            'Empty configuration. Please configure your configuration.')
        sys.exit(1)

    # 3 - Ensure the reequired credentials are entered
    if munchkin_id == '' or client_id == '' or client_secret == '':
        logging.error(
            'Credentials are missing: [Munchkin ID token], [Client ID Token], [Client Secret Token]')
        sys.exit(1)

    # 4 - ensure the input table is available for some endpoints
    endpoints_req = [
        'extract_leads_by_ids',
        'extract_leads_by_filter',
        'get_lead_activities',
        'get_companies'
    ]
    if method in endpoints_req and len(in_tables) == 0:
        logging.error(
            f'Input table is missing. Method [{method}] requies an input table.')
        sys.exit(1)

    # 5 - ensure the since_date is not larger than until date
    if since_date != '' and until_date != '':
        since_date_obj = datetime.strptime(since_date, '%Y-%m-%d')
        until_date_obj = datetime.strptime(until_date, '%Y-%m-%d')

        if until_date_obj < since_date_obj:
            logging.error('[From] date cannot exceed [To] date. Please validate your parameters.')
            sys.exit(1)

    # 6 - ensure startdate is not larger than today
    if since_date != '':
        since_date_obj = datetime.strptime(since_date, '%Y-%m-%d')

        if datetime.now() < since_date_obj:
            logging.error('[From] date is larger than today.')
            sys.exit(1)


if __name__ == "__main__":

    validate_user_parameters()
    main()

    logging.info("Component [kdsteam.ex-marketo] finished.")


================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          #- echo 'Pushing test image to repo. [tag=test]'
          #- export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          #- docker tag $APP_IMAGE:latest $REPOSITORY:test
          #- eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          #- docker push $REPOSITORY:test

  tags:
    '*':
      - step:
          deployment: production
          script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          - echo "Preparing KBC test image"
          - docker pull quay.io/keboola/developer-portal-cli-v2:latest
          # push test image to ECR - uncomment when initialised
          # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          # - docker tag $APP_IMAGE:latest $REPOSITORY:test
          # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          # - docker push $REPOSITORY:test
          # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
          # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
          - ./deploy.sh

================================================
File: /component_config/component_long_description.md
================================================
This component allows KBC to get data from a few endpoints of the Marketo REST API (http://developers.marketo.com/rest-api/). 
It relies heavily on package marketorestpython (https://github.com/jepcastelein/marketo-rest-python).

As of now, the component can be used for extracting data from 5 endpoints, but it can be easily extended:
- extract_leads_by_ids
- extract_leads_by_filter
- get_lead_activities
- get_lead_changes
- get_deleted_leads
- get_campaigns
- get_activity_types

## Parameters
There are 10 options in the UI:
- Munchkin ID
- Client ID
- Client Secret
- Input/Output tables. *The tables need to mapped to specific names (names can be found in main.py)*
- Desired Fields. *write down the column names you want to extract and separate them by white space*
- Method. *denotes the endpoint*
- Until Date
- Since Date
- Column with Filter Values *Denotes the column in the input file that contains the values you want to filter for*
- Field to filter on *Denotes the field in the API to filter on*
- How many days back you want to go? *Alternative to Since Date. The Until Date is automatically set to current date if this field is used.*

## Endpoints
- extract_leads_by_ids
    * Takes in _Desired Fields_ parameter
    * The output is mapped to `leads_by_ids`
    * The input needs to contain column with name `lead_id` with list of id
    * The _Desired Fields_ parameter denotes which fields should be retrieved
- extract_leads_by_filter
    * Takes in _Desired Fields_ and _Column with Filter Values_ parameter
    * The output is mapped to `leads_by_filter`
    * The input needs to contain column with the values to input to the filter (the column is specified by the _Filter Values Column_)
    * The _Desired Fields_ parameter denotes which fields should be retrieved
    * Current functionality allows only filtering on e-mail
- get_lead_activities
    * Takes in _Since Date_ and _Until_date_ parameter. These denote the date when the activity occured.
    * The output is mapped to `lead_activites`
    * The input needs to contain columns 'activity_type_ids' and 'lead_ids'
    * output file will contain columns based on the fields in extracted responses 
    * it can definitely happen that different runs will produce different number of columns!!
    * _Since Date_ and _until_date_ parameters are self-explanatory
- get_lead_changes
    * Takes in _Since Date, Until Date_ and _Desired Fields_ parameter. These denote the date of the change in a lead.
    * The output is mapped to `lead_changes`
    * Output file will contain columns `leadId`, `activityDate` and `activityTypeId`
    * _Since Date_  and _Until Date_ parameters are self-explanatory
    * _Desired Fields_ parameter denotes the list of field names to return changes for, field names can be retrieved with the Describe Lead API
- get_deleted_leads
    * Takes in the _Since Date_ parameter. This denotes the date when a lead was deleted.
    * The output is mapped to `deleted_leads`
- get_campaigns
    * Takes in the _Field to filter on_ and _Column with Filter Values_
    * If the above parameters are left blank, all campaigns are retrieved.
    * The output is mapped to `campaigns`
- get_activity_types
    * Doesn't require any parameters.
    * The output is mapped to `activity_types`.
    







================================================
File: /component_config/configuration_description.md
================================================
### Configurations

1. Desired Fields
    - Specifying the columns you want to extract
    - Relevant to the endpoints below
        1. `extract_leads_by_ids`
        2. `extract_leads_by_filter`
        3. `get_lead_changes`
        4. `get_companies`

2. Since Date (From)
    - Specifing the date range of the extraction
    - Relevant to the endpoints below
        1. `get_deleted_leads`
        2. `get_lead_changes`
        3. `get_lead_activities`

3. Until Date (To)
    - Specifing the date range of the extraction
    - Relevant to the endpoints below
        1. `get_lead_changes`
        2. `get_lead_activities`

4. Filter Column
    - Denotes the column in the input file that contains the values you want to filter
    - Relevant to the endpoints below
        1. `extract_leads_by_filter`
        2. `get_campaigns`

5. Field to filter on
    - The API fields you want to filter on

6. How many days back you want to go?
    - Alternative to `Since Date`. The `Until Date` is automatically set to current date if this field is used

================================================
File: /component_config/component_short_description.md
================================================
Marketo is a powerful marketing automation software.

================================================
File: /component_config/stack_parameters.json
================================================
{}

================================================
File: /component_config/configSchema.json
================================================
{
	"type": "object",
	"title": "Marketo Database",
	"required": [
		"#munchkin_id",
		"#client_id",
		"#client_secret",
		"method",
		"since_date",
		"until_date",
		"filter_column",
		"desired_fields",
		"filter_field",
		"dayspan"
	],
	"properties": {
		"#munchkin_id": {
			"type": "string",
			"title": "Munchkin ID token",
			"format": "password",
			"description": "Can be found in Admin > Web Services menu in the REST API section",
			"propertyOrder": 1
		},
		"#client_id": {
			"type": "string",
			"title": "Client ID token",
			"format": "password",
			"propertyOrder": 2
		},
		"#client_secret": {
			"type": "string",
			"title": "Client Secret token",
			"format": "password",
			"propertyOrder": 3
		},
		"desired_fields": {
			"type": "string",
			"title": "Desired Fields",
			"description": "Comma seperated. Column names you want to extract. Relevant to some endpoints only.",
			"propertyOrder": 4
		},
		"method": {
			"enum": [
				"extract_leads_by_ids",
				"extract_leads_by_filter",
				"get_deleted_leads",
				"get_lead_changes",
				"get_lead_activities",
				"get_companies",
				"get_campaigns",
				"get_activity_types"
			],
			"type": "string",
			"title": "Method",
			"default": "extract_leads_by_ids",
			"propertyOrder": 5
		},
		"since_date": {
			"type": "string",
			"title": "Since Date (From)",
			"description": "Start date for the extraction. Format: YYYY-MM-DD",
			"propertyOrder": 6
		},
		"until_date": {
			"type": "string",
			"title": "Until Date (To)",
			"description": "End date for the extraction. Format: YYYY-MM-DD",
			"propertyOrder": 7
		},
		"filter_column": {
			"type": "string",
			"title": "Column with Filter Values",
			"description": "Denotes the column in the input file that contains the values you want to filter for. Relevant only for [extract_leads_by_filter] and [get_campaigns].",
			"propertyOrder": 8
		},
		"filter_field": {
			"type": "string",
			"title": "Field to filter on",
			"description": "The API fields you want to filter on.",
			"propertyOrder": 9
		},
		"dayspan": {
			"type": "string",
			"title": "How many days back you want to go?",
			"description": "Alternative to Since Date. The Until Date is automatically set to current date if this field is used.",
			"propertyOrder": 10
		}
	}
}

================================================
File: /component_config/sample-config/config.json
================================================
{
	"storage": {
		"input": {
			"files": [],
			"tables": [{
					"source": "in.c-test.test",
					"destination": "test.csv",
					"limit": 50,
					"columns": [],
					"where_values": [],
					"where_operator": "eq"
				}
			]
		},
		"output": {
			"files": [],
			"tables": []
		}
	},
	"parameters": {
		"project_id": "1234",
		"#api_token": "123456",
		"period_from": "2018-12-01",
		"period_to": "2018-12-06",
		"relative_period": "",
		"backfill_mode": 0,
		"metrics": [{
				"metric": "NoVt",
				"modificator": "change"
			}
		]
	},
	"image_parameters": {
		"syrup_url": "https://syrup.keboola.com/"
	},
	"authorization": {
		"oauth_api": {
			"id": "OAUTH_API_ID",
			"credentials": {
				"id": "main",
				"authorizedFor": "Myself",
				"creator": {
					"id": "1234",
					"description": "me@keboola.com"
				},
				"created": "2016-01-31 00:13:30",
				"#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
				"oauthVersion": "2.0",
				"appKey": "000000004C184A49",
				"#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
			}
		}
	}
}


================================================
File: /component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: /component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /requirements.txt
================================================
https://github.com/keboola/python-docker-application/zipball/master#egg=keboola
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.1.0.zip#egg=kbc
pytz
python-dateutil
logging_gelf==0.0.18
pandas

================================================
File: /Dockerfile
================================================
FROM python:3.7.2-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

RUN pip install flake8
RUN pip install  --upgrade --no-cache-dir --ignore-installed logging_gelf
RUN pip install  --upgrade --no-cache-dir --ignore-installed marketorestpython

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/main.py"]


================================================
File: /LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /README.md
================================================
# Keboola Marketo Extractor

Keboola Connection docker app for extracting data from specific endpoints of Marketo REST API. Available under `radim-kasparek.ex-marketo`

## API Limitations
[Marketo API Limit](http://developers.marketo.com/rest-api/marketo-integration-best-practices/)

## Functionality
This component allows KBC to get data from a few endpoints of the Marketo REST API (http://developers.marketo.com/rest-api/). 
It relies heavily on package marketorestpython (https://github.com/jepcastelein/marketo-rest-python).

As of now, the component can be used for extracting data from 5 endpoints, but it can be easily extended:
- extract_leads_by_ids
- extract_leads_by_filter
- get_lead_activities
- get_lead_changes
- get_deleted_leads
- get_campaigns
- get_activity_types

## Parameters
There are 10 options in the UI:
- Munchkin ID
- Client ID
- Client Secret
- Input/Output tables. *The tables need to mapped to specific names (names can be found in main.py)*
- Desired Fields. *write down the column names you want to extract and separate them by white space*
- Method. *denotes the endpoint*
- Until Date
- Since Date
- Column with Filter Values *Denotes the column in the input file that contains the values you want to filter for*
- Field to filter on *Denotes the field in the API to filter on*
- How many days back you want to go? *Alternative to Since Date. The Until Date is automatically set to current date if this field is used.*

## Endpoints
- extract_leads_by_ids
    * Takes in _Desired Fields_ parameter
    * The output is mapped to `leads_by_ids`
    * The input needs to contain column with name `lead_id` with list of id
    * The _Desired Fields_ parameter denotes which fields should be retrieved
- extract_leads_by_filter
    * Takes in _Desired Fields_ and _Column with Filter Values_ parameter
    * The output is mapped to `leads_by_filter`
    * The input needs to contain column with the values to input to the filter (the column is specified by the _Filter Values Column_)
    * The _Desired Fields_ parameter denotes which fields should be retrieved
    * Current functionality allows only filtering on e-mail
- get_lead_activities
    * Takes in _Since Date_ and _Until_date_ parameter. These denote the date when the activity occured.
    * The output is mapped to `lead_activites`
    * The input needs to contain columns 'activity_type_ids' and 'lead_ids'
    * output file will contain columns based on the fields in extracted responses 
    * it can definitely happen that different runs will produce different number of columns!!
    * _Since Date_ and _until_date_ parameters are self-explanatory
- get_lead_changes
    * Takes in _Since Date, Until Date_ and _Desired Fields_ parameter. These denote the date of the change in a lead.
    * The output is mapped to `lead_changes`
    * Output file will contain columns `leadId`, `activityDate` and `activityTypeId`
    * _Since Date_  and _Until Date_ parameters are self-explanatory
    * _Desired Fields_ parameter denotes the list of field names to return changes for, field names can be retrieved with the Describe Lead API
- get_deleted_leads
    * Takes in the _Since Date_ parameter. This denotes the date when a lead was deleted.
    * The output is mapped to `deleted_leads`
- get_campaigns
    * Takes in the _Field to filter on_ and _Column with Filter Values_
    * If the above parameters are left blank, all campaigns are retrieved.
    * The output is mapped to `campaigns`
- get_activity_types
    * Doesn't require any parameters.
    * The output is mapped to `activity_types`.
    







