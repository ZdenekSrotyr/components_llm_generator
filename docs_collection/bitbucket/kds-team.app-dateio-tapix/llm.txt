Directory structure:
└── kds_consulting_team-kds-team.app-dateio-tapix/
    ├── README.md
    ├── Dockerfile
    ├── LICENSE.md
    ├── bitbucket-pipelines.yml
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── flake8.cfg
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   └── tables/
    │       │       ├── merchants.csv
    │       │       ├── shops.csv
    │       │       └── transactions.csv
    │       └── out/
    │           ├── state.json
    │           └── tables/
    │               ├── invalidations.csv
    │               ├── invalidations.csv.manifest
    │               ├── merchants.csv
    │               ├── merchants.csv.manifest
    │               ├── shops.csv
    │               ├── shops.csv.manifest
    │               ├── transactions.csv
    │               └── transactions.csv.manifest
    ├── scripts/
    │   ├── build_n_test.sh
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── client.py
    │   ├── component.py
    │   ├── result.py
    │   └── run.py
    └── tests/
        ├── __init__.py
        └── test_component.py

================================================
File: README.md
================================================
# TapiX Application

The Keboola application for TapiX by Dateio serves as a connector to the TapiX service developed by Dateio for processing and enriching bank transaction data.

**Table of contents:**  
  
[TOC]

## Configuration

The configuration of the application is pretty straightforward. The component requires 5 parameters and depending on the mode, an input table may be required.

A sample of the configuration can be found in the [component's repository](https://bitbucket.org/kds_consulting_team/kds-team.app-dateio-tapix/src/master/component_config/sample-config).

### Parameters

Following parameters are required by the application:

- `username` - a username to the TapiX API; beware, that this username may be different to the username used to access the API docs;
- `#password` - a password to the TapiX API; again, may be different to the password used to access the API docs;
- `mode` - specifies, in which mode the application will be launched;
- `incremental` - whether the outputs will be loaded into storage incrementally or not;
- `invalidations_since` - a date string in relative (e.g. 3 days ago), absolute (2020-01-01) format, or use keyword **last** to load latest data based on last saved state; the parameter is only needed for the `invalidations` mode.

#### Mode

The application can work in 4 different modes:

- `transactions` - utilizes endpoint `POST /shops/findByCardTransactionBatch` to find shops by transactions;
- `shops` - utilizes endpoint `GET /shops/{id}` to download details about shops;
- `merchants` - utilizes endpoint `GET /merchants/{id}` to download details about merchants;
- `invalidations` - utilizes endpoint `GET /invalidations` to download data about invalidations.

### Input Tables

All modes, except for `invalidations` mode, require a table on the input mapping to work properly.

For `transactions`, the table should be mapped to `transactions.csv` and should contain only column (attributes), which are supported by `/shops/findByCardTransactionBatch` endpoint. Additionally, `customId` column is required as the results need to be mapped back to their original records.

For `shops`, the table should be mapped to `shops.csv` and can contain arbitrary number of columns. Required column is `id`, which must contain the `uid` of the shop, for which the information should be downloaded.

For `merchants`, the table should be mapped to `merchants.csv` and can contain arbitrary number of columns. Required column is `id`, which must contain the `uid` of the merchant, for which the information should be downloaded.

## Development

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path in the docker-compose file:

```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone repo_path my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

## Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/).


================================================
File: Dockerfile
================================================
FROM python:3.7.2-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/run.py"]



================================================
File: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
File: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - echo 'Pushing test image to repo. [tag=debug]'
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:debug
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:debug

  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
File: change_log.md
================================================



================================================
File: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "asigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "asigning travis tag"
	export TAG="$TRAVIS_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
File: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
      - KBC_RUNID=12345.123456
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
      - KBC_RUNID=12345.123456
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
File: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example,
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
File: requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.5.1.zip#egg=kbc
dateparser
mock
freezegun


================================================
File: component_config/component_long_description.md
================================================
# TapiX Application

The Keboola application for TapiX by Dateio serves as a connector to the TapiX service developed by Dateio for processing and enriching bank transaction data.

# Configuration

The configuration of the application is pretty straightforward. The component requires 5 parameters and depending on the mode, an input table may be required.

A sample of the configuration can be found in the [component's repository](https://bitbucket.org/kds_consulting_team/kds-team.app-dateio-tapix/src/master/component_config/sample-config).


================================================
File: component_config/component_short_description.md
================================================
TapiX, by Dateio, converts your raw card transactional data into meaningful information containing shop name and logo, address with GPS and business categorization.


================================================
File: component_config/configRowSchema.json
================================================
{}


================================================
File: component_config/configSchema.json
================================================
{
    "title": "Parameters",
    "type": "object",
    "required": [
        "username",
        "#password",
        "mode",
        "invalidations_date",
        "output"
    ],
    "properties": {
        "username": {
            "title": "Username",
            "description": "Username used to log in to the TapiX Dateio API. This might be different from the username used to log in to the API docs.",
            "type": "string",
            "propertyOrder": 1000
        },
        "#password": {
            "title": "Password",
            "description": "Password used to log in to the TapiX Dateio API. This might be different from the password used to log in to the API docs.",
            "type": "string",
            "format": "password",
            "propertyOrder": 2000
        },
        "mode": {
            "title": "Mode",
            "description": "A mode, in which the application will be ran. See documentation for more information.",
            "type": "string",
            "enum": [
                "transactions",
                "invalidations",
                "shops",
                "merchants"
            ],
            "default": "transactions",
            "propertyOrder": 3000
        },
        "invalidations_date": {
            "title": "Invalidations Date Settings",
            "type": "object",
            "description": "Specifies date range for which invalidations will be downloaded.",
            "required": [
                "since",
                "until"
            ],
            "propertyOrder": 5000,
            "format": "grid-strict",
            "properties": {
                "since": {
                    "type": "string",
                    "title": "Date Since",
                    "description": "Date from which invalidations will be downloaded. Can be specified in:</br><ul><li>relative format, e.g. 3 days ago,</li><li>absolute format, e.g. 2020-12-01,</li><li>keyword \"last\" to utilize last saved value,</li><li>\"last - 1\" to utilize last saved value and subtract 1 day.</li></ul>If left empty, defaults to \"last\".",
                    "default": "last",
                    "propertyOrder": 100,
                    "options": {
                        "grid_columns": 6
                    }
                },
                "until": {
                    "type": "string",
                    "title": "Date Until",
                    "description": "Date until which invalidations will be downloaded. Can be specified in:</br><ul><li>relative format, e.g. now,</li><li>absolute format, e.g. 2020-12-01,</li></ul>If left empty, defaults to \"now\".</br>If \"last\" is used in <i>since</i> date, this date automatically defaults to \"now\".",
                    "default": "now",
                    "propertyOrder": 200,
                    "options": {
                        "grid_columns": 6
                    }
                }
            }
        },
        "output": {
            "title": "Output Table",
            "type": "object",
            "description": "Specifies destination in storage, where the resulting table will be stored.",
            "propertyOrder": 6000,
            "format": "grid-strict",
            "required": [
                "bucket",
                "table",
                "incremental"
            ],
            "properties": {
                "bucket": {
                    "title": "Output Bucket",
                    "description": "<i>Optional:</i> Identificator of the bucket, where the table should be loaded. Specify in <i>[in/out].c-bucket_name</i> format.</br>If not provided, the bucket name will be auto-generated.",
                    "type": "string",
                    "propertyOrder": 100,
                    "options": {
                        "grid_columns": 4
                    }
                },
                "table": {
                    "title": "Output Table",
                    "description": "<i>Optional:</i> Name of the table in storage, where the data will be loaded.</br>If not provided, name of the table will be set by the mode used.",
                    "type": "string",
                    "propertyOrder": 200,
                    "options": {
                        "grid_columns": 4
                    }
                },
                "incremental": {
                    "title": "Load Type",
                    "description": "Specifies load type back to storage.",
                    "propertyOrder": 300,
                    "type": "boolean",
                    "options": {
                        "enum_titles": [
                            "Incremental Load",
                            "Full Load"
                        ],
                        "grid_columns": 4
                    },
                    "default": true
                }
            }
        }
    }
}


================================================
File: component_config/configuration_description.md
================================================
A sample of the configuration can be found in the [component's repository](https://bitbucket.org/kds_consulting_team/kds-team.app-dateio-tapix/src/master/component_config/sample-config).

### Parameters

Following parameters are required by the application:

- `username` - a username to the TapiX API; beware, that this username may be different to the username used to access the API docs;
- `#password` - a password to the TapiX API; again, may be different to the password used to access the API docs;
- `mode` - specifies, in which mode the application will be launched;
- `incremental` - whether the outputs will be loaded into storage incrementally or not;
- `invalidations_since` - a date string in relative (e.g. 3 days ago), absolute (2020-01-01) format, or use keyword **last** to load latest data based on last saved state; the parameter is only needed for the `invalidations` mode.

#### Mode

The application can work in 4 different modes:

- `transactions` - utilizes endpoint `POST /shops/findByCardTransactionBatch` to find shops by transactions;
- `shops` - utilizes endpoint `GET /shops/{id}` to download details about shops;
- `merchants` - utilizes endpoint `GET /merchants/{id}` to download details about merchants;
- `invalidations` - utilizes endpoint `GET /invalidations` to download data about invalidations.

### Input Tables

All modes, except for `invalidations` mode, require a table on the input mapping to work properly.

For `transactions`, the table should be mapped to `transactions.csv` and should contain only column (attributes), which are supported by `/shops/findByCardTransactionBatch` endpoint. Additionally, `customId` column is required as the results need to be mapped back to their original records.

For `shops`, the table should be mapped to `shops.csv` and can contain arbitrary number of columns. Required column is `id`, which must contain the `uid` of the shop, for which the information should be downloaded.

For `merchants`, the table should be mapped to `merchants.csv` and can contain arbitrary number of columns. Required column is `id`, which must contain the `uid` of the merchant, for which the information should be downloaded.


================================================
File: component_config/stack_parameters.json
================================================
{}


================================================
File: component_config/sample-config/config.json
================================================
{
  "parameters": {
    "username": "USERNAME",
    "#password": "PASSWORD",
    "mode": "transactions",
    "debug": true,
    "incremental": true,
    "invalidations_since": "2020-12-01"
  },
  "image_parameters": {}
}



================================================
File: component_config/sample-config/in/state.json
================================================
{"invalidations": 1609465063}


================================================
File: component_config/sample-config/in/tables/merchants.csv
================================================
id
2kP5VKa7l5DsBW4EBJ0ZNB


================================================
File: component_config/sample-config/in/tables/shops.csv
================================================
id
MmPdedgnvjXiRZnBJzJQKb


================================================
File: component_config/sample-config/in/tables/transactions.csv
================================================
merchantId,posId,description,city,zip,country,mcc,channelCode,ecommerceFlag,associationName,acquirerId,customId
3124532,341231234,DESCRIPTION,BRNO,,CZE,6345,POS,1,MASTERCARD,432123,10


================================================
File: component_config/sample-config/out/state.json
================================================
{"invalidations": 1610593952}


================================================
File: component_config/sample-config/out/tables/invalidations.csv
================================================
"Ynp53ENkl4jCB4dJrkD1q0","merchant","shallow","12345.123456","1610593952"



================================================
File: component_config/sample-config/out/tables/invalidations.csv.manifest
================================================
{"incremental": true, "primary_key": ["uid", "type", "level", "run_id", "timestamp"], "columns": ["uid", "type", "level", "run_id", "timestamp"]}


================================================
File: component_config/sample-config/out/tables/merchants.csv
================================================
"2kP5VKa7l5DsBW4EBJ0ZNB","rohlik.cz","Groceries","https://s3.eu-central-1.amazonaws.com/files.node-prod.dateio.cz/m/3012760/l"



================================================
File: component_config/sample-config/out/tables/merchants.csv.manifest
================================================
{"incremental": true, "primary_key": ["uid"], "columns": ["uid", "name", "category", "logo"]}


================================================
File: component_config/sample-config/out/tables/shops.csv
================================================
"MmPdedgnvjXiRZnBJzJQKb","bricks","[""Hyper-Supermarket""]","538VqZ4gn33tVJJWEYa8d3","Skandinávská 144/25","Praha 5 - Třebonice","15500","CZ","24987531","50.049689","14.294543","https://itesco.cz",""



================================================
File: component_config/sample-config/out/tables/shops.csv.manifest
================================================
{"incremental": true, "primary_key": ["uid"], "columns": ["uid", "type", "tags", "merchantUid", "location_address_street", "location_address_city", "location_address_zip", "location_address_country", "location_address_unid", "location_coordinates_lat", "location_coordinates_long", "url", "googlePlaceId"]}


================================================
File: component_config/sample-config/out/tables/transactions.csv
================================================
"10","found","9V3Lgp9qB7NTpzq3mAALKZ","9V3Lgp9qB7NTpzq3mAALKZ"



================================================
File: component_config/sample-config/out/tables/transactions.csv.manifest
================================================
{"incremental": true, "primary_key": ["customId"], "columns": ["customId", "result", "handle", "shop_uid"]}


================================================
File: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
File: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
File: src/client.py
================================================
import json
import logging
import sys

from kbc.client_base import HttpClientBase
from urllib.parse import urljoin
from datetime import datetime

DT_RFC_3399 = '%Y-%m-%dT%H:%M:%SZ'


class Client(HttpClientBase):
    TAPIX_URL = 'https://tapix.dateio.eu/v5/'
    INVALIDATIONS_PAGE_SIZE = 250000
    INVALIDATIONS_LEVELS = 'shallow,deep,solved'

    def __init__(self, username, password):
        super().__init__(self.TAPIX_URL, auth=(username, password))

    def get_shops_by_transactions(self, transaction_batch: list) -> list:
        '''Returns a list of shops matched for each transaction in a batch'''

        shops_url = urljoin(self.base_url, 'shops/findByCardTransactionBatch')
        shops_hdr = {'Content-Type': 'application/json'}
        shops_bdy = {'requests': transaction_batch}

        shops_rsp = self.post_raw(url=shops_url, headers=shops_hdr, data=json.dumps(shops_bdy), timeout=50)
        shops_rsp_sc = shops_rsp.status_code

        if shops_rsp_sc == 200:
            return shops_rsp.json()

        elif shops_rsp_sc == 401:
            logging.error("Invalid credentials.")
            sys.exit(1)

        else:
            logging.error(' '.join(["Could not get data about shops from transactions.",
                                    f"Received: {shops_rsp_sc} - {shops_rsp.json()}."]))

            # logging.debug(f"URL: {shops_url}.\nHeaders: {shops_hdr}.")
            # logging.debug(f"Body: {shops_bdy}.")
            sys.exit(1)

    def get_shop_by_id(self, shop_id: str) -> dict:
        '''Returns detailed information about a shop from its handled or uid'''

        shop_url = urljoin(self.base_url, f'shops/{shop_id}')

        shop_rsp = self.get_raw(url=shop_url)
        shop_rsp_sc = shop_rsp.status_code

        if shop_rsp_sc == 200:
            return shop_rsp.json()

        elif shop_rsp_sc == 401:
            logging.error("Invalid credentials.")
            sys.exit(1)

        else:
            logging.warning(' '.join([f"Could not get detailed information for a shop with id {shop_id}.",
                                      f"Received: {shop_rsp_sc}."]))
            return {'uid': shop_id, 'type': 'NOT FOUND', 'merchantUid': 'NOT FOUND'}

    def get_merchant_by_id(self, merchant_id: str) -> dict:
        '''Returns detailed information about a merchant from its handle or uid'''

        merchant_url = urljoin(self.base_url, f'merchants/{merchant_id}')

        merchant_rsp = self.get_raw(url=merchant_url)
        merchant_rsp_sc = merchant_rsp.status_code

        if merchant_rsp_sc == 200:
            return merchant_rsp.json()

        elif merchant_rsp_sc == 401:
            logging.error("Invalid credentials.")
            sys.exit(1)

        else:
            logging.warning(' '.join([f"Could not get detailed information for a merchant with id {merchant_id}.",
                                      f"Received: {merchant_rsp_sc}."]))
            return {'uid': merchant_id, 'name': 'NOT FOUND', 'category': 'NOT FOUND'}

    def get_invalidations_range(self, date_from: datetime, date_to: datetime = None) -> dict:
        '''Returns a numeric range of all invalidations within specified time period'''

        date_from_rfc = date_from.strftime(DT_RFC_3399)
        date_to_rfc = date_to.strftime(DT_RFC_3399) if date_to is not None else None

        invalidations_url = urljoin(self.base_url, 'invalidations/item/range')
        invalidations_par = {'from': date_from_rfc, 'to': date_to_rfc}

        invalidations_rsp = self.get_raw(url=invalidations_url, params=invalidations_par)
        invalidations_rsp_sc = invalidations_rsp.status_code

        if invalidations_rsp_sc == 200:
            return invalidations_rsp.json()

        elif invalidations_rsp_sc == 401:
            logging.error("Invalid credentials.")
            sys.exit(1)

        else:
            logging.error(' '.join(["Could not download invalidations item range.",
                                    f"Received: {invalidations_rsp_sc}."]))
            sys.exit(1)

    def get_invalidations(self, from_id: str, to_id: str = None) -> dict:
        '''Returns a list of all invalidated items in the API'''

        invalidations_url = urljoin(self.base_url, 'invalidations')
        invalidations_par = {
            'fromId': from_id,
            'toId': to_id,
            'level': self.INVALIDATIONS_LEVELS,
            'pageSize': self.INVALIDATIONS_PAGE_SIZE
        }

        invalidations_rsp = self.get_raw(url=invalidations_url, params=invalidations_par)
        invalidations_rsp_sc = invalidations_rsp.status_code

        if invalidations_rsp_sc == 200:
            return invalidations_rsp.json()

        elif invalidations_rsp_sc == 401:
            logging.error("Invalid credentials.")
            sys.exit(1)

        else:
            logging.error(' '.join(["Could not download invalidations item range.",
                                    f"Received: {invalidations_rsp_sc}."]))
            sys.exit(1)



================================================
File: src/component.py
================================================
import _csv
import csv
import logging
import os
import sys
import time
from datetime import datetime, timedelta
from glob import glob

import dateparser
from kbc.env_handler import KBCEnvHandler

from client import Client, DT_RFC_3399
from result import ResultWriter

CONFIG_USERNAME_KEY = 'username'
CONFIG_PASSWORD_KEY = '#password'
CONFIG_MODE_KEY = 'mode'
CONFIG_DEBUG_KEY = 'debug'
CONFIG_INVALIDATIONS_DATE = 'invalidations_date'
CONFIG_OUTPUT_KEY = 'output'

CONFIG_INVALIDATIONS_SINCE = 'since'
CONFIG_INVALIDATIONS_UNTIL = 'until'

CONFIG_OUTPUT_BUCKET = 'bucket'
CONFIG_OUTPUT_TABLE = 'table'
CONFIG_OUTPUT_INCREMENTAL = 'incremental'

MANDATORY_PARAMS = [CONFIG_USERNAME_KEY, CONFIG_PASSWORD_KEY, CONFIG_MODE_KEY]

ENV_RUNID_KEY = 'KBC_RUNID'
ENV_CMPID_KEY = 'KBC_COMPONENTID'
ENV_CFGID_KEY = 'KBC_CONFIGID'

APP_VERSION = '1.2.2'
sys.tracebacklimit = 0

ALLOWED_MODES = ['transactions', 'invalidations', 'merchants', 'shops']

TRANSACTIONS_ATTRIBUTES_CAST = {
    'ecommerceFlag': int,
    'ignoreTime': bool,
    'gpsLat': float,
    'gpsLong': float,
    'refresh': bool
}


class UserException(Exception):
    pass


class Component(KBCEnvHandler):
    TRANSACTIONS_BATCH_SIZE = 1000
    INVALIDATIONS_DEFAULT_RANGE_SINCE = '7 days ago'
    INVALIDATIONS_DEFAULT_RANGE_UNTIL = 'now'

    def __init__(self):
        super().__init__(MANDATORY_PARAMS, data_path=None)

        logging.info(f"Running app version {APP_VERSION}.")

        if self.cfg_params.get(CONFIG_DEBUG_KEY, False) is True:
            logger = logging.getLogger()
            logger.setLevel(level='DEBUG')

            sys.tracebacklimit = 3

        try:
            self.validate_config(mandatory_params=self._mandatory_params)
        except ValueError as e:
            logging.exception(e)
            sys.exit(1)

        self.client = Client(self.cfg_params[CONFIG_USERNAME_KEY], self.cfg_params[CONFIG_PASSWORD_KEY])
        self.mode = self.cfg_params[CONFIG_MODE_KEY]

        self.run_id = os.environ[ENV_RUNID_KEY]
        self.cmp_id = os.environ[ENV_CMPID_KEY]
        self.cfg_id = os.environ[ENV_CFGID_KEY]

        invalidations_date_settings = self.cfg_params.get(CONFIG_INVALIDATIONS_DATE, {})
        self.invalidations_since = invalidations_date_settings.get(CONFIG_INVALIDATIONS_SINCE, 'last').strip()
        self.invalidations_until = invalidations_date_settings.get(CONFIG_INVALIDATIONS_UNTIL, 'now').strip()

        output = self.cfg_params.get(CONFIG_OUTPUT_KEY, {})
        self.output_bucket = output.get(CONFIG_OUTPUT_BUCKET, '').strip()
        self.output_table = output.get(CONFIG_OUTPUT_TABLE, '').strip()
        self.incremental = output.get(CONFIG_OUTPUT_INCREMENTAL, False)

        self.check_input_tables()

    def check_input_tables(self):

        if self.mode == 'invalidations':
            return

        tables = glob(os.path.join(self.tables_in_path, '*.csv'))

        if len(tables) == 0:
            logging.error("No input tables provided.")

        if self.mode not in ALLOWED_MODES:
            logging.error(f"Invalid mode provided: {self.mode}. Must be one of allowed: {', '.join(ALLOWED_MODES)}.")
            sys.exit(1)

        else:
            required_table = f"{self.mode}.csv"
            required_table_full_path = os.path.join(self.tables_in_path, required_table)
            if required_table_full_path not in tables:
                logging.error(f"Mode {self.mode} requires table {required_table} on the input mapping.")
                sys.exit(1)
            else:
                self.input_table = required_table_full_path

    def _retrieve_and_write_transactions(self, batch: list, writer: ResultWriter):

        if len(batch) == 0:
            return

        results = self.client.get_shops_by_transactions(transaction_batch=batch)
        writer.writerows(results)

    def process_transactions(self, reader: csv.DictReader):

        request_counter = 0

        writer = ResultWriter(self.tables_out_path, 'transactions', self.incremental, self.destination)

        if 'customId' not in reader.fieldnames:
            logging.error("Field 'customId' is required for transactions. Please, provide this non-empty field.")
            sys.exit(1)

        rows_to_send = []
        try:
            for row in reader:
                _tmp = dict()
                for key, value in row.items():
                    if key in TRANSACTIONS_ATTRIBUTES_CAST and value.strip() != '':
                        try:
                            _tmp[key] = TRANSACTIONS_ATTRIBUTES_CAST[key](value)
                        except ValueError:
                            logging.exception(''.join([f"Could not convert value {value} in column {key}",
                                                       f" to type {str(TRANSACTIONS_ATTRIBUTES_CAST[key])}."]),
                                              exc_info=True)
                            raise
                    else:
                        _tmp[key] = str(value)
                rows_to_send += [_tmp]

                if len(rows_to_send) == self.TRANSACTIONS_BATCH_SIZE:
                    # Process if limit is reached. Then reset back to empty list.
                    self._retrieve_and_write_transactions(rows_to_send, writer)
                    request_counter += 1
                    rows_to_send = []

                    if request_counter % 50 == 0:
                        logging.info(f"Made {request_counter} batch requests so far.")

                else:
                    continue
        except _csv.Error as err:
            raise UserException(f'Input file processing failed {err}', f'Last row: {row}') from err

        # Process final batch
        self._retrieve_and_write_transactions(rows_to_send, writer)
        logging.info(f"Made {request_counter + 1} batch requests in total.")

    def determine_date(self):
        if self.invalidations_since == 'last' or self.invalidations_since.replace(' ', '') == 'last-1':
            state = self.get_state_file()
            if state is None:
                logging.warn(f"No state file found. Defaulting to: {self.INVALIDATIONS_DEFAULT_RANGE_SINCE}.")
                _inv_date_since = dateparser.parse(self.INVALIDATIONS_DEFAULT_RANGE_SINCE)
            else:
                _ts = state.get('invalidations')

                if _ts is None:
                    logging.warn("No last state found for invalidations. Defaulting to past 7 days.")
                    _inv_date_since = dateparser.parse(self.INVALIDATIONS_DEFAULT_RANGE_SINCE)

                else:
                    _inv_date_since = datetime.utcfromtimestamp(_ts)

                    if self.invalidations_since.replace(' ', '') == 'last-1':
                        _inv_date_since = _inv_date_since - timedelta(days=1)

            _inv_date_until = dateparser.parse(self.INVALIDATIONS_DEFAULT_RANGE_UNTIL)

        elif self.invalidations_since.replace(' ', '') == 'last-1':
            state = self.get_state_file()
            if state is None:
                logging.warn(f"No state file found. Defaulting to: {self.INVALIDATIONS_DEFAULT_RANGE_SINCE}.")
                _inv_date_since = dateparser.parse(self.INVALIDATIONS_DEFAULT_RANGE_SINCE)
            else:
                _ts = state.get('invalidations')

        else:
            _inv_date_since = dateparser.parse(self.invalidations_since)
            _inv_date_until = dateparser.parse(self.invalidations_until)
            if _inv_date_since is None:
                logging.error(f"Invalid date since {self.invalidations_since} provided.")
                sys.exit(1)
            elif _inv_date_until is None:
                logging.error(f"Invalid date until {self.invalidations_until} provided.")
                sys.exit(1)
            else:
                pass

        return _inv_date_since, _inv_date_until

    def determine_destination(self):
        if self.output_bucket is None or self.output_bucket == '':
            _output_bucket = f"in.c-{self.cmp_id.replace('.', '-')}-{self.cfg_id}"
        else:
            _output_bucket = self.output_bucket

        if self.output_table is None or self.output_table == '':
            _output_table = self.mode
        else:
            _output_table = self.output_table

        return f'{_output_bucket}.{_output_table}'

    def get_invalidations(self):

        current_ts = int(time.time())

        inv_date_since, inv_date_until = self.determine_date()
        dt_since = inv_date_since.strftime(DT_RFC_3399)
        dt_until = inv_date_until.strftime(DT_RFC_3399)

        logging.info(f"Downloading invalidations from date {dt_since} to date {dt_until}.")

        writer = ResultWriter(self.tables_out_path, 'invalidations', self.incremental, self.destination)

        invalidations_range = self.client.get_invalidations_range(date_from=inv_date_since, date_to=inv_date_until)
        if invalidations_range['itemCount'] == 0:
            logging.info("No new invalidations")
            self.write_state_file({'invalidations': int(inv_date_until.timestamp())})
            return

        from_id = invalidations_range['fromId']
        to_id = invalidations_range['toId']
        is_complete = False

        while is_complete is False:
            invalidations = self.client.get_invalidations(from_id=from_id, to_id=to_id)

            batches = invalidations['data']['batches']

            for batch in batches:
                all_data = [{'type': batch['type'], 'level': batch['level'], 'timestamp': current_ts,
                             'run_id': self.run_id, 'since': dt_since, 'until': dt_until, 'uid': x}
                            for x in batch['ids']]
                writer.writerows(all_data)

            is_complete = invalidations['paging']['lastPage']
            from_id = invalidations['paging'].get('lastItemId')

        self.write_state_file({'invalidations': int(inv_date_until.timestamp())})

    def get_shops(self, reader: csv.DictReader):

        request_counter = 0

        if 'id' not in reader.fieldnames:
            logging.error("Field 'id' must be provided in the input table 'shops.csv'.")
            sys.exit(1)

        writer = ResultWriter(self.tables_out_path, 'shops', self.incremental, self.destination)

        for row in reader:
            shop_data = self.client.get_shop_by_id(row['id'])
            request_counter += 1
            writer.writerows([shop_data])

            if request_counter % 100 == 0:
                logging.info(f"Made {request_counter} requests so far.")

        logging.info(f"Made {request_counter} requests in total.")

    def get_merchants(self, reader: csv.DictReader):

        request_counter = 0

        if 'id' not in reader.fieldnames:
            logging.error("Field 'id' must be provided in the input table 'merchants.csv'.")
            sys.exit(1)

        writer = ResultWriter(self.tables_out_path, 'merchants', self.incremental, self.destination)

        for row in reader:
            merchant_data = self.client.get_merchant_by_id(row['id'])
            request_counter += 1
            writer.writerows([merchant_data])

            if request_counter % 100 == 0:
                logging.info(f"Made {request_counter} requests so far.")

        logging.info(f"Made {request_counter} requests in total.")

    def run(self):

        self.destination = self.determine_destination()

        if self.mode == 'invalidations':
            self.get_invalidations()

        else:
            with open(self.input_table) as mode_table:

                _rdr = csv.DictReader(mode_table)

                if self.mode == 'transactions':
                    self.process_transactions(_rdr)
                elif self.mode == 'merchants':
                    self.get_merchants(_rdr)
                elif self.mode == 'shops':
                    self.get_shops(_rdr)
                else:
                    logging.error("Unsupported mode.")
                    sys.exit(1)

        logging.info("Component finished.")



================================================
File: src/result.py
================================================
import os
import csv
import json
import logging

FIELDS_TRANSACTIONS = ['customId', 'result', 'handle', 'shop_uid']
FIELDS_R_TRANSACTIONS = FIELDS_TRANSACTIONS
PK_TRANSACTIONS = ['customId']
JSON_TRANSACTIONS = []

FIELDS_INVALIDATIONS = ['uid', 'type', 'level', 'run_id', 'timestamp', 'since', 'until']
FIELDS_R_INVALIDATIONS = FIELDS_INVALIDATIONS
PK_INVALIDATIONS = ['uid', 'type', 'level', 'run_id', 'timestamp']
JSON_INVALIDATIONS = []

FIELDS_MERCHANTS = ['uid', 'name', 'category', 'logo']
FIELDS_R_MERCHANTS = FIELDS_MERCHANTS
PK_MERCHANTS = ['uid']
JSON_MERCHANTS = []

FIELDS_SHOPS = ['uid', 'type', 'tags', 'merchantUid', 'location_address_street', 'location_address_city',
                'location_address_zip', 'location_address_country', 'location_address_unid',
                'location_coordinates_lat', 'location_coordinates_long', 'url', 'googlePlaceId']
FIELDS_R_SHOPS = FIELDS_SHOPS
PK_SHOPS = ['uid']
JSON_SHOPS = ['tags']


class ResultWriter:

    def __init__(self, tableOutPath, tableName, incremental, destination):

        self.path = tableOutPath
        self.table_name = tableName
        self.table = tableName + '.csv'
        self.table_path = os.path.join(self.path, self.table)
        self.fields = eval(f'FIELDS_{tableName.upper().replace("-", "_")}')
        self.fields_json = eval(f'JSON_{tableName.upper().replace("-", "_")}')
        self.primary_key = eval(f'PK_{tableName.upper().replace("-", "_")}')
        self.fields_renamed = eval(f'FIELDS_R_{tableName.upper().replace("-", "_")}')
        self.incremental = incremental
        self.destination = destination

        self.create_manifest()
        self.create_writer()

    def create_manifest(self):

        template = {
            'incremental': self.incremental,
            'primary_key': self.primary_key,
            'columns': self.fields_renamed,
            'destination': self.destination
        }

        path = self.table_path + '.manifest'

        with open(path, 'w') as manifest:
            json.dump(template, manifest)

    def create_writer(self):

        self.writer = csv.DictWriter(open(self.table_path, 'w', newline=''), fieldnames=self.fields,
                                     restval='', extrasaction='ignore', quotechar='\"', quoting=csv.QUOTE_ALL,
                                     lineterminator='\n')

    def writerows(self, listToWrite, parentDict=None):

        for row in listToWrite:
            row_f = self.flatten_json(x=row)

            if self.fields_json != []:
                for field in self.fields_json:
                    try:
                        row_f[field] = json.dumps(row.get(field, []))
                    except json.JSONDecodeError:
                        logging.warning(f"Could not decode JSON value {row.get(field, [])}.")
                        row_f[field] = json.dumps([])

            _dictToWrite = {}

            for key, value in row_f.items():

                if key in self.fields:
                    _dictToWrite[key] = value
                else:
                    continue

            if parentDict is not None:
                _dictToWrite = {**_dictToWrite, **parentDict}

            self.writer.writerow(_dictToWrite)

    def flatten_json(self, x, out=None, name=''):
        if out is None:
            out = dict()

        if type(x) is dict:
            for a in x:
                self.flatten_json(x[a], out, name + a + '_')
        else:
            out[name[:-1]] = x

        return out



================================================
File: src/run.py
================================================
import logging
import sys

from component import Component, UserException

if __name__ == '__main__':
    c = Component()
    try:
        c.run()
    except UserException as exc:
        detail = ''
        if len(exc.args) > 1:
            detail = exc.args[1]
        logging.exception(exc, extra={"full_message": detail})
        exit(1)
    except Exception as e:
        logging.exception(e)
        sys.exit(1)



================================================
File: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
File: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


