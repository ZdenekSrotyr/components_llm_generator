Directory structure:
└── kds_consulting_team-kds-team.app-compare-tables/
    ├── flake8.cfg
    ├── deploy.sh
    ├── docker-compose.yml
    ├── change_log.md
    ├── src/
    │   └── component.py
    ├── bitbucket-pipelines.yml
    ├── tests/
    │   ├── test_component.py
    │   └── __init__.py
    ├── data/
    │   ├── config.json
    │   ├── in/
    │   │   └── tables/
    │   │       ├── original.csv
    │   │       ├── new.csv
    │   │       ├── new.csv.manifest
    │   │       └── original.csv.manifest
    │   └── out/
    │       └── tables/
    │           ├── sample_mismatch.csv
    │           ├── original_table_unq_rows.csv
    │           ├── comparison_report.csv
    │           ├── column_stats.csv
    │           └── new_table_unq_rows.csv
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── configuration_description.md
    │   ├── component_short_description.md
    │   ├── stack_parameters.json
    │   ├── configSchema.json
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           └── tables/
    │               ├── original.csv
    │               ├── new.csv
    │               ├── new.csv.manifest
    │               └── original.csv.manifest
    ├── scripts/
    │   ├── update_dev_portal_properties.sh
    │   ├── run_kbc_tests.ps1
    │   ├── run.bat
    │   ├── build_n_run.ps1
    │   └── build_n_test.sh
    ├── requirements.txt
    ├── docs/
    │   └── imgs/
    ├── Dockerfile
    ├── LICENSE.md
    └── README.md

================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog


================================================
File: /src/component.py
================================================
'''
Template Component main class.

'''

import logging
import sys
import pandas as pd
import datacompy

from kbc.env_handler import KBCEnvHandler

# configuration variables
KEY_JOIN_COLUMNS = 'join_columns'
KEY_ORIGINAL_TABLE = 'original_table'

# #### Keep for debug
KEY_STDLOG = 'stdlogging'
KEY_DEBUG = 'debug'
MANDATORY_PARS = []
MANDATORY_IMAGE_PARS = []

APP_VERSION = '0.0.6'


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS, log_level=logging.DEBUG if debug else logging.INFO)
        # override debug from config
        if self.cfg_params.get(KEY_DEBUG):
            debug = True
        if debug:
            logging.getLogger().setLevel(logging.DEBUG)
        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        try:
            self.validate_config()
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as e:
            logging.exception(e)
            exit(1)

    def run(self):
        '''
        Main execution code
        '''
        params = self.cfg_params  # noqa
        in_tables = self.configuration.get_input_tables()
        join_column_list = params.get(KEY_JOIN_COLUMNS)
        original_table_name = params.get(KEY_ORIGINAL_TABLE)
        tables_out_path = self.tables_out_path

        if len(in_tables) < 2:
            logging.error('Please provide two (2) input mapping tables.')
            exit(1)

        if len(in_tables) > 2:
            logging.error('Please provide two (2) input mapping tables.')
            exit(1)

        logging.info("Loading dataset...")
        table_name1 = in_tables[0]['full_path']
        table_name2 = in_tables[1]['full_path']

        if table_name1.split('/')[-1] == original_table_name:
            df1 = pd.read_csv(table_name1)
            df2 = pd.read_csv(table_name2)
            logging.info('[INFO] Original table - ' + table_name1 + ' loaded.')
            logging.info('[INFO] New table - ' + table_name2 + ' loaded.')
        elif table_name2.split('/')[-1] == original_table_name:
            df1 = pd.read_csv(table_name2)
            df2 = pd.read_csv(table_name1)
            logging.info('[INFO] Original table - ' + table_name2 + ' loaded.')
            logging.info('[INFO] New table - ' + table_name1 + ' loaded.')
            table_name_holder = table_name1
            table_name1 = table_name2
            table_name2 = table_name_holder
        else:
            logging.error('[ERROR] Entered Original Table Name doesnt match any of the provided input tables.')

        table_name1 = table_name1.split('/')[-1]
        table_name2 = table_name2.split('/')[-1]

        try:
            compare = datacompy.Compare(
                df1,
                df2,
                join_columns=join_column_list,
                abs_tol=0,
                rel_tol=0,
                df1_name='Original',
                df2_name='New'
            )

            # general stats of all columns - cnt match, cnt unequal, data types,...
            column_stats = pd.DataFrame(compare.column_stats)
            column_stats['original_table_name'] = table_name1
            column_stats['new_table_name'] = table_name2
            column_stats.to_csv(tables_out_path + '/column_stats.csv', index=False)

            # rows that are unique to original dataframe (are not in new one)
            original_table_unq_rows = pd.DataFrame(compare.df1_unq_rows)
            original_table_unq_rows['original_table_name'] = table_name1
            original_table_unq_rows.to_csv(tables_out_path + '/original_table_unq_rows.csv', index=False)

            # rows that are unique to new dataframe (are not in original one)
            new_table_unq_rows = pd.DataFrame(compare.df2_unq_rows)
            new_table_unq_rows['new_table_name'] = table_name2
            new_table_unq_rows.to_csv(tables_out_path + '/new_table_unq_rows.csv', index=False)

            # sample of mismatch values for all columns
            sample_mismatch = pd.DataFrame({"join_column": [], "intersect_column": [],
                                            "original_table_value": [], "new_table_value": [],
                                            "original_table_name": [], "new_table_name": []})
            columns = list(set(list(compare.intersect_columns())) - set(compare.join_columns))
            for column in columns:
                temp_mismatch = compare.sample_mismatch(column)
                temp_mismatch_cols = temp_mismatch.shape[1]
                if temp_mismatch_cols > 3:
                    join_column_cnt = temp_mismatch_cols - 3
                    temp_mismatch['join_column'] = '+'.join(list(temp_mismatch.columns)[0:join_column_cnt+1])
                else:
                    join_column_cnt = 0
                    temp_mismatch['join_column'] = list(temp_mismatch.columns)[0]
                temp_mismatch = temp_mismatch.iloc[:, join_column_cnt+1:]
                temp_mismatch['intersect_column'] = column
                temp_mismatch['original_table_name'] = table_name1
                temp_mismatch['new_table_name'] = table_name2
                temp_mismatch.columns = ['original_table_value', 'new_table_value',
                                         'join_column', 'intersect_column',
                                         'original_table_name', 'new_table_name']
                sample_mismatch = sample_mismatch.append(temp_mismatch)
            sample_mismatch.to_csv(tables_out_path + '/sample_mismatch.csv', index=False)

            # overall comparison report - aggregated
            comparison_report = pd.DataFrame({"original_table_name": [table_name1],
                                              "new_table_name": [table_name2],
                                              "original_table_cols_cnt": [compare.df1.shape[1]],
                                              "new_table_cols_cnt": [compare.df2.shape[1]],
                                              "original_table_rows_cnt": [compare.df1.shape[0]],
                                              "new_table_rows_cnt": [compare.df2.shape[0]],
                                              "number_of_intersect_cols": [len(compare.intersect_columns())],
                                              "number_of_original_unq_cols": [len(compare.df1_unq_columns())],
                                              "number_of_new_unq_cols": [len(compare.df2_unq_columns())],
                                              "matched_on": [str(compare.join_columns)],
                                              "duplicates_on_match_values": ["Yes" if compare._any_dupes else "No"],
                                              "number_of_intersect_rows": [compare.intersect_rows.shape[0]],
                                              "number_of_original_unq_rows": [compare.df1_unq_rows.shape[0]],
                                              "number_of_new_unq_rows": [compare.df2_unq_rows.shape[0]],
                                              "number_of_rows_with_some_unequal": [compare.intersect_rows.shape[0] -
                                                                                   compare.count_matching_rows()],
                                              "number_of_rows_with_all_equal": [compare.count_matching_rows()],
                                              "number_of_cols_with_some_unequal": [
                                                  len([col for col in compare.column_stats if col["unequal_cnt"] > 0])],
                                              "number_of_cols_with_all_equal": [len(
                                                  [col for col in compare.column_stats if col["unequal_cnt"] == 0])],
                                              "total_number_of_values_unequal": [
                                                  sum([col["unequal_cnt"] for col in compare.column_stats])]
                                              })
            comparison_report.to_csv(tables_out_path + '/comparison_report.csv', index=False)
        except Exception as e:
            logging.error('[ERROR] Application error: ' + str(e) + '. Please contact application developer.')


"""
        Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug_arg = sys.argv[1]
    else:
        debug_arg = False
    try:
        comp = Component(debug_arg)
        comp.run()
    except Exception as e:
        logging.exception(e)
        exit(1)


================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
#          - echo 'Pushing test image to repo. [tag=test]'
#          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#          - docker tag $APP_IMAGE:latest $REPOSITORY:test
#          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /tests/test_component.py
================================================
if __name__ == "__main__":
    print('test')


================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /data/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.original",
          "destination": "original.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        },
        {
          "source": "in.c-test.new",
          "destination": "new.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": [
      ]
    }
  },
  "parameters": {
    "join_columns": ["id"],
    "original_table": "original.csv"
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  }
}

================================================
File: /data/in/tables/original.csv
================================================
"id","name","value"
"1","Tester","0"
"2","Testor","1"
"3","Testir","2"

================================================
File: /data/in/tables/new.csv
================================================
"id","name","value","value_new"
"1","Tester","10","1"
"2","Testor","11","2"

================================================
File: /data/in/tables/new.csv.manifest
================================================
{
    "id": "in.c-test.new",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.new",
    "name": "new",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /data/in/tables/original.csv.manifest
================================================
{
    "id": "in.c-test.original",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.original",
    "name": "original",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /data/out/tables/sample_mismatch.csv
================================================
join_column,intersect_column,original_table_value,new_table_value,original_table_name,new_table_name
id,value,1.0,11.0,original.csv,new.csv
id,value,0.0,10.0,original.csv,new.csv


================================================
File: /data/out/tables/original_table_unq_rows.csv
================================================
id,name,value,original_table_name
3,Testir,2,original.csv


================================================
File: /data/out/tables/comparison_report.csv
================================================
original_table_name,new_table_name,original_table_cols_cnt,new_table_cols_cnt,original_table_rows_cnt,new_table_rows_cnt,number_of_intersect_cols,number_of_original_unq_cols,number_of_new_unq_cols,matched_on,duplicates_on_match_values,number_of_intersect_rows,number_of_original_unq_rows,number_of_new_unq_rows,number_of_rows_with_some_unequal,number_of_rows_with_all_equal,number_of_cols_with_some_unequal,number_of_cols_with_all_equal,total_number_of_values_unequal
original.csv,new.csv,3,3,3,3,3,0,1,['id'],No,2,1,1,2,0,1,2,2


================================================
File: /data/out/tables/column_stats.csv
================================================
column,match_column,match_cnt,unequal_cnt,dtype1,dtype2,all_match,max_diff,null_diff,original_table_name,new_table_name
value,value_match,0,2,int64,int64,False,10.0,0,original.csv,new.csv
name,name_match,2,0,object,object,True,0.0,0,original.csv,new.csv
id,,2,0,int64,int64,True,0.0,0,original.csv,new.csv


================================================
File: /data/out/tables/new_table_unq_rows.csv
================================================
id,name,value,value_new,new_table_name


================================================
File: /component_config/component_long_description.md
================================================
Application compares two tables and generates comparison result as separated detailed tables.
It uses Python package named DataCompy documented here https://capitalone.github.io/datacompy/

================================================
File: /component_config/configuration_description.md
================================================
## Required input
The application requires two (2) input mapping tables. Tables should have at least one column named equally.

## Required configuration
__Join Column__: a column on which the join (for comparison) will be made is required.
__Original Table Name__: name of the table (as provided on input mapping) which should be taken as the original one. The
second table will be compared the original one.

================================================
File: /component_config/component_short_description.md
================================================
Application compares two tables and generates comparison result as separated detailed tables.

================================================
File: /component_config/stack_parameters.json
================================================
{}

================================================
File: /component_config/configSchema.json
================================================
{
   "title":"Application configuration",
   "type":"object",
   "required":[
      "join_columns",
      "original_table"
   ],
   "properties":{
      "join_columns":{
         "title":"Join Columns",
         "type":"array",
         "description":"List all the columns that should be used for comparison's join (usually same as primary key)",
         "default":"",
         "propertyOrder": 2
         },
      "original_table":{
         "title":"Original Table Name",
         "type":"string",
         "description":"Please provide a name of the original table from input mapping (for example original-table.csv).",
         "default":"",
         "propertyOrder": 1
         }
   }
}

================================================
File: /component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "original.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        },
        {
          "source": "in.c-test.test",
          "destination": "new.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": [
      ]
    }
  },
  "parameters": {
    "join_columns": ["id", "name"],
    "original_table": "original.csv"
  }
}

================================================
File: /component_config/sample-config/in/tables/original.csv
================================================
"id","name","value"
"1","Tester","0"
"2","Testor","1"
"3","Testir","2"

================================================
File: /component_config/sample-config/in/tables/new.csv
================================================
"id","name","value","value_new"
"1","Tester","0","1"
"2","Testor","1","2"

================================================
File: /component_config/sample-config/in/tables/new.csv.manifest
================================================
{
    "id": "in.c-test.new",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.new",
    "name": "new",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /component_config/sample-config/in/tables/original.csv.manifest
================================================
{
    "id": "in.c-test.original",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.original",
    "name": "original",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.7.zip#egg=kbc
datacompy
pandas

================================================
File: /Dockerfile
================================================
FROM python:3.7.2-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]


================================================
File: /LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2020 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /README.md
================================================
# Compare Tables

Application compares two tables provided on input mapping.

Compare Tables utilizes the __DataComPy__ Python library documented here: https://capitalone.github.io/datacompy/.
The application will try to join two dataframes on a list of join columns (can be just one, usually it's a primary key). 
If the two dataframes have duplicates based on join values, the match process sorts by the remaining fields and joins 
based on that row number. Column-wise comparisons attempt to match values even when dtypes don’t match. So if, for 
example, you have a column with decimal.Decimal values in one dataframe and an identically-named column with float64 
dtype in another, it will tell you that the dtypes are different but will still try to compare the values.

It calculates numbers of equal columns, rows and values and generates information
about mismatches as well as about new/deleted columns and creates multiple tables on output to document the comparison.

## Required input
The application requires two (2) input mapping tables. Tables should have at least one column (the join column) named 
equally.

## Required configuration
__Join Column__: a column on which the join (for comparison) will be made is required.
__Original Table Name__: Name (as provided on the input mapping, for example original.csv) that is used as the original 
table. The second one will be compared to the original.

## Generated output
The application generates five (5) output tables:
__comparison_report__ - overall information about count of matching/non-matching rows, count of columns/rows unique to 
original or new table etc.
__column_stats__ - details for individual columns. How many rows for each columns are matching, what is the maximum 
difference, what are the data types of both columns, etc.
__sample_mismatch__ - for each column, a list of non-matching rows is generated as a sample to see the original and the 
new value
__original_table_unq_rows__ - rows that are unique to original table (those IDs - used for join column) are not present 
in new table at all
__new_table_unq_rows__ - opposite to the table above - contains only rows that are unique to new table

## Usage
This application can be used for exampl when you perform changes in a production pipelines. When making a change, you 
can compare the result tables of new transformation to the result tables of the original transformation.

