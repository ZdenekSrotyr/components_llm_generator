Directory structure:
└── kds_consulting_team-kds-team.ex-hubspot-deprecated/
    ├── README.md
    ├── Dockerfile
    ├── LICENSE.md
    ├── bitbucket-pipelines.yml
    ├── deploy.sh
    ├── docker-compose.yml
    ├── flake8.cfg
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   ├── files/
    │       │   │   └── order1.xml
    │       │   └── tables/
    │       │       ├── test.csv
    │       │       └── test.csv.manifest
    │       └── out/
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               └── test.csv
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run.bat
    │   ├── run_kbc_tests.ps1
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── component.py
    │   └── hubspot/
    │       ├── __init__.py
    │       └── client_service.py
    └── tests/
        ├── __init__.py
        └── test_component.py

================================================
File: README.md
================================================
# Hubspot extractor

KBC Component for data retrieval from [Hubspot API](https://developers.hubspot.com/docs/overview).


## Functionality
Supports retrieval from several endpoints. Some endpoints allow retrieval of recently updated records, 
this is set by `Date From` parameter. In most of the cases maximum of last 30 days can be retrieved.

## Supported Endpoints
- [Companies](#Companies)
- [Contacts](#Contacts)
- [Deals](#Deals)
- [Pipelines](#Pipelines)
- [Campaigns](#Campaigns)
- [Email Events](#Email Events)
- [Engagements](#Engagements)
- [Contact Lists](#Contact Lists)
- [Owners](#Owners)

### Companies    

 [All companies](https://developers.hubspot.com/docs/methods/companies/get-all-companies) or 
 [recently modified (last 30 days) ](https://developers.hubspot.com/docs/methods/companies/get_companies_modified) can be retrieved. 
 NOTE: Fetches always 30 day period
 
 Following Company properties are fetched by default:
  
```python
 ["about_us", "name", "phone", "facebook_company_page", "city", "country", "website", 
 "industry", "annualrevenue", "linkedin_company_page", "hs_lastmodifieddate", "hubspot_owner_id", "notes_last_updated", 
 "description", "createdate", "numberofemployees", "hs_lead_status", "founded_year", "twitterhandle", "linkedinbio"] 
```
 
Custom properties may be specified in configuration, names must match with api names as specified by [Company Properties](https://developers.hubspot.com/docs/methods/companies/company-properties-overview)
 

### Contacts    
 [All contacts](https://developers.hubspot.com/docs/methods/contacts/get_contacts) or 
 [recently modified (max last 30 days) ](https://developers.hubspot.com/docs/methods/contacts/get_recently_updated_contacts) can be retrieved. 
 Recently modified period can be limited by `Date From` parameter 
 
 Following Contact properties are fetched by default:
  
```json
 ["hs_facebookid", "hs_linkedinid", "ip_city", "ip_country", "ip_country_code", "newsletter_opt_in", "firstname", 
 "linkedin_profile", "lastname", "email", "mobilephone", "phone", "city", "country", "region", "jobtitle", 
 "company", "website", "numemployees", "industry", "associatedcompanyid", "hs_lead_status", 
 "lastmodifieddate", "source", "hs_email_optout", "twitterhandle", "lead_type", "hubspot_owner_id", 
 "notes_last_updated", "hs_analytics_source", "opt_in", "createdate", "hs_twitterid", "lifecyclestage"]
```

 **Note:** Following properties will be fetched each time, regardless configuration and with option `propertyMode=value_and_history`. This is currently hardcoded and 
 all other properties are fetched with value only:
 
```json
 ["company",
  "firstname",
  "lastmodifieddate",
  "lastname"]
```
 
Custom properties may be specified in configuration, names must match with api names as specified by [Contact Properties](https://developers.hubspot.com/docs/methods/contacts/contact-properties-overview)
 
**Result tables** : `contacts.csv`, `contacts_form_submissions.csv`, `contacts_lists.csv`
 
### Deals    
 [All deals](https://developers.hubspot.com/docs/methods/deals/get-all-deals) or 
 [recently modified (last 30 days) ](https://developers.hubspot.com/docs/methods/deals/get_deals_modified) can be retrieved. 
 NOTE: Fetches max 30 day period, larger periods are cut to match the limit.
 
 Following Deal properties are fetched by default:
  
```json
["authority", "budget", "campaign_source", "hs_analytics_source", "hs_campaign", 
"hs_lastmodifieddate", "need", "timeframe", "dealname", "amount", "closedate", "pipeline", 
"createdate", "engagements_last_meeting_booked", "dealtype", "hs_createdate", "description", 
"start_date", "closed_lost_reason", "closed_won_reason", "end_date", "lead_owner", "tech_owner", 
"service_amount", "contract_type", "hubspot_owner_id", "partner_name", "notes_last_updated"]
```
 
Custom properties may be specified in configuration, names must match with api names as specified by [Deal Properties](https://developers.hubspot.com/docs/methods/deals/deal_properties_overview)
 
**Result tables** : `deals.csv`, `deals_stage_history.csv`, `deals_contacts_list.csv`

### Pipelines
[All pipelines](https://developers.hubspot.com/docs/methods/pipelines/get_pipelines_for_object_type) - gets all pipelines and its stages.

**Result tables** : `pipelines.csv`, `pipeline_stages.csv`

### Campaigns
[All Campaigns](https://developers.hubspot.com/docs/methods/email/get_campaigns_by_id) 

NOTE: Fetches max 30 day period

### Email Events
[All Email Events](https://developers.hubspot.com/docs/methods/email/get_events)  - possible to limit by `Date From` parameter.

NOTE: Fetches max 30 day period, larger periods are cut to match the limit.
 
### Engagements 
[All Activities](https://developers.hubspot.com/docs/methods/engagements/get-all-engagements) or 
 [recently modified (max last 30 days) ](https://developers.hubspot.com/docs/methods/engagements/get-recent-engagements) - 
 possible to limit by `Date From` parameter.

NOTE: Fetches max 30 day period, larger periods are cut to match the limit.

### Contact Lists
[All Lists](https://developers.hubspot.com/docs/methods/lists/get_lists) 

NOTE: Always fetches all available lists
 
### Owners
[All owners](https://developers.hubspot.com/docs/methods/owners/get_owners) 

NOTE: Always sets `include_inactive` to `True`
 
## Development
 
This example contains runnable container with simple unittest. For local testing it is useful to include `data` folder in the root
and use docker-compose commands to run the container or execute tests. 

If required, change local data folder path to your custom:

```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository and init the workspace with following command:

```
git clone https://bitbucket.org:kds_consulting_team/kds-team.ex-hubspot.git
cd kds-team.ex-hubspot
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```
 
# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/) 


================================================
File: Dockerfile
================================================
FROM python:3.7.1-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

RUN apt-get update && apt-get install -y build-essential
RUN pip install flake8

RUN pip3 install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
File: LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
File: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - echo 'Pushing test image to repo. [tag=test]'
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:test
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - echo 'Pushing test image to repo. [tag=test]'
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
File: deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
File: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
File: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
File: requirements.txt
================================================
https://github.com/keboola/python-docker-application/zipball/master#egg=keboola
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.1.0.zip#egg=kbc
pytz
python-dateutil
pandas
dateparser



================================================
File: component_config/component_long_description.md
================================================
This component uses the [HubSpot API](https://developers.hubspot.com/docs/overview) to download the following 
data from [HubSpot](http://www.hubspot.com/):

- Companies
- Contacts
- Deals
- Pipelines
- Campaigns
- Email Events
- Engagements
- Contact Lists
- Owners

To configure the extractor, you need to have

a working HubSpot account, and
a [HubSpot API Key](https://app.hubspot.com/keys/get).


The Contacts, Companies, Engagements, Email Events  and Deals support incremental load to limit the API calls.


================================================
File: component_config/component_short_description.md
================================================
The HubSpot CRM helps companies grow traffic, convert leads, get insights to close more deals, etc


================================================
File: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Configuration",
  "required": [
    "#api_token",
    "period_from",
    "endpoints",
    "company_properties",
    "contact_properties",
    "deal_properties",
    "property_attributes",
    "incremental_output"
  ],
  "properties": {
    "endpoints": {
      "type": "array",
      "items": {
        "enum": [
          "companies",
          "campaigns",
          "email_events",
          "activities",
          "lists",
          "owners",
          "contacts",
          "deals",
          "pipelines"
        ],
        "type": "string"
      },
      "format": "table",
      "default": [
        "companies",
        "campaigns",
        "email_events",
        "activities",
        "lists",
        "owners",
        "contacts",
        "deals",
        "pipelines"
      ],
      "uniqueItems": true,
      "propertyOrder": 360
    },
    "#api_token": {
      "type": "string",
      "title": "API token",
      "format": "password",
      "propertyOrder": 200
    },
    "period_from": {
      "type": "string",
      "title": "Period from date [including]",
      "description": "Date in YYYY-MM-DD format or dateparser string i.e. 5 days ago, 1 month ago, yesterday, etc. If left empty, all records are downloaded.",
      "propertyOrder": 300
    },
    "incremental_output": {
          "type": "boolean",
          "title": "Incremental output",
          "description": "If set to true, the result tables will be updated based on primary key. <font color=\"#e06666\"><b>NOTE</b>: If you wish to remove deleted records, this needs to be set to false and the <i>Period from</i> attribute empty. </font>",
      "propertyOrder": 365,
          "default": true
        },
    "property_attributes": {
      "type": "object",
      "title": "Property additional attributes",
      "description": "Add additional attributes to each custom property. <font color=\"#e06666\"><b>NOTE</b>: Applicable for Contacts, Companies and Deals. </font>",
      "options": {
        "expand_height": true
      },
      "propertyOrder": 400,
      "required": [
        "include_versions",
        "include_source",
        "include_timestamp"
      ],
      "format": "grid",
      "properties": {
        "include_versions": {
          "type": "boolean",
          "title": "Include attribute versions",
          "description": "A list of previous versions of the property. The first item in the list will be the current version",
          "default": true
        },
        "include_source": {
          "type": "boolean",
          "title": "Include source attribute",
          "description": "The method by which this version was set.",
          "default": true
        },
        "include_timestamp": {
          "type": "boolean",
          "title": "Include timestamp attribute?",
          "description": "A Unix timestamp (in milliseconds) of the time when this version was set",
          "default": true
        }
      }
    },
    "deal_properties": {
      "type": "string",
      "title": "Deal additional properties",
      "format": "textarea",
      "default": "authority, budget, campaign_source, hs_analytics_source, hs_campaign, hs_lastmodifieddate, need, timeframe, dealname, amount, closedate, pipeline, createdate, engagements_last_meeting_booked, dealtype, hs_createdate, description, start_date, closed_lost_reason, closed_won_reason, end_date, lead_owner, tech_owner, service_amount, contract_type, hubspot_owner_id, partner_name, notes_last_updated",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of Deal properties to fetch. The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched. <font color=\"#e06666\"><b>NOTE</b>: applies only if the endpoint is selected.</font>",
      "uniqueItems": true,
      "propertyOrder": 700
    },
    "company_properties": {
      "type": "string",
      "title": "Company additional properties",
      "format": "textarea",
      "default": "about_us, name, phone, facebook_company_page, city, country, website, industry, annualrevenue, linkedin_company_page, hs_lastmodifieddate, hubspot_owner_id, notes_last_updated, description, createdate, numberofemployees, hs_lead_status, founded_year, twitterhandle, linkedinbio",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of Company properties to fetch. The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched. <font color=\"#e06666\"><b>NOTE</b>: applies only if the endpoint is selected.</font>",
      "uniqueItems": true,
      "propertyOrder": 500
    },
    "contact_properties": {
      "type": "string",
      "title": "Contact properties",
      "format": "textarea",
      "default": "hs_facebookid, hs_linkedinid, ip_city, ip_country, ip_country_code, newsletter_opt_in, firstname, linkedin_profile, lastname, email, mobilephone, phone, city, country, region, jobtitle, company, website, numemployees, industry, associatedcompanyid, hs_lead_status, lastmodifieddate, source, hs_email_optout, twitterhandle, lead_type, hubspot_owner_id, notes_last_updated, hs_analytics_source, opt_in, createdate, hs_twitterid, lifecyclestage",
      "options": {
        "input_height": "100px"
      },
      "description": "Comma separated list of contact properties to fetch. e.g. `'firstname', 'lastname'` The values must match valid company properties, otherwise an empty value is returned. If left empty, default properties will be fetched. <font color=\"#e06666\"><b>NOTE</b>: applies only if the endpoint is selected.</font>",
      "uniqueItems": true,
      "propertyOrder": 600
    }
  }
}



================================================
File: component_config/configuration_description.md
================================================
Additional documentation is available [here](https://bitbucket.org/kds_consulting_team/kds-team.ex-hubspot/src/master/README.md)


================================================
File: component_config/stack_parameters.json
================================================
{}


================================================
File: component_config/sample-config/config.json
================================================
{
	"storage": {
		"input": {
			"files": [],
			"tables": [{
					"source": "in.c-test.test",
					"destination": "test.csv",
					"limit": 50,
					"columns": [],
					"where_values": [],
					"where_operator": "eq"
				}
			]
		},
		"output": {
			"files": [],
			"tables": []
		}
	},
	"parameters": {
		"#api_token": "demo",
		"period_from": "2 months ago",
		"endpoints": [
			"companies",
			"campaigns",
			"email_events",
			"activities",
			"lists",
			"owners",
			"contacts",
			"deals",
			"pipelines"
		],
		"company_properties": "about_us, name, phone, facebook_company_page, city, country, website, industry, annualrevenue, linkedin_company_page, hs_lastmodifieddate, hubspot_owner_id, notes_last_updated, description, createdate, numberofemployees, hs_lead_status, founded_year, twitterhandle, linkedinbio",
		"contact_properties": "hs_facebookid, hs_linkedinid, ip_city, ip_country, ip_country_code, newsletter_opt_in, firstname, linkedin_profile, lastname, email, mobilephone, phone, city, country, region, jobtitle, company, website, numemployees, industry, associatedcompanyid, hs_lead_status, lastmodifieddate, source, hs_email_optout, twitterhandle, lead_type, hubspot_owner_id, notes_last_updated, hs_analytics_source, opt_in, createdate, hs_twitterid, lifecyclestage",
		"deal_properties": "authority, budget, campaign_source, hs_analytics_source, hs_campaign, hs_lastmodifieddate, need, timeframe, dealname, amount, closedate, pipeline, createdate, engagements_last_meeting_booked, dealtype, hs_createdate, description, start_date, closed_lost_reason, closed_won_reason, end_date, lead_owner, tech_owner, service_amount, contract_type, hubspot_owner_id, partner_name, notes_last_updated",
		"debug": true
	},
	"image_parameters": {
		"syrup_url": "https://syrup.keboola.com/"
	},
	"authorization": {
		"oauth_api": {
			"id": "OAUTH_API_ID",
			"credentials": {
				"id": "main",
				"authorizedFor": "Myself",
				"creator": {
					"id": "1234",
					"description": "me@keboola.com"
				},
				"created": "2016-01-31 00:13:30",
				"#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
				"oauthVersion": "2.0",
				"appKey": "000000004C184A49",
				"#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
			}
		}
	}
}



================================================
File: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
File: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
File: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
File: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
File: component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
File: component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
File: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
File: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
File: scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag


================================================
File: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
File: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
File: src/component.py
================================================
'''
Template Component main class.

'''

import logging
import os
from datetime import datetime

import pandas as pd
from hubspot.client_service import HubspotClientService
from kbc.env_handler import KBCEnvHandler

# primary keys
PIPELINE_STAGE_PK = ['PIPELINE_ID', 'stageId']
PIPELINE_PK = ['pipelineId']
OWNER_PK = ['ownerId']
LISTS_PK = ['listId']
ACTIVITIES_PK = ['engagement_id ']
EMAIL_EVENTS_PK = ['id', 'created']
CAMPAIGNS_PK = ['id']
DEAL_C_LIST_PK = ['Deal_ID', 'Contact_ID']
DEAL_STAGE_HIST_PK = ['DEAL_ID', 'sourceVid', 'sourceId', 'timestamp']
DEAL_PK = ['dealId ']
CONTACT_LIST_PK = ['internal_list_id', 'static_list_id', 'CONTACT_ID']
C_SUBMISSION_PK = ['form_id', 'CONTACT_ID', 'portal_id', 'conversion_id', 'page_id', 'page_url']
CONTACT_PK = ['vid', 'portal_id']
COMPANY_ID_COL = ['companyId']

# config keys
KEY_API_TOKEN = '#api_token'
KEY_PERIOD_FROM = 'period_from'
KEY_ENDPOINTS = 'endpoints'
KEY_INCR_OUT = 'incremental_output'

KEY_COMPANY_PROPERTIES = 'company_properties'
KEY_CONTACT_PROPERTIES = 'contact_properties'
KEY_DEAL_PROPERTIES = 'deal_properties'

KEY_PROPERTY_ATTRIBUTES = "property_attributes"

SUPPORTED_ENDPOINTS = ['companies', 'campaigns', 'email_events', 'activities', 'lists', 'owners', 'contacts', 'deals',
                       'pipelines']

MANDATORY_PARS = [KEY_API_TOKEN]
MANDATORY_IMAGE_PARS = []

# columns
CONTACT_FORM_SUBISSION_COLS = ["contact-associated-by", "conversion-id", "form-id", "form-type", "meta-data",
                               "page-id", "page-url", "portal-id", "timestamp", "title", 'CONTACT_ID']
CONTACT_LISTS_COLS = ["internal-list-id", "is-member", "static-list-id", "timestamp", "vid", "CONTACT_ID"]
DEAL_STAGE_HIST_COLS = ['name', 'source', 'sourceId', 'sourceVid', 'timestamp', 'value', 'DEAL_ID']

APP_VERSION = '0.0.1'


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS)
        # override debug from config
        if self.cfg_params.get('debug'):
            debug = True

        self.set_default_logger('DEBUG' if debug else 'INFO')
        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        try:
            self.validate_config(MANDATORY_PARS)
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as e:
            logging.error(e)
            exit(1)

        self.incremental = self.cfg_params.get(KEY_INCR_OUT)

    def run(self):
        '''
        Main execution code
        '''
        params = self.cfg_params  # noqa
        token = params[KEY_API_TOKEN]
        client_service = HubspotClientService(token)

        if params.get(KEY_PERIOD_FROM):
            start_date, end_date = self.get_date_period_converted(params.get(KEY_PERIOD_FROM),
                                                                  datetime.utcnow().strftime('%Y-%m-%d'))
            recent = True
        else:
            start_date = None
            recent = False
        endpoints = params.get(KEY_ENDPOINTS, SUPPORTED_ENDPOINTS)
        property_attributes = params.get(KEY_PROPERTY_ATTRIBUTES,
                                         {"include_versions": True, "include_source": True, "include_timestamp": True})

        if 'companies' in endpoints:
            logging.info('Extracting Companies')
            res_file_path = os.path.join(self.tables_out_path, 'companies.csv')
            self._get_simple_ds(res_file_path, COMPANY_ID_COL, client_service.get_companies, property_attributes,
                                recent, self._parse_props(params.get(KEY_COMPANY_PROPERTIES)))

        if 'campaigns' in endpoints:
            logging.info('Extracting Campaigns from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'campaigns.csv')
            self._get_simple_ds(res_file_path, CAMPAIGNS_PK, client_service.get_campaigns, recent)

        if 'email_events' in endpoints:
            logging.info('Extracting Email Events from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'email_events.csv')
            self._get_simple_ds(res_file_path, EMAIL_EVENTS_PK, client_service.get_email_events, start_date)

        if 'activities' in endpoints:
            logging.info('Extracting Activities from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'activities.csv')
            self._get_simple_ds(res_file_path, ACTIVITIES_PK, client_service.get_activities, start_date)

        if 'lists' in endpoints:
            logging.info('Extracting Lists from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'lists.csv')
            self._get_simple_ds(res_file_path, LISTS_PK, client_service.get_lists)

        if 'owners' in endpoints:
            logging.info('Extracting Owners from HubSpot CRM')
            res_file_path = os.path.join(self.tables_out_path, 'owners.csv')
            self._get_simple_ds(res_file_path, OWNER_PK, client_service.get_owners, recent)

        if 'contacts' in endpoints:
            logging.info('Extracting Contacts from HubSpot CRM')
            self.get_contacts(client_service, start_date, self._parse_props(params.get(KEY_CONTACT_PROPERTIES)),
                              property_attributes)

        if 'deals' in endpoints:
            logging.info('Extracting Deals from HubSpot CRM')
            self.get_deals(client_service, start_date, self._parse_props(params.get(KEY_DEAL_PROPERTIES)),
                           property_attributes)

        if 'pipelines' in endpoints:
            logging.info('Extracting Pipelines from HubSpot CRM')
            self.get_pipelines(client_service)

    def _get_simple_ds(self, res_file_path, pkey, ds_getter, *fpars):
        """
        Generic method to get simple objects
        :param res_file_path:
        :param pkey:
        :param ds_getter:
        :return:
        """
        for res in ds_getter(*fpars):
            self.output_file(res, res_file_path, res.columns)

        # store manifest
        if os.path.isfile(res_file_path):
            self.configuration.write_table_manifest(file_name=res_file_path, primary_key=pkey,
                                                    incremental=self.incremental)

    # CONTACTS
    def get_contacts(self, client: HubspotClientService, start_time, fields, property_attributes):
        res_file_path = os.path.join(self.tables_out_path, 'contacts.csv')
        for res in client.get_contacts(property_attributes, start_time, fields):
            if len(res.columns.values) == 0:
                logging.info("No contact records for specified period.")
                continue
            if 'form-submissions' in res.columns or 'list-memberships' in res.columns:
                self._store_contact_submission_and_list(res)
                res.drop(['form-submissions', 'list-memberships'], 1, inplace=True)
            self.output_file(res, res_file_path, res.columns)

        # store manifests
        if os.path.isfile(res_file_path):
            self.configuration.write_table_manifest(file_name=res_file_path, primary_key=CONTACT_PK,
                                                    incremental=self.incremental)

        c_subform_path = os.path.join(self.tables_out_path, 'contacts_form_submissions.csv')
        c_lists_path = os.path.join(self.tables_out_path, 'contacts_lists.csv')
        if os.path.isfile(c_subform_path):
            self.configuration.write_table_manifest(file_name=c_subform_path, primary_key=C_SUBMISSION_PK,
                                                    incremental=self.incremental)
        if os.path.isfile(c_lists_path):
            self.configuration.write_table_manifest(file_name=c_lists_path, primary_key=CONTACT_LIST_PK,
                                                    incremental=self.incremental)

    def _store_contact_submission_and_list(self, contacts):

        c_subform_path = os.path.join(self.tables_out_path, 'contacts_form_submissions.csv')
        c_lists_path = os.path.join(self.tables_out_path, 'contacts_lists.csv')
        # Create table with Contact's form submissions and lists and drop column afterwards
        for index, row in contacts.iterrows():

            if len(row['form-submissions']) > 0:
                temp_contacts_sub_forms = pd.DataFrame(row['form-submissions'])
                temp_contacts_sub_forms['CONTACT_ID'] = row['canonical-vid']
                res_cols = CONTACT_FORM_SUBISSION_COLS
                temp_contacts_sub_forms = temp_contacts_sub_forms.loc[:, res_cols].fillna('')

                # save res
                self.output_file(temp_contacts_sub_forms, c_subform_path, temp_contacts_sub_forms.columns)

            if len(row['list-memberships']) > 0:
                temp_contacts_lists = pd.DataFrame(row['list-memberships'])
                temp_contacts_lists['CONTACT_ID'] = row['canonical-vid']
                res_cols = CONTACT_LISTS_COLS
                temp_contacts_lists = temp_contacts_lists.loc[:, res_cols].fillna('')
                # save res
                self.output_file(temp_contacts_lists, c_lists_path, temp_contacts_lists.columns)

    # DEALS
    def get_deals(self, client: HubspotClientService, start_time, fields, property_attributes):
        logging.info('Extracting Companies from HubSpot CRM')
        res_file_path = os.path.join(self.tables_out_path, 'deals.csv')
        for res in client.get_deals(property_attributes, start_time, fields):
            self.output_file(res, res_file_path, res.columns)
            self._store_deals_stage_hist_and_list(res)

        # store manifests
        self.configuration.write_table_manifest(file_name=res_file_path, primary_key=DEAL_PK,
                                                incremental=self.incremental)
        stage_hist_path = os.path.join(self.tables_out_path, 'deals_stage_history.csv')
        c_lists_path = os.path.join(self.tables_out_path, 'deals_contacts_list.csv')

        if os.path.isfile(stage_hist_path):
            self.configuration.write_table_manifest(file_name=stage_hist_path, primary_key=DEAL_STAGE_HIST_PK,
                                                    incremental=self.incremental)
        if os.path.isfile(c_lists_path):
            self.configuration.write_table_manifest(file_name=c_lists_path, primary_key=DEAL_C_LIST_PK,
                                                    incremental=self.incremental)

    def _store_deals_stage_hist_and_list(self, deals):

        stage_hist_path = os.path.join(self.tables_out_path, 'deals_stage_history.csv')
        c_lists_path = os.path.join(self.tables_out_path, 'deals_contacts_list.csv')
        # Create table with Deals' Stage History & Deals' Contacts List

        for index, row in deals.iterrows():

            if row.get('properties.dealstage.versions') and str(
                    row['properties.dealstage.versions']) != 'nan' and len(row['properties.dealstage.versions']) > 0:
                temp_stage_history = pd.DataFrame(row['properties.dealstage.versions'])
                temp_stage_history['DEAL_ID'] = row['dealId']
                # fix columns - sometimes there are some missing in the response
                temp_stage_history = temp_stage_history.loc[:, DEAL_STAGE_HIST_COLS].fillna('')

                self.output_file(temp_stage_history, stage_hist_path, temp_stage_history.columns)

            if row.get('associations.associatedVids') and len(row['associations.associatedVids']) != '[]':
                temp_deals_contacts_list = pd.DataFrame(row['associations.associatedVids'],
                                                        columns=['Contact_ID'])
                temp_deals_contacts_list['Deal_ID'] = row['dealId']
                self.output_file(temp_deals_contacts_list, c_lists_path, temp_deals_contacts_list.columns)

    # PIPELINES
    def get_pipelines(self, client: HubspotClientService):
        logging.info('Extracting Companies from HubSpot CRM')
        res_file_path = os.path.join(self.tables_out_path, 'pipelines.csv')
        for res in client.get_pipelines():
            self.output_file(res, res_file_path, res.columns)
            self._store_pipeline_stages(res)

        # store manifests
        self.configuration.write_table_manifest(file_name=res_file_path, primary_key=PIPELINE_PK,
                                                incremental=self.incremental)

        stage_hist_path = os.path.join(self.tables_out_path, 'pipeline_stages.csv')
        self.configuration.write_table_manifest(file_name=stage_hist_path, primary_key=PIPELINE_STAGE_PK,
                                                incremental=self.incremental)

    def _store_pipeline_stages(self, pipelines):

        stage_hist_path = os.path.join(self.tables_out_path, 'pipeline_stages.csv')
        # Create table with Pipelines' Stages.
        for index, row in pipelines.iterrows():

            if len(row['stages']) > 0:
                temp_pipelines_stages = pd.DataFrame(row['stages'])
                temp_pipelines_stages['PIPELINE_ID'] = row['pipelineId']
                self.output_file(temp_pipelines_stages, stage_hist_path, temp_pipelines_stages.columns)

    def output_file(self, data_output, file_output, column_headers):
        """
        Output the dataframe input to destination file
        Append to the file if file does not exist
        * row by row
        """
        if data_output.empty:
            logging.info("No results for %s", file_output)
            return

        if not os.path.isfile(file_output):
            with open(file_output, 'w+', encoding='utf-8', newline='') as b:
                data_output.to_csv(b, index=False, columns=column_headers, line_terminator="")
            b.close()
        else:
            with open(file_output, 'a', encoding='utf-8', newline='') as b:
                data_output.to_csv(b, index=False, header=False, columns=column_headers, line_terminator="")
            b.close()

    def _parse_props(self, param):
        cols = []
        if param:
            cols = [p.strip() for p in param.split(",")]
        return cols


"""
        Main entrypoint
"""
if __name__ == "__main__":
    comp = Component()
    comp.run()



================================================
File: src/hubspot/__init__.py
================================================



================================================
File: src/hubspot/client_service.py
================================================
import json
from _datetime import timedelta

import pandas as pd
from collections.abc import Iterable
from datetime import datetime
from kbc.client_base import HttpClientBase
from pandas.io.json import json_normalize

COMPANIES_DEFAULT_COLS = ["additionalDomains", "companyId", "isDeleted", "mergeAudits", "portalId", "stateChanges"]
COMPANY_DEFAULT_PROPERTIES = ['about_us', 'name', 'phone', 'facebook_company_page', 'city', 'country', 'website',
                              'industry', 'annualrevenue', 'linkedin_company_page',
                              'hs_lastmodifieddate', 'hubspot_owner_id', 'notes_last_updated', 'description',
                              'createdate', 'numberofemployees', 'hs_lead_status', 'founded_year',
                              'twitterhandle',
                              'linkedinbio']

DEAL_DEFAULT_COLS = ["associations.associatedCompanyIds",
                     "associations.associatedDealIds",
                     "associations.associatedVids",
                     "dealId",
                     "imports",
                     "isDeleted",
                     "portalId",
                     "stateChanges"]

DEAL_DEFAULT_PROPERTIES = ["hs_object_id", 'authority', 'budget', 'campaign_source', 'hs_analytics_source',
                           'hs_campaign',
                           'hs_lastmodifieddate', 'need', 'timeframe', 'dealname', 'amount', 'closedate', 'pipeline',
                           'createdate', 'engagements_last_meeting_booked', 'dealtype', 'hs_createdate', 'description',
                           'start_date', 'closed_lost_reason', 'closed_won_reason', 'end_date', 'lead_owner',
                           'tech_owner', 'service_amount', 'contract_type',
                           'hubspot_owner_id',
                           'partner_name', 'notes_last_updated']

CONTACTS_DEFAULT_COLS = ["addedAt",
                         "canonical-vid",
                         "form-submissions",
                         "identity-profiles",
                         "is-contact",
                         "list-memberships",
                         "merge-audits",
                         "merged-vids",
                         "portal-id",
                         "profile-token",
                         "profile-url",
                         "vid"]

CONTACT_DEFAULT_PROPERTIES = ['hs_facebookid', 'hs_linkedinid', 'ip_city', 'ip_country',
                              'ip_country_code', 'newsletter_opt_in', 'linkedin_profile',
                              'email', 'mobilephone', 'phone', 'city',
                              'country', 'region', 'jobtitle', 'website', 'numemployees',
                              'industry', 'associatedcompanyid', 'hs_lead_status', 'lastmodifieddate',
                              'source', 'hs_email_optout', 'twitterhandle', 'lead_type',
                              'hubspot_owner_id', 'notes_last_updated', 'hs_analytics_source', 'opt_in',
                              'createdate', 'hs_twitterid', 'lifecyclestage']

LISTS_COLS = ['archived', 'authorId', 'createdAt', 'deleteable', 'dynamic', 'filters',
              'internalListId', 'listId', 'listType', 'metaData.error',
              'metaData.lastProcessingStateChangeAt', 'metaData.lastSizeChangeAt',
              'metaData.listReferencesCount', 'metaData.parentFolderId',
              'metaData.processing', 'metaData.size', 'name', 'portalId', 'readOnly',
              'updatedAt']

EMAIL_EVENTS_COLS = ['appId', 'appName', 'browser', 'browser.family', 'browser.name', 'browser.producer',
                     'browser.producerUrl', 'browser.type', 'browser.url', 'browser.version', 'causedBy.created',
                     'causedBy.id', 'created', 'deviceType', 'duration', 'emailCampaignId', 'filteredEvent', 'id',
                     'ipAddress', 'location', 'location.city', 'location.country', 'location.state', 'portalId',
                     'recipient', 'sentBy.created', 'sentBy.id', 'smtpId', 'type', 'userAgent']

ENGAGEMENTS_COLS = ['metadata.isBot', 'metadata.endTime', 'metadata.postSendStatus', 'associations.quoteIds',
                    'metadata.from.raw', 'engagement.createdBy', 'metadata.to',
                    'metadata.agentResponseTimeMilliseconds', 'metadata.visitorStartTime', 'metadata.messageId',
                    'metadata.durationMilliseconds', 'metadata.externalUrl', 'engagement.source',
                    'associations.contactIds', 'engagement.lastUpdated', 'metadata.body', 'metadata.forObjectType',
                    'metadata.categoryId', 'metadata.sessionClosedAt', 'metadata.visitorEndTime', 'metadata.from.email',
                    'metadata.recordingUrl', 'associations.ticketIds', 'associations.contentIds',
                    'metadata.fromfirstName', 'metadata.numVisitorMessages', 'engagement.ownerId',
                    'metadata.facsimileSendId', 'metadata.createdFromLinkId', 'metadata.sessionDurationMilliseconds',
                    'metadata.startTime', 'metadata.subject', 'associations.ownerIds', 'associations.dealIds',
                    'metadata.html', 'metadata.source', 'metadata.sendDefaultReminder', 'engagement.createdAt',
                    'engagement.sourceId', 'metadata.category', 'metadata.fromemail', 'metadata.sender.email',
                    'metadata.online', 'metadata.from.lastName', 'engagement.uid', 'engagement.allAccessibleTeamIds',
                    'metadata.from.firstName', 'metadata.text', 'metadata.conversationSource', 'metadata.toNumber',
                    'associations.workflowIds', 'metadata.sentVia', 'attachments', 'metadata.numAgentMessages',
                    'metadata.url', 'metadata.agentJoinTime', 'engagement.id', 'engagement.type',
                    'associations.companyIds', 'metadata.title', 'metadata.disposition', 'metadata.state',
                    'engagement.modifiedBy', 'metadata.fromlastName', 'metadata.externalId', 'metadata.sourceId',
                    'metadata.fromNumber', 'metadata.cc', 'metadata.externalAccountId',
                    'metadata.visitorWaitTimeMilliseconds', 'engagement.portalId', 'metadata.trackerKey',
                    'metadata.preMeetingProspectReminders', 'metadata.attachedVideoOpened',
                    'metadata.validationSkipped', 'metadata.loggedFrom', 'metadata.mediaProcessingStatus',
                    'metadata.threadId', 'metadata.reminders', 'metadata.status', 'metadata.name',
                    'engagement.timestamp', 'metadata.contentId', 'metadata.campaignGuid', 'metadata.taskType',
                    'metadata.bcc', 'scheduledTasks', 'engagement.active', 'metadata.attachedVideoWatched',
                    'engagement.teamId', 'metadata.to.email', 'metadata.calleeObjectId', 'metadata.calleeObjectType',
                    'metadata.emailSendEventId.created', 'metadata.emailSendEventId.id', 'metadata.errorMessage']

CAMPAIGNS = 'email/public/v1/campaigns/'

LISTS = 'contacts/v1/lists'

ENGAGEMENTS_PAGED = 'engagements/v1/engagements/paged'
ENGAGEMENTS_PAGED_SINCE = 'engagements/v1/engagements/recent/modified'

EMAIL_EVENTS = 'email/public/v1/events'

CAMPAIGNS_BY_ID = 'email/public/v1/campaigns/by-id'
CAMPAIGNS_BY_ID_RECENT = 'email/public/v1/campaigns'

DEALS_ALL = 'deals/v1/deal/paged'
DEALS_RECENT = 'deals/v1/deal/recent/modified'

COMPANIES_ALL = 'companies/v2/companies/paged'
COMPANIES_RECENT = 'companies/v2/companies/recent/modified'

MAX_RETRIES = 10
BASE_URL = 'https://api.hubapi.com/'

# endpoints
CONTACTS_ALL = 'contacts/v1/lists/all/contacts/all'
CONTACTS_RECENT = 'contacts/v1/lists/recently_updated/contacts/recent'

COMPANY_PROPERTIES = 'properties/v1/companies/properties/'


class HubspotClientService(HttpClientBase):

    def __init__(self, token):
        HttpClientBase.__init__(self, base_url=BASE_URL, max_retries=MAX_RETRIES, backoff_factor=0.3,
                                status_forcelist=(429, 500, 502, 504), default_params={"hapikey": token})

    def _get_paged_result_pages(self, endpoint, parameters, res_obj_name, limit_attr, offset_req_attr, offset_resp_attr,
                                has_more_attr, offset, limit, default_cols=None):

        has_more = True
        while has_more:
            final_df = pd.DataFrame()
            parameters[offset_req_attr] = offset
            parameters[limit_attr] = limit

            req = self.get_raw(self.base_url + endpoint, params=parameters)
            resp_text = str.encode(req.text, 'utf-8')
            req_response = json.loads(resp_text)

            if req_response[has_more_attr]:
                has_more = True
            else:
                has_more = False
            offset = req_response[offset_resp_attr]
            final_df = final_df.append(json_normalize(req_response[res_obj_name]))
            if default_cols and not final_df.empty:
                # dedupe
                default_cols = list(set(default_cols))
                final_df = final_df.loc[:, default_cols].fillna('')
            # sort cols
            final_df = final_df.reindex(sorted(final_df.columns), axis=1)
            yield final_df

    def _get_all_pages_result(self, endpoint, parameters, res_obj_name, limit_attr, offset_attr, has_more_attr, offset,
                              limit):
        final_df = pd.DataFrame()

        has_more = True
        while has_more:
            parameters[offset_attr] = offset
            parameters[limit_attr] = limit

            req = self.get_raw(self.base_url + endpoint, params=parameters)
            resp_text = str.encode(req.text, 'utf-8')
            req_response = json.loads(resp_text)

            if req_response[has_more_attr]:
                has_more = True
            else:
                has_more = False
            offset = req_response[offset_attr]
            return final_df.append(json_normalize(req_response[res_obj_name]))

    def get_contacts(self, property_attributes, start_time=None, fields=None) -> Iterable:
        """
        Get either all available contacts or recent ones specified by start_time.

        API supports more options, possible to extend in the future
        :type fields: list list of contact properties to get
        :param start_time: datetime
        :return: generator object with all available pages
        """
        offset = -1

        if not fields:
            contact_properties = CONTACT_DEFAULT_PROPERTIES
            expected_contact_cols = CONTACTS_DEFAULT_COLS + self._build_property_cols(
                CONTACTS_DEFAULT_COLS, property_attributes)
        else:
            contact_properties = fields
            expected_contact_cols = CONTACTS_DEFAULT_COLS + self._build_property_cols(fields, property_attributes)

        parameters = {'property': contact_properties, 'formSubmissionMode': 'all', 'showListMemberships': 'true'}

        # hubspot api allows only 30 days back
        if start_time and (datetime.utcnow() - start_time).days >= 30:
            start_time = datetime.now() + timedelta(-30)
        parameters['propertyMode'] = 'value_and_history'
        if start_time:
            return self._get_paged_result_pages(CONTACTS_RECENT, parameters, 'contacts', 'count', 'timeOffset',
                                                'time-offset', 'has-more', int(start_time.timestamp() * 1000), 100,
                                                default_cols=expected_contact_cols)
        else:
            return self._get_paged_result_pages(CONTACTS_ALL, parameters, 'contacts', 'count', 'vidOffset',
                                                'vid-offset', 'has-more', offset, 100,
                                                default_cols=expected_contact_cols)

    def get_companies(self, property_attributes, recent=False, fields=None):

        offset = 0
        if not fields:
            company_properties = COMPANY_DEFAULT_PROPERTIES
            expected_company_cols = COMPANIES_DEFAULT_COLS + self._build_property_cols(
                COMPANY_DEFAULT_PROPERTIES, property_attributes)
        else:
            company_properties = fields
            expected_company_cols = COMPANIES_DEFAULT_COLS + self._build_property_cols(fields, property_attributes)

        parameters = {'properties': company_properties}

        if recent:
            return self._get_paged_result_pages(COMPANIES_RECENT, parameters, 'results', 'count', 'offset', 'offset',
                                                'hasMore',
                                                offset, 200, default_cols=expected_company_cols)
        else:
            return self._get_paged_result_pages(COMPANIES_ALL, parameters, 'companies', 'limit', 'offset', 'offset',
                                                'has-more', offset, 250, default_cols=expected_company_cols)

    def get_company_properties(self):
        req = self.get_raw(self.base_url + COMPANY_PROPERTIES)
        req_response = req.json()
        return req_response

    def _build_property_cols(self, properties, property_attributes):
        # get flattened property cols
        prop_cols = []
        for p in properties:
            if property_attributes.get('include_source', True):
                prop_cols.append('properties.' + p + '.source')
                prop_cols.append('properties.' + p + '.sourceId')
            if property_attributes.get('include_timestamp', True):
                prop_cols.append('properties.' + p + '.timestamp')
            if property_attributes.get('include_versions', True):
                prop_cols.append('properties.' + p + '.versions')

            prop_cols.append('properties.' + p + '.value')
        return prop_cols

    def _build_contact_property_cols(self, properties):
        # get flattened property cols
        prop_cols = []
        for p in properties:
            prop_cols.append('properties.' + p + '.value')
            prop_cols.append('properties.' + p + '.versions')
        return prop_cols

    def get_deals(self, property_attributes, start_time=None, fields=None) -> Iterable:
        """
        Get either all available deals or recent ones specified by start_time.

        API supports more options, possible to extend in the future
        :type fields: list list of deal properties to get
        :param start_time: datetime
        :return: generator object with all available pages
        """
        offset = 0
        if not fields:
            deal_properties = DEAL_DEFAULT_PROPERTIES
            expected_deal_cols = DEAL_DEFAULT_COLS + self._build_property_cols(
                DEAL_DEFAULT_PROPERTIES, property_attributes)
        else:
            if 'dealstage' in fields:
                fields.remove('dealstage')
            deal_properties = fields
            expected_deal_cols = DEAL_DEFAULT_COLS + self._build_property_cols(fields, property_attributes)

        property_attributes['include_versions'] = True
        expected_deal_cols += self._build_property_cols(['dealstage'], property_attributes)
        parameters = {'properties': deal_properties,
                      'propertiesWithHistory': 'dealstage',
                      'includeAssociations': 'true'}
        if start_time:
            parameters['since'] = int(start_time.timestamp() * 1000)
            return self._get_paged_result_pages(DEALS_RECENT, parameters, 'results', 'count', 'offset', 'offset',
                                                'hasMore',
                                                offset, 100, default_cols=expected_deal_cols)
        else:
            return self._get_paged_result_pages(DEALS_ALL, parameters, 'deals', 'limit', 'offset', 'offset', 'hasMore',
                                                offset, 250, default_cols=expected_deal_cols)

    def get_campaigns(self, recent=False):
        final_df = pd.DataFrame()
        if recent:
            url = CAMPAIGNS_BY_ID_RECENT
        else:
            url = CAMPAIGNS_BY_ID

        for res in self._get_paged_result_pages(url, {}, 'campaigns', 'limit', 'offset', 'offset', 'hasMore', None,
                                                1000):

            for index, row in res.iterrows():
                req = self.get_raw(self.base_url + CAMPAIGNS + str(row['id']))
                req_response = req.json()

                final_df = final_df.append(json_normalize(req_response))

            yield final_df[['counters.open', 'counters.click', 'id', 'name']]

    def get_email_events(self, start_date: datetime) -> Iterable:
        offset = ''
        timestamp = None
        if start_date:
            timestamp = int(start_date.timestamp() * 1000)
        parameters = {'eventType': 'OPEN', 'startTimestamp': timestamp}
        for open_ev in self._get_paged_result_pages(EMAIL_EVENTS, parameters, 'events', 'limit', 'offset', 'offset',
                                                    'hasMore',
                                                    offset, 1000, default_cols=EMAIL_EVENTS_COLS):
            yield open_ev

        parameters = {'eventType': 'CLICK', 'startTimestamp': timestamp}
        for click_ev in self._get_paged_result_pages(EMAIL_EVENTS, parameters, 'events', 'limit', 'offset', 'offset',
                                                     'hasMore',
                                                     offset, 1000, default_cols=EMAIL_EVENTS_COLS):
            yield click_ev

    def get_activities(self, start_time: datetime) -> Iterable:
        offset = 0

        if start_time:
            pages = self._get_paged_result_pages(ENGAGEMENTS_PAGED_SINCE, {"since": int(start_time.timestamp() * 1000)},
                                                 'results', 'count', 'offset', 'offset', 'hasMore', offset, 250,
                                                 default_cols=ENGAGEMENTS_COLS)
        else:
            pages = self._get_paged_result_pages(ENGAGEMENTS_PAGED, {}, 'results', 'limit', 'offset', 'offset',
                                                 'hasMore',
                                                 offset, 250, default_cols=ENGAGEMENTS_COLS)
        for pg_res in pages:
            if 'metadata.text' in pg_res.columns:
                pg_res.drop(['metadata.text'], 1)
            if 'metadata.html' in pg_res.columns:
                pg_res.drop(['metadata.html'], 1)
            yield pg_res

    def get_lists(self):
        offset = 0

        return self._get_paged_result_pages(LISTS, {}, 'lists', 'limit', 'offset', 'offset', 'has-more',
                                            offset, 250, default_cols=LISTS_COLS)

    def get_pipelines(self, include_inactive=None):
        final_df = pd.DataFrame()

        req = self.get_raw('https://api.hubapi.com/deals/v1/pipelines', params={'include_inactive': include_inactive})
        req_response = req.json()

        final_df = final_df.append(json_normalize(req_response))

        return [final_df]

    def get_owners(self, include_inactive=True):
        final_df = pd.DataFrame()

        req = self.get_raw('https://api.hubapi.com/owners/v2/owners/', params={'include_inactive': include_inactive})
        req_response = req.json()

        final_df = final_df.append(json_normalize(req_response))

        return [final_df]



================================================
File: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
File: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest

from component import Component


class TestComponent(unittest.TestCase):


    def testRunEmptyFails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


