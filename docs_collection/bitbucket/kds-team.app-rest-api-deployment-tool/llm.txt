Directory structure:
└── kds_consulting_team-kds-team.app-rest-api-deployment-tool/
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    ├── change_log.md
    ├── Dockerfile
    ├── resources/
    │   ├── tr2lambda/
    │   │   ├── __init__.py
    │   │   ├── requirements.txt
    │   │   └── app.py
    │   └── template.yaml
    ├── flake8.cfg
    ├── src/
    │   └── component.py
    ├── LICENSE.md
    ├── docs/
    │   └── imgs/
    ├── requirements.txt
    ├── bitbucket-pipelines.yml
    ├── component_config/
    │   ├── component_short_description.md
    │   ├── .DS_Store
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── component_long_description.md
    │   └── sample-config/
    │       ├── in/
    │       │   ├── .DS_Store
    │       │   ├── tables/
    │       │   │   └── .DS_Store
    │       │   └── files/
    │       │       └── order1.xml
    │       ├── .DS_Store
    │       ├── out/
    │       │   ├── .DS_Store
    │       │   ├── tables/
    │       │   │   └── .DS_Store
    │       │   └── files/
    │       │       └── .DS_Store
    │       └── config.json
    ├── deploy.sh
    ├── docker-compose.yml
    ├── scripts/
    │   ├── build_n_test.sh
    │   ├── update_dev_portal_properties.sh
    │   ├── build_n_run.ps1
    │   ├── run.bat
    │   └── run_kbc_tests.ps1
    └── README.md

================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: /change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog


================================================
File: /Dockerfile
================================================
FROM linuxbrew/debian
ENV PYTHONIOENCODING utf-8
RUN brew tap aws/tap; \
brew install awscli aws-sam-cli

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
USER root
RUN apt-get update && apt-get install -y build-essential && apt-get install -y python3-pip
RUN pip3 install flake8

COPY . /code/
RUN pip3 install -r /code/requirements.txt

WORKDIR /code/

ENV HOME /tmp

CMD ["python3", "-u", "/code/src/component.py"]


================================================
File: /resources/tr2lambda/requirements.txt
================================================
pandas
boto3

================================================
File: /resources/tr2lambda/app.py
================================================
import base64
import binascii
import gzip
import json
import os
import sys
import ast

import pandas as pd
import shutil

KEY_API_KEYS_ENV = 'api_keys'


class LambdaException(Exception):
    """
    Lambda ex
    """


def check_authorization(event):
    # api keys are stored in environment varialble
    keys_dict = ast.literal_eval(os.getenv(KEY_API_KEYS_ENV))
    keys = [k['#secret'] for k in keys_dict]
    if not event['headers'].get('Authorization') or event['headers'].get('Authorization') not in keys:
        return False
    else:
        return True


def cleanup_tmp_folder():
    if not os.path.exists('/tmp'):
        return
    for root, dirs, files in os.walk('/tmp'):
        for name in files:
            print(os.path.join(root, name))
            os.remove(os.path.join(root, name))
        for name in dirs:
            print(os.path.join(root, name))
            shutil.rmtree(os.path.join(root, name))


def prepare_common_interface(body):
    # cleanup tmp folder (may be shared)
    cleanup_tmp_folder()
    # build folder structure
    if not os.path.exists('/tmp'):
        os.makedirs('/tmp')
    if not os.path.exists('/tmp/in/tables'):
        os.makedirs('/tmp/in/tables')
    if not os.path.exists('/tmp/out/tables'):
        os.makedirs('/tmp/out/tables')

    body = json.loads(body)
    error = ''
    for key in body:
        try:
            tb = pd.read_json(json.dumps(body[key]), orient='split')
            tb.to_csv('/tmp/in/tables/' + key, index=False)
        except Exception as e:
            error += F"Parsing of table definition '{key}' failed with error: '{str(e)}'"
    if error:
        raise LambdaException(error)
    os.chdir('/tmp')


def build_response_body():
    body = dict()
    for filename in os.listdir('/tmp/out/tables'):
        tb = pd.read_csv(os.path.join('/tmp/out/tables', filename))
        dct = tb.to_dict(orient='split')
        dct.pop('index', None)
        body = {**body, **{filename: dct}}
    return json.dumps(body)


def lambda_handler(event, context):
    """Sample pure Lambda function

    Parameters
    ----------
    event: dict, required
        API Gateway Lambda Proxy Input Format

        Event doc:
        https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-input-format

    context: object, required
        Lambda Context runtime methods and attributes

        Context doc: https://docs.aws.amazon.com/lambda/latest/dg/python-context-object.html

    Returns
    ------
    API Gateway Lambda Proxy Output Format: dict

        Return doc: https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html
    """

    try:
        print('Authorization')
        if not check_authorization(event):
            return {"body": json.dumps('Invalid token or missing Authorization header!'),
                    "statusCode": 401}

        print('Collecting request')
        c_type = event['headers']['Content-Type']
        # c_enc = event['headers'].get('Content-Encoding')

        print('Parsing input')
        if c_type == 'application/x-gzip':
            body = gzip.decompress(binascii.a2b_base64(event['body'].encode('utf-8'))).decode()
        else:
            body = event['body']

        print("Preparing environment")
        prepare_common_interface(body)

        print("Executing python transformation function")
        try:
            curr_directory = os.path.dirname(os.path.realpath(__file__))
            script = open(os.path.join(curr_directory, 'keboola_python_tr.py')).read()
            exec(script)
        except Exception as e:
            raise LambdaException(F"Excecution of the Python Transformation code failed with error: '{str(e)}'")

        body = build_response_body()
        if sys.getsizeof(body) > 5000000:
            body = gzip.compress(bytes(body, 'utf-8'))
            body = json.dumps(str(base64.b64encode(body), 'utf-8'))
            response = {"body": body, "statusCode": 200,
                        "isBase64Encoded": True,
                        "headers": {
                            "Content-Encoding": "gzip"
                        }}
        else:
            response = {"body": body, "statusCode": 200}

        return response
    except LambdaException as e:
        print(e)
        return {"body": json.dumps({"error": str(e)}),
                "statusCode": 400
                }
    except Exception as e:
        # Send some context about this error to Lambda Logs
        print(e)
        return {"body": json.dumps({"error": str(e)}),
                "statusCode": 500
                }


================================================
File: /resources/template.yaml
================================================
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: >
  Generated template for Keboola Python transformation 2 Lambda conversion

# More info about Globals: https://github.com/awslabs/serverless-application-model/blob/master/docs/globals.rst
Globals:
  Function:
    Timeout: 3

Resources:
  KeboolaBasicAWSApiGateway:
    Type: AWS::Serverless::Api
    Properties:
      Name: Keboola AWS Api Gateway
      MinimumCompressionSize: 5000000
      BinaryMediaTypes:
        # In case of gzipped request sent as binary file (to bypass api gtw/lambda payload limits)
        - application~1x-gzip
      StageName: kbc
  KeboolaTr2Lambda:
    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction
    Properties:
      CodeUri: tr2lambda/
      Handler: app.lambda_handler
      Runtime: python3.7
      Timeout: 30
      MemorySize: 256
      Environment:
        Variables:
          api_keys: '[{"name": "key", "secret": "sec"}]'
      Events:
        Tr2Lambda:
          Type: Api # More info about API Event Source: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#api
          Properties:
            Path: /keboola-test
            Method: post
            RestApiId: !Ref KeboolaBasicAWSApiGateway
Outputs:
  KeboolaBasicAWSApiGateway:
    Description: 'API Gateway endpoint URL for Keboola converted Python transformation'
    Value: !Sub 'https://${KeboolaBasicAWSApiGateway}.execute-api.${AWS::Region}.amazonaws.com/kbc/%(url_path)s/'
  KeboolaBasicAWSApiGatewayRestApiId:
    Description: 'API Gateway ARN for Basic AWS API Gateway'
    Value: !Ref KeboolaBasicAWSApiGateway
    Export:
      Name: BasicAWSApiGateway-RestApiId
  KeboolaBasicAWSApiGatewayyRootResourceId:
    Value: !GetAtt KeboolaBasicAWSApiGateway.RootResourceId
    Export:
      Name: KeboolaBasicAWSApiGateway-RootResourceId


================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    resources
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /src/component.py
================================================
'''
Template Component main class.

'''

import json
import logging
import os
import sys
import urllib
from distutils import dir_util

import boto3
import cfn_tools
import requests
import subprocess
from kbc.env_handler import KBCEnvHandler

# configuration variables
PACKAGES = 'packages'
REGION = 'region'
BUCKET_ID = 'bucket_id'
TRANSFORMATION_ID = 'transformation_id'
STORAGE_TOKEN = '#storage_token'

# aws params
KEY_AWS_PARAMS = 'aws_parameters'
KEY_AWS_API_KEY_ID = 'api_key_id'
KEY_AWS_API_KEY_SECRET = '#api_key_secret'
KEY_AWS_REGION = 'aws_region'
KEY_AWS_S3_BUCKET = 's3_bucket'

# lambda params
KEY_LAMBDA_CFG = 'lambda_config'
KEY_CF_STACK_NAME = 'cf_stack_name'
KEY_URL_PATH = 'url_path'
KEY_MEMORY_AVAILABLE = 'memory_available'
KEY_API_KEYS = 'api_keys'
KEY_SECRET = 'secret'
KEY_NAME = 'name'

KEY_SOURCE_TR = 'source_transformation'
# #### Keep for debug
KEY_STDLOG = 'stdlogging'
KEY_DEBUG = 'debug'

MANDATORY_PARS = [KEY_LAMBDA_CFG, KEY_AWS_PARAMS, KEY_SOURCE_TR]
MANDATORY_IMAGE_PARS = []

# current working dir
PAR_DIRPATH = os.path.dirname(os.path.abspath(os.path.join(os.path.abspath(__file__), os.pardir)))
TMP_FOLDER_PATH = os.path.join('/tmp')
RESOURCES_FOLDER_PATH = os.path.join(PAR_DIRPATH, 'resources')

PAR_SECRET_MGR_TAG = 'keboola_tr2lambda'

APP_VERSION = '0.0.1'


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS)
        # override debug from config
        if self.cfg_params.get(KEY_DEBUG):
            debug = True

        log_level = logging.DEBUG if debug else logging.INFO
        # setup GELF if available
        if os.getenv('KBC_LOGGER_ADDR', None):
            self.set_gelf_logger(log_level)
        else:
            self.set_default_logger(log_level)
        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        try:
            self.validate_config()
            self.validate_parameters(self.cfg_params[KEY_LAMBDA_CFG],
                                     [KEY_MEMORY_AVAILABLE, KEY_URL_PATH, KEY_API_KEYS], KEY_LAMBDA_CFG)
            self.validate_parameters(self.cfg_params[KEY_AWS_PARAMS],
                                     [KEY_AWS_API_KEY_ID, KEY_AWS_API_KEY_SECRET, KEY_AWS_REGION, KEY_AWS_S3_BUCKET],
                                     KEY_AWS_PARAMS)
            self.validate_parameters(self.cfg_params[KEY_SOURCE_TR],
                                     [BUCKET_ID, PACKAGES, TRANSFORMATION_ID],
                                     KEY_AWS_PARAMS)
        except ValueError as e:
            logging.exception(e)
            exit(1)

        self.packages_string = self.cfg_params[KEY_SOURCE_TR][PACKAGES]
        self.bucket_id = self.cfg_params[KEY_SOURCE_TR][BUCKET_ID]
        self.transformation_id = self.cfg_params[KEY_SOURCE_TR][TRANSFORMATION_ID]

        # init variables
        self.storage_token = self.get_storage_token()
        self.stack_id = os.environ["KBC_STACKID"]

    def run(self):
        '''
        Main execution code
        '''

        # get the list of packages, split it to a list, and then write it out to txt
        packages_list = [p.strip() for p in self.packages_string.split(",")]

        lambda_pars = self.cfg_params[KEY_LAMBDA_CFG]
        aws_pars = self.cfg_params[KEY_AWS_PARAMS]

        logging.info("Preparing package folder.")
        tr_name, tr_id = self.prepare_package_folder(packages_list)

        logging.info("Setting up AWS cli parameters.")
        self.setup_aws_config(aws_pars)

        logging.info("Generating CloudFormation template.")
        self.prepare_cf_template(lambda_pars, aws_pars, tr_name, tr_id)

        logging.info("Deploying SAM package.")
        self.build_n_deploy_sam_package(lambda_pars, aws_pars)

        logging.info("Getting generated endpoint URL")
        client = boto3.client('apigateway')
        apis = client.get_rest_apis()
        api_id = [r['id'] for r in apis['items'] if r['name'] == lambda_pars[KEY_CF_STACK_NAME]]
        api_url = f'https://{api_id[0]}.execute-api.{aws_pars[KEY_AWS_REGION]}' \
                  f'.amazonaws.com/kbc/{lambda_pars[KEY_URL_PATH]}/'

        logging.info(f"Api created! Your function is available at {api_url}")

        # get created stack
        client = boto3.client('cloudformation')
        stacks = client.list_stacks(
            StackStatusFilter=['CREATE_COMPLETE', 'UPDATE_COMPLETE', 'UPDATE_ROLLBACK_COMPLETE', 'IMPORT_COMPLETE',
                               'IMPORT_ROLLBACK_COMPLETE'])
        stack_id = [r['StackId'] for r in stacks['StackSummaries'] if r['StackName'] == lambda_pars[KEY_CF_STACK_NAME]]
        stack_url = f'https://{aws_pars[KEY_AWS_REGION]}.console.aws.amazon.com/cloudformation/home?'
        stack_query = urllib.parse.quote(f'region={aws_pars[KEY_AWS_REGION]}#/stacks/outputs?stackId={stack_id[0]}')

        logging.info(f"You can also review the created stack {lambda_pars[KEY_CF_STACK_NAME]} "
                     f"in the console: {stack_url + stack_query}")

    def prepare_package_folder(self, packages_list):
        requirements_file = os.path.join(TMP_FOLDER_PATH, 'requirements.txt')
        os.makedirs(TMP_FOLDER_PATH, exist_ok=True)
        # copy template files to tmp stage
        dir_util.copy_tree(RESOURCES_FOLDER_PATH, TMP_FOLDER_PATH, update=1)

        with open(requirements_file, 'a') as f:
            for row, item in enumerate(packages_list):
                if row < len(packages_list) - 1:
                    f.write("%s\n" % item)
                elif row == len(packages_list) - 1:
                    f.write("%s" % item)

        transformation_config = self._get_configuration(
            stack_id=self.stack_id,
            bucket_id=self.bucket_id,
            transformation_id=self.transformation_id,
            storage_token=self.storage_token)

        query = transformation_config['configuration']['queries'][0]

        python_tr_file = os.path.join(TMP_FOLDER_PATH, 'tr2lambda', 'keboola_python_tr.py')

        with open(python_tr_file, 'w') as f:
            f.write(query)
        return transformation_config['name'], transformation_config['id']

    def prepare_cf_template(self, lambda_pars, aws_pars, tr_name, tr_id):
        with open(os.path.join(TMP_FOLDER_PATH, "template.yaml"), 'r') as input:
            template = cfn_tools.load_yaml(input)
        if lambda_pars.get(KEY_CF_STACK_NAME):
            stack_name = KEY_CF_STACK_NAME
        else:
            nm_norm = tr_name.lower().replace(' ', '-')
            stack_name = '-'.join(['keboola-tr', nm_norm, tr_id])
            lambda_pars[KEY_CF_STACK_NAME] = stack_name

        gtw_props = template['Resources']['KeboolaBasicAWSApiGateway']['Properties']
        gtw_props['Name'] = stack_name

        lmbd_props = template['Resources']['KeboolaTr2Lambda']['Properties']
        # set api keys env
        lmbd_props['Environment']['Variables']['api_keys'] = json.dumps(lambda_pars[KEY_API_KEYS])

        lmbd_props['MemorySize'] = lambda_pars[KEY_MEMORY_AVAILABLE]
        lmbd_props['Events']['Tr2Lambda']['Properties']['Path'] = '/' + str(lambda_pars[KEY_URL_PATH])

        output_pr = template['Outputs']['KeboolaBasicAWSApiGateway']
        output_pr['Value']['Fn::Sub'] = output_pr['Value']['Fn::Sub'] % {'url_path': lambda_pars[KEY_URL_PATH]}

        # set export keys
        stackid = stack_name.replace(' ', '')
        apigtw_export_nm = template['Outputs']['KeboolaBasicAWSApiGatewayRestApiId']['Export']['Name']
        template['Outputs']['KeboolaBasicAWSApiGatewayRestApiId']['Export']['Name'] = apigtw_export_nm.replace(
            'RestApiId', stackid)

        root_res = template['Outputs']['KeboolaBasicAWSApiGatewayyRootResourceId']
        root_res['Export']['Name'] = root_res['Export']['Name'].replace(
            'RootResourceId', stackid)

        with open(os.path.join(TMP_FOLDER_PATH, "template.yaml"), 'w') as tmpl_out:
            tmpl_out.write(cfn_tools.dump_yaml(template))

    def setup_aws_config(self, aws_params):
        # set HOME just in case
        os.environ["HOME"] = str(TMP_FOLDER_PATH)
        cfg__dir_path = os.path.join(TMP_FOLDER_PATH, '.aws')
        os.makedirs(cfg__dir_path, exist_ok=True)
        creds = f"""
        [default]
        aws_access_key_id = {aws_params[KEY_AWS_API_KEY_ID]}
        aws_secret_access_key = {aws_params[KEY_AWS_API_KEY_SECRET]}
        """
        with open(os.path.join(cfg__dir_path, 'credentials'), 'w+') as credentials:
            credentials.write(creds)

        cfg = f"""
        [default]
        region = {aws_params[KEY_AWS_REGION]}
        output = json
        """
        with open(os.path.join(cfg__dir_path, 'config'), 'w+') as config:
            config.write(cfg)

    def build_n_deploy_sam_package(self, lambda_pars, aws_pars):
        os.chdir(TMP_FOLDER_PATH)
        logging.info("Building sam package to S3.")
        # sam build
        process = subprocess.run(['sam', 'build'],
                                 capture_output=True,
                                 universal_newlines=True)
        if process.returncode > 0:
            raise Exception(f"Building sam package failed: {process.stderr}")
        else:
            logging.info(process.stdout)

        logging.info("Packaging and uploading to S3...")
        # sam package
        process = subprocess.run(['sam', 'package', '--output-template-file', 'packaged.yaml', '--s3-bucket',
                                  aws_pars[KEY_AWS_S3_BUCKET]],
                                 capture_output=True,
                                 universal_newlines=True)
        if process.returncode > 0:
            raise Exception(f"Running sam package failed: {process.stderr}")
        else:
            logging.info(process.stdout)

        logging.info("Deploying the CloudFormation stack..")
        # sam deploy
        process = subprocess.run(['sam', 'deploy', '--template-file', 'packaged.yaml', '--stack-name',
                                  lambda_pars[KEY_CF_STACK_NAME], '--capabilities', 'CAPABILITY_IAM'],
                                 capture_output=True,
                                 universal_newlines=True)
        if process.returncode > 0:
            raise Exception(f"Running sam deploy failed: {process.stderr}")
        else:
            logging.info(process.stdout)

    def _get_configuration(self, stack_id, bucket_id, transformation_id, storage_token):

        base_url = 'https://' + stack_id + '/v2/storage/'

        url_configuration = os.path.join(
            base_url, f'components/transformation/configs/{bucket_id}/rows/{transformation_id}')
        header_configuration = {
            'x-storageapi-token': storage_token
        }

        req_configuration = requests.get(
            url_configuration, headers=header_configuration)
        sc_configuration, js_configuration = req_configuration.status_code, req_configuration.json()

        if sc_configuration != 200:

            raise Exception(
                f"Could not obtain configuration of bucket {bucket_id} for transformation id {transformation_id}.")

        else:

            return js_configuration


"""
        Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug = sys.argv[1]
    else:
        debug = False
    try:
        comp = Component()
        comp.run()
    except Exception as e:
        logging.exception(e)
        exit(1)


================================================
File: /LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.5.zip#egg=kbc
mock
freezegun
requests
pyaml
cfn_flip
boto3

================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python3 -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
#          - echo 'Pushing test image to repo. [tag=test]'
#          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#          - docker tag $APP_IMAGE:latest $REPOSITORY:test
#          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python3 -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - echo 'Pushing test image to repo. [tag=test]'
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python3 -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /component_config/component_short_description.md
================================================
This is the short description.


================================================
File: /component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Configuration",
  "required": [
    "source_transformation",
    "aws_parameters",
    "lambda_config"
  ],
  "properties": {
    "source_transformation": {
      "type": "object",
      "title": "Source Transformation",
      "required": [
        "bucket_id",
        "transformation_id",
        "packages"
      ],
      "description": "Source transformation parameters",
      "propertyOrder": 0,
      "properties": {
        "bucket_id": {
          "type": "string",
          "title": "Source transformation bucket ID",
          "description": "Can be found in the transformation URL: https://connection.{region}.keboola.com/admin/projects/{pid}/transformations/bucket/{BUCKET_ID}}/transformation/{TRANSFORMATION_ID}",
          "propertyOrder": 1
        },
        "transformation_id": {
          "type": "string",
          "title": "Source transformation ID",
          "description": "Can be found in the transformation URL: https://connection.{region}.keboola.com/admin/projects/{pid}/transformations/bucket/{BUCKET_ID}}/transformation/{TRANSFORMATION_ID}",
          "propertyOrder": 2
        },
        "packages": {
          "type": "string",
          "title": "Required Python Packages",
          "format": "textarea",
          "options": {
            "input_height": "50px"
          },
          "description": "Comma separated list of required Python packages, not available by default in the Python distribution.",
          "uniqueItems": true,
          "propertyOrder": 3
        }
      }
    },
    "aws_parameters": {
      "type": "object",
      "format": "grid",
      "title": "AWS config",
      "required": [
        "api_key_id",
        "#api_key_secret",
        "s3_bucket",
        "aws_region"
      ],
      "propertyOrder": 30,
      "properties": {
        "api_key_id": {
          "type": "string",
          "title": "AWS API Key ID",
          "options": {
            "grid_columns": "2"
          },
          "propertyOrder": 1
        },
        "#api_key_secret": {
          "type": "string",
          "title": "AWS API Key Secret",
          "options": {
            "grid_columns": "2"
          },
          "format": "password",
          "propertyOrder": 2
        },
        "s3_bucket": {
          "type": "string",
          "title": "AWS S3 bucket name",
          "description": "An existing S3 bucket name that will be used for lambda function package staging.",
          "propertyOrder": 3
        },
        "aws_region": {
          "type": "string",
          "title": "AWS Region",
          "enum": [
            "us-east-1",
            "us-west-1",
            "us-west-2",
            "ap-east-1",
            "ap-south-1",
            "ap-northeast-2",
            "ap-southeast-1",
            "ap-southeast-2",
            "ap-northeast-1",
            "ca-central-1",
            "cn-north-1",
            "cn-northwest-1",
            "eu-central-1",
            "eu-west-1",
            "eu-west-2",
            "eu-west-3",
            "eu-north-1",
            "me-south-1",
            "sa-east-1",
            "us-gov-east-1",
            "us-gov-west-1"
          ],
          "default": "eu-central-1",
          "propertyOrder": 4
        }
      }
    },
    "lambda_config": {
      "type": "object",
      "title": "Lambda function parameters",
      "required": [
        "memory_available",
        "url_path",
        "api_keys"
      ],
      "description": "Parameters of the lambda function",
      "propertyOrder": 40,
      "properties": {
        "memory_available": {
          "type": "number",
          "title": "Available memory (MB)",
          "description": "Available memory of the function. (May affect cost)",
          "default": 128,
          "propertyOrder": 1
        },
        "url_path": {
          "type": "string",
          "title": "Lambda function URL path.",
          "description": "The resulting function will be available at  https://{APP_ID}.execute-api.{REGION}.amazonaws.com/kbc/{URL_PATH}/",
          "default": "execute",
          "propertyOrder": 2
        },
        "api_keys": {
          "type": "array",
          "title": "Function Authorized API Keys",
          "description": "List of API Keys that will be authorized for the endpoint use. The keys are stored in the environment variables of the Lambda function.",
          "items": {
            "format": "grid",
            "type": "object",
            "title": "Key",
            "required": [
              "name",
              "#secret"
            ],
            "properties": {
              "name": {
                "type": "string",
                "title": "Unique name of the key",
                "options": {
                  "grid_columns": "2"
                },
                "propertyOrder": 1000
              },
              "#secret": {
                "type": "string",
                "format": "password",
                "options": {
                  "grid_columns": "2"
                },
                "title": "Secret key",
                "description": "Api key to use in Authorization header. Store the key for reference.",
                "propertyOrder": 2000
              }
            }
          }
        }
      }
    }
  }
}

================================================
File: /component_config/configuration_description.md
================================================
**NOTE:** The resulting API endpoint will be displayed in the **job log**.

![log](https://i.ibb.co/B2jqvWV/log.png)

Example Request:

```bash
curl -X POST \
  https://{APP_ID}.execute-api.{REGION}.amazonaws.com/kbc/{URL_PATH}/ \
  -H 'Accept: */*' \
  -H 'Accept-Encoding: gzip, deflate' \
  -H 'Authorization: 3QoRjpZUsLx7' \
  -H 'Content-Type: application/json' \

  -d '{"table1.csv": 
{"columns":["B Bid","B Ask","P Bid","P Ask"],
"data":[
  [64.4,64.9,81.3,82.3],
  [0,0,0,0]
]},
"table2.csv": {"columns":["start_date","end_date"], "data":[["11/30/2018","12/31/2021"]]}
}
'
```

================================================
File: /component_config/component_long_description.md
================================================
This is the long description.


================================================
File: /component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /README.md
================================================
# KBC rest api deployment tool

Application that allows to "apify" any KBC Python transformation. 

The only limitation on the transformation design is using relative paths for `data/` folder interactions.
 e.g. use `in/tables` rather than `/data/in/tables`.
 
 ## Configuration
 
 ### Source Transformation
 
Source transformation that will be converted to API.
**Parameters**
- Source transformation bucket ID 
    
    Can be found in the transformation URL: https://connection.{region}.keboola.com/admin/projects/{pid}/transformations/bucket/{BUCKET_ID}}/transformation/{TRANSFORMATION_ID}

- Source transformation ID
    
    Can be found in the transformation URL: https://connection.{region}.keboola.com/admin/projects/{pid}/transformations/bucket/{BUCKET_ID}}/transformation/{TRANSFORMATION_ID}

- Required Python Packages

    Comma separated list of required Python packages, not available by default in the Python distribution.

### AWS config

AWS config parameters.
- AWS API Key ID
- AWS API Key Secret
- AWS S3 bucket name

    An existing S3 bucket name that will be used for lambda function package staging.

- AWS Region

### Lambda function parameters

Parameters of the lambda function

- Available memory (MB)
- Lambda function URL path.

    The resulting function will be available at https://{APP_ID}.execute-api.{REGION}.amazonaws.com/kbc/{URL_PATH}/

### Function Authorized API Keys

List of API Keys that will be authorized for the endpoint use. The keys are stored in the environment variables of the Lambda function.

## Development
 
This example contains runnable container with simple unittest. For local testing it is useful to include `data` folder in the root
and use docker-compose commands to run the container or execute tests. 

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path:
```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone https://bitbucket.org:kds_consulting_team/kbc-python-template.git my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/) 

