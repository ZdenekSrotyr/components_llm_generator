Directory structure:
└── kds_consulting_team-kds-team.ex-airtable/
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    ├── change_log.md
    ├── Dockerfile
    ├── flake8.cfg
    ├── src/
    │   ├── transformation.py
    │   └── component.py
    ├── LICENSE.md
    ├── docs/
    │   └── imgs/
    ├── requirements.txt
    ├── bitbucket-pipelines.yml
    ├── component_config/
    │   ├── uiOptions.json
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── stack_parameters.json
    │   ├── configuration_description.md
    │   ├── configRowSchema.json
    │   ├── component_long_description.md
    │   ├── sample-config/
    │   │   ├── in/
    │   │   │   ├── tables/
    │   │   │   │   ├── test.csv
    │   │   │   │   └── test.csv.manifest
    │   │   │   ├── state.json
    │   │   │   └── files/
    │   │   │       └── order1.xml
    │   │   ├── out/
    │   │   │   ├── tables/
    │   │   │   │   └── test.csv
    │   │   │   └── files/
    │   │   │       └── order1.xml
    │   │   └── config.json
    │   ├── loggerConfiguration.json
    │   └── logger
    ├── deploy.sh
    ├── docker-compose.yml
    ├── README_template.md
    ├── scripts/
    │   ├── build_n_test.sh
    │   ├── update_dev_portal_properties.sh
    │   ├── build_n_run.ps1
    │   ├── run.bat
    │   └── run_kbc_tests.ps1
    └── README.md

================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: /change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog


================================================
File: /Dockerfile
================================================
FROM python:3.10-slim
ENV PYTHONIOENCODING utf-8

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY requirements.txt /code/requirements.txt
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]


================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /src/transformation.py
================================================
import json
from dataclasses import dataclass, field
from enum import Enum
from types import NoneType
from typing import Any, Callable, Dict, Optional, Union, Type, List, MutableMapping

import typeguard
from typeguard import TypeCheckError

SUBOBJECT_SEP = "_"
CHILD_TABLE_SEP = "__"
RECORD_ID_FIELD_NAME = "record_id"
ARRAY_OBJECTS_ID_FIELD_NAME = "id"
PARENT_ID_COLUMN_NAME = "parent_id"

ELEMENTARY_TYPE = Union[int, float, str, bool, NoneType]


def is_type(val, type: Type) -> bool:
    try:
        typeguard.check_type(val, type)
    except TypeCheckError:
        return False
    else:
        return True


def flatten_dict(
        dictionary: Dict,
        parent_key: Optional[str] = None,
        separator: str = SUBOBJECT_SEP,
        flatten_lists: bool = False,
):
    items = []
    for key, value in dictionary.items():
        new_key = str(parent_key) + separator + key if parent_key else key
        if isinstance(value, MutableMapping):
            items.extend(flatten_dict(dict(value), new_key, separator).items())
        elif flatten_lists and isinstance(value, list):
            for k, v in enumerate(value):
                items.extend(flatten_dict({str(k): v}, new_key).items())
        else:
            items.append((new_key, value))
    return dict(items)


class ColumnType(Enum):
    ELEMENTARY = ELEMENTARY_TYPE
    OBJECT = Dict
    ARRAY_OF_ELEMENTARY = List[ELEMENTARY_TYPE]
    ARRAY_OF_OBJECTS = List[Dict]

    @classmethod
    def from_example_value(cls, example_value):
        for t in cls:
            if is_type(example_value, t.value):
                return t
        raise ValueError(
            f'Unexpected field data type. Got value of "{example_value}"'
            f' as type "{type(example_value)}".'
        )


@dataclass(slots=True)
class ResultTable:
    name: str
    id_column_names: List[str]
    rows: List[Dict[str, Any]] = field(default_factory=list)
    child_tables: Dict[str, "ResultTable"] = field(default_factory=dict)

    @classmethod
    def from_dicts(
            cls,
            name: str,
            dicts: List[Dict[str, Any]],
            id_column_names: List[str] = [RECORD_ID_FIELD_NAME]
    ):
        if len(dicts) < 1:
            return None
        table = cls(name=name, id_column_names=id_column_names)
        for row_dict in dicts:
            table.add_row(row_dict)
        return table

    def add_row(self, row_dict: Dict[str, Any]):

        def add_value_to_row(column_name: str, value, row_dict: Dict[str, Any]):
            if value is None:
                return
            column_type = ColumnType.from_example_value(value)
            if column_type is ColumnType.ELEMENTARY:
                row_dict[column_name] = value  # no need to do anything
            elif column_type is ColumnType.OBJECT:
                flattened_dict = flatten_dict(value, parent_key=column_name)
                for flattened_key, flattened_value in flattened_dict.items():
                    add_value_to_row(flattened_key, flattened_value, row_dict)
            elif column_type is ColumnType.ARRAY_OF_ELEMENTARY:
                row_dict[column_name] = json.dumps(
                    value
                )  # TODO?: maybe create child table instead?
            elif isinstance(value, list) and len(value) < 1:
                row_dict[column_name] = ""
            elif isinstance(value, list) and isinstance(value[0], dict) and value[0].get("error", None):
                row_dict[column_name] = value[0].get("error")
            elif column_type is ColumnType.ARRAY_OF_OBJECTS:
                child_table_name = f"{self.name}{CHILD_TABLE_SEP}{column_name}"
                child_table = self.child_tables.get(
                    child_table_name,
                    self.__class__(
                        name=child_table_name,
                        id_column_names=[
                            ARRAY_OBJECTS_ID_FIELD_NAME, PARENT_ID_COLUMN_NAME]
                    ),
                )
                self.child_tables[child_table_name] = child_table
                # Add parent id to child table
                for child_dict in value:
                    child_dict: Dict
                    if RECORD_ID_FIELD_NAME in row_dict:
                        child_dict[PARENT_ID_COLUMN_NAME] = row_dict[RECORD_ID_FIELD_NAME]
                    child_table.add_row(child_dict)
            else:
                raise ValueError(f"Invalid column data type: {column_type}.")

        processed_dict = {}
        # first process ID columns
        for id_column in self.id_column_names:
            id_value = row_dict[id_column]
            add_value_to_row(id_column, id_value, processed_dict)
        # next other columns
        for column_name, value in row_dict.items():
            if column_name not in self.id_column_names:
                add_value_to_row(column_name, value, processed_dict)
        self.rows.append(processed_dict)

    def rename_columns(self, rename_function: Callable[[str], str]):
        self.rows = [
            {rename_function(k): v for k, v in row.items()} for row in self.rows
        ]

    def to_dicts(self) -> List[Dict[str, Any]]:
        return self.rows


================================================
File: /src/component.py
================================================
import logging
import os
from typing import Dict, List, Optional
from datetime import datetime, timezone
import dateparser

import pyairtable
import pyairtable.metadata
from keboola.component import ComponentBase
from keboola.component.base import sync_action
from keboola.component.dao import TableDefinition
from keboola.component.exceptions import UserException
from keboola.utils.header_normalizer import DefaultHeaderNormalizer
from pyairtable import Api, Base, retry_strategy, Table as ApiTable
from requests import HTTPError

from keboola.csvwriter import ElasticDictWriter
from transformation import ResultTable, RECORD_ID_FIELD_NAME

# Configuration variables
KEY_API_KEY = "#api_key"
KEY_BASE_ID = "base_id"
KEY_TABLE_NAME = "table_name"
KEY_USE_VIEW = 'use_view'
KEY_VIEW_NAME = "view_name"
KEY_FIELDS = "fields"
KEY_INCREMENTAL_LOAD = "incremental_loading"
KEY_GROUP_DESTINATION = "destination"

# Sync options variables
KEY_SYNC_OPTIONS = "sync_options"
KEY_SYNC_MODE = "sync_mode"
KEY_SYNC_MODE_INCREMENTAL = "incremental_sync"
KEY_SYNC_DATE_FROM = "date_from"
KEY_SYNC_DATE_TO = "date_to"

# State variables
KEY_STATE_LAST_RUN = "last_run"
KEY_TABLES_COLUMNS = "tables_columns"

# list of mandatory parameters => if some is missing,
# component will fail with readable message on initialization.
REQUIRED_PARAMETERS = [KEY_API_KEY, KEY_BASE_ID,
                       KEY_TABLE_NAME]
REQUIRED_IMAGE_PARS = []

RECORD_CREATED_TIME_FIELD_NAME = "record_created_time"

SUB = "_"
HEADER_NORMALIZER = DefaultHeaderNormalizer(forbidden_sub=SUB)


def normalize_name(name: str):
    return HEADER_NORMALIZER.normalize_header([name])[0]


def process_record(record: Dict) -> Dict:
    fields = record["fields"]
    output_record = {
        RECORD_ID_FIELD_NAME: record["id"],
        RECORD_CREATED_TIME_FIELD_NAME: record["createdTime"],
        **fields,
    }
    return output_record


class Component(ComponentBase):
    """
    Extends base class for general Python components. Initializes the CommonInterface
    and performs configuration validation.

    For easier debugging the data folder is picked up by default from `../data` path,
    relative to working directory.

    If `debug` parameter is present in the `config.json`, the default logger is set to verbose DEBUG mode.
    """

    def __init__(self):
        super().__init__()

        self.table_definitions: Dict[str, TableDefinition] = {}
        self.csv_writers: Dict[str, ElasticDictWriter] = {}
        self.tables_columns = dict()
        self.incremental_destination: bool = False
        self.last_run = int()
        self.state = dict()

    def run(self):
        """
        Main execution code
        """
        # Check for missing configuration parameters
        self.validate_configuration_parameters(REQUIRED_PARAMETERS)
        self.validate_image_parameters(REQUIRED_IMAGE_PARS)
        self.state = self.get_state_file()
        self.last_run = self.state.get(KEY_STATE_LAST_RUN, {}) or []
        self.state[KEY_STATE_LAST_RUN] = datetime.now(timezone.utc).strftime(
            "%Y-%m-%d %H:%M:%S")
        self.date_from = self._get_date_from()
        self.date_to = self._get_date_to()
        self.state[KEY_TABLES_COLUMNS] = self.tables_columns = self.state.get(
            KEY_TABLES_COLUMNS, {}
        )

        params: dict = self.configuration.parameters
        # Access parameters in data/config.json
        api_key: str = params[KEY_API_KEY]
        base_id: str = params[KEY_BASE_ID]
        table_id: str = params[KEY_TABLE_NAME]
        view_id: Optional[str] = params.get(KEY_VIEW_NAME)
        fields: Optional[List[str]] = params.get(KEY_FIELDS, None)
        self.incremental_destination: bool = params.get(KEY_GROUP_DESTINATION, {KEY_INCREMENTAL_LOAD: True}) \
            .get(KEY_INCREMENTAL_LOAD)

        api_options = {}
        if self._fetching_is_incremental():
            api_options["formula"] = self._create_filter()
        if fields:
            api_options["fields"] = fields
        if view_id:
            api_options["view"] = view_id

        retry = retry_strategy(status_forcelist=(429, 500, 502, 503, 504), backoff_factor=0.5, total=10)

        try:
            api_table = pyairtable.Table(api_key, base_id, table_id, retry_strategy=retry)
            destination_table_name = self._get_result_table_name(api_table, table_id)

            logging.info(f"Downloading table: {destination_table_name}")
            for i, record_batch in enumerate(api_table.iterate(**api_options)):
                record_batch_processed = [process_record(r) for r in record_batch]
                result_table = ResultTable.from_dicts(destination_table_name, record_batch_processed,
                                                      id_column_names=[RECORD_ID_FIELD_NAME])

                if result_table:
                    self.process_table(result_table, str(i))
                else:
                    logging.warning("The result is empty!")

        except HTTPError as err:
            self._handle_http_error(err)

        self.finalize_all_tables()
        self.write_state_file(self.state)

    def process_table(self, table: ResultTable, slice_name: str):
        table.rename_columns(normalize_name)
        table.name = normalize_name(table.name)

        self.table_definitions[table.name] = table_def = self.table_definitions.get(
            table.name,
            self.create_out_table_definition(
                name=f"{table.name}.csv",
                incremental=self.incremental_destination,
                primary_key=table.id_column_names,
                is_sliced=True,
            ),
        )
        self.csv_writers[table.name] = csv_writer = self.csv_writers.get(
            table.name,
            ElasticDictWriter(
                file_path=f"{table_def.full_path}/{slice_name}.csv",
                fieldnames=self.tables_columns.get(table.name, []),
            ),
        )
        os.makedirs(table_def.full_path, exist_ok=True)

        for row in table.to_dicts():
            try:
                csv_writer.writerow(row)
            except UnicodeEncodeError:
                new_row = self.remove_non_utf8(row)
                csv_writer.writerow(new_row)

        for child_table in table.child_tables.values():
            self.process_table(child_table, slice_name)

    @staticmethod
    def remove_non_utf8(row_dict):
        new_row = {}
        for key, value in row_dict.items():
            if isinstance(value, str):
                original_value = value
                new_value = ''.join(char for char in value if char.isprintable())

                if original_value != new_value:
                    logging.info(f"Removed non-printable characters for key '{key}': '{new_value}'")

                new_row[key] = new_value

        return new_row

    def finalize_all_tables(self):
        for table_name in self.csv_writers:
            csv_writer = self.csv_writers[table_name]
            table_def = self.table_definitions[table_name]
            self.tables_columns[table_name] = table_def.columns = csv_writer.fieldnames
            self.write_manifest(table_def)
            csv_writer.close()

    def _fetching_is_incremental(self) -> bool:
        params = self.configuration.parameters
        loading_options = params.get(KEY_SYNC_OPTIONS, {})
        load_type = loading_options.get(KEY_SYNC_MODE)
        return load_type == "incremental_sync"

    def _get_date_from(self) -> Optional[str]:
        params = self.configuration.parameters
        loading_options = params.get(KEY_SYNC_OPTIONS, {})
        incremental = self._fetching_is_incremental()
        return self._get_parsed_date(loading_options.get(KEY_SYNC_DATE_FROM)) if incremental else None

    def _get_date_to(self) -> Optional[str]:
        params = self.configuration.parameters
        loading_options = params.get(KEY_SYNC_OPTIONS, {})
        incremental = self._fetching_is_incremental()
        return self._get_parsed_date(loading_options.get(KEY_SYNC_DATE_TO)) if incremental else None

    @staticmethod
    def _handle_http_error(error: HTTPError):
        json_message = error.response.json()["error"]

        if error.response.status_code == 401:
            message = 'Request failed. Invalid credentials. Please verify your PAT token and the scopes allowed. ' \
                      f'Detail: {json_message["type"]}, {json_message["message"]}'
        else:
            message = f'Request failed: {json_message["type"]}. Details: {json_message["message"]}'
        raise UserException(message) from error

    def _get_result_table_name(self, api_table: pyairtable.Table, table_name: str) -> str:

        destination_name = self.configuration.parameters.get(KEY_GROUP_DESTINATION, {KEY_TABLE_NAME: ''}).get(
            KEY_TABLE_NAME)

        if not destination_name:
            # see comments in list_fields() why it is necessary to use get_base_schema()
            tables = pyairtable.metadata.get_base_schema(api_table)
            destination_name = next(
                table['name'] for table in tables['tables'] if table['id'] == table_name)
        return destination_name

    def _get_parsed_date(self, date_input: Optional[str]) -> Optional[str]:
        if not date_input:
            parsed_date = None
        elif date_input.lower() in ["last", "last run"] and self.last_run:
            parsed_date = dateparser.parse(self.last_run)
        elif date_input.lower() in ["now", "today"]:
            parsed_date = datetime.now(timezone.utc)
        elif date_input.lower() in ["last", "last run"] and not self.last_run:
            parsed_date = dateparser.parse("1990-01-01")
        else:
            try:
                parsed_date = dateparser.parse(date_input).date()
            except (AttributeError, TypeError) as err:
                raise UserException(
                    f"Cannot parse date input {date_input}") from err
        if parsed_date:
            parsed_date = parsed_date.strftime("%Y-%m-%d %H:%M:%S")
        return parsed_date

    def _create_filter(self) -> str:
        date_from = f"SET_TIMEZONE('{self._get_date_from()}','UTC')"
        date_to = f"SET_TIMEZONE('{self._get_date_to()}','UTC')"
        c_time = "SET_TIMEZONE(CREATED_TIME(),'UTC')"
        l_time = "SET_TIMEZONE(LAST_MODIFIED_TIME(),'UTC')"
        if_not = f"IF(NOT(LAST_MODIFIED_TIME()),{c_time},{l_time})"
        after = f"IS_AFTER({if_not},{date_from})"
        before = f"IS_BEFORE({if_not},{date_to})"
        filter = f"AND({after},{before})"
        return filter

    def _get_table_in_base_schema(self, ):
        params: dict = self.configuration.parameters
        api_key: str = params.get(KEY_API_KEY)
        if not api_key:
            raise UserException('API key or personal token is missing')
        base_id: str = params.get(KEY_BASE_ID)
        if not base_id:
            raise UserException('Base ID is missing')
        table_name: str = params.get(KEY_TABLE_NAME)
        if not table_name:
            raise UserException('ResultTable name is missing')
        table = ApiTable(api_key, base_id, table_name)
        base_schema = pyairtable.metadata.get_base_schema(table)
        table_record = None
        for record in base_schema.get("tables", []):
            if record["id"] == table_name:
                table_record = record
                break
        return table_record

    def _list_table_attributes(self, key):
        table = self._get_table_in_base_schema()
        if not table:
            return []
        attributes = [dict(value=field['id'], label=f"{field['name']} ({field['id']})") for field in
                      table.get(key, [])]
        return attributes

    @sync_action('list_fields')
    def list_fields(self):
        fields = self._list_table_attributes('fields')
        return fields

    @sync_action('list_views')
    def list_views(self):
        views = self._list_table_attributes('views')
        return views

    @sync_action('list_bases')
    def list_bases(self):
        params: dict = self.configuration.parameters
        api_key: str = params.get(KEY_API_KEY)
        if not api_key:
            raise UserException('API key or personal token missing')
        api = Api(api_key)
        bases = pyairtable.metadata.get_api_bases(api)
        resp = [dict(
            value=base['id'], label=f"{base['name']} ({base['id']})") for base in bases['bases']]
        return resp

    @sync_action('testConnection')
    def test_connection(self):
        params: dict = self.configuration.parameters
        api_key: str = params.get(KEY_API_KEY)
        if not api_key:
            raise UserException('API key or personal token missing')
        api = Api(api_key)
        try:
            pyairtable.metadata.get_api_bases(api)
        except Exception as e:
            raise UserException(
                "Login failed! Please check your API Token.") from e

    @sync_action('list_tables')
    def list_tables(self):
        params: dict = self.configuration.parameters
        api_key: str = params.get(KEY_API_KEY)
        if not api_key:
            raise UserException('API key or personal token is missing')
        base_id: str = params.get(KEY_BASE_ID)
        if not base_id:
            raise UserException('Base ID is missing')
        base = Base(api_key, base_id)
        tables = pyairtable.metadata.get_base_schema(base)
        resp = [dict(
            value=table['id'], label=f"{table['name']} ({table['id']})") for table in tables['tables']]
        return resp


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)


================================================
File: /LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /requirements.txt
================================================
mock==5.0.2
freezegun==1.2.2
keboola.component==1.4.3
keboola.http-client==1.0.0
keboola.utils==1.1.0
pyairtable==1.4.0
typeguard==3.0.2
keboola.csvwriter==1.0.1



================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG


  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
            - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:$TEST_TAG
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP 897817378 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /component_config/uiOptions.json
================================================
[
    "genericDockerUI",
    "genericDockerUI-rows"
]

================================================
File: /component_config/component_short_description.md
================================================
Airtable is an easy to use, yet powerful database service that allows you to quickly create, organize and collaborate on
any project.

================================================
File: /component_config/configSchema.json
================================================
{
  "title": "Airtable Credentials",
  "type": "object",
  "required": [
    "#api_key"
  ],
  "properties": {
    "#api_key": {
      "propertyOrder": 20,
      "title": "Personal Access Token (PAT)",
      "type": "string",
      "description": "A valid Airtable PAT token with required read only scopes (`data.records:read`, `schema.bases:read`). Learn more in the <a href=\"https://support.airtable.com/docs/creating-and-using-api-keys-and-access-tokens\">documentation</a<",
      "format": "password"
    },
    "test_connection": {
      "type": "button",
      "format": "test-connection"
    }
  }
}

================================================
File: /component_config/stack_parameters.json
================================================
{}

================================================
File: /component_config/configuration_description.md
================================================
You must know your Airtable API key and ID of the base.

- You can obtain the API key from your [Airtable account overview page](https://airtable.com/account). Since read only access is sufficient, we recommend you create a read only account and provide its API key. For instructions how to do that, see the [this Airtable support article](https://support.airtable.com/hc/en-us/articles/360056249614).
- To obtain the base ID, open your base API documentation from the [Airtable API documentation list](https://airtable.com/api) and find the base ID in your base's API documentation.

You must also know the names of the tables in your Airtable base you want to extract. If you want to use filtering or download just a subset of the given table's fields (columns), you also need to provide the filter [formula](https://support.airtable.com/hc/en-us/articles/203255215-Formula-Field-Reference) and/or the names of the fields (columns) you want to download respectively.

================================================
File: /component_config/configRowSchema.json
================================================
{
  "title": "Row configuration",
  "type": "object",
  "required": [
    "base_id",
    "table_name"
  ],
  "properties": {
    "base_id": {
      "propertyOrder": 10,
      "title": "Base ID",
      "type": "string",
      "enum": [],
      "format": "select",
      "options": {
        "async": {
          "label": "Re-load available base ids",
          "action": "list_bases",
          "autoload": []
        }
      },
      "description": "The ID of the base you want to extract tables from."
    },
    "table_name": {
      "propertyOrder": 20,
      "title": "Table name",
      "type": "string",
      "enum": [],
      "format": "select",
      "options": {
        "async": {
          "label": "Re-load available tables",
          "action": "list_tables",
          "autoload": ["parameters.base_id"]
        }
      },
      "description": "The name of the table in Airtable base you want to download."
    },
    "use_view": {
      "type": "boolean",
      "format": "checkbox",
      "title": "Use View",
      "description": "if checked - extraction is done from the view not the table itself",
      "default": false,
      "propertyOrder": 24
    },
    "view_name": {
      "propertyOrder": 25,
      "title": "View name",
      "type": "string",
      "enum": [],
      "format": "select",
      "options": {
        "async": {
          "label": "Re-load available views",
          "action": "list_views",
          "autoload": ["parameters.table_name"]
        },
        "dependencies": {"use_view": true}
      },
      "description": "The name of the view in Airtable base you want to download."
    },
    "fields": {
      "propertyOrder": 30,
      "title": "Fields",
      "type": "array",
      "description": "The fields you want to download. You may leave this empty to download all fields. Note: in case od use_view even hidden fields will be retrieved.",
      "format": "select",
      "uniqueItems": true,
      "items": {
        "enum": [],
        "type": "string"
      },
      "options": {
        "async": {
          "label": "Re-load available columns",
          "action": "list_fields",
          "autoload": ["parameters.table_name"]
        }
      }
    },
    "sync_options": {
      "type": "object",
      "title": "Sync Options",
      "propertyOrder": 40,
      "properties": {
        "sync_mode": {
          "type": "string",
          "required": true,
          "title": "Sync Mode",
          "enum": [
            "full_sync",
            "incremental_sync"
          ],
          "options": {
            "enum_titles": [
              "Full Sync",
              "Incremental Sync"
            ]
          },
          "default": "full_sync",
          "description": "Full Sync downloads all data from the source every run. Incremental Sync downloads data created or updated in a specified time range by field CREATED_TIME() or LAST_MODIFIED_TIME(), fields are described in <a href='https://support.airtable.com/docs/formula-field-reference'>Airtable - Formula field reference</a>.",
          "propertyOrder": 10
        },
        "date_from": {
          "type": "string",
          "title": "Date From",
          "default": "last run",
          "description": "Date from which data is downloaded. Either date in YYYY-MM-DD format or dateparser string i.e. 5 days ago, 1 month ago, yesterday, etc. You can also set this as last run, which will fetch data from the last run of the component.",
          "propertyOrder": 20,
          "options": {
            "dependencies": {
              "sync_mode": "incremental_sync"
            }
          }
        },
        "date_to": {
          "type": "string",
          "title": "Date to",
          "default": "now",
          "description": "Date to which data is downloaded. Either date in YYYY-MM-DD format or dateparser string i.e. 5 days ago, 1 month ago, now, etc.",
          "propertyOrder": 30,
          "options": {
            "dependencies": {
              "sync_mode": "incremental_sync"
            }
          }
        }
      }
    },
    "destination": {
      "title": "Destination",
      "type": "object",
      "propertyOrder": 70,
      "required": [
        "incremental_loading"
      ],
      "properties": {
        "table_name": {
          "type": "string",
          "title": "Storage Table Name",
          "propertyOrder": 10,
          "description": "Name of the destination table. Source table name will be used if not specified."
        },
        "incremental_loading": {
          "enum": [
            false,
            true
          ],
          "type": "boolean",
          "title": "Load Type",
          "default": false,
          "options": {
            "enum_titles": [
              "Full Load",
              "Incremental Load"
            ]
          },
          "description": "If Full load is used, the destination table will be overwritten every run. If Incremental Load is used, data will be upserted into the destination table.",
          "propertyOrder": 20
        }
      }
    }
  }
}

================================================
File: /component_config/component_long_description.md
================================================
Airtable Data Source allows you to download data from Airtable Bases. It supports custom fields selection and filtering by
formulas.

================================================
File: /component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: /component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}


================================================
File: /component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}

================================================
File: /component_config/logger
================================================
gelf

================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "asigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "asigning travis tag"
	export TAG="$TRAVIS_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /README_template.md
================================================
# KBC Component

Description

**Table of contents:**  
  
[TOC]


# Functionality notes


# Configuration

## Param 1

## Param 2


## Development

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path in the docker-compose file:

```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone repo_path my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/) 

================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
fi

echo "Updating logger settings"

value=`cat component_config/logger`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} logger --value="$value"
else
    echo "logger type is empty!"
fi

echo "Updating logger configuration"
value=`cat component_config/loggerConfiguration.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} loggerConfiguration --value="$value"
else
    echo "loggerConfiguration is empty!"
fi

echo "Updating UI helpers to use"
value=`cat component_config/uiOptions.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} uiOptions --value="$value"
else
    echo "uiOptions is empty!"
fi

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /README.md
================================================
Airtable extractor
==================

This is an extractor that allows you to extract tables from Airtable bases.

**Table of contents:**

[TOC]

<!-- Functionality notes
=================== -->

Prerequisites
=============

Create Airtable PAT token.

- You can create the PAT token in the [Airtable developer hub](https://airtable.com/account). Create read only access for following scopes: `data.records:read` and `schema.bases:read`
- For more information about PAT tokens, see [the documentation](https://support.airtable.com/docs/creating-and-using-api-keys-and-access-tokens).



Configuration
=============

1. [Create a new configuration](https://help.keboola.com/components/#creating-component-configuration) of the Airtable
   datasource.
2. In the authorization section, enter the obtained PAT token. See prerequisites section.
3. Create a new configuration row.

4. Select `Base ID`. To reload available Bases click the `RELOAD AVAILABLE BASE IDS` button.
5. Select `Table` you wish to Sync. To reload available tables for selected Base click the `RELOAD AVAILABLE TABLES`
   button.
6. Optionally, insert a custom `Filter Formula`. For syntax please refer [here](https://support.airtable.com/docs/formula-field-reference).
   1. e.g. `DATETIME_DIFF(NOW(), CREATED_TIME(), 'minutes') < 130`
7. Optionally, select subset of fields you wish to sync. If left empty, all fields are downloaded.
8. Configure `Destination` section
    1. Optionally, set the resulting `Storage Table Name`. If left empty, name of the source table will be automatically
       used.
    2. Select `Load Type`. If Full Load is used, the destination table will be overwritten every run. If Incremental
       load is used, data will be "upserted" into the destination table.



## Configuration parameters
 - Debug (debug) - [OPT] Whether you want to run the configuration in debug mode.
 - API key (#api_key) - [REQ] The API key to authenticate the connection with Airtable.
 - Base ID (base_id) - [REQ] The ID of the base you want to extract tables from.
 - Table name (table_name) - [REQ] The name of the table in Airtable base you want to download.
 - Filter by formula (filter_by_formula) - [OPT] A predicate (expression that evaluates to true or false) [Airtable field formula](https://support.airtable.com/hc/en-us/articles/203255215-Formula-Field-Reference).
 - Fields (fields) - [OPT] The fields you want to download. You may leave this empty to download all fields.
 - Incremental loading (incremental_loading) - [OPT] Whether incremental loading should be used.


Sample Configuration
=============
```json
{
    "parameters": {
        "debug": true,
        "#api_key": "SECRET_VALUE",
        "base_id": "appxDZ88j6DBq80NU",
        "table_name": "Order items",
        "filter_by_formula": "DATETIME_DIFF(NOW(), CREATED_TIME(), 'minutes') < 130",
        "fields": [
            "Order",
            "Product",
            "Amount"
        ],
        "incremental_loading": true
    }
}
```

Output
======
<!-- List of tables, foreign keys, schema. -->
Output for each configuration row will consist of the main table being extracted as well as child tables created from certain fields of the main table.

- If a field's JSON representation consists of a list of objects, it will be omitted from the main table and instead a row for each such object will be created in a child table named `{table_name}__{field_name}`.
- If a field's JSON representation consists of a list of simple values, it will be represented as a JSON string in the output table.
- If a field's JSON representation consists of an object, it will be flattened into its table as columns named `{table_field_name}_{object_key}`.

Development
===========

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path in
the `docker-compose.yml` file:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Clone this repository, init the workspace and run the component with following command:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
docker-compose build
docker-compose run --rm dev
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Run the test suite and lint check using this command:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
docker-compose run --rm test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Integration
===========

For information about deployment and integration with KBC, please refer to the
[deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/)

