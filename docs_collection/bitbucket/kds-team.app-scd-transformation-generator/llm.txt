Directory structure:
└── kds_consulting_team-kds-team.app-scd-transformation-generator/
    ├── README.md
    ├── Dockerfile
    ├── LICENSE.md
    ├── bitbucket-pipelines.yml
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── flake8.cfg
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   ├── files/
    │       │   │   └── order1.xml
    │       │   └── tables/
    │       │       ├── test.csv
    │       │       └── test.csv.manifest
    │       └── out/
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               └── test.csv
    ├── docs/
    │   └── imgs/
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run.bat
    │   ├── run_kbc_tests.ps1
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── kbc_scripts/
    │   │   ├── __init__.py
    │   │   └── kbcapi_scripts.py
    │   └── sql_generator/
    │       ├── __init__.py
    │       └── scd_sql_generators.py
    └── tests/
        ├── __init__.py
        └── test_component.py

================================================
File: README.md
================================================
# Snapshot (SCD) transformation generator

An application that generates transformation code and required initial tables to perform SCD type 2 and 4.

**Table of contents:**  
  
[TOC]

# Configuration


## Source table
The table you wish to snapshot.
   
- `ID`-  e.g. "out.c-test_snapshot.out_table" - full ID of the source table that you want to snapshot
- `Primary Key` - Comma separated list primary key columns of the source table
- `Monitored parameters` - Comma separated list of columns that will be "monitored". This means that these columns 
will be checked for changes and a new (SCD2) snapshot will be created whenever a change is recognized.

## Destination table
The result table where the snapshot will be stored.

- `ID` - Full Storage ID of the table e.g. "in.c-snapshots.test_snapshot_gen" - destination snapshot table

## Snapshot parameters
Configuration parameters affecting how the resulting snapshot is generated.

### SCD Type

Type of the snapshot generated snapshot / slowly changing dimension.

#### SCD Type 2
This is a most common type of slowly changing dimension. It produces a row whenever some of the monitored attributes is 
changed. For more information about type 2 [see here](https://en.wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row).

**Example**:

Consider we want to snapshot the input table with `Add is_deleted flag ` and `Include Time` parameters checked 
and monitoring the `VALUE` column.

The input table looks like this:

![input](docs/imgs/input_table.png)

The SCD Type 2 snapshot first run result will look like this:

![input](docs/imgs/first_result.png)

Now the column `VALUE` of the row ID 1 changes, the next run of the generated snapshot is going to look like this:

![input](docs/imgs/second_result.png)


#### SCD Type 4

This is the simplest type of snapshot, that tracks the state of the records each run (day), regardless the actual changes.

The `Monitored columns` parameter in this case works just a selector of which column should be included in the snapshot.


### Parameters

- **Add is_deleted flag** - If set, `is_deleted` column is included flagging records that had been removed 
from the source.
- **Include Time** - If set, the `snapshot_date` column will include time. Otherwise daily snapshots are captured.
- **Keep deleted active** - If set, deleted items will stay active i.e. `actual=1`
- **Timezone** - important parameter specifying timezone you want to snapshot in. If specified incorrectly, the current 
date generated may be different from what you expect.


## Result Transformation

The application generates Snowflake transformation along with all required input mappings and also creates 
the destination table that is included in the input mapping for feedback.

The code will vary based on the `Snapshot parameters`. The resulting transformation is ready to run / orchestrate 
and the snapshots will be created incrementally as you trigger them.

### Parameters:

- ** Transformation name** - [REQ] The resulting name of the transformation (row)
- ** Transformation bucket** [REQ] Parameters of the resulting transformation bucket.
    - `name` - The resulting bucket name
    - `ID` - The resulting bucket ID. Required only if the `Create new` parameter is set to `No`
    - `Create new` - When set to yes, a new bucket is created. When set to no, the `ID` parameter is required and the 
    resulting transformation will be injected into an existing bucket.
    
Below is example of generated input mapping:

![input](docs/imgs/generated_mapping.png)


# Output structure

The result table looks like this:

![input](docs/imgs/second_result.png)

## Columns
- **snap_pk** - primary key formed from Source table primary key columns and `snapshot_date`
- **source table cols** - any columns selected by `Monitored columns` parameter.
- **start_date** - Date from when the record is active
- **end_date** - Date until when the record were active. If still active the 9999 date is added.
- **snapshot_date** - Snapshot date, available for SCD type 4, instead of `start_date` and `from_date`__
- **actual** - Flag whether it is the latest actual record.
- **is_deleted** - If setup, this flags that the item was deleted from the monitored source table
    


# Development
 
This example contains runnable container with simple unittest. For local testing it is useful to include `data` folder in the root
and use docker-compose commands to run the container or execute tests. 

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path:
```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone https://bitbucket.org:kds_consulting_team/kbc-python-template.git my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/) 


================================================
File: Dockerfile
================================================
FROM python:3.7.2-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential
RUN apt-get install -y git

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
File: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
File: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
#          - echo 'Pushing test image to repo. [tag=test]'
#          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#          - docker tag $APP_IMAGE:latest $REPOSITORY:test
#          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
File: change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog



================================================
File: deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi



================================================
File: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
File: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
File: requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.5.zip#egg=kbc
mock
freezegun
sqlparse
git+https://github.com/keboola/sapi-python-client.git#egg=kbcstorage


================================================
File: component_config/component_long_description.md
================================================
Generate transformation setup and code for SCD type 2 or 4


================================================
File: component_config/component_short_description.md
================================================
Generate transformation setup and code for SCD type 2 or 4 on any input dataset.


================================================
File: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Configuration",
  "required": [
    "source_table",
    "destination_table",
    "scd_parameters",
    "output_tr_bucket"
  ],
  "properties": {
    "source_table": {
      "type": "object",
      "format": "grid",
      "title": "Source table",
      "required": [
        "src_table",
        "monitored_params",
        "primary_key"
      ],
      "description": "Source table that will be snapshot.",
      "propertyOrder": 30,
      "properties": {
        "src_table": {
          "type": "string",
          "title": "ID",
          "options": {
        "grid_columns": "2"
          },
          "propertyOrder": 1
        },
        "monitored_params": {
          "type": "string",
          "title": "Monitored parameters",
          "format": "textarea",
          "options": {
            "input_height": "50px"
          },
          "description": "Comma separated list of columns that will be monitored, e.g. changes in these columns will be considered for snapshotting.",
          "uniqueItems": true,
          "propertyOrder": 3
        },
        "primary_key": {
          "type": "string",
          "title": "Primary key",
          "format": "textarea",
          "options": {
            "input_height": "50px"
          },
          "description": "Comma separated list of columns that form a unique key of the source table.",
          "uniqueItems": true,
          "propertyOrder": 2
        }
      }
    },
    "destination_table": {
      "type": "object",
      "title": "Destination table",
      "required": [
        "dst_table_id"
      ],
      "description": "Destination table where the snapshot will be stored. It must not exist, otherwise an exception is thrown.",
      "propertyOrder": 40,
      "properties": {
        "dst_table_id": {
          "type": "string",
          "title": "ID",
          "propertyOrder": 400
        }
      }
    },
    "scd_parameters": {
      "type": "object",
      "format": "grid",
      "title": "Snapshot parameters",
      "required": [
        "scd_type",
        "timezone",
        "use_datetime",
        "keep_del_active",
        "deleted_flag"
      ],
      "description": "Parameters of generated snapshots / slowly changing dimension",
      "propertyOrder": 50,
      "properties": {
        "scd_type": {
          "enum": [
            "scd2",
            "scd4"
          ],
          "options": {
            "enum_titles": [
              "SCD Type 2",
              "SCD Type 4"
            ],
            "grid_columns": "2"
          },
          "type": "string",
          "title": "SCD Type",
          "default": "SCD Type 2",
          "propertyOrder": 20
        },
        "timezone": {
          "type": "string",
          "title": "Timezone",
          "enum": [
            "Africa/Abidjan",
            "Africa/Accra",
            "Africa/Addis_Ababa",
            "Africa/Algiers",
            "Africa/Asmara",
            "Africa/Asmera",
            "Africa/Bamako",
            "Africa/Bangui",
            "Africa/Banjul",
            "Africa/Bissau",
            "Africa/Blantyre",
            "Africa/Brazzaville",
            "Africa/Bujumbura",
            "Africa/Cairo",
            "Africa/Casablanca",
            "Africa/Ceuta",
            "Africa/Conakry",
            "Africa/Dakar",
            "Africa/Dar_es_Salaam",
            "Africa/Djibouti",
            "Africa/Douala",
            "Africa/El_Aaiun",
            "Africa/Freetown",
            "Africa/Gaborone",
            "Africa/Harare",
            "Africa/Johannesburg",
            "Africa/Juba",
            "Africa/Kampala",
            "Africa/Khartoum",
            "Africa/Kigali",
            "Africa/Kinshasa",
            "Africa/Lagos",
            "Africa/Libreville",
            "Africa/Lome",
            "Africa/Luanda",
            "Africa/Lubumbashi",
            "Africa/Lusaka",
            "Africa/Malabo",
            "Africa/Maputo",
            "Africa/Maseru",
            "Africa/Mbabane",
            "Africa/Mogadishu",
            "Africa/Monrovia",
            "Africa/Nairobi",
            "Africa/Ndjamena",
            "Africa/Niamey",
            "Africa/Nouakchott",
            "Africa/Ouagadougou",
            "Africa/Porto-Novo",
            "Africa/Sao_Tome",
            "Africa/Timbuktu",
            "Africa/Tripoli",
            "Africa/Tunis",
            "Africa/Windhoek",
            "America/Adak",
            "America/Anchorage",
            "America/Anguilla",
            "America/Antigua",
            "America/Araguaina",
            "America/Argentina/Buenos_Aires",
            "America/Argentina/Catamarca",
            "America/Argentina/ComodRivadavia",
            "America/Argentina/Cordoba",
            "America/Argentina/Jujuy",
            "America/Argentina/La_Rioja",
            "America/Argentina/Mendoza",
            "America/Argentina/Rio_Gallegos",
            "America/Argentina/Salta",
            "America/Argentina/San_Juan",
            "America/Argentina/San_Luis",
            "America/Argentina/Tucuman",
            "America/Argentina/Ushuaia",
            "America/Aruba",
            "America/Asuncion",
            "America/Atikokan",
            "America/Atka",
            "America/Bahia",
            "America/Bahia_Banderas",
            "America/Barbados",
            "America/Belem",
            "America/Belize",
            "America/Blanc-Sablon",
            "America/Boa_Vista",
            "America/Bogota",
            "America/Boise",
            "America/Buenos_Aires",
            "America/Cambridge_Bay",
            "America/Campo_Grande",
            "America/Cancun",
            "America/Caracas",
            "America/Catamarca",
            "America/Cayenne",
            "America/Cayman",
            "America/Chicago",
            "America/Chihuahua",
            "America/Coral_Harbour",
            "America/Cordoba",
            "America/Costa_Rica",
            "America/Creston",
            "America/Cuiaba",
            "America/Curacao",
            "America/Danmarkshavn",
            "America/Dawson",
            "America/Dawson_Creek",
            "America/Denver",
            "America/Detroit",
            "America/Dominica",
            "America/Edmonton",
            "America/Eirunepe",
            "America/El_Salvador",
            "America/Ensenada",
            "America/Fort_Wayne",
            "America/Fortaleza",
            "America/Glace_Bay",
            "America/Godthab",
            "America/Goose_Bay",
            "America/Grand_Turk",
            "America/Grenada",
            "America/Guadeloupe",
            "America/Guatemala",
            "America/Guayaquil",
            "America/Guyana",
            "America/Halifax",
            "America/Havana",
            "America/Hermosillo",
            "America/Indiana/Indianapolis",
            "America/Indiana/Knox",
            "America/Indiana/Marengo",
            "America/Indiana/Petersburg",
            "America/Indiana/Tell_City",
            "America/Indiana/Vevay",
            "America/Indiana/Vincennes",
            "America/Indiana/Winamac",
            "America/Indianapolis",
            "America/Inuvik",
            "America/Iqaluit",
            "America/Jamaica",
            "America/Jujuy",
            "America/Juneau",
            "America/Kentucky/Louisville",
            "America/Kentucky/Monticello",
            "America/Knox_IN",
            "America/Kralendijk",
            "America/La_Paz",
            "America/Lima",
            "America/Los_Angeles",
            "America/Louisville",
            "America/Lower_Princes",
            "America/Maceio",
            "America/Managua",
            "America/Manaus",
            "America/Marigot",
            "America/Martinique",
            "America/Matamoros",
            "America/Mazatlan",
            "America/Mendoza",
            "America/Menominee",
            "America/Merida",
            "America/Metlakatla",
            "America/Mexico_City",
            "America/Miquelon",
            "America/Moncton",
            "America/Monterrey",
            "America/Montevideo",
            "America/Montreal",
            "America/Montserrat",
            "America/Nassau",
            "America/New_York",
            "America/Nipigon",
            "America/Nome",
            "America/Noronha",
            "America/North_Dakota/Beulah",
            "America/North_Dakota/Center",
            "America/North_Dakota/New_Salem",
            "America/Ojinaga",
            "America/Panama",
            "America/Pangnirtung",
            "America/Paramaribo",
            "America/Phoenix",
            "America/Port-au-Prince",
            "America/Port_of_Spain",
            "America/Porto_Acre",
            "America/Porto_Velho",
            "America/Puerto_Rico",
            "America/Rainy_River",
            "America/Rankin_Inlet",
            "America/Recife",
            "America/Regina",
            "America/Resolute",
            "America/Rio_Branco",
            "America/Rosario",
            "America/Santa_Isabel",
            "America/Santarem",
            "America/Santiago",
            "America/Santo_Domingo",
            "America/Sao_Paulo",
            "America/Scoresbysund",
            "America/Shiprock",
            "America/Sitka",
            "America/St_Barthelemy",
            "America/St_Johns",
            "America/St_Kitts",
            "America/St_Lucia",
            "America/St_Thomas",
            "America/St_Vincent",
            "America/Swift_Current",
            "America/Tegucigalpa",
            "America/Thule",
            "America/Thunder_Bay",
            "America/Tijuana",
            "America/Toronto",
            "America/Tortola",
            "America/Vancouver",
            "America/Virgin",
            "America/Whitehorse",
            "America/Winnipeg",
            "America/Yakutat",
            "America/Yellowknife",
            "Antarctica/Casey",
            "Antarctica/Davis",
            "Antarctica/DumontDUrville",
            "Antarctica/Macquarie",
            "Antarctica/Mawson",
            "Antarctica/McMurdo",
            "Antarctica/Palmer",
            "Antarctica/Rothera",
            "Antarctica/South_Pole",
            "Antarctica/Syowa",
            "Antarctica/Troll",
            "Antarctica/Vostok",
            "Arctic/Longyearbyen",
            "Asia/Aden",
            "Asia/Almaty",
            "Asia/Amman",
            "Asia/Anadyr",
            "Asia/Aqtau",
            "Asia/Aqtobe",
            "Asia/Ashgabat",
            "Asia/Ashkhabad",
            "Asia/Baghdad",
            "Asia/Bahrain",
            "Asia/Baku",
            "Asia/Bangkok",
            "Asia/Beirut",
            "Asia/Bishkek",
            "Asia/Brunei",
            "Asia/Calcutta",
            "Asia/Chita",
            "Asia/Choibalsan",
            "Asia/Chongqing",
            "Asia/Chungking",
            "Asia/Colombo",
            "Asia/Dacca",
            "Asia/Damascus",
            "Asia/Dhaka",
            "Asia/Dili",
            "Asia/Dubai",
            "Asia/Dushanbe",
            "Asia/Gaza",
            "Asia/Harbin",
            "Asia/Hebron",
            "Asia/Ho_Chi_Minh",
            "Asia/Hong_Kong",
            "Asia/Hovd",
            "Asia/Irkutsk",
            "Asia/Istanbul",
            "Asia/Jakarta",
            "Asia/Jayapura",
            "Asia/Jerusalem",
            "Asia/Kabul",
            "Asia/Kamchatka",
            "Asia/Karachi",
            "Asia/Kashgar",
            "Asia/Kathmandu",
            "Asia/Katmandu",
            "Asia/Khandyga",
            "Asia/Kolkata",
            "Asia/Krasnoyarsk",
            "Asia/Kuala_Lumpur",
            "Asia/Kuching",
            "Asia/Kuwait",
            "Asia/Macao",
            "Asia/Macau",
            "Asia/Magadan",
            "Asia/Makassar",
            "Asia/Manila",
            "Asia/Muscat",
            "Asia/Nicosia",
            "Asia/Novokuznetsk",
            "Asia/Novosibirsk",
            "Asia/Omsk",
            "Asia/Oral",
            "Asia/Phnom_Penh",
            "Asia/Pontianak",
            "Asia/Pyongyang",
            "Asia/Qatar",
            "Asia/Qyzylorda",
            "Asia/Rangoon",
            "Asia/Riyadh",
            "Asia/Saigon",
            "Asia/Sakhalin",
            "Asia/Samarkand",
            "Asia/Seoul",
            "Asia/Shanghai",
            "Asia/Singapore",
            "Asia/Srednekolymsk",
            "Asia/Taipei",
            "Asia/Tashkent",
            "Asia/Tbilisi",
            "Asia/Tehran",
            "Asia/Tel_Aviv",
            "Asia/Thimbu",
            "Asia/Thimphu",
            "Asia/Tokyo",
            "Asia/Ujung_Pandang",
            "Asia/Ulaanbaatar",
            "Asia/Ulan_Bator",
            "Asia/Urumqi",
            "Asia/Ust-Nera",
            "Asia/Vientiane",
            "Asia/Vladivostok",
            "Asia/Yakutsk",
            "Asia/Yekaterinburg",
            "Asia/Yerevan",
            "Atlantic/Azores",
            "Atlantic/Bermuda",
            "Atlantic/Canary",
            "Atlantic/Cape_Verde",
            "Atlantic/Faeroe",
            "Atlantic/Faroe",
            "Atlantic/Jan_Mayen",
            "Atlantic/Madeira",
            "Atlantic/Reykjavik",
            "Atlantic/South_Georgia",
            "Atlantic/St_Helena",
            "Atlantic/Stanley",
            "Australia/ACT",
            "Australia/Adelaide",
            "Australia/Brisbane",
            "Australia/Broken_Hill",
            "Australia/Canberra",
            "Australia/Currie",
            "Australia/Darwin",
            "Australia/Eucla",
            "Australia/Hobart",
            "Australia/LHI",
            "Australia/Lindeman",
            "Australia/Lord_Howe",
            "Australia/Melbourne",
            "Australia/NSW",
            "Australia/North",
            "Australia/Perth",
            "Australia/Queensland",
            "Australia/South",
            "Australia/Sydney",
            "Australia/Tasmania",
            "Australia/Victoria",
            "Australia/West",
            "Australia/Yancowinna",
            "Brazil/Acre",
            "Brazil/DeNoronha",
            "Brazil/East",
            "Brazil/West",
            "CET",
            "CST6CDT",
            "Canada/Atlantic",
            "Canada/Central",
            "Canada/East-Saskatchewan",
            "Canada/Eastern",
            "Canada/Mountain",
            "Canada/Newfoundland",
            "Canada/Pacific",
            "Canada/Saskatchewan",
            "Canada/Yukon",
            "Chile/Continental",
            "Chile/EasterIsland",
            "Cuba",
            "EET",
            "EST5EDT",
            "Egypt",
            "Eire",
            "Etc/GMT",
            "Etc/GMT+0",
            "Etc/GMT+1",
            "Etc/GMT+10",
            "Etc/GMT+11",
            "Etc/GMT+12",
            "Etc/GMT+2",
            "Etc/GMT+3",
            "Etc/GMT+4",
            "Etc/GMT+5",
            "Etc/GMT+6",
            "Etc/GMT+7",
            "Etc/GMT+8",
            "Etc/GMT+9",
            "Etc/GMT-0",
            "Etc/GMT-1",
            "Etc/GMT-10",
            "Etc/GMT-11",
            "Etc/GMT-12",
            "Etc/GMT-13",
            "Etc/GMT-14",
            "Etc/GMT-2",
            "Etc/GMT-3",
            "Etc/GMT-4",
            "Etc/GMT-5",
            "Etc/GMT-6",
            "Etc/GMT-7",
            "Etc/GMT-8",
            "Etc/GMT-9",
            "Etc/GMT0",
            "Etc/Greenwich",
            "Etc/UCT",
            "Etc/UTC",
            "Etc/Universal",
            "Etc/Zulu",
            "Europe/Amsterdam",
            "Europe/Andorra",
            "Europe/Athens",
            "Europe/Belfast",
            "Europe/Belgrade",
            "Europe/Berlin",
            "Europe/Bratislava",
            "Europe/Brussels",
            "Europe/Bucharest",
            "Europe/Budapest",
            "Europe/Busingen",
            "Europe/Chisinau",
            "Europe/Copenhagen",
            "Europe/Dublin",
            "Europe/Gibraltar",
            "Europe/Guernsey",
            "Europe/Helsinki",
            "Europe/Isle_of_Man",
            "Europe/Istanbul",
            "Europe/Jersey",
            "Europe/Kaliningrad",
            "Europe/Kiev",
            "Europe/Lisbon",
            "Europe/Ljubljana",
            "Europe/London",
            "Europe/Luxembourg",
            "Europe/Madrid",
            "Europe/Malta",
            "Europe/Mariehamn",
            "Europe/Minsk",
            "Europe/Monaco",
            "Europe/Moscow",
            "Europe/Nicosia",
            "Europe/Oslo",
            "Europe/Paris",
            "Europe/Podgorica",
            "Europe/Prague",
            "Europe/Riga",
            "Europe/Rome",
            "Europe/Samara",
            "Europe/San_Marino",
            "Europe/Sarajevo",
            "Europe/Simferopol",
            "Europe/Skopje",
            "Europe/Sofia",
            "Europe/Stockholm",
            "Europe/Tallinn",
            "Europe/Tirane",
            "Europe/Tiraspol",
            "Europe/Uzhgorod",
            "Europe/Vaduz",
            "Europe/Vatican",
            "Europe/Vienna",
            "Europe/Vilnius",
            "Europe/Volgograd",
            "Europe/Warsaw",
            "Europe/Zagreb",
            "Europe/Zaporozhye",
            "Europe/Zurich",
            "GB",
            "GB-Eire",
            "GMT",
            "GMT0",
            "Greenwich",
            "Hongkong",
            "Iceland",
            "Indian/Antananarivo",
            "Indian/Chagos",
            "Indian/Christmas",
            "Indian/Cocos",
            "Indian/Comoro",
            "Indian/Kerguelen",
            "Indian/Mahe",
            "Indian/Maldives",
            "Indian/Mauritius",
            "Indian/Mayotte",
            "Indian/Reunion",
            "Iran",
            "Israel",
            "Jamaica",
            "Japan",
            "Kwajalein",
            "Libya",
            "MET",
            "MST7MDT",
            "Mexico/BajaNorte",
            "Mexico/BajaSur",
            "Mexico/General",
            "NZ",
            "NZ-CHAT",
            "Navajo",
            "PRC",
            "PST8PDT",
            "Pacific/Apia",
            "Pacific/Auckland",
            "Pacific/Bougainville",
            "Pacific/Chatham",
            "Pacific/Chuuk",
            "Pacific/Easter",
            "Pacific/Efate",
            "Pacific/Enderbury",
            "Pacific/Fakaofo",
            "Pacific/Fiji",
            "Pacific/Funafuti",
            "Pacific/Galapagos",
            "Pacific/Gambier",
            "Pacific/Guadalcanal",
            "Pacific/Guam",
            "Pacific/Honolulu",
            "Pacific/Johnston",
            "Pacific/Kiritimati",
            "Pacific/Kosrae",
            "Pacific/Kwajalein",
            "Pacific/Majuro",
            "Pacific/Marquesas",
            "Pacific/Midway",
            "Pacific/Nauru",
            "Pacific/Niue",
            "Pacific/Norfolk",
            "Pacific/Noumea",
            "Pacific/Pago_Pago",
            "Pacific/Palau",
            "Pacific/Pitcairn",
            "Pacific/Pohnpei",
            "Pacific/Ponape",
            "Pacific/Port_Moresby",
            "Pacific/Rarotonga",
            "Pacific/Saipan",
            "Pacific/Samoa",
            "Pacific/Tahiti",
            "Pacific/Tarawa",
            "Pacific/Tongatapu",
            "Pacific/Truk",
            "Pacific/Wake",
            "Pacific/Wallis",
            "Pacific/Yap",
            "Poland",
            "Portugal",
            "ROK",
            "Singapore",
            "SystemV/AST4",
            "SystemV/AST4ADT",
            "SystemV/CST6",
            "SystemV/CST6CDT",
            "SystemV/EST5",
            "SystemV/EST5EDT",
            "SystemV/HST10",
            "SystemV/MST7",
            "SystemV/MST7MDT",
            "SystemV/PST8",
            "SystemV/PST8PDT",
            "SystemV/YST9",
            "SystemV/YST9YDT",
            "Turkey",
            "UCT",
            "US/Alaska",
            "US/Aleutian",
            "US/Arizona",
            "US/Central",
            "US/East-Indiana",
            "US/Eastern",
            "US/Hawaii",
            "US/Indiana-Starke",
            "US/Michigan",
            "US/Mountain",
            "US/Pacific",
            "US/Pacific-New",
            "US/Samoa",
            "UTC",
            "Universal",
            "W-SU",
            "WET",
            "Zulu",
            "EST",
            "HST",
            "MST",
            "ACT",
            "AET",
            "AGT",
            "ART",
            "AST",
            "BET",
            "BST",
            "CAT",
            "CNT",
            "CST",
            "CTT",
            "EAT",
            "ECT",
            "IET",
            "IST",
            "JST",
            "MIT",
            "NET",
            "NST",
            "PLT",
            "PNT",
            "PRT",
            "PST",
            "SST",
            "VST"
          ],
          "default": "Europe/Prague",
          "options": {
            "grid_columns": "2"
          },
          "propertyOrder": 30
        },
        "use_datetime": {
          "title": "Include Time",
          "type": "boolean",
          "format": "checkbox"
        },
        "keep_del_active": {
          "title": "Keep deleted active",
          "type": "boolean",
          "format": "checkbox"
        },
        "deleted_flag": {
          "title": "Add is_deleted flag",
          "type": "boolean",
          "format": "checkbox"
        }
      }
    },
    "output_tr_bucket": {
      "type": "object",
      "title": "Result Transformation",
      "required": [
        "dst_tr_name",
        "tr_bucket"
      ],
      "description": "Output transformation that will be generated.",
      "propertyOrder": 400,
      "properties": {
        "dst_tr_name": {
          "type": "string",
          "title": "Transformation name",
          "propertyOrder": 1
        },
        "tr_bucket": {
          "type": "object",
          "title": "Transformation bucket",
          "required": [
            "name",
            "id",
            "tr_bucket_override"
          ],
          "propertyOrder": 400,
          "properties": {
            "name": {
              "type": "string",
              "title": "Name",
              "propertyOrder": 1
            },
            "id": {
              "type": "string",
              "title": "ID",
              "description": "Required when using existing bucket",
              "propertyOrder": 2
            },
            "tr_bucket_override": {
              "title": "Create new",
              "description": "Create new bucket or use existing",
              "type": "number",
              "enum": [
                1,
                0
              ],
              "options": {
                "enum_titles": [
                  "Yes",
                  "No"
                ]
              },
              "propertyOrder": 2
            }
          }
        }
      }
    }
  }
}


================================================
File: component_config/configuration_description.md
================================================



================================================
File: component_config/stack_parameters.json
================================================
{}


================================================
File: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#storage_token": "1124-179922-xxxx",
    "region": "US",
    "src_table": "out.c-test_snapshot.out_table",
    "dst_table": "in.c-snapshots.test_snapshot_gen",
    "monitored_params": "VALUE, NOT_MONITORED",
    "primary_key": "ID",
    "tr_bucket": {
      "tr_bucket_override": true,
      "name": "test-snapshot"
    },
    "dst_tr_name": "Generated snapshots",
    "scd_type": "scd2",
    "use_datetime": true,
    "timezone": "Europe/Prague",
    "keep_del_active": false,
    "deleted_flag": false,
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}



================================================
File: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
File: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
File: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
File: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
File: component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
File: component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"




================================================
File: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
File: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
File: scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag


================================================
File: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
File: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
File: src/component.py
================================================
'''
Template Component main class.

'''

import logging
import os
import sys

import requests
from kbc.env_handler import KBCEnvHandler
from kbcstorage import tables

from kbc_scripts import kbcapi_scripts
from sql_generator import scd_sql_generators

# global constants'

# configuration variables
KEY_STORAGE_TOKEN = '#storage_token'
KEY_REGION = 'region'

KEY_SRC_TABLE = 'source_table'
KEY_DST_TABLE = 'destination_table'
KEY_SCD_PARAMETERS = 'scd_parameters'
KEY_OUTPUT_TR_BUCKET = 'output_tr_bucket'

KEY_SRC_TABLE_ID = 'src_table'
KEY_DST_TABLE_ID = 'dst_table_id'

KEY_MONITORED_PARAMS = 'monitored_params'
KEY_PRIMARY_KEY = 'primary_key'
KEY_KEEP_UNMONITORED = 'keep_unmonitored'

KEY_DST_BUCKET = 'tr_bucket'
KEY_BUCKET_OVERRIDE = 'tr_bucket_override'
KEY_TR_NAME = 'dst_tr_name'

KEY_USE_DATETIME = 'use_datetime'
KEY_SCD_TYPE = 'scd_type'
KEY_TIMEZONE = 'timezone'
KEY_KEEP_DELETED_ACTIVE = 'keep_del_active'
DELETED_FLAG = 'deleted_flag'

SUPPORTED_SCD_TYPES = ['scd2', 'scd4']

URL_REGION = {"connection.keboola.com": "US",
              "connection.eu-central-1.keboola.com": "EU"}

# #### Keep for debug
KEY_DEBUG = 'debug'
MANDATORY_PARS = [KEY_SRC_TABLE, KEY_DST_TABLE, KEY_SCD_PARAMETERS, KEY_OUTPUT_TR_BUCKET]
MANDATORY_IMAGE_PARS = []

APP_VERSION = '0.0.1'


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS)
        # override debug from config
        if self.cfg_params.get(KEY_DEBUG):
            debug = True

        log_level = logging.DEBUG if debug else logging.INFO
        if not os.getenv('KBC_LOGGER_PORT', None):
            # for debug purposes
            self.set_default_logger(log_level)
        else:
            self.set_gelf_logger(log_level)
        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        try:
            self.validate_config()
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as e:
            logging.exception(e)
            exit(1)

        # init variables
        self.storage_token = self.get_storage_token()
        self.stack_id = os.environ["KBC_STACKID"]

        # fill defaults
        self.cfg_params[KEY_SCD_PARAMETERS][KEY_KEEP_UNMONITORED] = self.cfg_params.get(KEY_SCD_PARAMETERS, {}).get(
            KEY_KEEP_UNMONITORED, False)
        self.cfg_params[KEY_SCD_PARAMETERS][KEY_KEEP_DELETED_ACTIVE] = self.cfg_params.get(KEY_SCD_PARAMETERS, {}).get(
            KEY_KEEP_DELETED_ACTIVE, False)
        self.cfg_params[KEY_SCD_PARAMETERS][DELETED_FLAG] = self.cfg_params.get(KEY_SCD_PARAMETERS, {}).get(
            DELETED_FLAG, False)
        self.cfg_params[KEY_SCD_PARAMETERS][KEY_USE_DATETIME] = self.cfg_params.get(KEY_SCD_PARAMETERS, {}).get(
            KEY_USE_DATETIME, False)

    def run(self):
        '''
        Main execution code
        '''
        params = self.cfg_params  # noqa
        self.validate_cfg()
        scd_type = params[KEY_SCD_PARAMETERS][KEY_SCD_TYPE]
        src_table = params[KEY_SRC_TABLE][KEY_SRC_TABLE_ID].strip()
        table_details = ''
        try:
            table_details = self._get_storage_table_detail(src_table, self.storage_token, self.stack_id)
        except Exception as ex:
            logging.error(F'Failed to get the specified storage table "{src_table}"')
            logging.exception(ex)
            exit(1)
        logging.info("Validating configuration..")
        self.validate_output_objects()

        monitored_columns = self._parse_comma_sep_params(params[KEY_SRC_TABLE][KEY_MONITORED_PARAMS])
        primary_key_cols = self._parse_comma_sep_params(params[KEY_SRC_TABLE][KEY_PRIMARY_KEY])

        self.validate_columns(table_details, monitored_columns)
        self.validate_columns(table_details, primary_key_cols)

        conf_tr_bucket = params[KEY_OUTPUT_TR_BUCKET][KEY_DST_BUCKET]

        dst_table = None
        logging.info(F"Creating destination table {params[KEY_DST_TABLE][KEY_DST_TABLE_ID]}")
        try:
            dst_table = self._create_dst_table(params, [scd_sql_generators.COL_SNAP_PK],
                                               monitored_columns + primary_key_cols, scd_type)
        except Exception as ex:
            logging.error(F'Failed to create the destination table "{params[KEY_DST_TABLE][KEY_DST_TABLE_ID]}"')
            logging.exception(ex)

        dest_bucket = None

        if conf_tr_bucket[KEY_BUCKET_OVERRIDE]:
            logging.info(F'Creating trnasformation bucket {conf_tr_bucket["name"]}')
            dest_bucket = self.create_tr_bucket(conf_tr_bucket)
        else:
            dest_bucket = self.get_tr_bucket(conf_tr_bucket)

        if scd_type == 'scd2':
            logging.info("Generating SCD type 2 transformation SQL code.")
            sql_queries = scd_sql_generators.generate_scd2_code(monitored_columns, primary_key_cols,
                                                                params[KEY_SCD_PARAMETERS][KEY_KEEP_UNMONITORED],
                                                                params[KEY_SCD_PARAMETERS][KEY_KEEP_DELETED_ACTIVE],
                                                                params[KEY_SCD_PARAMETERS][DELETED_FLAG],
                                                                params[KEY_SCD_PARAMETERS][KEY_TIMEZONE],
                                                                params[KEY_SCD_PARAMETERS][KEY_USE_DATETIME])
            new_tr = self.create_transformation_row(sql_queries, table_details['id'], dst_table, dest_bucket['id'],
                                                    [scd_sql_generators.COL_SNAP_PK])
        elif scd_type == 'scd4':
            logging.info("Generating SCD type 4 transformation SQL code.")
            sql_queries = scd_sql_generators.generate_scd4_code(monitored_columns, primary_key_cols,
                                                                params[KEY_SCD_PARAMETERS][KEY_KEEP_UNMONITORED],
                                                                params[KEY_SCD_PARAMETERS][KEY_KEEP_DELETED_ACTIVE],
                                                                params[KEY_SCD_PARAMETERS][DELETED_FLAG],
                                                                params[KEY_SCD_PARAMETERS][KEY_TIMEZONE],
                                                                params[KEY_SCD_PARAMETERS][KEY_USE_DATETIME])
            new_tr = self.create_transformation_row(sql_queries, table_details['id'], dst_table, dest_bucket['id'],
                                                    [scd_sql_generators.COL_SNAP_PK])

        logging.info(
            f'Transformation created sucessfully! The new configuration named "{new_tr["name"]}" '
            f'is available in transformation bucket name: "{dest_bucket["name"]}" ID {dest_bucket["id"]}')

    def validate_cfg(self):
        validation_errors = ''
        try:
            self.validate_parameters(self.cfg_params[KEY_SRC_TABLE],
                                     [KEY_PRIMARY_KEY, KEY_MONITORED_PARAMS, KEY_SRC_TABLE_ID], 'Source table')
        except ValueError as e:
            validation_errors += str(e)

        try:
            self.validate_parameters(self.cfg_params[KEY_DST_TABLE], [KEY_DST_TABLE_ID], 'Destination table')
        except ValueError as e:
            validation_errors += str(e)

        try:
            self.validate_parameters(self.cfg_params[KEY_OUTPUT_TR_BUCKET], [KEY_TR_NAME], 'Transformation bucket')
        except ValueError as e:
            validation_errors += str(e)

        if not self.cfg_params[KEY_OUTPUT_TR_BUCKET][KEY_DST_BUCKET].get(KEY_BUCKET_OVERRIDE) \
                and not self.cfg_params[KEY_OUTPUT_TR_BUCKET][KEY_DST_BUCKET].get('id'):
            validation_errors += '\nYou must specify transformation bucket ID if you wish to reuse the bucket!'

        if self.cfg_params[KEY_OUTPUT_TR_BUCKET].get(KEY_BUCKET_OVERRIDE) \
                and not self.cfg_params[KEY_OUTPUT_TR_BUCKET].get('name'):
            validation_errors += '\nYou must specify transformation bucket name if you wish to create new one!'

        if validation_errors:
            raise ValueError(validation_errors)

    def create_transformation_row(self, queries, input_table_id, dst_table_id, dst_config_id, pkey_cols):
        row_cfg = self._build_tr_config(queries, dst_table_id, input_table_id,
                                        self.cfg_params[KEY_OUTPUT_TR_BUCKET][KEY_TR_NAME],
                                        pkey_cols)
        return kbcapi_scripts._create_config_row(self.storage_token, URL_REGION[self.stack_id], 'transformation',
                                                 dst_config_id,
                                                 self.cfg_params[KEY_OUTPUT_TR_BUCKET][KEY_TR_NAME], row_cfg)

    def _build_tr_config(self, queries, dst_table, input_table, name, pkey_columns):
        pkey_columns = [pk.lower() for pk in pkey_columns]
        cfg = {
            "output": [
                {
                    "primaryKey": pkey_columns,
                    "destination": dst_table,
                    "source": "final_snapshot",
                    "incremental": True
                }
            ],
            "queries": queries,
            "input": [
                {
                    "source": dst_table,
                    "destination": "curr_snapshot",
                    "whereColumn": "actual",
                    "whereValues": [
                        "1"
                    ],
                    "whereOperator": "eq",
                    "columns": []
                },
                {
                    "source": input_table,
                    "destination": "in_table"
                }
            ],
            "name": name,
            "phase": "1",
            "backend": "snowflake",
            "type": "simple",
            "disabled": False,
            "description": ""
        }
        return cfg

    def _get_storage_table_detail(self, table_id, token, stack_id):
        tablescl = tables.Tables('https://' + stack_id, token)
        return tablescl.detail(table_id)

        # if scd_type = 'scd2':

    def validate_columns(self, table_details, cols):
        missing_cols = list()
        for c in cols:
            if c not in table_details["columns"]:
                missing_cols.append(c)
        if missing_cols:
            raise ValueError(
                F'Some specified columns {missing_cols} were not found in specified table {table_details["id"]}. '
                F'Valid columns are {table_details["columns"]}')

    def _parse_comma_sep_params(self, param):
        cols = []
        if param:
            cols = [p.strip() for p in param.split(",")]
        return cols

    def create_tr_bucket(self, conf_tr_bucket):
        if conf_tr_bucket.get('id'):
            cfg = self.get_tr_bucket(conf_tr_bucket)
            if cfg:
                raise ValueError(F'Specified transformation bucket ID "{conf_tr_bucket.get("id")}" already exists. '
                                 F'Use different one or change override type to append.')

        return kbcapi_scripts._create_config(self.storage_token, URL_REGION[self.stack_id], 'transformation',
                                             conf_tr_bucket['name'], 'Generated by snapshotting tool', dict(),
                                             configurationId=conf_tr_bucket.get('id'))

    def get_tr_bucket(self, conf_tr_bucket):
        try:
            return kbcapi_scripts._get_config_detail(self.storage_token, URL_REGION[self.stack_id], 'transformation',
                                                     conf_tr_bucket.get('id'))
        except Exception as e:
            logging.error(F'Failed to get the specified transformation ID "{conf_tr_bucket.get("id")}"')
            raise e

    def validate_output_objects(self):
        err = []

        try:
            table_details = self._get_storage_table_detail(self.cfg_params[KEY_DST_TABLE][KEY_DST_TABLE_ID],
                                                           self.storage_token,
                                                           self.stack_id)
            if table_details:
                err.append(
                    F'Specified destination table already exists "{self.cfg_params[KEY_DST_TABLE][KEY_DST_TABLE_ID]}"')
            conf_tr_bucket = self.cfg_params[KEY_OUTPUT_TR_BUCKET][KEY_DST_BUCKET]
            if conf_tr_bucket[KEY_BUCKET_OVERRIDE]:
                cfg = self.get_tr_bucket(conf_tr_bucket)
                if cfg:
                    err.append(
                        F'Specified transformation bucket ID "{conf_tr_bucket.get("id")}" already exists. '
                        F'Use different one or change override type to append.')

        except requests.exceptions.HTTPError as ex:
            if ex.response.status_code == 404:
                pass
            else:
                logging.exception(ex)

        if err:
            raise ValueError(F'Output objects cannot be created: {err}')

    def _create_dst_table(self, params, pk_cols, monitored_cols, scd_type):
        table_id = params[KEY_DST_TABLE][KEY_DST_TABLE_ID].split('.')
        tablescl = tables.Tables('https://' + self.stack_id, self.storage_token)
        with open(os.path.join(self.data_path, 'dst.csv'), 'w+') as f:
            f.write(','.join(self._build_dst_header(pk_cols, monitored_cols, scd_type)))
        return tablescl.create(table_id[0] + '.' + table_id[1], table_id[2], file_path=f.name,
                               primary_key=[scd_sql_generators.COL_SNAP_PK])

    def _build_dst_header(self, pk_cols, monitored_cols, scd_type):
        header = [scd_sql_generators.COL_SNAP_PK]
        header.extend(pk_cols)
        header.extend(monitored_cols)
        if scd_type == 'scd2':
            header.extend([scd_sql_generators.COL_START_DATE, scd_sql_generators.COL_END_DATE])
        elif scd_type == 'scd4':
            header.extend([scd_sql_generators.COL_SNAP_DATE])

        header.extend([scd_sql_generators.COL_ACTUAL])
        if self.cfg_params[KEY_SCD_PARAMETERS][DELETED_FLAG]:
            header.append('is_deleted')
        header = list(set(header))
        return [h.lower() for h in header]


"""
        Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug_arg = sys.argv[1]
    else:
        debug_arg = False
    try:
        comp = Component(debug_arg)
        comp.run()
    except Exception as e:
        logging.exception(e)
        exit(1)



================================================
File: src/kbc_scripts/__init__.py
================================================



================================================
File: src/kbc_scripts/kbcapi_scripts.py
================================================
import json
import os
import urllib

import requests
from kbcstorage.base import Endpoint
from kbcstorage.buckets import Buckets
from kbcstorage.tables import Tables

# uncomment in sandbox
# import subprocess
# import sys
# subprocess.call([sys.executable, '-m', 'pip', 'install', 'git+https://github.com/keboola/sapi-python-client.git'])

URL_SUFFIXES = {"US": ".keboola.com",
                "EU": ".eu-central-1.keboola.com"}

"""
Various Adhoc scripts for KBC api manipulations.

"""


def run_config(component_id, config_id, token, region='US'):
    values = {
        "config": config_id
    }

    headers = {
        'Content-Type': 'application/json',
        'X-StorageApi-Token': token
    }
    response = requests.post('https://syrup' + URL_SUFFIXES[region] + '/docker/' + component_id + '/run',
                             data=json.dumps(values),
                             headers=headers)

    try:
        response.raise_for_status()
    except requests.HTTPError as e:
        raise e
    else:
        return response.json()


def get_job_status(token, url):
    headers = {
        'Content-Type': 'application/json',
        'X-StorageApi-Token': token
    }
    response = requests.get(url, headers=headers)
    try:
        response.raise_for_status()
    except requests.HTTPError as e:
        raise e
    else:
        return response.json()


def _get_config_detail(token, region, component_id, config_id):
    """

    :param region: 'US' or 'EU'
    """
    cl = Endpoint('https://connection' + URL_SUFFIXES[region], 'components', token)
    url = '{}/{}/configs/{}'.format(cl.base_url, component_id, config_id)
    return cl._get(url)


def _get_config_rows(token, region, component_id, config_id):
    """
    Retrieves component's configuration detail.

    Args:
        component_id (str or int): The id of the component.
        config_id (int): The id of configuration
        region: 'US' or 'EU'
    Raises:
        requests.HTTPError: If the API request fails.
    """
    cl = Endpoint('https://connection' + URL_SUFFIXES[region], 'components', token)
    url = '{}/{}/configs/{}/rows'.format(cl.base_url, component_id, config_id)

    return cl._get(url)


def _create_config(token, region, component_id, name, description, configuration, configurationId=None, state=None,
                   changeDescription='', **kwargs):
    """
    Create a new table from CSV file.

    Args:
        component_id (str):
        name (str): The new table name (only alphanumeric and underscores)
        configuration (dict): configuration JSON; the maximum allowed size is 4MB
        state (dict): configuration JSON; the maximum allowed size is 4MB
        changeDescription (str): Escape character used in the CSV file.
        region: 'US' or 'EU'

    Returns:
        table_id (str): Id of the created table.

    Raises:
        requests.HTTPError: If the API request fails.
    """
    cl = Endpoint('https://connection' + URL_SUFFIXES[region], 'components', token)
    url = '{}/{}/configs'.format(cl.base_url, component_id)
    parameters = {}
    if configurationId:
        parameters['configurationId'] = configurationId
    parameters['configuration'] = json.dumps(configuration)
    parameters['name'] = name
    parameters['description'] = description
    parameters['changeDescription'] = changeDescription
    if state:
        parameters['state'] = json.dumps(state)
    header = {'Content-Type': 'application/x-www-form-urlencoded'}
    data = urllib.parse.urlencode(parameters)
    return cl._post(url, data=data, headers=header)


def _create_config_row(token, region, component_id, configuration_id, name, configuration,
                       description='', rowId=None, state=None, changeDescription='', isDisabled=False, **kwargs):
    """
    Create a new table from CSV file.

    Args:
        component_id (str):
        name (str): The new table name (only alphanumeric and underscores)
        configuration (dict): configuration JSON; the maximum allowed size is 4MB
        state (dict): configuration JSON; the maximum allowed size is 4MB
        changeDescription (str): Escape character used in the CSV file.
        region: 'US' or 'EU'

    Returns:
        table_id (str): Id of the created table.

    Raises:
        requests.HTTPError: If the API request fails.
    """
    cl = Endpoint('https://connection' + URL_SUFFIXES[region], 'components', token)
    url = '{}/{}/configs/{}/rows'.format(cl.base_url, component_id, configuration_id)
    parameters = {}
    # convert objects to string
    parameters['configuration'] = json.dumps(configuration)
    parameters['name'] = name
    parameters['description'] = description
    if rowId:
        parameters['rowId'] = rowId
    parameters['changeDescription'] = changeDescription
    parameters['isDisabled'] = isDisabled
    if state:
        parameters['state'] = json.dumps(state)

    header = {'Content-Type': 'application/x-www-form-urlencoded'}
    data = urllib.parse.urlencode(parameters)
    return cl._post(url, data=data, headers=header)


def clone_orchestration(src_token, dest_token, src_region, dst_region, orch_id):
    """
    Clones orchestration. Note that all component configs that are part of the tasks need to be migrated first using
    the migrate_config function. Otherwise it will fail.
    :param src_token:
    :param orch_id:
    :param dest_token:
    :param region:
    :return:
    """
    src_config = _get_config_detail(src_token, src_region, 'orchestrator', orch_id)
    return _create_orchestration(dest_token, dst_region, src_config['name'], src_config['configuration']['tasks'])


def _create_orchestration(token, region, name, tasks):
    values = {
        "name": name,
        "tasks": tasks
    }

    headers = {
        'Content-Type': 'application/json',
        'X-StorageApi-Token': token
    }
    response = requests.post('https://syrup' + URL_SUFFIXES[region] + '/orchestrator/orchestrations',
                             data=json.dumps(values),
                             headers=headers)

    try:
        response.raise_for_status()
    except requests.HTTPError as e:
        raise e
    else:
        return response.json()


def run_orchestration(orch_id, token, region='US'):
    headers = {
        'Content-Type': 'application/json',
        'X-StorageApi-Token': token
    }
    response = requests.post(
        'https://syrup' + URL_SUFFIXES[region] + '/orchestrator/orchestrations/' + str(orch_id) + '/jobs',
        headers=headers)

    try:
        response.raise_for_status()
    except requests.HTTPError as e:
        raise e
    else:
        return response.json()


def get_orchestrations(token, region='US'):
    syrup_cl = Endpoint('https://syrup' + URL_SUFFIXES[region], 'orchestrator', token)

    url = syrup_cl.root_url + '/orchestrator/orchestrations'
    res = syrup_cl._get(url)
    return res


def _download_table(table, client: Tables, out_file):
    print('Downloading table %s into %s from source project', table['id'], out_file)
    res_path = client.export_to_file(table['id'], out_file, is_gzip=True, changed_until='')

    return res_path


PAR_WORKDIRPATH = os.path.dirname(os.path.join(os.path.abspath('')))


def transfer_storage_bucket(from_token, to_token, src_bucket_id, region_from='EU', region_to='EU', dest_bucket_id=None,
                            tmp_folder=os.path.join(PAR_WORKDIRPATH, 'data')):
    storage_api_url_from = 'https://connection' + URL_SUFFIXES[region_from]
    storage_api_url_to = 'https://connection' + URL_SUFFIXES[region_to]
    from_tables = Tables(storage_api_url_from, from_token)
    from_buckets = Buckets(storage_api_url_from, from_token)
    to_tables = Tables(storage_api_url_to, to_token)
    to_buckets = Buckets(storage_api_url_to, to_token)
    print('Getting tables from bucket %s', src_bucket_id)
    tables = from_buckets.list_tables(src_bucket_id)

    if dest_bucket_id:
        new_bucket_id = dest_bucket_id
    else:
        new_bucket_id = src_bucket_id

    bucket_exists = (new_bucket_id in [b['id'] for b in to_buckets.list()])

    for tb in tables:
        tb['new_id'] = tb['id'].replace(src_bucket_id, new_bucket_id)
        tb['new_bucket_id'] = new_bucket_id

        if bucket_exists and tb['new_id'] in [b['id'] for b in to_buckets.list_tables(new_bucket_id)]:
            print('Table %s already exists in destination bucket, skipping..', tb['new_id'])
            continue

        local_path = _download_table(tb, from_tables, tmp_folder)

        b_split = tb['new_bucket_id'].split('.')

        if not bucket_exists:
            print('Creating new bucket %s in destination project', tb['new_bucket_id'])
            to_buckets.create(b_split[1].replace('c-', ''), b_split[0])
            bucket_exists = True

        print('Creating table %s in the destination project', tb['id'])

        to_tables.create(tb['new_bucket_id'], tb['name'], local_path,
                         primary_key=tb['primaryKey'])
        # , compress=True)

        print('Deleting temp file')
        os.remove(local_path)
        # os.remove(local_path + '.gz')

    print('Finished.')


def migrate_configs(src_token, dst_token, src_config_id, component_id, src_region='EU', dst_region='EU',
                    use_src_id=False):
    """
    Super simple method, getting all table config objects and updating/creating them in the destination configuration.
    Includes all attributes, even the ones that are not updateble => API service will ignore them.

    :par use_src_id: If true the src config id will be used in the destination

    """
    src_config = _get_config_detail(src_token, src_region, component_id, src_config_id)
    src_config_rows = _get_config_rows(src_token, src_region, component_id, src_config_id)

    dst_config = src_config.copy()
    # add component id
    dst_config['component_id'] = component_id

    if use_src_id:
        dst_config['configurationId'] = src_config['id']

    # add token and region to use wrapping
    dst_config['token'] = dst_token
    dst_config['region'] = dst_region

    print('Transfering config..')
    new_cfg = _create_config(**dst_config)

    print('Transfering config rows')
    for row in src_config_rows:
        row['component_id'] = component_id
        row['configuration_id'] = new_cfg['id']
        row['configuration'].pop('id', {})
        row['configuration'].pop('rowId', {})
        row.pop('rowId', {})

        # add token and region to use wrapping
        row['token'] = dst_token
        row['region'] = dst_region

        _create_config_row(**row)



================================================
File: src/sql_generator/__init__.py
================================================



================================================
File: src/sql_generator/scd_sql_generators.py
================================================
import sqlparse

COL_SNAP_PK = 'snap_pk'
COL_START_DATE = 'start_date'
COL_END_DATE = 'end_date'
COL_ACTUAL = 'actual'
COL_IS_DELETED = 'is_deleted'

COL_SNAP_DATE = 'snapshot_date'


def generate_scd2_code(monitored_columns, primary_key_cols, keep_all_cols, keep_deleted_active, deleted_flag, timezone,
                       use_datetime=False):
    query_raw_template = """
        -- Auto-generated code using Snapshotting Generator. Modify as you please.

        SET CURR_DATE = (SELECT CONVERT_TIMEZONE('%(timezone)s', current_timestamp()))::DATE;
        SET CURR_TIMESTAMP = (SELECT CONVERT_TIMEZONE('%(timezone)s', current_timestamp())::TIMESTAMP_NTZ);
        SET CURR_DATE_TXT = (SELECT TO_CHAR($CURR_DATE, 'YYYY-MM-DD'));
        SET CURR_TIMESTAMP_TXT = (SELECT TO_CHAR($CURR_TIMESTAMP, 'YYYY-MM-DD HH:Mi:SS'));


        CREATE OR REPLACE TABLE changed_records_snapshot AS
        WITH
            diff_records AS (
                -- get records that have changed or have been added
                SELECT
                    %(input_table_cols_w_alias)s
                FROM "in_table" input
                MINUS
                SELECT
                    %(snap_table_cols_w_alias)s
                FROM "curr_snapshot" snap
                WHERE
                    "actual" = 1
            )
        SELECT
            %(input_table_cols)s

          , %(curr_date_value)s   AS "start_date"
          , '9999-12-31 00:00:00' AS "end_date"
          , 1                     AS "actual"
          , 0 AS "is_deleted"
        FROM diff_records;



        CREATE OR REPLACE TABLE deleted_records_snapshot AS
        SELECT
            %(snap_table_cols_w_alias)s
          , snap."start_date"   AS "start_date"
          , %(actual_deleted_timestamp)s AS "end_date"
          , %(actual_deleted_value)s                   AS "actual"
          , 1 AS "is_deleted"
        FROM
            "curr_snapshot" snap
                LEFT JOIN "in_table" input
                          ON %(snap_input_join_condition)s
        WHERE
            snap."actual" = 1 AND input.%(input_random_col)s IS NULL;

        CREATE OR REPLACE TABLE updated_snapshots AS
        SELECT
            %(snap_table_cols_w_alias)s

          , snap."start_date"
          , %(curr_date_value)s AS "end_date"
          , 0                   AS "actual"
          , 0 AS "is_deleted"
        FROM
            "curr_snapshot" snap
                JOIN changed_records_snapshot input
                     ON %(snap_input_join_condition)s
        WHERE
            snap."actual" = 1;

        -- final snapshot table
        CREATE OR REPLACE TABLE "final_snapshot" AS
        SELECT
        %(snap_primary_key_lower)s
          ,%(snap_table_cols)s
          ,%(snap_default_cols)s
        FROM deleted_records_snapshot
        UNION
        SELECT
        %(snap_primary_key_lower)s
          ,%(snap_table_cols)s
          ,%(snap_default_cols)s
        FROM updated_snapshots
        UNION
        SELECT
        %(snap_primary_key)s
          ,%(input_table_cols)s
          ,%(snap_default_cols)s
        FROM changed_records_snapshot
        ;
        """
    placeholder_values = dict()
    # input table columns pk + monitored
    input_table_cols_raw = list(set(primary_key_cols + monitored_columns))
    input_table_cols_w_alias_raw = ['input.' + '"' + i + '"' for i in input_table_cols_raw]

    placeholder_values['input_table_cols'] = ','.join(['"' + i + '"' for i in input_table_cols_raw])
    placeholder_values['input_table_cols_w_alias'] = ','.join(input_table_cols_w_alias_raw)

    # snap table cols normalized to lowercase
    snap_table_cols_raw = ['"' + i.lower() + '"' for i in input_table_cols_raw]
    snap_table_cols_w_alias_raw = ['snap.' + i for i in snap_table_cols_raw]

    placeholder_values['snap_table_cols'] = ','.join(snap_table_cols_raw)
    placeholder_values['snap_table_cols_w_alias'] = ','.join(snap_table_cols_w_alias_raw)

    # default cols
    snap_default_cols_raw = [COL_START_DATE, COL_END_DATE, COL_ACTUAL]
    if deleted_flag:
        snap_default_cols_raw.append(COL_IS_DELETED)
    placeholder_values['snap_default_cols'] = ','.join(['"' + i + '"' for i in snap_default_cols_raw])

    if use_datetime:
        curr_date_value = '$CURR_TIMESTAMP_TXT'
    else:
        curr_date_value = '$CURR_DATE_TXT'

    placeholder_values['curr_date_value'] = curr_date_value

    if not keep_deleted_active:
        placeholder_values['actual_deleted_value'] = 0
        placeholder_values['actual_deleted_timestamp'] = curr_date_value
    else:
        placeholder_values['actual_deleted_value'] = 1
        placeholder_values['actual_deleted_timestamp'] = "'9999-12-31 00:00:00'"

    placeholder_values['input_random_col'] = '"' + primary_key_cols[0] + '"'

    snap_input_join_condition = ['snap."' + pk.lower() + '"=' + 'input."' + pk + '"' for pk in primary_key_cols]
    placeholder_values['snap_input_join_condition'] = ' AND '.join(snap_input_join_condition)

    snap_primary_key = primary_key_cols + [COL_START_DATE]
    snap_primary_key_query_lower = ['"' + k.lower() + '"' for k in snap_primary_key]
    snap_primary_key_query = ['"' + k + '"' for k in snap_primary_key]
    placeholder_values['snap_primary_key_lower'] = "|| '|' ||".join(
        snap_primary_key_query_lower) + 'AS "' + COL_SNAP_PK + '"'
    placeholder_values['snap_primary_key'] = "|| '|' ||".join(snap_primary_key_query) + 'AS "' + COL_SNAP_PK + '"'
    placeholder_values['timezone'] = timezone

    res_sql = query_raw_template % placeholder_values
    statements = sqlparse.split(res_sql)
    formatted_sql_queries = list()
    for s in statements:
        formatted_sql_queries.append(sqlparse.format(s, reindent=True, keyword_case='upper'))
    return formatted_sql_queries


def generate_scd4_code(monitored_columns, primary_key_cols, keep_all_cols, keep_deleted_active, deleted_flag, timezone,
                       use_datetime=False):
    query_raw_template = """
        -- Auto-generated code using Snapshotting Generator. Modify as you please.


        SET CURR_DATE =  (SELECT CONVERT_TIMEZONE('Europe/Prague', current_timestamp()))::DATE;
        SET CURR_TIMESTAMP =  (SELECT CONVERT_TIMEZONE('Europe/Prague', current_timestamp())::TIMESTAMP_NTZ);
        SET CURR_DATE_TXT =  (SELECT TO_CHAR($CURR_DATE, 'YYYY-MM-DD'));
        SET CURR_TIMESTAMP_TXT =  (SELECT TO_CHAR($CURR_TIMESTAMP, 'YYYY-MM-DD HH:Mi:SS'));

        -- actual snapshot
        CREATE OR REPLACE TABLE records_snapshot AS
        SELECT %(input_table_cols)s
               , $CURR_TIMESTAMP_TXT AS "snapshot_date"
               , 1 AS "actual"
               , 0 AS "is_deleted"
        FROM "in_table";

        -- last snapshot rows to update actual flag
        CREATE OR REPLACE TABLE last_curr_records AS
        SELECT %(snap_table_cols)s
               , "snapshot_date"
               , 0 AS "actual"
               %(is_deleted_flag)s
        FROM "curr_snapshot"
        WHERE "actual" = 1 ;

        CREATE OR REPLACE TABLE deleted_records_snapshot AS
        SELECT %(snap_table_cols_w_alias)s
               , $CURR_TIMESTAMP_TXT AS "snapshot_date"
               , 1 AS "actual"
               , 1 AS "is_deleted"
        FROM "curr_snapshot" snap
        LEFT JOIN "in_table" INPUT ON snap."id"=input."ID"
        WHERE snap."actual" = 1
          AND input."ID" IS NULL;

        -- final snapshot table

        CREATE OR REPLACE TABLE "final_snapshot" AS
        SELECT %(snap_primary_key_lower)s
                  ,%(snap_table_cols)s
                  ,%(snap_default_cols)s
        FROM last_curr_records
        %(deleted_snap_query)s
        UNION
        SELECT     %(snap_primary_key)s
                  ,%(input_table_cols)s
                  ,%(snap_default_cols)s
        FROM records_snapshot ;
"""

    deleted_records_query = """
    UNION
    SELECT %(snap_primary_key_lower)s
              ,%(snap_table_cols)s
              ,%(snap_default_cols)s
    FROM deleted_records_snapshot
    """
    placeholder_values = dict()
    # input table columns pk + monitored
    input_table_cols_raw = list(set(primary_key_cols + monitored_columns))
    input_table_cols_w_alias_raw = ['input.' + '"' + i + '"' for i in input_table_cols_raw]

    placeholder_values['input_table_cols'] = ','.join(['"' + i + '"' for i in input_table_cols_raw])
    placeholder_values['input_table_cols_w_alias'] = ','.join(input_table_cols_w_alias_raw)

    # snap table cols normalized to lowercase
    snap_table_cols_raw = ['"' + i.lower() + '"' for i in input_table_cols_raw]
    snap_table_cols_w_alias_raw = ['snap.' + i for i in snap_table_cols_raw]

    placeholder_values['snap_table_cols'] = ','.join(snap_table_cols_raw)
    placeholder_values['snap_table_cols_w_alias'] = ','.join(snap_table_cols_w_alias_raw)

    # default cols
    snap_default_cols_raw = [COL_SNAP_DATE, COL_ACTUAL]
    placeholder_values['is_deleted_flag'] = ''
    if deleted_flag:
        snap_default_cols_raw.append(COL_IS_DELETED)
        placeholder_values['is_deleted_flag'] = ', "is_deleted"'
    placeholder_values['snap_default_cols'] = ','.join(['"' + i + '"' for i in snap_default_cols_raw])

    if use_datetime:
        curr_date_value = '$CURR_TIMESTAMP_TXT'
    else:
        curr_date_value = '$CURR_DATE_TXT'

    placeholder_values['curr_date_value'] = curr_date_value

    placeholder_values['input_random_col'] = '"' + primary_key_cols[0] + '"'

    snap_input_join_condition = ['snap."' + pk.lower() + '"=' + 'input."' + pk + '"' for pk in primary_key_cols]
    placeholder_values['snap_input_join_condition'] = ' AND '.join(snap_input_join_condition)

    snap_primary_key = primary_key_cols + [COL_SNAP_DATE]
    snap_primary_key_query_lower = ['"' + k.lower() + '"' for k in snap_primary_key]
    snap_primary_key_query = ['"' + k + '"' for k in snap_primary_key]
    placeholder_values['snap_primary_key_lower'] = "|| '|' ||".join(
        snap_primary_key_query_lower) + 'AS "' + COL_SNAP_PK + '"'
    placeholder_values['snap_primary_key'] = "|| '|' ||".join(snap_primary_key_query) + 'AS "' + COL_SNAP_PK + '"'
    placeholder_values['timezone'] = timezone

    # if deleted add union
    placeholder_values['deleted_snap_query'] = ''
    if keep_deleted_active:
        placeholder_values['deleted_snap_query'] = deleted_records_query % placeholder_values

    res_sql = query_raw_template % placeholder_values
    statements = sqlparse.split(res_sql)
    formatted_sql_queries = list()
    for s in statements:
        formatted_sql_queries.append(sqlparse.format(s, reindent=True, keyword_case='upper'))
    return formatted_sql_queries



================================================
File: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
File: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


