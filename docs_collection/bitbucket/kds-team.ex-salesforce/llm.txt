Directory structure:
└── kds_consulting_team-kds-team.ex-salesforce/
    ├── README.md
    ├── Dockerfile
    ├── LICENSE.md
    ├── accountsupd.csv
    ├── bitbucket-pipelines.yml
    ├── config.json
    ├── configSchema.json
    ├── deploy.sh
    ├── pom.xml
    ├── testConfig.json
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   ├── files/
    │       │   │   └── order1.xml
    │       │   └── tables/
    │       │       ├── test.csv
    │       │       └── test.csv.manifest
    │       └── out/
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               └── test.csv
    ├── img/
    ├── main/
    │   └── keboola/
    │       └── salesforce/
    │           └── extractor/
    │               ├── CsvWriter.java
    │               ├── ExtractionException.java
    │               ├── Extractor.java
    │               ├── FileHandler.java
    │               ├── KBCException.java
    │               └── config/
    │                   ├── JsonConfigParser.java
    │                   ├── JsonlStateWriter.java
    │                   ├── KBCConfig.java
    │                   ├── KBCOutputMapping.java
    │                   ├── KBCParameters.java
    │                   ├── KBCStorage.java
    │                   ├── KBCTablesList.java
    │                   ├── LastState.java
    │                   ├── ManifestBuilder.java
    │                   ├── ManifestFile.java
    │                   ├── ObjectsClass.java
    │                   └── ValidationException.java
    └── scripts/
        ├── build_n_run.ps1
        ├── build_n_test.sh
        ├── run.bat
        ├── run_kbc_tests.ps1
        └── update_dev_portal_properties.sh

================================================
File: README.md
================================================
Salesforce Extractor component for Keboola Connection.

## Functionality

The component exports data from the Salesforce based on SOQL you provide and save it into the out/tables directory in file named as object.csv.

### Configuration

#### Authentication

**Login Name** - (REQ) your user name, when exporting data from sandbox don't forget to add .sandboxname at the end
**Password** - (REQ) your password
**Security Token** - (REQ) your security token, don't forget it is different for sandbox
**sandbox** - (REQ) true when you want to export data from sandbox


**Object** - Salesforce object identifier, eg. Account.

##### Incremental loading 

Set `incremental` to true to download only new records since the last extraction. The `Id` column must be included if you use custom query as it will be the primary key.

**NOTE**: The extractor uses the `LastModifiedDate` to perform the incremental fetching. Some datasets like `*_history` do not have the `LastModifiedDate` column defined. 

In such cases, specify the increments using the custom query and use the `JSON EDITOR` to setup manifest to control the incremental loads.



**Example:**

```json
{
  "parameters": {
    "sinceLast": false,
    "objects": [
      {
        "name": "CaseHistory",
        "soql": "SELECT Id, CaseId, Field, CreatedById, CreatedDate, IsDeleted, NewValue, OldValue FROM CaseHistory where CreatedDate = LAST_N_DAYS:5"
      }
    ]
  },
  "processors": {
    "after": [
      {
        "definition": {
          "component": "keboola.processor-create-manifest"
        },
        "parameters": {
          "incremental": true,
          "primary_key": [
            "Id"
          ]
        }
      }
    ]
  }
}
```


As of Spring '17 release this connector allow querying for deleted records (use IsDeleted).

================================================
File: Dockerfile
================================================
FROM maven:3.6.2-jdk-8-slim
MAINTAINER KDS Team <data_ca@keboola.com>

ENV APP_VERSION 1.1.0

# set switch that enables correct JVM memory allocation in containers
ENV MAVEN_OPTS="-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"
ENV JAVA_OPTS="-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap"

COPY . /code/
WORKDIR /code/

RUN mvn compile

# https://github.com/keboola/docker-bundle/issues/198
RUN chmod a+rw ./ -R
RUN mkdir -p /var/www/.m2 && chmod a+rw /var/www/.m2 -R

# https://github.com/carlossg/docker-maven#running-as-non-root
# ENV MAVEN_CONFIG=/var/maven/.m2

ENTRYPOINT mvn -q -e exec:java -Dexec.args=/data  

================================================
File: LICENSE.md
================================================
#Terms and Conditions

The Salesforce Extractor is built and offered as a third party component. It is provided as-is, without guarantees and support, and for no additional charge.  
It does not remove your data from or store it elsewhere than within your KBC account, maintaining all Keboola recommended security standards along the way.


##Developed by
 Martin Humpolec
kbc@htns.cz

================================================
File: accountsupd.csv
================================================
Id,Name,BillingCity
001i000001usCnh,Firma2,Brno

================================================
File: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          # push test image to ecr - uncomment for testing before deployment
          - echo 'Pushing test image to repo. [tag=test]'
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:test
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            # push test image to ecr - uncomment for testing before deployment
            - echo 'Pushing test image to repo. [tag=test]'
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: config.json
================================================
{
    "storage": {
        "input": {
            "tables": [
                {
                    "source": "in.c-main.test",
                    "destination": "source.csv",
                    "limit": 50,
                    "columns": [],
                    "where_values": [],
                    "where_operator": "eq"
                }
            ],
            "files": []
        },
        "output": {
            "tables": [
                {
                    "source": "destination.csv",
                    "destination": "out.c-main.test",
                    "incremental": false,
                    "primary_key": [],
                    "delete_where_values": [],
                    "delete_where_operator": "eq",
                    "delimiter": ",",
                    "enclosure": "\"",
                    "escaped_by": ""
                }
            ],
            "files": []
        }
    },
    "parameters": {
        "multiplier": 2,
	"loginname": "loginname",
"#password": "pass",
"#securitytoken": "kuftucbEqltNDp2fAQEMC5u3",
"sandbox": false,
 "objects": [
    {
      "name": "User",
      "soql": ""
    },
    {
      "name": "Account",
      "soql": "SELECT Id, Name FROM Account"
    }
  ]
    },
    "image_parameters": []
}

================================================
File: configSchema.json
================================================
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "type": "object",
"title":"Object query",
  "format": "table",
  "properties": {
    "object_name": {
      "title": "Object name",
      "description": "Salesforce object identifier, eg. Account",
      "type": "string",
      "default": "",
      "required": true,
      "propertyOrder": 10
    },
    "soql": {
      "title": "SOQL",
      "description": "Specify the SOQL query for the given object. If the SOQL is not specified, extractor will download all columns",
      "default": "",
      "type": "string",
      "propertyOrder": 20
    },
    "sinceLast": {
      "type": "boolean",
      "format": "checkbox",
      "title": "Incremental",
      "propertyOrder": 30,
      "description": "Download only new records since the last extraction. The `Id` column must be included if you use custom query as it will be the primary key.",
      "default": false
    }
  },
  "required": [
    "object_name",
    "soql",
    "sinceLast"
  ]
}

================================================
File: deploy.sh
================================================
#!/bin/sh
set -e

# Obtain the component repository and log--in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi

================================================
File: pom.xml
================================================
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
	<modelVersion>4.0.0</modelVersion>
	<groupId>mhumpolec</groupId>
	<artifactId>KBC_Salesforce_Ext</artifactId>
	<version>0.1</version>
	<packaging>jar</packaging>
	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
		<maven.compiler.source>1.8</maven.compiler.source>
		<maven.compiler.target>1.8</maven.compiler.target>
		<jackson.version>2.7.4</jackson.version>
		<slf4j.version>1.7.30</slf4j.version>
	</properties>

	<build>
		<sourceDirectory>${project.basedir}/main</sourceDirectory>
		<plugins>
			<plugin>
				<groupId>org.codehaus.mojo</groupId>
				<artifactId>exec-maven-plugin</artifactId>
				<version>1.2.1</version>
				<executions>
					<execution>
						<goals>
							<goal>java</goal>
						</goals>
					</execution>
				</executions>
				<configuration>
					<mainClass>keboola.salesforce.extractor.Extractor</mainClass>
				</configuration>
			</plugin>
		</plugins>
	</build>
	<dependencies>
		<dependency>
			<groupId>com.fasterxml.jackson.datatype</groupId>
			<artifactId>jackson-datatype-jsr310</artifactId>
			<version>${jackson.version}</version>
		</dependency>
		<dependency>
			<groupId>com.fasterxml.jackson.core</groupId>
			<artifactId>jackson-databind</artifactId>
			<version>${jackson.version}</version>
		</dependency>
		<dependency>
			<groupId>com.fasterxml.jackson.core</groupId>
			<artifactId>jackson-annotations</artifactId>
			<version>${jackson.version}</version>
		</dependency>
		<dependency>
			<groupId>com.fasterxml.jackson.core</groupId>
			<artifactId>jackson-core</artifactId>
			<version>${jackson.version}</version>
		</dependency><!-- https://mvnrepository.com/artifact/com.force.api/force-partner-api -->
		<dependency>
			<groupId>com.force.api</groupId>
			<artifactId>force-partner-api</artifactId>
			<version>43.0.0</version>
		</dependency>
		<dependency>
			<groupId>com.evanlennick</groupId>
			<artifactId>retry4j</artifactId>
			<version>0.15.0</version>
		</dependency>
		<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-api</artifactId>
    <version>${slf4j.version}</version>
</dependency>
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-simple</artifactId>
    <version>${slf4j.version}</version>
</dependency>
	</dependencies>
</project>

================================================
File: testConfig.json
================================================
{
 "loginname": "username@nekde.cz", "#password": "12345678",
      "#securitytoken": "vcxgdgde",
   "sandbox": false,
 "objects": [
    {
      "name": "User",
      "soql": ""
    },
    {
      "name": "Account",
      "soql": "SELECT Id, Name FROM Account"
    }
  ]
}



================================================
File: component_config/component_long_description.md
================================================
The component exports data from the Salesforce based on SOQL you provide.


================================================
File: component_config/component_short_description.md
================================================
Extract data from Salesforce

================================================
File: component_config/configRowSchema.json
================================================
{
  "$schema": "http://json-schema.org/draft-04/schema#",
  "title": "Salesforce Credentials",
  "type": "object",
  "format": "table",
  "properties": {
    "loginname": {
      "title": "Login Name",
      "type": "string",
      "description": "Login name for Salesforce",
      "minLength": 1,
      "default": ""
    },
    "#password": {
      "title": "Password",
      "type": "string",
      "format": "password",
      "description": "Salesforce password",
      "minLength": 1,
      "default": ""
    },
    "#securitytoken": {
      "title": "Security token",
      "type": "string",
      "description": "Salesforce security token",
      "minLength": 1,
      "default": "",
      "format": "password"
    },
    "sandbox": {
      "title": "Sandbox",
      "type": "boolean",
      "format": "checkbox",
      "description": "True if you wish to download records from sandbox"
    },
    "sinceLast": {
      "type": "boolean",
      "description": "True if you wish to download only records changed from last run",
      "default": true
    },
    "objects": {
      "title": "Objects",
      "description": "Objects from which you want to export data",
      "default": "",
      "type": "array",
      "items": {
        "title": "Object",
        "type": "object",
        "properties": {
          "name": {
            "title": "Object name",
            "description": "Salesforce object identifier, eg. Account",
            "type": "string",
            "default": "",
            "required": true
          },
          "soql": {
            "title": "SOQL",
            "description": "Specify the SOQL query for the given object. If the SOQL is not specified, extractor will download all columns",
            "default": "",
            "type": "string"
          }
        }
      }
    }
  },
  "required": [
    "loginname",
    "#password",
    "#securitytoken",
    "sandbox",
    "objects",
    "sinceLast"
  ]
}

================================================
File: component_config/configSchema.json
================================================
{
	"$schema": "http://json-schema.org/draft-04/schema#",
	"title": "Salesforce Credentials",
	"type": "object",
	"format": "table",
	"properties": {
		"loginname": {
			"title": "Login Name",
			"type": "string",
			"description": "Login name for Salesforce",
			"minLength": 1,
			"default": "",
			"propertyOrder": 10
		},
		"#password": {
			"title": "Password",
			"type": "string",
			"format": "password",
			"description": "Salesforce password",
			"minLength": 1,"propertyOrder":20,
			"default": ""
		},
		"#securitytoken": {
			"title": "Security token",
			"type": "string",
			"description": "Salesforce security token",
			"minLength": 1,
			"default": "",
			"format": "password",
			"propertyOrder": 30
		},
		"sandbox": {
			"title": "Sandbox",
			"type": "boolean",
			"format": "checkbox",
			"description": "Download records from sandbox instead of the production environment.",
			"propertyOrder": 40
		}
	},
	"required": [
		"loginname",
		"#password",
		"#securitytoken",
		"sandbox"
	]
}

================================================
File: component_config/configuration_description.md
================================================
### Configuration

#### Authentication

- **Login Name** - (REQ) your user name, when exporting data from sandbox don't forget to add .sandboxname at the end
- **Password** - (REQ) your password
- **Security Token** - (REQ) your security token, don't forget it is different for sandbox
- **sandbox** - (REQ) true when you want to export data from sandbox


- **Object** - Salesforce object identifier, eg. Account.

##### Incremental loading 

Set `incremental` to true to download only new records since the last extraction. The `Id` column must be included if you use custom query as it will be the primary key.

**NOTE**: The extractor uses the `LastModifiedDate` to perform the incremental fetching. Some datasets like `*_history` do not have the `LastModifiedDate` column defined. 

In such cases, specify the increments using the custom query and use the `JSON EDITOR` to setup manifest to control the incremental loads.



**Example:**

```json
{
  "parameters": {
    "sinceLast": false,
    "objects": [
      {
        "name": "CaseHistory",
        "soql": "SELECT Id, CaseId, Field, CreatedById, CreatedDate, IsDeleted, NewValue, OldValue FROM CaseHistory where CreatedDate = LAST_N_DAYS:5"
      }
    ]
  },
  "processors": {
    "after": [
      {
        "definition": {
          "component": "keboola.processor-create-manifest"
        },
        "parameters": {
          "incremental": true,
          "primary_key": [
            "Id"
          ]
        }
      }
    ]
  }
}
```


As of Spring '17 release this connector allow querying for deleted records (use IsDeleted).

================================================
File: component_config/stack_parameters.json
================================================
{}

================================================
File: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}


================================================
File: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: main/keboola/salesforce/extractor/CsvWriter.java
================================================
package keboola.salesforce.extractor;

import java.io.PrintWriter;
import java.io.Writer;

/**
 * CSV writer
 *
 * <p/>
 * User: mcheenath
 * Date: Nov 1, 2010
 */

public class CsvWriter {
    private PrintWriter writer;

    public CsvWriter( Writer w) {
        writer = new PrintWriter(w, true);
    }

    public void writeRecord(String[] values) {
    	assert values != null;
    	try {
    		writeFirstField(values[0]);

    		for (int i=1; i<values.length; i++) {
    			writeField(values[i]);
    		}

    		endRecord();
    	} catch (Exception e) {
    		e.printStackTrace();
    		System.err.println( "CSV Writer writeRecord Exception on " + e.getStackTrace()[0].getLineNumber());
    		System.err.println( "Values lenght: " + values.length);
    	//	System.err.println( "Values: " + values);
    	}
    }

    public void endDocument() {
        writer.close();
    }

    public void endRecord() {
        writer.println();
    }

    public void writeField(String value) {
        writer.print(",");
        writeFirstField(value);
    }

    public void writeFirstField(String value) {
        if (value == null) {
            return;
        }

        writer.print(value);
    }
}

================================================
File: main/keboola/salesforce/extractor/ExtractionException.java
================================================
package keboola.salesforce.extractor;
/**
 * @author David Esner
 */
public class ExtractionException extends KBCException {

	public ExtractionException(String message, int severity) {
		super(message, severity);
		// TODO Auto-generated constructor stub
	}

	public ExtractionException(String message) {
		super(message);
		// TODO Auto-generated constructor stub
	}

}


================================================
File: main/keboola/salesforce/extractor/Extractor.java
================================================
package keboola.salesforce.extractor;

import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Arrays;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.TimeZone;
import java.util.concurrent.Callable;

import com.evanlennick.retry4j.CallExecutorBuilder;
import com.evanlennick.retry4j.Status;
import com.evanlennick.retry4j.config.RetryConfig;
import com.evanlennick.retry4j.config.RetryConfigBuilder;
import com.evanlennick.retry4j.exception.RetriesExhaustedException;
import com.evanlennick.retry4j.exception.UnexpectedException;
import com.sforce.async.AsyncApiException;
import com.sforce.async.BatchInfo;
import com.sforce.async.BatchStateEnum;
import com.sforce.async.BulkConnection;
import com.sforce.async.ConcurrencyMode;
import com.sforce.async.ContentType;
import com.sforce.async.JobInfo;
import com.sforce.async.OperationEnum;
import com.sforce.async.QueryResultList;
import com.sforce.soap.partner.DescribeSObjectResult;
import com.sforce.soap.partner.GetServerTimestampResult;
import com.sforce.soap.partner.PartnerConnection;
import com.sforce.ws.ConnectionException;
import com.sforce.ws.ConnectorConfig;

import keboola.salesforce.extractor.config.JsonConfigParser;
import keboola.salesforce.extractor.config.JsonlStateWriter;
import keboola.salesforce.extractor.config.KBCConfig;
import keboola.salesforce.extractor.config.LastState;
import keboola.salesforce.extractor.config.ManifestBuilder;
import keboola.salesforce.extractor.config.ManifestFile;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @author Martin Humpolec <martin.humpolec at gmail.com>
 * @created 2016
 */
public class Extractor {
	//

	Calendar serverTime;

	public static void main(String[] args)
			throws AsyncApiException, ConnectionException, IOException {
		if (args.length == 0) {
			System.err.println("No parameters provided.");
			System.exit(1);
		}

		String dataPath = args[0];
		String outTablesPath = dataPath + File.separator + "out" + File.separator + "tables"
				+ File.separator;

		KBCConfig config = null;
		File confFile = new File(args[0] + File.separator + "config.json");
		if (!confFile.exists()) {
			System.out.println("config.json does not exist!");
			System.err.println("config.json does not exist!");
			System.exit(1);
		}
		// Parse config file
		try {
			if (confFile.exists() && !confFile.isDirectory()) {
				config = JsonConfigParser.parseFile(confFile);
			}
		} catch (Exception ex) {
			System.out.println("Failed to parse config file");
			System.err.println(ex.getMessage());
			System.exit(1);
		}
		if (!config.validate()) {
			System.out.println(config.getValidationError());
			System.err.println(config.getValidationError());
			System.exit(1);
		}

		LastState lastState = retrieveStateFile(dataPath);
		Map<String, Date> lastBulkRequests = null;
		Calendar lastSync = null;
		if (lastState != null) {
			try {
				lastBulkRequests = lastState.getBulkRequests();
			} catch (NullPointerException ex) {
				System.out.println("No matching state.");
			}
		}
		LastState newState = new LastState(new HashMap());

		if (config.getParams().getObjects() == null) {
			System.err.println("No object to extract specified.");
			System.exit(1);
		}

		System.out.println("Everything ready, let's get some data from Salesforce, loginname: "
				+ config.getParams().getLoginname());

		Extractor sfdown = new Extractor();

		sfdown.runQueries(config.getParams().getLoginname(),
				config.getParams().getPassword() + config.getParams().getSecuritytoken(),
				config.getParams().getSandbox(), outTablesPath, config.getParams().getObjects(),
				lastBulkRequests, lastState, newState, config.getParams().getSinceLast());

		/* Write state file */
		try {
			JsonlStateWriter.writeStateFile(
					dataPath + File.separator + "out" + File.separator + "state.json", newState);
		} catch (IOException ex) {
			System.err.println("Error building state file " + ex.getMessage());
			System.exit(1);
		}

		System.out.println("All done.");
	}

	/**
	 * generate SELECT * for the object
	 */
	public String getSOQL(String object, PartnerConnection connection) {
		String soql = "";
		try {
			// Make the describe call
			DescribeSObjectResult describeSObjectResult = connection.describeSObject(object);

			// Get sObject metadata
			if (describeSObjectResult != null) {

				// Get the fields
				com.sforce.soap.partner.Field[] fields = describeSObjectResult.getFields();

				// Iterate through each field and gets its properties
				for (int i = 0; i < fields.length; i++) {
					com.sforce.soap.partner.Field field = fields[i];
					// System.out.println( field.getName() + " - " +
					// field.getType());

					if (field.getType() != com.sforce.soap.partner.FieldType.address
							&& field.getType() != com.sforce.soap.partner.FieldType.location
							&& field.getType() != com.sforce.soap.partner.FieldType.base64) {

						// if not formula field publish it
						if (soql == "") {
							soql = field.getName();
						} else {
							soql = soql + "," + field.getName();
						}
					}

				}
			}
		} catch (ConnectionException ce) {
			ce.printStackTrace();
		}
		if (soql != "") {
			soql = "SELECT " + soql + " FROM " + object;
		}
		// System.out.println( "SOQL: " + soql);

		return soql;
	}

	/**
	 * Creates a Bulk API job and uploads batches for a CSV file.
	 */
	public int runQueries(String loginname, String password, Boolean sandbox, String filesDirectory,
			List<keboola.salesforce.extractor.config.ObjectsClass> objects,
			Map<String, Date> lastBulkRequests, LastState lastState, LastState newState,
			Boolean getSinceLast) throws AsyncApiException, ConnectionException, IOException {
		BulkConnection bulkconnection = getBulkConnection(loginname, password, sandbox);
		PartnerConnection connection = getConnection(loginname, password, sandbox);

		if (connection != null) {

			for (keboola.salesforce.extractor.config.ObjectsClass object : objects) {

				String sname = object.getName();
				if (sname.indexOf(',') > 0) {
					List<String> oos = Arrays.asList(sname.split("\\s*,\\s*"));
					for (String os : oos) {
						String soql = getSOQL(os, connection);
						System.out.println("SOQL: " + soql);
						runQuery(bulkconnection, filesDirectory, os, soql, lastBulkRequests,
								lastState, newState, getSinceLast);
					}
				} else {
					String soql = object.getSoql();
					if (soql == "" || soql == null) {
						soql = getSOQL(object.getName(), connection);
					}
					System.out.println("SOQL: " + soql);
					runQuery(bulkconnection, filesDirectory, object.getName(), soql,
							lastBulkRequests, lastState, newState, getSinceLast);
				}
			}
		}
		return 0;
	}

	/**
	 * Creates a Bulk API job and download batches for a CSV file.
	 */
	public int runQuery(BulkConnection bulkconnection, String filesDirectory, String object,
			String soql, Map<String, Date> lastBulkRequests, LastState lastState,
			LastState newState, Boolean getSinceLast)
			throws AsyncApiException, ConnectionException, IOException {

		Date lastRun = serverTime.getInstance().getTime();

		Calendar lastSync = null;
		if (lastBulkRequests != null && getSinceLast) {
			lastSync = Calendar.getInstance(TimeZone.getTimeZone("UTC"));
			Date dt = lastBulkRequests.get(object);
			SimpleDateFormat formatter, formatter2;

			formatter = new SimpleDateFormat("yyyy-MM-dd", Locale.US);
			formatter2 = new SimpleDateFormat("hh:mm:ss", Locale.US);

			if (dt != null) {
				// we have last run time, update soql
				lastSync.setTime(dt);
				if (soql.toUpperCase().indexOf(" WHERE ") > 0) {
					soql = soql + " AND LastModifiedDate >= " + formatter.format(dt) + "T"
							+ formatter2.format(dt) + "Z";
				} else {
					soql = soql + " where LastModifiedDate >= " + formatter.format(dt) + "T"
							+ formatter2.format(dt) + "Z";
				}
				System.out.println("Modified SOQL: " + soql);
			}
		}
		// save state every run
		final String finalSoql = soql;
		newState.getBulkRequests().put(object, lastRun);

		System.out.println("Processing object: " + object);
		final JobInfo job = new JobInfo();
		job.setObject(object);

		job.setOperation(OperationEnum.queryAll);
		job.setConcurrencyMode(ConcurrencyMode.Parallel);
		job.setContentType(ContentType.CSV);

		// retry policy
		Callable<Integer> callable = () -> {
			return this.processBulkRequest(filesDirectory, object, job, bulkconnection, finalSoql);
		};

		RetryConfig config = new RetryConfigBuilder()
				.retryOnSpecificExceptions(ExtractionException.class).exponentialBackoff5Tries5Sec()
				.build();
		int processedRecords = 0;
		try {
			Status<Integer> status = new CallExecutorBuilder().config(config).build()
					.execute(callable);
			processedRecords = status.getResult(); // the result of the callable
													// logic, if it returns one
		} catch (RetriesExhaustedException ree) {
			ree.printStackTrace();
			System.err.println("Batch job failed after 5 retries:" + ree.getMessage());
			System.exit(1);
		} catch (UnexpectedException ue) {
			if (AsyncApiException.class.isInstance(ue.getCause())) {
				ue.printStackTrace();
				System.err.println("AsyncApiException" + ue.getStackTrace()[0].getLineNumber());
				System.exit(1);
			} else if (InterruptedException.class.isInstance(ue.getCause())) {
				ue.printStackTrace();
				System.err.println("InterruptedException");
				System.exit(1);
			} else {
				System.err.println("Bulk job failed with non-transient error: " + ue.getMessage());
				System.exit(1);
			}
		}

		if (processedRecords > 0) {
			if (getSinceLast) {
				ManifestFile manFile = new ManifestFile.Builder(object + ".csv")
						.setIncrementalLoad(true).setPrimaryKey(new String[] { "Id" })
						.setDelimiter(",").setEnclosure("\"").build();

				try {
					ManifestBuilder.buildManifestFile(manFile, filesDirectory, object + ".csv");
				} catch (IOException ex) {
					System.err.println("Error building manifest file " + ex.getMessage());
					System.exit(2);
				}
			}

			return processedRecords;
		}
		// something went wrong here, return 0 to catch an error back in main
		return 0;

	}

	private Integer processBulkRequest(String filesDirectory, String object, JobInfo job,
			BulkConnection bulkconnection, String soql)
			throws AsyncApiException, InterruptedException, ExtractionException, IOException {
		JobInfo submittedJob = bulkconnection.createJob(job);
		assert submittedJob.getId() != null;

		submittedJob = bulkconnection.getJobStatus(submittedJob.getId());

		BatchInfo info = null;
		ByteArrayInputStream bout = new ByteArrayInputStream(soql.getBytes());
		info = bulkconnection.createBatchFromStream(submittedJob, bout);

		String[] queryResults = null;

		double resultsNr = 0;
		for (int i = 1; i < 60000; i++) {
			// Thread.sleep(i==0 ? 30 * 1000 : 30 * 1000); //30 sec
			Thread.sleep(i < 12 ? i * 1000 * 5 : 60 * 1000); // 60 sec
			info = bulkconnection.getBatchInfo(submittedJob.getId(), info.getId());

			if (info.getState() == BatchStateEnum.Completed) {
				System.out.println("Completed, getting results.");
				QueryResultList list = bulkconnection.getQueryResultList(submittedJob.getId(),
						info.getId());
				queryResults = list.getResult();
				System.out.println("Processed " + info.getNumberRecordsProcessed() + " records.");
				resultsNr = info.getNumberRecordsProcessed();
				break;
			} else if (info.getState() == BatchStateEnum.Failed) {
				System.err.println("-------------- failed ----------" + info);
				bulkconnection.closeJob(submittedJob.getId());
				throw new ExtractionException("Batch job failed: " + info, 1);
			} else {
				System.out.println("-------------- waiting " + (i < 12 ? i * 5 : 60)
						+ " seconds ----------" /* + info */);
			}
		}

		int out = 0;
		if (queryResults != null && resultsNr > 0) {
			System.out.println("Write everything into " + filesDirectory + object + ".csv");
			Boolean firstFile = true;
			for (String resultId : queryResults) {
				// grabs result stream and passes it to csv writer
				FileHandler.writeCSVFromStream(
						bulkconnection.getQueryResultStream(submittedJob.getId(), info.getId(), resultId),
						object, filesDirectory, firstFile);
				// grabs results to ensure integrity
				bulkconnection.getQueryResultList(submittedJob.getId(), info.getId()).getResult();
				firstFile = false;
			}
			System.out.println("Close file");

			// notify user of job complete
			// return number of records complete for data check and close job
			out = info.getNumberRecordsProcessed();
			bulkconnection.closeJob(submittedJob.getId());
			System.out.println("Processed " + out + " records.");
		}

		return out;
	}

	/**
	 * Create the BulkConnection used to call Bulk API operations.
	 */
	private BulkConnection getBulkConnection(String userName, String password, boolean sandbox)
			throws ConnectionException, AsyncApiException {
		try {
			ConnectorConfig partnerConfig = new ConnectorConfig();
			partnerConfig.setUsername(userName);
			partnerConfig.setPassword(password);
			if (sandbox == true) {
				System.out.println("Connecting to Salesforce Sandbox (Bulk API)");
				partnerConfig.setAuthEndpoint("https://test.salesforce.com/services/Soap/u/39.0");
			} else {
				System.out.println("Connecting to Salesforce Production (Bulk API)");
				partnerConfig.setAuthEndpoint("https://login.salesforce.com/services/Soap/u/39.0");
			}
			// Creating the connection automatically handles login and stores
			// the session in partnerConfig
			PartnerConnection pc = new PartnerConnection(partnerConfig);
			// When PartnerConnection is instantiated, a login is implicitly
			// executed and, if successful,
			// a valid session is stored in the ConnectorConfig instance.
			// Use this key to initialize a BulkConnection:
			ConnectorConfig config = new ConnectorConfig();
			config.setSessionId(partnerConfig.getSessionId());

			// The endpoint for the Bulk API service is the same as for the
			// normal
			// SOAP uri until the /Soap/ part. From here it's
			// '/async/versionNumber'
			String soapEndpoint = partnerConfig.getServiceEndpoint();
			String apiVersion = "39.0";
			String restEndpoint = soapEndpoint.substring(0, soapEndpoint.indexOf("Soap/"))
					+ "async/" + apiVersion;
			config.setRestEndpoint(restEndpoint);

			// This should only be false when doing debugging.
			config.setCompression(true);
			// Set this to true to see HTTP requests and responses on stdout
			config.setTraceMessage(false);
			BulkConnection connection = new BulkConnection(config);
			return connection;

		} catch (Exception ex) {
			System.err.println("Error logging into the system - " + ex.getMessage());
			System.err.println(
					"If you changed password don't forget to change security token as well.");
			System.exit(1);
			return null;
		}
	}

	/**
	 * Create the Connection used to call Describe operations.
	 */
	private PartnerConnection getConnection(String userName, String password, boolean sandbox)
			throws ConnectionException, AsyncApiException {
		try {
			ConnectorConfig partnerConfig = new ConnectorConfig();

			partnerConfig.setUsername(userName);
			partnerConfig.setPassword(password);
			if (sandbox == true) {
				System.out.println("Connecting to Salesforce Sandbox");
				partnerConfig.setAuthEndpoint("https://test.salesforce.com/services/Soap/u/39.0");
			} else {
				System.out.println("Connecting to Salesforce Production");
				partnerConfig.setAuthEndpoint("https://login.salesforce.com/services/Soap/u/39.0");
			}
			// Creating the connection automatically handles login and stores
			// the session in partnerConfig
			PartnerConnection connection = new PartnerConnection(partnerConfig);

			GetServerTimestampResult result = connection.getServerTimestamp();
			serverTime = result.getTimestamp();

			return connection;

			// When PartnerConnection is instantiated, a login is implicitly
			// executed and, if successful,
			// a valid session is stored in the ConnectorConfig instance.
		} catch (Exception ex) {
			System.err.println("Error logging into the system - " + ex.getMessage());
			System.err.println(
					"If you changed password don't forget to change security token as well.");
			System.exit(1);
			return null;
		}

	}

	private static LastState retrieveStateFile(String dataPath) {
		File stateFile = new File(dataPath + File.separator + "in" + File.separator + "state.json");
		LastState lastState = null;
		if (stateFile.exists()) {
			try {
				lastState = (LastState) JsonConfigParser.parseFile(stateFile, LastState.class);
			} catch (IOException ex) {
				System.err.println(ex + " " + ex.getStackTrace()[0].getLineNumber());
			}
		} else {
			System.out.println("State file does not exist. (first run?)");
		}
		return lastState;
	}
}

================================================
File: main/keboola/salesforce/extractor/FileHandler.java
================================================
package keboola.salesforce.extractor;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.util.ArrayList;
 
 
public class FileHandler
{       
    public static void writeCSVFromStream(InputStream in, String object, String filesDirectory, Boolean firstFile) throws IOException{
        //gets QueryResultStream from BulkExample and object name, creates BufferedReader and output file in specified folder
        //makes output folder if it doesnt exist
        try {
	    	BufferedReader read = new BufferedReader(new InputStreamReader(in));
	        //writes csv file to output folder
	
	        BufferedWriter csvFile = new BufferedWriter( new FileWriter(filesDirectory+object+".csv", true));
	        
	        String line = "";
	        Boolean firstLine = true;
//	        CsvWriter writer = new CsvWriter(csvFile);
	        //makes array of results
	        while((line = read.readLine()) != null){
	                if(line.length()>0){
	                        line.trim();
//	                        String[] row = line.split("___");
	                        if( !firstFile && firstLine) { 
	                        } else {
	                        	csvFile.write( line);
		                        csvFile.newLine();
		                        csvFile.flush();
								line = null; // try to null the variable so it can be cleaned, no idea why it consume so much memory
	                        }
//	                        writer.writeRecord(row);
	                }
                    firstLine = false;	           
	        }
	        //closes stream
	   		csvFile.close();
//	        writer.endDocument();
        } catch (Exception e) {
			e.printStackTrace();
			System.err.println( "FileWriter Exception on " + e.getStackTrace()[0].getLineNumber());
//			System.exit(1);
        }
    }
}

================================================
File: main/keboola/salesforce/extractor/KBCException.java
================================================
/*
 */
package keboola.salesforce.extractor;

/**
 * Abstract class implementing exception with severity indicator
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2016
 */
public abstract class KBCException extends Exception {

    private final int severity;

    /**
     * Create exception with default severity = 1 (User Exception)
     *
     * @param message
     */
    public KBCException(String message) {
        super(message);
        severity = 1;
    }

    /**
     * Create exception with custom severity
     *
     * @param message
     * @param severity - severity code (Exit status = 1 will be considered as an
     * user exception, all other as application exceptions.)
     */
    public KBCException(String message, int severity) {
        super(message);
        this.severity = severity;
    }

    public int getSeverity() {
        return severity;
    }

}


================================================
File: main/keboola/salesforce/extractor/config/JsonConfigParser.java
================================================
package keboola.salesforce.extractor.config;

import com.fasterxml.jackson.core.JsonFactory;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.File;
import java.io.IOException;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2015
 */
public class JsonConfigParser {

    public static KBCConfig parseFile(File file) throws IOException {
   		System.out.println( "KBC Config parseFile start");

        final ObjectMapper mapper = new ObjectMapper(new JsonFactory());
        mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
        mapper.findAndRegisterModules();
   		System.out.println( "KBC Config parseFile end");
        return mapper.readValue(file, KBCConfig.class);
    }

    public static Object parseFile(File file, Class type) throws IOException {
   		System.out.println( "KBC Config parseFile object start");
        final ObjectMapper mapper = new ObjectMapper(new JsonFactory());
        mapper.findAndRegisterModules();
        mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
        return mapper.readValue(file, type);
    }
    
}


================================================
File: main/keboola/salesforce/extractor/config/JsonlStateWriter.java
================================================

package keboola.salesforce.extractor.config;
import com.fasterxml.jackson.core.JsonFactory;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import java.io.File;
import java.io.IOException;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2016
 */
public class JsonlStateWriter {

    public static void writeStateFile(String resultStateFilePath, LastState lstate) throws IOException {

    	System.out.println( "Writing state.json");
    	
        final ObjectMapper mapper = new ObjectMapper(new JsonFactory());
        mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);

        File stateFile = new File(resultStateFilePath);
        mapper.writeValue(stateFile, lstate);
    }

}

================================================
File: main/keboola/salesforce/extractor/config/KBCConfig.java
================================================
/*
 */
package keboola.salesforce.extractor.config;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import java.util.List;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @updated Martin Humpolec <kbc at htns.cz>
 * @created 2015
 */
@JsonIgnoreProperties(ignoreUnknown = true)
public class KBCConfig {

    String validationError;
    @JsonProperty("storage")
    private KBCStorage storage;
    @JsonProperty("parameters")
    private KBCParameters params;

    public KBCConfig() {
        validationError = null;
    }

    public KBCConfig(KBCStorage storage, KBCParameters params) {
        this.storage = storage;
        this.params = params;
    }

    public boolean validate() {
        try {
            return params.validateParametres();
        } catch (ValidationException ex) {
            this.validationError = ex.getMessage();
            return false;
        }

    }

    public String getValidationError() {
        return validationError;
    }

    private void setValidationError(List<String> missingFields) {
        this.validationError = "Required config fields are missing: ";
        int i = 0;
        for (String fld : missingFields) {
            if (i < missingFields.size()) {
                this.validationError += fld + ", ";
            } else {
                this.validationError += fld;
            }
        }
    }

    public KBCStorage getStorage() {
        return storage;
    }

    public void setStorage(KBCStorage storage) {
        this.storage = storage;
    }

    public KBCParameters getParams() {
        return params;
    }

    public void setParams(KBCParameters params) {
        this.params = params;
    }

    public List<KBCOutputMapping> getOutputTables() {
        return this.storage.getOutputTables().getTables();
    }
}


================================================
File: main/keboola/salesforce/extractor/config/KBCOutputMapping.java
================================================
/*
 */
package keboola.salesforce.extractor.config;

import com.fasterxml.jackson.annotation.JsonProperty;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2015
 */
public class KBCOutputMapping {

    @JsonProperty("source")
    private String source;
    @JsonProperty("destination")
    private String destination;

    public KBCOutputMapping() {
    }

    public KBCOutputMapping(String source, String destination) {
        this.source = source;
        this.destination = destination;
    }

    public String getSource() {
        return source;
    }

    public void setSource(String source) {
        this.source = source;
    }

    public String getDestination() {
        return destination;
    }

    public void setDestination(String destination) {
        this.destination = destination;
    }
}


================================================
File: main/keboola/salesforce/extractor/config/KBCParameters.java
================================================
/*
 */
package keboola.salesforce.extractor.config;

import java.text.ParseException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @updated Martin Humpolec <kbc at htns.cz>
 * @created 2015
 */

public class KBCParameters {

	private final static String[] REQUIRED_FIELDS = { "loginname", "password", "securitytoken" };
	private final Map<String, Object> parametersMap;

	@JsonProperty("loginname")
	private String loginname;
	@JsonProperty("#password")
	private String password;
	@JsonProperty("#securitytoken")
	private String securitytoken;
	@JsonProperty("sandbox")
	private Boolean sandbox;
	@JsonProperty("sinceLast")
	private Boolean sinceLast;
	@JsonProperty("objects")
	private List<ObjectsClass> objects;
	@JsonProperty("object_name")
	private String objectName;
	@JsonProperty("soql")
	private String soql;

	public KBCParameters() {
		parametersMap = new HashMap();

	}

	@JsonCreator
	public KBCParameters(@JsonProperty("loginname") String loginname,
			@JsonProperty("#password") String password,
			@JsonProperty("#securitytoken") String securitytoken,
			@JsonProperty("objects") List<ObjectsClass> objects,
			@JsonProperty("sandbox") Boolean sandbox, @JsonProperty("sinceLast") Boolean sinceLast,
			@JsonProperty("object_name") String objectName, @JsonProperty("soql") String soql

	) throws ParseException {
		parametersMap = new HashMap();
		this.loginname = loginname;
		this.password = password;
		this.securitytoken = securitytoken;
		this.sandbox = sandbox;

		// backward compatibility
		if (objects == null && objectName != null) {
			ObjectsClass obj = new ObjectsClass(objectName, soql);
			this.objects = new ArrayList<ObjectsClass>();
			this.objects.add(obj);
		} else {
			this.objects = objects;
		}

		if (sinceLast == null) {
			this.sinceLast = false;
		} else {
			this.sinceLast = sinceLast;
		}
		System.out.println("sinceLast: " + this.sinceLast + "/" + sinceLast);

		// set param map
		parametersMap.put("loginname", loginname);
		parametersMap.put("password", password);
		parametersMap.put("securitytoken", securitytoken);
		parametersMap.put("sandbox", sandbox);
		parametersMap.put("objects", objects);

	}

	/**
	 * Returns list of required fields missing in config
	 *
	 * @return
	 */
	private List<String> getMissingFields() {
		List<String> missing = new ArrayList<String>();
		for (int i = 0; i < REQUIRED_FIELDS.length; i++) {
			Object value = parametersMap.get(REQUIRED_FIELDS[i]);
			if (value == null) {
				missing.add(REQUIRED_FIELDS[i]);
			}
		}

		if (missing.isEmpty()) {
			return null;
		}
		return missing;
	}

	private String missingFieldsMessage() {
		List<String> missingFields = getMissingFields();
		String msg = "";
		if (missingFields != null && missingFields.size() > 0) {
			msg = "Required config fields are missing: ";
			int i = 0;
			for (String fld : missingFields) {
				if (i < missingFields.size()) {
					msg += fld + ", ";
				} else {
					msg += fld;
				}
			}
		}
		return msg;
	}

	public boolean validateParametres() throws ValidationException {
		// validate date format
		String error = "";

		error += missingFieldsMessage();

		if (error.equals("")) {
			return true;
		} else {

			throw new ValidationException("Validation error: " + error);
		}
	}

	public String getLoginname() {
		return loginname;
	}

	public void setLoginname(String loginname) {
		this.loginname = loginname;
	}

	public String getPassword() {
		return password;
	}

	public void setPassword(String password) {
		this.password = password;
	}

	public String getSecuritytoken() {
		return securitytoken;
	}

	public void setSecuritytoken(String securitytoken) {
		this.securitytoken = securitytoken;
	}

	public boolean getSandbox() {
		return sandbox;
	}

	public void setSandbox(boolean sandbox) {
		this.sandbox = sandbox;
	}

	public Boolean getSinceLast() {
		return sinceLast;
	}

	public List<ObjectsClass> getObjects() {
		return objects;
	}

	public void setObjects(List<ObjectsClass> objects) {
		this.objects = objects;
	}

}

================================================
File: main/keboola/salesforce/extractor/config/KBCStorage.java
================================================
/*
 */
package keboola.salesforce.extractor.config;

import com.fasterxml.jackson.annotation.JsonProperty;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2015
 */
public class KBCStorage {

    @JsonProperty("input")
    private KBCTablesList inputTables;
    @JsonProperty("output")
    private KBCTablesList outputTables;

    public KBCStorage() {
    }

    public KBCStorage(KBCTablesList outputTables) {
        this.outputTables = outputTables;
    }

    public KBCTablesList getOutputTables() {
        return outputTables;
    }

    public void setOutputTables(KBCTablesList outputTables) {
        this.outputTables = outputTables;
    }

    public KBCTablesList getInputTables() {
        return inputTables;
    }

    public void setInputTables(KBCTablesList inputTables) {
        this.inputTables = inputTables;
    }

}


================================================
File: main/keboola/salesforce/extractor/config/KBCTablesList.java
================================================
/*
 */
package keboola.salesforce.extractor.config;

import com.fasterxml.jackson.annotation.JsonProperty;
import java.util.List;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2015
 */
public class KBCTablesList {

    @JsonProperty("tables")
    private List<KBCOutputMapping> tables;

    public KBCTablesList() {
    }

    public KBCTablesList(List<KBCOutputMapping> tables) {
        this.tables = tables;
    }

    public List<KBCOutputMapping> getTables() {
        return tables;
    }

    public void setTables(List<KBCOutputMapping> tables) {
        this.tables = tables;
    }
}


================================================
File: main/keboola/salesforce/extractor/config/LastState.java
================================================

package keboola.salesforce.extractor.config;
import com.fasterxml.jackson.annotation.JsonProperty;
import java.util.Date;
import java.util.Map;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2016
 */
public class LastState {

    @JsonProperty("bulkRequests")
    private Map<String, Date> bulkRequests;

    public LastState(Map<String, Date> bulkRequests) {
        this.bulkRequests = bulkRequests;
    }

    public Map<String, Date> getBulkRequests() {
        return bulkRequests;
    }

    public void setBulkRequests(Map<String, Date> bulkRequests) {
        this.bulkRequests = bulkRequests;
    }

    public LastState() {
    }

}

================================================
File: main/keboola/salesforce/extractor/config/ManifestBuilder.java
================================================
/*
 */
package keboola.salesforce.extractor.config;

import java.io.File;
import java.io.IOException;

import com.fasterxml.jackson.core.JsonFactory;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2016
 */
public class ManifestBuilder {

    public static void buildManifestFile(ManifestFile file, String folderPath, String resFileName) throws IOException {
        /*Build manifest file*/
        File resFile = new File(folderPath + File.separator + resFileName + ".manifest");
        ObjectMapper mapper = new ObjectMapper(new JsonFactory());
        mapper.configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);
        mapper.writeValue(resFile, file);

//        String manifest = "destination: " + resFileName + "\n"
//                + "incremental: " + file.isIncremental() + "\n"
//                + "delimiter: \"" + file.getDelimiter() + "\"\n"
//                + "enclosure: \"" + file.getEnclosure() + "\"\n";
//        if (file.getPrimaryKey() != null) {
//            manifest += "primary_key: \"" + file.getPrimaryKey() + "\"";
//        }
//
//        Files.write(manifest, resFile, Charset.forName("UTF-8"));
    }
}


================================================
File: main/keboola/salesforce/extractor/config/ManifestFile.java
================================================
/*
 */
package keboola.salesforce.extractor.config;

import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 *
 * author David Esner <code>&lt;esnerda at gmail.com&gt;</code>
 * created 2016
 */
@JsonInclude(Include.NON_NULL)
public class ManifestFile {

    @JsonProperty("incremental")
    private final boolean incremental;
    //"," default
    @JsonProperty("delimiter")
    private final String delimiter;
    //"\"" default
    @JsonProperty("enclosure")
    private final String enclosure;
    @JsonProperty("primary_key")
    private final String[] primaryKey;
    @JsonProperty("columns")
    private final String[] columns;
    @JsonProperty("rows_count")
    private final Integer rows_count;
    @JsonProperty("data_size_bytes")
    private final Integer data_size_bytes;

    @JsonIgnore
    private String name;

    public ManifestFile(String name, boolean incremental, String[] primaryKey, String delimiter, String enclosure) {
        this.name = name;
        this.primaryKey = primaryKey;
        this.incremental = incremental;
        this.delimiter = delimiter;
        this.enclosure = enclosure;

        this.columns = null;
        this.rows_count = null;
        this.data_size_bytes = null;
    }

    @JsonCreator
    public ManifestFile(@JsonProperty("incremental") boolean incremental,
            @JsonProperty("primary_key") String[] primaryKey, @JsonProperty("delimiter") String delimiter,
            @JsonProperty("enclosure") String enclosure, @JsonProperty("rows_count") Integer rows_count,
            @JsonProperty("data_size_bytes") Integer data_size_bytes, @JsonProperty("columns") String[] columns) {

        this.rows_count = rows_count;
        this.data_size_bytes = data_size_bytes;

        this.incremental = incremental;
        this.primaryKey = primaryKey;
        //default values
        if (delimiter == null) {
            this.delimiter = ",";
        } else {
            this.delimiter = delimiter;
        }
        if (enclosure == null) {
            this.enclosure = "\"";
        } else {
            this.enclosure = enclosure;
        }
        this.columns = columns;
    }

    private ManifestFile(Builder builder) {
        this.name = builder.name;
        this.rows_count = builder.rows_count;
        this.data_size_bytes = builder.data_size_bytes;
        this.incremental = builder.incremental;
        this.primaryKey = builder.primaryKey;
        this.delimiter = builder.delimiter;
        this.enclosure = builder.enclosure;
        this.columns = builder.columns;
    }

    public String[] getColumns() {
        return columns;
    }

    public Integer getRows_count() {
        return rows_count;
    }

    public Integer getData_size_bytes() {
        return data_size_bytes;
    }

    public String[] getPrimaryKey() {
        return primaryKey;
    }

    public boolean isIncremental() {
        return incremental;
    }

    public String getDelimiter() {
        return delimiter;
    }

    public String getEnclosure() {
        return enclosure;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public static class Builder {

        private boolean incremental;

        //"," default
        private String delimiter;

        //"\"" default
        private String enclosure;

        private String[] primaryKey;
        private String[] columns;
        private Integer rows_count;
        private Integer data_size_bytes;
        private final String name;

        /**
         * Creates builder with required parameters set to default values.
         * Default delimiter(,) and enclosure(") characters and incremental load
         * set to false.
         *
         * @param name - name of the source csv table
         * @param destination - destination SAPI path, i.e. in.c-main.tables
         */
        public Builder(String name) {
            this.name = name;
            //set default values for required params
            this.delimiter = ",";
            this.enclosure = "\"";
            this.incremental = false;
        }

        public Builder setDelimiter(String delimiter) {
            this.delimiter = delimiter;
            return this;
        }

        public Builder setEnclosure(String enclosure) {
            this.enclosure = enclosure;
            return this;
        }

        public Builder setIncrementalLoad(boolean incremental) {
            this.incremental = incremental;
            return this;
        }

        public Builder setPrimaryKey(String[] pkey) {
            this.primaryKey = pkey;
            return this;
        }

        public Builder setColumns(String[] cols) {
            this.columns = cols;
            return this;
        }

        public Builder setRowsCount(int rCount) {
            this.rows_count = rCount;
            return this;
        }

        public Builder setDataSize(int data_size_bytes) {
            this.data_size_bytes = data_size_bytes;
            return this;
        }

        public ManifestFile build(){
            if (incremental && (primaryKey == null || primaryKey.length == 0)) {
                setIncrementalLoad(false);
            }
            return new ManifestFile(this);
        }

    }
}


================================================
File: main/keboola/salesforce/extractor/config/ObjectsClass.java
================================================

package keboola.salesforce.extractor.config;

public class ObjectsClass {
	private String name;
	private String soql;

	public ObjectsClass(String name, String soql) {
		super();
		this.name = name;
		this.soql = soql;
	}

	public String getName() {
		return name;
	}

	public void setName(String objectname) {
		this.name = objectname;
	}

	public String getSoql() {
		return soql;
	}

	public void setSoql(String objectsoql) {
		this.soql = objectsoql;
	}

}

================================================
File: main/keboola/salesforce/extractor/config/ValidationException.java
================================================
/*
 */
package keboola.salesforce.extractor.config;

/**
 *
 * @author David Esner <esnerda at gmail.com>
 * @created 2016
 */
public class ValidationException extends Exception {

    public ValidationException(String message) {
        super(message);
    }

}


================================================
File: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

