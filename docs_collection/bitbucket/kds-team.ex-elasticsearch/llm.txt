Directory structure:
└── kds_consulting_team-kds-team.ex-elasticsearch/
    ├── README.md
    ├── bitbucket-pipelines.yml
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── legacy_config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_test.sh
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── client/
    │   │   ├── es_client.py
    │   │   └── ssh_utils.py
    │   └── legacy_client/
    │       ├── legacy_es_client.py
    │       ├── result.py
    │       └── ssh_client.py
    └── tests/
        ├── __init__.py
        └── test_component.py

================================================
FILE: README.md
================================================
# Elasticsearch Extractor

Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.

The component allows to download data from indexes in an Elasticseach engine directly to Keboola without complicated setup.

**Table of contents:**  
  
[TOC]

# Notes on functionality

The extractor utilizes Elasticsearch [Search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html) to download the data from an index. Users are able to define their own request by specifying a JSON request body, which will be appended to a request. For all allowed request body specifications, please refer to [Request Body in Search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html#search-search-api-request-body) documentation.

# Configuration

The sample `/data` folder can be found in the [component's repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-elasticsearch/src/master/component_config/sample-config/). The [config.json](https://bitbucket.org/kds_consulting_team/kds-team.ex-elasticsearch/src/master/component_config/sample-config/config.json) file represents the configuration, that should be passed to the component in order for the component to run successfully.

In Keboola, the component is set up as a row-based component and thus certain parameters (SSH & DB settings) have to be configured only once, while index specific settings can be configured for each index separately.

### Database (`db`) settings

The database host and port needs to be provided to correctly connect to the engine and download index data.

Required parameters are:

- **Hostname** (`db.hostname`) - specifies the IP address or URL at which the database is located. NOTE: If you are using ssh tunnel with automatic port forwarding, you can use `localhost` as the hostname.
- **Port** (`db.port`) - specifies the accompanying port to the hostname.

The correct JSON specification of the database settings then takes the following form.

```json
{
  "db": {
      "hostname": "localhost",
      "port": 8080
    }
}
```

## Authentication methods

Elasticsearch extractor currently supports following authentication methods:
- **No auth**
- **Basic** - Username + password combination
- **API key**
- **SSH + Any method mentioned above** - You can use connection over shh tunnel and any of the above-mentioned methods.

**note: ** You also have to specify scheme elasticsearch parameter, which can be either http or https.

### SSH (`ssh`) settings

Connection to the Elasticsearch instance via an SSH server is supported by the extractor

Required parameters for SSH section of the configuration are:

- **SSH Hostname** - SSH host, to which a connection shall be made. 
- **SSH Port**
- **SSH Username**  - A user, which will be used for SSH authentication.
- **SSH Private Key** - An SSH private key in RSA format.

The final SSH configuration should then look like the one below.

```json
  "ssh_options": {
    "enabled": true,
    "keys": {
      "public": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQCn4a...== dominik_novotny@keboola.com",
      "#private": "-----BEGIN OPENSSH PRIVATE KEY-----\nxxxxxxxx\n-----END OPENSSH PRIVATE KEY-----"
    },
        "sshHost": "66.66.66.142",
        "user": "dominik_novotny_keboola_com",
        "sshPort": 22
  }
```

*Note:* If you're using a predefined JSON configuration schema, the new lines in SSH private key will be automatically replaced by `\n`. However, if you're using the raw JSON to configure the component, you need to escape all new lines by `\n`, in order to inject the private key into the configuration properly.

*Note:* For local bind port, the port of target database will be used.


## Row (index) configuration

Index configuration is tied to a specific index you'd like to download. Users are able to configure the extraction according to their needs by specifying a request body, which will be sent along with the request. Additionally, a `{{date}}` placeholder can be used for a specified date to be injected into an index name (please see **Date** section for more information).

### Index Name (`index_name`)

The `index_name` parameter specifies the name of the index in an Elasticsearch index, which will be downloaded. [Search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html) is utilized to download all data from an index. You can use `{{date}}` placeholder to be replaced by settings specified in the placeholder settings section.

### Request Body (`request_body`)

In `request_body`, users are able to specify their custom JSON request body, which will be sent along with a request. For a list of all available attributes, which can be specified in the request body, please see [Request body in Search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html) documentation.

It's also possible to specify `size` and `scroll` parameters, to control size of the returned page and length of its availability. If `size` or `scroll` are not specified, default values are used for either of the parameters.

An example of specifying a request body may be shown by using the `_source` parameter to only extract requested fields. The request body would then take the following form:

```json
{
    "_source": [
        "_id",
        "_index",
        "_score",
        "_type",
        "click.clicked_at",
        "click.result.display_text",
        "click.result.serp_position",
        "click.result.uri",
        "event",
        "market",
        "offset",
        "query.current_value",
        "query.entered_at",
        "serp.displayed_at",
        "session_id",
        "user_hash"
    ]
}
```

### Date Placeholder Replacement (`date`)

A date placeholder `{{date}}` can be used in specifying an index name. This is especially useful if name of your index changes each day (e.g. data for each day are stored in a separate index).

The date placeholder will be automatically replaced based on the specification of the parameters below.

Parameters:

- **Date Shift** (`shift`) - A date in absolute (`YYYY-MM-DD`) format, or relative format (e.g. today, yesterday, 3 days ago, etc.), specifying by which date the placeholder will be replaced.
- **Date Format** (`format`) - Date format, which will replace the date placeholder. Accepted formats are listed in [Python strftime documentation](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes).
- **Time Zone** (`time_zone`) - A time zone, at which the date replacement will be evaluated. Accepted format is any standard [DB timezone specification](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List).


### Output Table Name (`storage_table`)

Name of the output table, under which the downloaded index will be stored in Keboola storage.

### Primary Keys (`primary_keys`)

An array of columns, specifying a primary key for the storage table inside Keboola.

### Load Type (`incremental`)

Specifies, whether to use incremental load (`true`) or full load (`false`).


## Development

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path in the docker-compose file:

```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone repo_path my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/) 


================================================
FILE: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG


  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - echo 'Pushing test image to repo. [tag=test]'
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "3"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
    deploy:
      resources:
        limits:
          memory: 256m

  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh



================================================
FILE: Dockerfile
================================================
FROM python:3.11.7-slim
ENV PYTHONIOENCODING utf-8

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY requirements.txt /code/requirements.txt
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential curl

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example,
    venv,
    es_builds
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: requirements.txt
================================================
keboola.component==1.4.3
keboola.csvwriter
keboola.json-to-csv
elasticsearch==8.10.1
dateparser
paramiko==3.4.0
retry
furl
mock
freezegun
https://bitbucket.org/kds_consulting_team/datadirtest/get/1.5.1.zip#egg=datadirtest
pytest
faker
cryptography==36.0.2 # to supress Blowfish warnings from Paramiko
sshtunnel==0.4.0


================================================
FILE: component_config/component_long_description.md
================================================
# Elasticsearch Extractor

Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.

The component allows to download data from indexes in an Elasticseach engine directly to Keboola without complicated setup.


# Notes on functionality

The extractor utilizes Elasticsearch [Search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html) to download the data from an index. Users are able to define their own request by specifying a JSON request body, which will be appended to a request. For all allowed request body specifications, please refer to [Request Body in Search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html#search-search-api-request-body) documentation.


================================================
FILE: component_config/component_short_description.md
================================================
Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.


================================================
FILE: component_config/configRowSchema.json
================================================
{
    "title": "Index",
    "type": "object",
    "required": [
        "date",
        "storage_table",
        "primary_keys",
        "incremental"
    ],
    "properties": {
        "index_name": {
            "title": "Index Name",
            "description": "Specify a name of the index, which will be downloaded from Elasticsearch instance. Use placeholder <strong>{{date}}</strong> to replace the placeholder with date value.",
            "type": "string",
            "propertyOrder": 100
        },
        "request_body": {
            "title": "Query",
            "description": "Query (in JSON), which will be sent together with a request. Allows to specify returned columns and other useful parameters. See <a href='https://www.elastic.co/guide/en/elasticsearch/reference/7.x/search-search.html#search-search-api-request-body' target='_blank'>Search API documentation</a> for all possible attributes.</br>You can also specify <a href='https://www.elastic.co/guide/en/elasticsearch/reference/current/scroll-api.html#scroll-api-query-params' target='_blank'><scroll>scroll</scroll></a> parameter to manage scroll length.",
            "type": "string",
            "format": "textarea",
            "options": {
                "input_height": "150px"
            },
            "propertyOrder": 200
        },
        "date": {
            "title": "Date Placeholder Replacement",
            "description": "If placeholder <strong>{{date}}</strong> is used in the index name, it will be automatically replaced by settings specified below.",
            "type": "object",
            "required": [
                "shift",
                "format",
                "time_zone"
            ],
            "propertyOrder": 300,
            "format": "grid-strict",
            "properties": {
                "shift": {
                    "title": "Date Shift",
                    "description": "Specify date in absolute format (YYYY-MM-DD) or relative format (yesterday, 3 days ago, etc.).",
                    "type": "string",
                    "propertyOrder": 200,
                    "options": {
                        "grid_columns": 6
                    },
                    "default": "yesterday"
                },
                "format": {
                    "title": "Date Format",
                    "description": "Specifies the date format, in which the date placeholder will be replaced. See <a href='https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes' target='_blank'>strftime</a> documentation for supported formats.",
                    "type": "string",
                    "default": "%Y-%m-%d",
                    "propertyOrder": 300,
                    "options": {
                        "grid_columns": 6
                    }
                },
                "time_zone": {
                    "title": "Time Zone",
                    "type": "string",
                    "description": "Specifies the time zone, at which the replacement date should be evaluated.</br>If not specified, defaults to \"UTC\".",
                    "enum": ["Africa/Abidjan", "Africa/Accra", "Africa/Algiers", "Africa/Bissau", "Africa/Cairo", "Africa/Casablanca", "Africa/Ceuta", "Africa/El_Aaiun", "Africa/Johannesburg", "Africa/Juba", "Africa/Khartoum", "Africa/Lagos", "Africa/Maputo", "Africa/Monrovia", "Africa/Nairobi", "Africa/Ndjamena", "Africa/Sao_Tome", "Africa/Tripoli", "Africa/Tunis", "Africa/Windhoek", "America/Adak", "America/Anchorage", "America/Araguaina", "America/Argentina/Buenos_Aires", "America/Argentina/Catamarca", "America/Argentina/Cordoba", "America/Argentina/Jujuy", "America/Argentina/La_Rioja", "America/Argentina/Mendoza", "America/Argentina/Rio_Gallegos", "America/Argentina/Salta", "America/Argentina/San_Juan", "America/Argentina/San_Luis", "America/Argentina/Tucuman", "America/Argentina/Ushuaia", "America/Asuncion", "America/Atikokan", "America/Bahia", "America/Bahia_Banderas", "America/Barbados", "America/Belem", "America/Belize", "America/Blanc-Sablon", "America/Boa_Vista", "America/Bogota", "America/Boise", "America/Cambridge_Bay", "America/Campo_Grande", "America/Cancun", "America/Caracas", "America/Cayenne", "America/Chicago", "America/Chihuahua", "America/Costa_Rica", "America/Creston", "America/Cuiaba", "America/Curacao", "America/Danmarkshavn", "America/Dawson", "America/Dawson_Creek", "America/Denver", "America/Detroit", "America/Edmonton", "America/Eirunepe", "America/El_Salvador", "America/Fort_Nelson", "America/Fortaleza", "America/Glace_Bay", "America/Goose_Bay", "America/Grand_Turk", "America/Guatemala", "America/Guayaquil", "America/Guyana", "America/Halifax", "America/Havana", "America/Hermosillo", "America/Indiana/Indianapolis", "America/Indiana/Knox", "America/Indiana/Marengo", "America/Indiana/Petersburg", "America/Indiana/Tell_City", "America/Indiana/Vevay", "America/Indiana/Vincennes", "America/Indiana/Winamac", "America/Inuvik", "America/Iqaluit", "America/Jamaica", "America/Juneau", "America/Kentucky/Louisville", "America/Kentucky/Monticello", "America/La_Paz", "America/Lima", "America/Los_Angeles", "America/Maceio", "America/Managua", "America/Manaus", "America/Martinique", "America/Matamoros", "America/Mazatlan", "America/Menominee", "America/Merida", "America/Metlakatla", "America/Mexico_City", "America/Miquelon", "America/Moncton", "America/Monterrey", "America/Montevideo", "America/Nassau", "America/New_York", "America/Nipigon", "America/Nome", "America/Noronha", "America/North_Dakota/Beulah", "America/North_Dakota/Center", "America/North_Dakota/New_Salem", "America/Nuuk", "America/Ojinaga", "America/Panama", "America/Pangnirtung", "America/Paramaribo", "America/Phoenix", "America/Port-au-Prince", "America/Port_of_Spain", "America/Porto_Velho", "America/Puerto_Rico", "America/Punta_Arenas", "America/Rainy_River", "America/Rankin_Inlet", "America/Recife", "America/Regina", "America/Resolute", "America/Rio_Branco", "America/Santarem", "America/Santiago", "America/Santo_Domingo", "America/Sao_Paulo", "America/Scoresbysund", "America/Sitka", "America/St_Johns", "America/Swift_Current", "America/Tegucigalpa", "America/Thule", "America/Thunder_Bay", "America/Tijuana", "America/Toronto", "America/Vancouver", "America/Whitehorse", "America/Winnipeg", "America/Yakutat", "America/Yellowknife", "Antarctica/Casey", "Antarctica/Davis", "Antarctica/DumontDUrville", "Antarctica/Macquarie", "Antarctica/Mawson", "Antarctica/Palmer", "Antarctica/Rothera", "Antarctica/Syowa", "Antarctica/Troll", "Antarctica/Vostok", "Asia/Almaty", "Asia/Amman", "Asia/Anadyr", "Asia/Aqtau", "Asia/Aqtobe", "Asia/Ashgabat", "Asia/Atyrau", "Asia/Baghdad", "Asia/Baku", "Asia/Bangkok", "Asia/Barnaul", "Asia/Beirut", "Asia/Bishkek", "Asia/Brunei", "Asia/Chita", "Asia/Choibalsan", "Asia/Colombo", "Asia/Damascus", "Asia/Dhaka", "Asia/Dili", "Asia/Dubai", "Asia/Dushanbe", "Asia/Famagusta", "Asia/Gaza", "Asia/Hebron", "Asia/Ho_Chi_Minh", "Asia/Hong_Kong", "Asia/Hovd", "Asia/Irkutsk", "Asia/Jakarta", "Asia/Jayapura", "Asia/Jerusalem", "Asia/Kabul", "Asia/Kamchatka", "Asia/Karachi", "Asia/Kathmandu", "Asia/Khandyga", "Asia/Kolkata", "Asia/Krasnoyarsk", "Asia/Kuala_Lumpur", "Asia/Kuching", "Asia/Macau", "Asia/Magadan", "Asia/Makassar", "Asia/Manila", "Asia/Nicosia", "Asia/Novokuznetsk", "Asia/Novosibirsk", "Asia/Omsk", "Asia/Oral", "Asia/Pontianak", "Asia/Pyongyang", "Asia/Qatar", "Asia/Qostanay", "Asia/Qyzylorda", "Asia/Riyadh", "Asia/Sakhalin", "Asia/Samarkand", "Asia/Seoul", "Asia/Shanghai", "Asia/Singapore", "Asia/Srednekolymsk", "Asia/Taipei", "Asia/Tashkent", "Asia/Tbilisi", "Asia/Tehran", "Asia/Thimphu", "Asia/Tokyo", "Asia/Tomsk", "Asia/Ulaanbaatar", "Asia/Urumqi", "Asia/Ust-Nera", "Asia/Vladivostok", "Asia/Yakutsk", "Asia/Yangon", "Asia/Yekaterinburg", "Asia/Yerevan", "Atlantic/Azores", "Atlantic/Bermuda", "Atlantic/Canary", "Atlantic/Cape_Verde", "Atlantic/Faroe", "Atlantic/Madeira", "Atlantic/Reykjavik", "Atlantic/South_Georgia", "Atlantic/Stanley", "Australia/Adelaide", "Australia/Brisbane", "Australia/Broken_Hill", "Australia/Darwin", "Australia/Eucla", "Australia/Hobart", "Australia/Lindeman", "Australia/Lord_Howe", "Australia/Melbourne", "Australia/Perth", "Australia/Sydney", "Etc/GMT", "Etc/GMT+1", "Etc/GMT+10", "Etc/GMT+11", "Etc/GMT+12", "Etc/GMT+2", "Etc/GMT+3", "Etc/GMT+4", "Etc/GMT+5", "Etc/GMT+6", "Etc/GMT+7", "Etc/GMT+8", "Etc/GMT+9", "Etc/GMT-1", "Etc/GMT-10", "Etc/GMT-11", "Etc/GMT-12", "Etc/GMT-13", "Etc/GMT-14", "Etc/GMT-2", "Etc/GMT-3", "Etc/GMT-4", "Etc/GMT-5", "Etc/GMT-6", "Etc/GMT-7", "Etc/GMT-8", "Etc/GMT-9", "UTC", "Europe/Amsterdam", "Europe/Andorra", "Europe/Astrakhan", "Europe/Athens", "Europe/Belgrade", "Europe/Berlin", "Europe/Brussels", "Europe/Bucharest", "Europe/Budapest", "Europe/Chisinau", "Europe/Copenhagen", "Europe/Dublin", "Europe/Gibraltar", "Europe/Helsinki", "Europe/Istanbul", "Europe/Kaliningrad", "Europe/Kiev", "Europe/Kirov", "Europe/Lisbon", "Europe/London", "Europe/Luxembourg", "Europe/Madrid", "Europe/Malta", "Europe/Minsk", "Europe/Monaco", "Europe/Moscow", "Europe/Oslo", "Europe/Paris", "Europe/Prague", "Europe/Riga", "Europe/Rome", "Europe/Samara", "Europe/Saratov", "Europe/Simferopol", "Europe/Sofia", "Europe/Stockholm", "Europe/Tallinn", "Europe/Tirane", "Europe/Ulyanovsk", "Europe/Uzhgorod", "Europe/Vienna", "Europe/Vilnius", "Europe/Volgograd", "Europe/Warsaw", "Europe/Zaporozhye", "Europe/Zurich", "Factory", "Indian/Chagos", "Indian/Christmas", "Indian/Cocos", "Indian/Kerguelen", "Indian/Mahe", "Indian/Maldives", "Indian/Mauritius", "Indian/Reunion", "Pacific/Apia", "Pacific/Auckland", "Pacific/Bougainville", "Pacific/Chatham", "Pacific/Chuuk", "Pacific/Easter", "Pacific/Efate", "Pacific/Enderbury", "Pacific/Fakaofo", "Pacific/Fiji", "Pacific/Funafuti", "Pacific/Galapagos", "Pacific/Gambier", "Pacific/Guadalcanal", "Pacific/Guam", "Pacific/Honolulu", "Pacific/Kiritimati", "Pacific/Kosrae", "Pacific/Kwajalein", "Pacific/Majuro", "Pacific/Marquesas", "Pacific/Nauru", "Pacific/Niue", "Pacific/Norfolk", "Pacific/Noumea", "Pacific/Pago_Pago", "Pacific/Palau", "Pacific/Pitcairn", "Pacific/Pohnpei", "Pacific/Port_Moresby", "Pacific/Rarotonga", "Pacific/Tahiti", "Pacific/Tarawa", "Pacific/Tongatapu", "Pacific/Wake", "Pacific/Wallis"],
                    "default": "UTC",
                    "options": {
                        "grid_columns": 12
                    }
                }

            }
        },
        "storage_table": {
            "title": "Output Table Name",
            "description": "Name of the output table in storage.",
            "type": "string",
            "propertyOrder": 400
        },
        "primary_keys": {
            "title": "Primary Keys",
            "description": "Specify primary keys for the storage table.",
            "type": "array",
            "format": "select",
            "items": {
                  "type": "string"
                },
            "options": {
                  "tags": true
                },
            "propertyOrder": 500,
            "uniqueItems": true
        },
        "incremental": {
            "title": "Load Type",
            "description": "Whether to load data into storage incrementally, or utilizing the full load.",
            "type": "boolean",
            "default": true,
            "propertyOrder": 600,
            "options": {
                "enum_titles":[
                    "Incremental Load",
                    "Full Load"
                ]
            }
        }
    }
}


================================================
FILE: component_config/configSchema.json
================================================
{
  "title": "Authorization",
  "type": "object",
  "required": [
    "db"
  ],
  "properties": {
    "db": {
      "title": "Database",
      "type": "object",
      "required": [
        "hostname",
        "port"
      ],
      "propertyOrder": 1,
      "properties": {
        "hostname": {
          "title": "Hostname",
          "type": "string",
          "propertyOrder": 10
        },
        "port": {
          "title": "Port",
          "type": "integer",
          "propertyOrder": 20,
          "default": 9200
        }
      }
    },
    "authentication": {
      "title": "Authentication",
      "propertyOrder": 2,
      "properties": {
        "auth_type": {
          "title": "Authentication Type",
          "type": "string",
          "enum": [
            "basic",
            "api_key",
            "no_auth"
          ],
          "default": "no_auth",
          "options": {
            "enum_titles": [
              "Basic",
              "API key",
              "No Authentication"
            ]
          },
          "propertyOrder": 1
        },
        "username": {
          "title": "Username",
          "type": "string",
          "options": {
            "dependencies": {
              "auth_type": "basic"
            }
          },
          "propertyOrder": 2
        },
        "#password": {
          "title": "Password",
          "type": "string",
          "format": "password",
          "options": {
            "dependencies": {
              "auth_type": "basic"
            }
          },
          "propertyOrder": 3
        },
        "api_key_id": {
          "title": "API Key ID",
          "type": "string",
          "options": {
            "dependencies": {
              "auth_type": "api_key"
            }
          },
          "propertyOrder": 4
        },
        "#api_key": {
          "title": "API Key",
          "type": "string",
          "format": "password",
          "options": {
            "dependencies": {
              "auth_type": "api_key"
            }
          },
          "propertyOrder": 5
        }
      }
    },
    "scheme": {
      "title": "Scheme",
      "type": "string",
      "enum": [
        "http",
        "https"
      ],
      "default": "http",
      "propertyOrder": 4
    },
        "ssh_options": {
          "type": "object",
          "format": "ssh-editor",
          "propertyOrder": 5
        }
  }
}


================================================
FILE: component_config/configuration_description.md
================================================
The sample `/data` folder can be found in the [component's repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-elasticsearch/src/master/component_config/sample-config/). The [config.json](https://bitbucket.org/kds_consulting_team/kds-team.ex-elasticsearch/src/master/component_config/sample-config/config.json) file represents the configuration, that should be passed to the component in order for the component to run successfully.

In Keboola, the component is set up as a row-based component and thus certain parameters (SSH & DB settings) have to be configured only once, while index specific settings can be configured for each index separately.

## Database and SSH Settings

Elasticsearch extractor currently supports only connection to the Elasticsearch instance over SSH tunnel. For successful connection, all database and SSH properties must be configured.

### Database (`db`) settings

The database host and port need to be provided to correctly connect to the engine and download index data.

Required parameters are:

- **Hostname** (`db.hostname`) - specifies the IP address or URL at which the database is located;
- **Port** (`db.port`) - specifies the accompanying port to the hostname.

The correct JSON specification of the database settings then takes the following form.

```json
{
  ...
  "db": {
      "hostname": "127.0.0.1",
      "port": 8080
    }
  ...
}
```

### SSH (`ssh`) settings

Connection to the Elasticsearch instance via an SSH server is supported by the extractor

Required parameters for SSH section of the configuration are:

- **SSH Hostname** (`ssh.hostname`) - a SSH host, to which a connection shall be made;
- **SSH Port** (`ssh.port`) - an accompanying SSH port to `ssh.hostname`;
- **SSH Username** (`ssh.username`) - a user, which will be used for SSH authentication;
- **SSH Private Key** (`ssh.#private_key`) - an SSH private key.

The final SSH configuration should then look like the one below.

```json
{
  ...
  "ssh": {
      "hostname": "ssh-host-url.cz",
      "port": 22,
      "username": "user-ssh",
      "#private_key": "-----BEGIN OPENSSH PRIVATE KEY-----\nENCRYPTED\nSSH\nKEY\n-----END OPENSSH PRIVATE KEY-----"
    }
  ...
}
```

*Note:* If you're using a predefined JSON configuration schema, the new lines in SSH private key will be automatically replaced by `\n`. However, if you're using the raw JSON to configure the component, you need to escape all new lines by `\n`, in order to inject the private key into the configuration properly.


## Row (index) configuration

Index configuration is tied to a specific index you'd like to download. Users are able to configure the extraction according to their needs by specifying a request body, which will be sent along with the request. Additionally, a `{{date}}` placeholder can be used for a specified date to be injected into an index name (please see **Date** section for more information).

### Index Name (`index_name`)

The `index_name` parameter specifies the name of the index in an Elasticsearch index, which will be downloaded. [Search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html) is utilized to download all data from an index. You can use `{{date}}` placeholder to be replaced by settings specified in the placeholder settings section.

### Request Body (`request_body`)

In `request_body`, users are able to specify their custom JSON request body, which will be sent along with a request. For a list of all available attributes, which can be specified in the request body, please see [Request body in Search API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html) documentation.

It's also possible to specify `size` and `scroll` parameters, to control size of the returned page and length of its availability. If `size` or `scroll` are not specified, default values are used for either of the parameters.

An example of sepcifying a request body may be shown by using the `_source` parameter to only extract requested fields. The request body would then take the following form:

```json
{
    "_source": [
        "_id",
        "_index",
        "_score",
        "_type",
        "click.clicked_at",
        "click.result.display_text",
        "click.result.serp_position",
        "click.result.uri",
        "event",
        "market",
        "offset",
        "query.current_value",
        "query.entered_at",
        "serp.displayed_at",
        "session_id",
        "user_hash"
    ]
}
```

### Date Placeholder Replacement (`date`)

A date placeholder `{{date}}` can be used in specifying an index name. This is especially useful if name of your index changes each day (e.g. data for each day are stored in a separate index).

The date placeholder will be automatically replaced based on the specification of the parameters below.

Parameters:

- **Date Shift** (`shift`) - a date in absolute (`YYYY-MM-DD`) format, or relative format (e.g. today, yesterday, 3 days ago, etc.), specifying by which date the placeholder will be replaced;
- **Date Format** (`format`) - the format of date, which will replace the date placeholder. Accepted formats are listed in [Python strftime documentation](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes).
- **Time Zone** (`time_zone`) - a time zone, at which the date replacement will be evaluated. Accepted format is any standard [DB timezone specification](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List).


### Output Table Name (`storage_table`)

Name of the output table, under which the downloaded index will be stored in Keboola storage.

### Primary Keys (`primary_keys`)

An array of columns, specifying a primary key for the storage table inside Keboola.

### Load Type (`incremental`)

Specifies, whether to use incremental load (`true`) or full load (`false`).


================================================
FILE: component_config/stack_parameters.json
================================================
{}


================================================
FILE: component_config/sample-config/legacy_config.json
================================================
{
  "parameters": {
    "ssh": {
      "hostname": "ssh-host-url.cz",
      "port": 22,
      "username": "user-ssh",
      "#private_key": "-----BEGIN OPENSSH PRIVATE KEY-----\nENCRYPTED\nSSH\nKEY\n-----END OPENSSH PRIVATE KEY-----"
    },
    "index_name": "search_analytics-{{date}}",
    "request_body": "{\"_source\":[\"_id\",\"_index\",\"_score\",\"_type\",\"click.clicked_at\",\"click.result.display_text\",\"click.result.serp_position\",\"click.result.uri\",\"event\",\"market\",\"offset\",\"query.current_value\",\"query.entered_at\",\"serp.displayed_at\",\"session_id\",\"user_hash\"]}",
    "date": {
      "append_date": true,
      "format": "%Y.%m.%d",
      "shift": "yesterday"
    },
    "primary_keys": ["id", "index"],
    "incremental": true,
    "storage_table": "search_analytics",
    "db": {
      "hostname": "127.0.0.1",
      "port": 8080
    },
    "debug": true
  },
  "image_parameters": {}
}


================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
FILE: src/component.py
================================================
import json
import logging
import uuid
import os
import shutil
import dateparser
import pytz

from keboola.component.base import ComponentBase
from keboola.component.exceptions import UserException
from keboola.csvwriter import ElasticDictWriter

from client.es_client import ElasticsearchClient
from legacy_client.legacy_es_client import LegacyClient
from client.ssh_utils import SomeSSHException, get_private_key
from sshtunnel import SSHTunnelForwarder, BaseSSHTunnelForwarderError

# configuration variables
KEY_GROUP_DB = 'db'
KEY_DB_HOSTNAME = 'hostname'
KEY_DB_PORT = 'port'
KEY_QUERY = 'request_body'  # this is named like this for backwards compatibility
KEY_INDEX_NAME = 'index_name'
KEY_STORAGE_TABLE = 'storage_table'
KEY_PRIMARY_KEYS = 'primary_keys'
KEY_INCREMENTAL = 'incremental'
KEY_GROUP_AUTH = 'authentication'
KEY_AUTH_TYPE = 'auth_type'
KEY_USERNAME = 'username'
KEY_PASSWORD = '#password'
KEY_API_KEY_ID = 'api_key_id'
KEY_API_KEY = '#api_key'
KEY_BEARER = '#bearer'
KEY_SCHEME = 'scheme'

KEY_GROUP_DATE = 'date'
KEY_DATE_APPEND = 'append_date'
KEY_DATE_FORMAT = 'format'
KEY_DATE_SHIFT = 'shift'
KEY_DATE_TZ = 'time_zone'
DATE_PLACEHOLDER = '{{date}}'
DEFAULT_DATE = 'yesterday'
DEFAULT_DATE_FORMAT = '%Y-%m-%d'
DEFAULT_TZ = 'UTC'

KEY_SSH = "ssh_options"
KEY_USE_SSH = "enabled"
KEY_SSH_KEYS = "keys"
KEY_SSH_PRIVATE_KEY = "#private"
KEY_SSH_USERNAME = "user"
KEY_SSH_TUNNEL_HOST = "sshHost"
KEY_SSH_TUNNEL_PORT = "sshPort"

LOCAL_BIND_ADDRESS = "127.0.0.1"

KEY_LEGACY_SSH = 'ssh'

REQUIRED_PARAMETERS = [KEY_GROUP_DB]

RSA_HEADER = "-----BEGIN RSA PRIVATE KEY-----"


class Component(ComponentBase):

    def __init__(self):
        super().__init__()

    def run(self):
        self.validate_configuration_parameters(REQUIRED_PARAMETERS)
        params = self.configuration.parameters

        if params.get(KEY_LEGACY_SSH):
            self.run_legacy_client()
        else:
            out_table_name = params.get(KEY_STORAGE_TABLE, False)
            if not out_table_name:
                out_table_name = "ex-elasticsearch-result"
                logging.info(f"Using default output table name: {out_table_name}")

            user_defined_pk = params.get(KEY_PRIMARY_KEYS, [])
            incremental = params.get(KEY_INCREMENTAL, False)

            index_name, query = self.parse_index_parameters(params)
            statefile = self.get_state_file()

            ssh_options = params.get(KEY_SSH)
            # fix eternal KBC issue
            if not isinstance(ssh_options, list):
                if ssh_options.get(KEY_USE_SSH, False):
                    self._create_and_start_ssh_tunnel(params)

            client = self.get_client(params)

            temp_folder = os.path.join(self.data_folder_path, "temp")
            os.makedirs(temp_folder, exist_ok=True)

            columns = statefile.get(out_table_name, [])
            out_table = self.create_out_table_definition(out_table_name, primary_key=user_defined_pk,
                                                         incremental=incremental)

            try:
                with ElasticDictWriter(out_table.full_path, columns) as wr:
                    for result in client.extract_data(index_name, query):
                        wr.writerow(result)
                    wr.writeheader()
            except Exception as e:
                raise UserException(f"Error occured while extracting data from Elasticsearch: {e}")
            finally:
                if hasattr(self, 'ssh_tunnel') and self.ssh_tunnel.is_active:
                    self.ssh_tunnel.stop()

            self.write_manifest(out_table)
            statefile[out_table_name] = wr.fieldnames
            self.write_state_file(statefile)
            self.cleanup(temp_folder)

    @staticmethod
    def run_legacy_client() -> None:
        client = LegacyClient()
        client.run()

    @staticmethod
    def cleanup(temp_folder: str):
        shutil.rmtree(temp_folder)

    def get_client(self, params: dict) -> ElasticsearchClient:
        auth_params = params.get(KEY_GROUP_AUTH)
        if not auth_params:
            return self.get_client_legacy(params)

        db_params = params.get(KEY_GROUP_DB)
        db_hostname = db_params.get(KEY_DB_HOSTNAME)
        db_port = db_params.get(KEY_DB_PORT)
        scheme = params.get(KEY_SCHEME, "http")

        auth_type = auth_params.get(KEY_AUTH_TYPE, False)
        if auth_type not in ["basic", "api_key", "bearer", "no_auth"]:
            raise UserException(f"Invalid auth_type: {auth_type}")

        setup = {"host": db_hostname, "port": db_port, "scheme": scheme}

        logging.info(f"The component will use {auth_type} type authorization.")

        if auth_type == "basic":
            username = auth_params.get(KEY_USERNAME)
            password = auth_params.get(KEY_PASSWORD)

            if not (username and password):
                raise UserException("You must specify both username and password for basic type authorization")

            auth = (username, password)
            client = ElasticsearchClient([setup], scheme, http_auth=auth)

        elif auth_type == "api_key":
            api_key_id = auth_params.get(KEY_API_KEY_ID)
            api_key = auth_params.get(KEY_API_KEY)
            api_key = (api_key_id, api_key)
            client = ElasticsearchClient([setup], scheme, api_key=api_key)

        elif auth_type == "no_auth":
            client = ElasticsearchClient([setup], scheme)

        else:
            raise UserException(f"Unsupported auth_type: {auth_type}")

        try:
            p = client.ping(error_trace=True)
            if not p:
                raise UserException(f"Connection to Elasticsearch instance {db_hostname}:{db_port} failed.")
        except Exception as e:
            raise UserException(f"Connection to Elasticsearch instance {db_hostname}:{db_port} failed. {str(e)}")

        return client

    @staticmethod
    def get_client_legacy(params) -> ElasticsearchClient:
        db_params = params.get(KEY_GROUP_DB)
        db_hostname = db_params.get(KEY_DB_HOSTNAME)
        db_port = db_params.get(KEY_DB_PORT)

        setup = {"host": db_hostname, "port": db_port, "scheme": "http"}
        client = ElasticsearchClient([setup])

        return client

    def parse_index_parameters(self, params):
        index = params.get(KEY_INDEX_NAME, "")
        date_config = params.get(KEY_GROUP_DATE, {})
        query = self._parse_query(params)

        if DATE_PLACEHOLDER in index:
            index = self._replace_date_placeholder(index, date_config)

        return index, query

    @staticmethod
    def _parse_query(params):
        _query = params.get(KEY_QUERY, '{}').strip()
        query_string = _query if _query != '' else '{}'

        try:
            logging.info(f"Using query: {query_string}")
            return json.loads(query_string)
        except ValueError:
            raise UserException("Could not parse request body string to JSON.")

    def _replace_date_placeholder(self, index, date_config):
        _date = dateparser.parse(date_config.get(KEY_DATE_SHIFT, DEFAULT_DATE))
        if _date is None:
            raise UserException(f"Could not parse value {date_config[KEY_DATE_SHIFT]} to date.")

        _date = _date.replace(tzinfo=pytz.UTC)
        _tz = self._validate_timezone(date_config.get(KEY_DATE_TZ, DEFAULT_TZ))
        _date_tz = pytz.timezone(_tz).normalize(_date)
        _date_formatted = _date_tz.strftime(date_config.get(KEY_DATE_FORMAT, DEFAULT_DATE_FORMAT))

        logging.info(f"Replaced date placeholder with value {_date_formatted}. "
                     f"Downloading data from index {index.replace(DATE_PLACEHOLDER, _date_formatted)}.")
        return index.replace(DATE_PLACEHOLDER, _date_formatted)

    @staticmethod
    def _validate_timezone(tz):
        if tz not in pytz.all_timezones:
            raise UserException(f"Incorrect timezone {tz} provided. Timezone must be a valid DB timezone name. "
                                "See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List.")
        return tz

    @staticmethod
    def _save_results(results: list, destination: str) -> None:
        full_path = os.path.join(destination, f"{uuid.uuid4()}.json")
        with open(full_path, "w") as json_file:
            json.dump(results, json_file, indent=4)

    def _create_and_start_ssh_tunnel(self, params) -> None:
        ssh_params = params.get(KEY_SSH)
        ssh_username = ssh_params.get(KEY_SSH_USERNAME)
        keys = ssh_params.get(KEY_SSH_KEYS)
        private_key = keys.get(KEY_SSH_PRIVATE_KEY)
        ssh_tunnel_host = ssh_params.get(KEY_SSH_TUNNEL_HOST)
        ssh_tunnel_port = ssh_params.get(KEY_SSH_TUNNEL_PORT, 22)
        db_params = params.get(KEY_GROUP_DB)
        db_hostname = db_params.get(KEY_DB_HOSTNAME)
        db_port = db_params.get(KEY_DB_PORT)
        self._create_ssh_tunnel(ssh_username, private_key, ssh_tunnel_host, ssh_tunnel_port,
                                db_hostname, db_port)

        try:
            self.ssh_server.start()
        except BaseSSHTunnelForwarderError as e:
            raise UserException(
                "Failed to establish SSH connection. Recheck all SSH configuration parameters") from e

        logging.info("SSH tunnel is enabled.")

    @staticmethod
    def is_valid_rsa(rsa_key) -> (bool, str):
        if not rsa_key.startswith(RSA_HEADER):
            return False, f"The RSA key does not start with the correct header: {RSA_HEADER}"
        if "\n" not in rsa_key:
            return False, "The RSA key does not contain any newline characters."
        return True, ""

    def _create_ssh_tunnel(self, ssh_username, private_key, ssh_tunnel_host, ssh_tunnel_port,
                           db_hostname, db_port) -> None:

        is_valid, error_message = self.is_valid_rsa(private_key)
        if is_valid:
            logging.info("SSH tunnel is enabled.")
        else:
            raise UserException(f"Invalid RSA key provided: {error_message}")

        try:
            private_key = get_private_key(private_key, None)
        except SomeSSHException as e:
            raise UserException(e) from e

        try:
            db_port = int(db_port)
        except ValueError as e:
            raise UserException("Remote port must be a valid integer") from e

        self.ssh_server = SSHTunnelForwarder(ssh_address_or_host=ssh_tunnel_host,
                                             ssh_port=ssh_tunnel_port,
                                             ssh_pkey=private_key,
                                             ssh_username=ssh_username,
                                             remote_bind_address=(db_hostname, db_port),
                                             local_bind_address=(LOCAL_BIND_ADDRESS, db_port),
                                             ssh_config_file=None,
                                             allow_agent=False)


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)



================================================
FILE: src/client/es_client.py
================================================
import json
import typing as t
from typing import Iterable

from elasticsearch import Elasticsearch
from elasticsearch.exceptions import ApiError, TransportError

DEFAULT_SIZE = 10_000
SCROLL_TIMEOUT = '15m'


class ElasticsearchClientException(Exception):
    pass


class ElasticsearchClient(Elasticsearch):

    def __init__(self, hosts: list, scheme: str = None, http_auth: tuple = None, api_key: tuple = None):
        options = {"hosts": hosts, "timeout": 30, "retry_on_timeout": True, "max_retries": 5}

        if scheme == "https":
            options.update({"verify_certs": False, "ssl_show_warn": False})

        if http_auth:
            options.update({"http_auth": http_auth})
        elif api_key:
            options.update({"api_key": api_key})

        super().__init__(**options)

    def extract_data(self, index_name: str, query: str) -> Iterable:
        """
        Extracts data from the specified Elasticsearch index based on the given query.

        Parameters:
            index_name (str): Name of the Elasticsearch index.
            query (dict): Elasticsearch DSL query.

        Yields:
            dict
        """
        response = self.search(index=index_name, size=DEFAULT_SIZE, scroll=SCROLL_TIMEOUT, body=query)
        for r in self._process_response(response):
            yield r

        while len(response['hits']['hits']):
            response = self.scroll(scroll_id=response["_scroll_id"], scroll=SCROLL_TIMEOUT)
            for r in self._process_response(response):
                yield r

    def _process_response(self, response: dict) -> Iterable:
        results = [hit["_source"] for hit in response['hits']['hits']]
        for result in results:
            yield self.flatten_json(result)

    def ping(
        self,
        *,
        error_trace: t.Optional[bool] = None,
        filter_path: t.Optional[t.Union[t.List[str], str]] = None,
        human: t.Optional[bool] = None,
        pretty: t.Optional[bool] = None,
    ) -> bool:
        """
        Returns True if a successful response returns from the info() API,
        otherwise returns False. This API call can fail either at the transport
        layer (due to connection errors or timeouts) or from a non-2XX HTTP response
        (due to authentication or authorization issues).

        If you want to discover why the request failed you should use the ``info()`` API.

        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html>`_
        """
        __path = "/"
        __query: t.Dict[str, t.Any] = {}
        if error_trace is not None:
            __query["error_trace"] = error_trace
        if filter_path is not None:
            __query["filter_path"] = filter_path
        if human is not None:
            __query["human"] = human
        if pretty is not None:
            __query["pretty"] = pretty
        __headers = {"accept": "application/json"}
        try:
            self.perform_request("HEAD", __path, params=__query, headers=__headers)
            return True
        except (ApiError, TransportError) as e:
            raise ElasticsearchClientException(e)

    def flatten_json(self, x, out=None, name=''):
        if out is None:
            out = dict()
        if type(x) is dict:
            for a in x:
                self.flatten_json(x[a], out, name + a + '.')

        elif type(x) is list:
            out[name[:-1]] = json.dumps(x)

        else:
            out[name[:-1]] = x

        return out



================================================
FILE: src/client/ssh_utils.py
================================================
import base64
import binascii
import contextlib
import paramiko
from io import StringIO
from typing import Tuple


class SomeSSHException(Exception):
    pass


def get_private_key(input_key, private_key_password):
    key = _get_decoded_key(input_key)
    try:
        if private_key_password:
            return paramiko.RSAKey.from_private_key(StringIO(key), password=private_key_password)
        else:
            return paramiko.RSAKey.from_private_key(StringIO(key))
    except paramiko.ssh_exception.SSHException as pkey_error:
        raise SomeSSHException("Invalid private key") from pkey_error


def _get_decoded_key(input_key):
    """
        Have to satisfy both encoded and not encoded keys
    """
    b64_decoded_input_key = ""
    with contextlib.suppress(binascii.Error):
        b64_decoded_input_key = base64.b64decode(input_key, validate=True).decode('utf-8')

    is_valid_b64, message_b64 = validate_ssh_private_key(b64_decoded_input_key)
    is_valid, message = validate_ssh_private_key(input_key)
    if is_valid_b64:
        final_key = b64_decoded_input_key
    elif is_valid:
        final_key = input_key
    else:
        raise SomeSSHException("\n".join([message, message_b64]))
    return final_key


def validate_ssh_private_key(ssh_private_key: str) -> Tuple[bool, str]:
    if "\n" not in ssh_private_key:
        return False, "SSH Private key is invalid, make sure it \\n characters as new lines"
    return True, ""



================================================
FILE: src/legacy_client/legacy_es_client.py
================================================
import json
import logging
from dataclasses import dataclass
from keboola.csvwriter import ElasticDictWriter
import csv

import dateparser
import pytz
from keboola.component.base import ComponentBase
from keboola.component.exceptions import UserException

from legacy_client.ssh_client import SshClient
from legacy_client.result import Fetcher

KEY_INDEX_NAME = 'index_name'
KEY_REQUEST_BODY = 'request_body'
KEY_STORAGE_TABLE = 'storage_table'
KEY_INCREMENTAL = 'incremental'
KEY_PRIMARY_KEYS = 'primary_keys'

KEY_DATE = 'date'
KEY_DATE_APPEND = 'append_date'
KEY_DATE_FORMAT = 'format'
KEY_DATE_SHIFT = 'shift'
KEY_DATE_TZ = 'time_zone'

KEY_SSH = 'ssh'
KEY_SSH_USE = 'use_ssh'
KEY_SSH_HOST = 'hostname'
KEY_SSH_PORT = 'port'
KEY_SSH_USERNAME = 'username'
KEY_SSH_PKEY = '#private_key'

KEY_DB = 'db'
KEY_DB_HOST = 'hostname'
KEY_DB_PORT = 'port'

KEY_DEBUG = 'debug'

MANDATORY_PARAMS = [KEY_INDEX_NAME, KEY_DB, KEY_STORAGE_TABLE, KEY_SSH]


@dataclass
class SshTunnel:
    hostname: str
    port: int
    username: str
    key: str


@dataclass
class Database:
    host: str
    port: str


class LegacyClient(ComponentBase):
    BATCH_PROCESSING_SIZE = 100000

    def __init__(self):
        super().__init__()

        logging.info("Running legacy ssh client.")

        if self.configuration.parameters.get('debug', False) is True:
            logger = logging.getLogger()
            logger.setLevel(level='DEBUG')

        try:
            self.validate_configuration_parameters(MANDATORY_PARAMS)
        except ValueError as e:
            raise UserException(e)

        _db_object = self._parse_db_parameters()
        _ssh_object = self._parse_ssh_parameters()
        self.index, self.index_params = self._parse_index_parameters()

        self.client = SshClient(_ssh_object, _db_object)

        self.fetcher = Fetcher(self.tables_out_path, self.configuration.parameters[KEY_STORAGE_TABLE],
                               self.configuration.parameters.get(KEY_INCREMENTAL, True),
                               self.configuration.parameters.get(KEY_PRIMARY_KEYS, []))

    def _parse_ssh_parameters(self):

        ssh_config = self.configuration.parameters.get(KEY_SSH, {})

        if ssh_config == {}:  # or ssh_config.get(KEY_SSH_USE) is False:
            raise UserException("SSH configuration not specified.")
        else:
            try:
                ssh_object = SshTunnel(ssh_config[KEY_SSH_HOST], ssh_config[KEY_SSH_PORT],
                                       ssh_config[KEY_SSH_USERNAME], ssh_config[KEY_SSH_PKEY])
            except KeyError as e:
                raise UserException(f"Missing mandatory field {e} in SSH configuration.")

            return ssh_object

    def _parse_db_parameters(self):
        db_config = self.configuration.parameters[KEY_DB]
        try:
            db_object = Database(db_config[KEY_DB_HOST], db_config[KEY_DB_PORT])
        except KeyError as e:
            raise UserException(f"Missing mandatory field {e} in DB configuration.")
        return db_object

    def _parse_index_parameters(self):

        index = self.configuration.parameters[KEY_INDEX_NAME]
        date_config = self.configuration.parameters.get(KEY_DATE, {})

        _req_body = self.configuration.parameters.get(KEY_REQUEST_BODY, '{}').strip()
        request_body_string = _req_body if _req_body != '' else '{}'

        if '{{date}}' in index:
            _date = dateparser.parse(date_config.get(KEY_DATE_SHIFT, 'yesterday'))

            if _date is None:
                raise UserException(f"Could not parse value {date_config[KEY_DATE_SHIFT]} to date.")

            _date = _date.replace(tzinfo=pytz.UTC)

            _tz = date_config.get(KEY_DATE_TZ, 'UTC')

            if _tz not in pytz.all_timezones:
                raise UserException(f"Incorrect timezone {_tz} provided. Timezone must be a valid DB timezone name. "
                                    "See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List.")

            _date_tz = pytz.timezone(_tz).normalize(_date)
            _date_formatted = _date_tz.strftime(date_config.get(KEY_DATE_FORMAT, '%Y-%m-%d'))

            index = index.replace('{{date}}', _date_formatted)
            logging.info(f"Replaced date placeholder with value {_date_formatted}. " +
                         f"Downloading data from index {index}.")

        else:
            logging.info(f"No date placeholder found in index name {index}.")

        try:
            request_body = json.loads(request_body_string)
        except ValueError:
            raise UserException("Could not parse request body string to JSON.")

        return index, request_body

    def parse_curl_stdout(self, stdout):
        stdout_split = stdout.split('\r\n\r\n')
        rsp_status = stdout_split[0].split(' ')[1]
        rsp_body = stdout_split[1]
        return rsp_status, rsp_body

    def parse_scroll(self, scroll_response):

        try:
            scroll_json = json.loads(scroll_response)
        except ValueError as e:
            raise UserException(f"Could not parse JSON response - {e}.")

        return scroll_json.get('_scroll_id'), scroll_json['hits']['total'], scroll_json['hits']['hits']

    def run(self):

        previous_state = self.get_state_file()
        if previous_state:
            columns = previous_state.get("columns", [])
            logging.info(f"Using table columns from state file: {columns}")
        else:
            columns = []
        is_complete = False

        _fp_out, _fp_err = self.client.get_first_page(self.index, self.index_params)

        if _fp_out == '' and _fp_err != '':
            raise UserException(f"Could not download data. Error: {_fp_err}")

        elif _fp_out == '' and _fp_err == '':
            raise UserException("No data returned.")

        else:
            pass

        logging.debug("Parsing first page.")
        stdout_sc, stdout_body = self.parse_curl_stdout(_fp_out)

        if stdout_sc != '200':
            raise UserException(f"Could not download data. Error: {stdout_body}.")

        else:
            _scroll_id, _nr_results, _results = self.parse_scroll(stdout_body)

        logging.info(f"{_nr_results} rows will be downloaded from index {self.index}.")
        all_results = [self.fetcher.flatten_json(r) for r in _results]

        already_written = 0
        with ElasticDictWriter(self.fetcher.get_table_path(), fieldnames=columns, restval='',
                               quoting=csv.QUOTE_ALL, quotechar='\"') as wr:
            for row in self.fetcher.fetch_results(all_results):
                wr.writerow(row)

            already_written += len(_results)

            if len(_results) < self.client._default_size:
                is_complete = True

            while not is_complete:

                _scroll_out, _scroll_err = self.client.get_scroll(_scroll_id)

                if _scroll_out == '':
                    raise UserException(f"Could not download data for scroll {_scroll_id}.\n" +
                                        f"STDERR: {_scroll_err}.")

                else:
                    pass

                stdout_sc, stdout_body = self.parse_curl_stdout(_scroll_out)

                if stdout_sc != '200':
                    raise UserException(f"Could not download data. Error: {stdout_body}.")

                else:
                    _scroll_id, _, _results = self.parse_scroll(stdout_body)

                all_results = [self.fetcher.flatten_json(r) for r in _results]

                if len(_results) < self.client._default_size:
                    is_complete = True

                for row in self.fetcher.fetch_results(all_results):
                    wr.writerow(row)
                already_written += len(_results)

                if already_written % self.BATCH_PROCESSING_SIZE == 0:
                    logging.info(f"Parsed {already_written} results so far.")

        logging.info(f"Downloaded all data for index {self.index}. Parsed {already_written} rows.")
        if already_written > 0:
            self.fetcher.create_manifest(wr.fieldnames, self.fetcher.incremental, self.fetcher.primary_keys)
            self.write_state_file({"columns": wr.fieldnames})

        logging.info("Component finished.")



================================================
FILE: src/legacy_client/result.py
================================================
import json
import logging
import os
import sys
from typing import List


class Fetcher:

    def __init__(self, path: str, table_name: str, incremental: bool = False, primary_keys: List = []):

        if (_tn := table_name.strip()) == '':
            logging.error("No table name provided.")
            sys.exit(1)
        else:
            self.table_name = _tn

        self.incremental = bool(incremental)
        self.table_path = path
        self.result_schema = None

        if isinstance(primary_keys, List) is False:
            logging.error("Primary keys must be provided as an array.")
            sys.exit(1)
        else:
            self.primary_keys = primary_keys

    def flatten_json(self, x, out=None, name=''):
        if out is None:
            out = dict()
        if type(x) is dict:
            for a in x:
                self.flatten_json(x[a], out, name + a + '.')

        elif type(x) is list:
            out[name[:-1]] = json.dumps(x)

        else:
            out[name[:-1]] = x

        return out

    def create_manifest(self, columns, incremental, primary_keys):

        with open(os.path.join(self.table_path, self.table_name) + '.manifest', 'w') as man_file:
            json.dump(
                {
                    'columns': columns,
                    'incremental': incremental,
                    'primary_key': primary_keys
                }, man_file
            )

    @staticmethod
    def fetch_results(results):
        for row in results:
            yield row

    def get_table_path(self):
        return os.path.join(self.table_path, self.table_name)



================================================
FILE: src/legacy_client/ssh_client.py
================================================
import io
import json
import logging
import socket
import sys
from typing import List, Tuple
from retry import retry
import paramiko
from furl import furl

Headers = List[Tuple[str, str]]

# Workaround for re-key timeout: https://github.com/paramiko/paramiko/issues/822
paramiko.packet.Packetizer.REKEY_BYTES = 1e12

SIZE_PARAM = 'size'
SCROLL_PARAM = 'scroll'

DEFAULT_SIZE = 2000
DEFAULT_SCROLL = '15m'
SSH_COMMAND_TIMEOUT = 60  # this is in seconds


class SshClient:

    def __init__(self, SshTunnel, Database):

        self.SshTunnel = SshTunnel

        pkey_file = io.StringIO(SshTunnel.key)
        self.pkey = self._parse_private_key(pkey_file)

        # Set up SSH paramiko client
        self.ssh = paramiko.SSHClient()
        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())

        self.connect_ssh()

        self.db = Database
        self._default_size = DEFAULT_SIZE

    def connect_ssh(self):
        try:
            self.ssh.connect(hostname=self.SshTunnel.hostname, port=self.SshTunnel.port,
                             username=self.SshTunnel.username, pkey=self.pkey)
        except (socket.gaierror, paramiko.ssh_exception.AuthenticationException):
            logging.exception("Could not establish SSH tunnel. Check that all SSH parameters are correct.")
            sys.exit(1)

    def _parse_private_key(self, keyfile):
        # try all versions of encryption keys
        pkey = None
        failed = False
        try:
            pkey = paramiko.RSAKey.from_private_key(keyfile)
        except paramiko.SSHException as e:
            logging.debug(e)
            logging.warning("RSS Private key invalid, trying DSS.")
            failed = True
        except IndexError:
            logging.exception("Could not read RSS Private Key - have you provided it correctly?")
            sys.exit(1)
        # DSS
        if failed:
            try:
                pkey = paramiko.DSSKey.from_private_key(keyfile)
                failed = False
            except paramiko.SSHException:
                logging.warning("DSS Private key invalid, trying ECDSAKey.")
                failed = True
            except IndexError:
                logging.exception("Could not read DSS Private Key - have you provided it correctly?")
                sys.exit(1)
        # ECDSAKey
        if failed:
            try:
                pkey = paramiko.ECDSAKey.from_private_key(keyfile)
                failed = False
            except paramiko.SSHException:
                logging.warning("ECDSAKey Private key invalid, trying Ed25519Key.")
                failed = True
            except IndexError:
                logging.exception("Could not read ECDSAKey Private Key - have you provided it correctly?")
                sys.exit(1)
        # Ed25519Key
        if failed:
            try:
                pkey = paramiko.Ed25519Key.from_private_key(keyfile)
            except paramiko.SSHException as e:
                logging.warning("Ed25519Key Private key invalid.")
                raise e
            except IndexError:
                logging.exception("Could not read Ed25519Key Private Key - have you provided it correctly?")
                sys.exit(1)

        return pkey

    def build_curl(self, url, request_type, headers=None, json_body=None):

        # Start of curl string
        curl = 'curl -i'
        curl += f' --request {request_type}'

        # Add headers
        _header_string = ''
        for header in headers:
            _header_string += f' -H "{header[0]}: {header[1]}"'

        curl += _header_string

        # Add JSON body
        if json_body is not None:
            curl += f' --data \'{json.dumps(json_body)}\''

        curl += f' {url}'

        logging.debug(f"Constructed cURL: {curl}.")
        return curl

    @retry(paramiko.ssh_exception.SSHException, delay=3, tries=5, backoff=3)
    def _execute_ssh_command(self, curl):
        """
        Wrapped func to execute ssh command with timeout defined in SSH_COMMAND_TIMEOUT
        """
        _, stdout, stderr = self.ssh.exec_command(command=curl, timeout=SSH_COMMAND_TIMEOUT)
        return _, stdout, stderr

    def execute_ssh_command(self, curl):
        """
        Executes ssh command with timeout defined in SSH_COMMAND_TIMEOUT
        """
        try:
            _, stdout, stderr = self._execute_ssh_command(curl)
        except paramiko.ssh_exception.SSHException:
            try:
                logging.info("Failed to execute SSH command, resetting connection and trying again...")
                self.connect_ssh()
                _, stdout, stderr = self._execute_ssh_command(curl)
            except paramiko.ssh_exception.SSHException:
                logging.exception(f"Maximum number of retries (3) reached when executing ssh_command {curl}")
                sys.exit(1)
            return _, stdout, stderr
        return _, stdout, stderr

    def get_first_page(self, index, body):

        db_url = furl(f'{self.db.host}:{self.db.port}')
        db_url /= f'{index}/_search'

        self._default_scroll = body.pop(SCROLL_PARAM, DEFAULT_SCROLL)
        db_url.args[SCROLL_PARAM] = self._default_scroll

        if SIZE_PARAM in body:
            self._default_size = body[SIZE_PARAM]
        else:
            body[SIZE_PARAM] = self._default_size

        logging.info(f"Default size: {self._default_size}")

        curl = self.build_curl(db_url, 'POST', [('Content-Type', 'application/json')], body)

        _, stdout, stderr = self.execute_ssh_command(curl)
        out, err = stdout.read(), stderr.read()

        return out.decode().strip(), err.decode().strip()

    def get_scroll(self, scroll_id):
        db_url = furl(f'{self.db.host}:{self.db.port}')
        db_url /= '_search/scroll'

        data = {'scroll': self._default_scroll, 'scroll_id': scroll_id}

        curl = self.build_curl(db_url, 'POST', [('Content-Type', 'application/json')], data)

        _, stdout, stderr = self.execute_ssh_command(curl)
        out, err = stdout.read(), stderr.read()

        return out.decode().strip(), err.decode().strip()



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")



================================================
FILE: tests/test_component.py
================================================
"""
Created on 12. 11. 2018

@author: esner
"""
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {"KBC_DATADIR": "./non-existing-dir"})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


