Directory structure:
└── kds_consulting_team-sapi-client-scripts/
    ├── README.md
    ├── clone_configuration.py
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── LICENSE.txt
    └── kbcstorage/
        ├── __init__.py
        ├── base.py
        ├── buckets.py
        ├── client.py
        ├── components.py
        ├── files.py
        ├── jobs.py
        ├── tables.py
        └── workspaces.py

================================================
FILE: README.md
================================================
[![Build Status](https://travis-ci.org/keboola/sapi-python-client.svg?branch=master)](https://travis-ci.org/keboola/sapi-python-client)


# Modified Python client for the Keboola Storage API
Extends some functionality, e.g. components endpoint and provides some useful utility scripts.

Client for using [Keboola Connection Storage API](http://docs.keboola.apiary.io/). This API client provides client methods to get data from KBC and store data in KBC. The endpoints 
for working with buckets, tables and workspaces are covered.

## Scripts

### Clone component configuration

#### Usage:
```
usage: clone_configuration.py [-h]
                              from_config from_token to_token component_id
                              [storage_api_url_from] [storage_api_url_to]

    Simple CMD to migrate component config from one config to another. Even across projects



positional arguments:
  from_config
  from_token
  to_token
  component_id
  storage_api_url_from  [https://connection.keboola.com]
  storage_api_url_to    [https://connection.keboola.com]

optional arguments:
  -h, --help            show this help message and exit
```

**Migrate one transformation config to another project:** 

- source project in default US Region, destination project in EU region

```python
python duplicateConfig.py 12345 SRC_PROJ_TOKEN DEST_PROJ_TOKEN 'transformation' 'https://connection.keboola.com' 
'https://connection.eu-central-1.keboola.com'
```


## Client Class Usage
```
from kbcstorage.client import Client

client = Client('https://connection.keboola.com', 'your-token')

# get table data into local file
client.tables.export_to_file(table_id='in.c-demo.some-table', path_name='/data/')

# save data
client.tables.create(name='some-table-2', bucket_id='in.c-demo', file_path='/data/some-table')

# list buckets
client.buckets.list()

# list bucket tables
client.buckets.list_tables('in.c-demo')

# get table info
client.tables.detail('in.c-demo.some-table')

```

## Endpoint Classes Usage 
```
from kbcstorage.tables import Tables
from kbcstorage.buckets import Buckets

tables = Tables('https://connection.keboola.com', 'your-token')

# get table data into local file
tables.export_to_file(table_id='in.c-demo.some-table', path_name='/data/')

# save data
tables.create(name='some-table-2', bucket_id='in.c-demo', file_path='/data/some-table')

# list buckets
buckets = Buckets('https://connection.keboola.com', 'your-token')
buckets.list()

# list bucket tables
buckets.list_tables('in.c-demo')

# get table info
tables.detail('in.c-demo.some-table')

```

## Docker image
Docker image with pre-installed library is also available, run it via:

```
docker run -i -t quay.io/keboola/sapi-python-client
```

## Tests

```bash
$ git clone https://github.com/keboola/sapi-python-client.git && cd sapi-python-client
$ python setup.py test
```

or 

```bash
$ docker-compose run --rm -e KBC_TEST_TOKEN -e KBC_TEST_API_URL sapi-python-client -m unittest discover
```

## Contribution Guide
The client is far from supporting the entire API, all contributions are very welcome. New API endpoints should 
be implemeneted in their own class extending `Endpoint`. Naming conventions should follow existing naming conventions
or those of the [API](http://docs.keboola.apiary.io/#). If the method contains some processing of the request or response, consult the corresponing [PHP implementation](https://github.com/keboola/storage-api-php-client) for reference. New code should be covered by tests.

Note that if you submit a PR from your own forked repository, the automated functional tests will fail. This is limitation of [Travis](https://docs.travis-ci.com/user/pull-requests/#Pull-Requests-and-Security-Restrictions). Either run the tests locally (set `KBC_TEST_TOKEN` (your token to test project) and `KBC_TEST_API_URL` (https://connection.keboola.com) variables) or ask for access. In case, you need a project for local testing, feel free to [ask for one](https://developers.keboola.com/#development-project).

The recommended workflow for making a pull request is:

```bash
git clone https://github.com/keboola/sapi-python-client.git
git checkout master
git pull
git checkout -b my-new-feature
# work on branch my-new-feature
git push origin my-new-feature:my-new-feature
```

This will create a new branch which can be used to make a pull request for your new feature.



================================================
FILE: clone_configuration.py
================================================
import logging

import plac

from kbcstorage.components import Components


class ConfigDuplicator:
    def __init__(self, client_from: Components, client_to: Components):
        """
        Initialise a client.
        Args:
            client_from (Components): Client object for source configuration.
            client_to (Components): Client object for destination configuration.

        """

        self.client_from = client_from
        self.client_to = client_to

    def migrate_configs(self, src_config_id, component_id):
        """
        Super simple method, getting all table config objects and updating/creating them in the destination configuration.
        Includes all attributes, even the ones that are not updateble => API service will ignore them.

        """
        src_config = self.client_from.get_config_detail(component_id, src_config_id)

        src_config_rows = self.client_from.get_config_rows(component_id, src_config_id)

        dst_config = src_config.copy()
        # add component id
        dst_config['component_id'] = 'transformation'

        logging.info('Transfering config..')
        new_cfg = self.client_to.create(**dst_config)

        logging.info('Transfering config rows')
        for row in src_config_rows:
            row['component_id'] = component_id
            row['configuration_id'] = new_cfg['id']
            test=row['configuration'].pop('id')
            test =row['configuration'].pop('rowId',{})
            test =row.pop('rowId',{})

            self.client_to.create_config_row(**row)


def main(from_config, from_token, to_token, component_id, storage_api_url_from='https://connection.keboola.com',
         storage_api_url_to='https://connection.keboola.com'):
    """
    Simple CMD to migrate component config from one config to another. Even across projects

    """
    from_client = Components(storage_api_url_from, from_token)
    to_client = Components(storage_api_url_to, to_token)

    duplicator = ConfigDuplicator(from_client, to_client)

    logging.info('Migrating component %s configuration from config ID: %s ', component_id, from_config)
    try:
        duplicator.migrate_configs(from_config, component_id)
    except Exception as err:
        logging.error("Failed to migrate the configuration: %s", err)
        exit()


if __name__ == '__main__':
    plac.call(main)



================================================
FILE: deploy.sh
================================================
#!/bin/bash
set -e

docker login -u="$QUAY_USERNAME" -p="$QUAY_PASSWORD" quay.io
docker tag ${KBC_APP_REPOSITORY} quay.io/${KBC_APP_REPOSITORY}:${TRAVIS_TAG}
docker tag ${KBC_APP_REPOSITORY} quay.io/${KBC_APP_REPOSITORY}:latest
docker images
docker push quay.io/${KBC_APP_REPOSITORY}:${TRAVIS_TAG}
docker push quay.io/${KBC_APP_REPOSITORY}:latest



================================================
FILE: docker-compose.yml
================================================
version: '2'
services:
  sapi-python-client:
    build: .
    image: keboola/sapi-python-client
    tty: true
    stdin_open: true
    command: bash
    volumes:
      - ./:/code/



================================================
FILE: Dockerfile
================================================
FROM python:3.6

WORKDIR /code
COPY . /code/
RUN pip3 install --no-cache-dir flake8 responses
RUN python setup.py install
ENTRYPOINT ["python"]



================================================
FILE: LICENSE.txt
================================================
Copyright (c) 2017 Keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.



================================================
FILE: kbcstorage/__init__.py
================================================
from pkg_resources import get_distribution, DistributionNotFound
try:
    release = get_distribution('kbcstorage').version
    __version__ = '.'.join(release.split('.')[:2])
except DistributionNotFound:
    # package is not installed
    pass



================================================
FILE: kbcstorage/base.py
================================================
"""
Base classes for constructing the client.

Primarily exposes a base Endpoint class which deduplicates functionality across
various endpoints, such as tables, workspaces, jobs, etc. as described in the
`Storage API documentation`.


.. _Storage API documentation:
    http://docs.keboola.apiary.io/
"""
import requests


class Endpoint:
    """
    Base class for implementing a single endpoint related to a single entities
    as described in the Storage API.

    Attributes:
        base_url (str): The base URL for this endpoint.
        token (str): A key for the Storage API.
    """
    def __init__(self, root_url, path_component, token):
        """
        Create an endpoint.

        Args
            root_url (str): Root url of API. eg.
                "https://connection.keboola.com/"
            path_component (str): The section of the path specific to the
                endpoint. eg. "buckets"
            token (str): A key for the Storage API. Can be found in the storage
                console.
        """
        if not root_url:
            raise ValueError("Root URL is required.")
        if not path_component:
            raise ValueError("Path component is required.")
        if not token:
            raise ValueError("Token is required.")
        self.root_url = root_url
        self.base_url = '{}/v2/storage/{}'.format(root_url.strip('/'),
                                                  path_component.strip('/'))
        self.token = token
        self._auth_header = {'X-StorageApi-Token': self.token,
                             'Accept-Encoding': 'gzip',
                             'User-Agent': 'Keboola Storage API Python Client'}

    def _get_raw(self, url, params=None, **kwargs):
        """
        Construct a requests GET call with args and kwargs and process the
        results.


        Args:
            url (str): requested url
            params (dict): additional url params to be passed to the underlying
                requests.get
            **kwargs: Key word arguments to pass to the get requests.get

        Returns:
            r (requests.Response): object

        Raises:
            requests.HTTPError: If the API request fails.
        """
        headers = kwargs.pop('headers', {})
        headers.update(self._auth_header)

        r = requests.get(url, params, headers=headers, **kwargs)
        try:
            r.raise_for_status()
        except requests.HTTPError:
            # Handle different error codes
            raise
        else:
            return r

    def _get(self, url, params=None, **kwargs):
        """
        Make authenticated GET request and return json

        Args:
            url (str): requested url
            params (dict): additional url params to be passed to the underlying
                requests.get
            **kwargs: Key word arguments to pass to the get requests.get

        Returns:
           body: Response body parsed from json.

        Raises:
            requests.HTTPError: If the API request fails.

        """
        return self._get_raw(url, params, **kwargs).json()

    def _post(self, *args, **kwargs):
        """
        Construct a requests POST call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
            **kwargs: Key word arguments to pass to the post request.

        Returns:
            body: Response body parsed from json.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        headers = kwargs.pop('headers', {})
        headers.update(self._auth_header)
        r = requests.post(headers=headers, *args, **kwargs)
        try:
            r.raise_for_status()
        except requests.HTTPError:
            # Handle different error codes
            raise
        else:
            return r.json()

    def _put(self, *args, **kwargs):
        """
        Construct a requests PUT call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
            **kwargs: Key word arguments to pass to the post request.

        Returns:
            body: Response body parsed from json.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        headers = kwargs.pop('headers', {})
        headers.update(self._auth_header)
        r = requests.put(headers=headers, *args, **kwargs)
        try:
            r.raise_for_status()
        except requests.HTTPError:
            # Handle different error codes
            raise
        else:
            return r.json()

    def _delete(self, *args, **kwargs):
        """
        Construct a requests DELETE call with args and kwargs and process the
        result

        Args:
            *args: Positional arguments to pass to the delete request.
            **kwargs: Key word arguments to pass to the delete request.

        Returns:
            body: Response body parsed from json.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        headers = kwargs.pop('headers', {})
        headers.update(self._auth_header)
        r = requests.delete(headers=headers, *args, **kwargs)
        try:
            r.raise_for_status()
        except requests.HTTPError:
            # Handle different error codes
            raise
        # Should delete return something on success?



================================================
FILE: kbcstorage/buckets.py
================================================
"""
Manages calls to the Storage API relating to buckets

Full documentation `here`.

.. _here:
    http://docs.keboola.apiary.io/#reference/buckets/
"""
from kbcstorage.base import Endpoint


class Buckets(Endpoint):
    """
    Buckets Endpoint
    """
    def __init__(self, root_url, token):
        """
        Create a Buckets endpoint.

        Args:
            root_url (:obj:`str`): The base url for the API.
            token (:obj:`str`): A storage API key.
        """
        super().__init__(root_url, 'buckets', token)

    def list(self):
        """
        List all buckets in project.

        Returns:
            response_body: The parsed json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """

        return self._get(self.base_url)

    def list_tables(self, bucket_id, include=None):
        """
        List all tables in a bucket.

        Args:
            bucket_id (str): Id of the bucket
            include (list): Properties to list (attributes, columns)
        Returns:
            response_body: The parsed json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        headers = {'X-StorageApi-Token': self.token}

        url = '{}/{}/tables'.format(self.base_url, bucket_id)
        params = {}
        if include is not None and isinstance(include, list):
            params['include'] = ','.join(include)
        return self._get(url, headers=headers, params=params)

    def detail(self, bucket_id):
        """
        Retrieves information about a given bucket.

        Args:
            bucket_id (str): The id of the bucket.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}'.format(self.base_url, bucket_id)

        return self._get(url)

    def create(self, name, stage='in', description='', backend=None):
        """
        Create a new bucket.

        Args:
            name (str): The new bucket name (only alphanumeric and underscores)
            stage (str): The new bucket stage. Can be one of ``in`` or ``out``.
                Default ``in``.
            description (str): The new bucket description.
            backend (str): The new bucket backend. Cand be one of
                ``snowflake``, ``redshift`` or ``mysql``. Default determined by
                project settings.
        Returns:
            response_body: The parsed json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        # Separating create and link into two distinct functions...
        # Need to check args...
        body = {
            'name': name,
            'stage': stage,
            'description': description,
            'backend': backend
        }

        return self._post(self.base_url, data=body)

    def delete(self, bucket_id, force=False):
        """
        Delete a bucket referenced by ``bucket_id``.

        By default, only empty buckets without dependencies (aliases etc) can
        be deleted. The optional ``force`` parameter allows for the deletion
        of non-empty buckets.

        Args:
            bucket_id (str): The id of the bucket to be deleted.
            force (bool): If ``True``, deletes the bucket even if it is not
                empty. Default ``False``.
        """
        # How does the API handle it when force == False and the bucket is non-
        # empty?
        url = '{}/{}'.format(self.base_url, bucket_id)
        params = {'force': force}
        self._delete(url, params=params)

    def link(self, *args, **kwargs):
        """
        **Not implemented**

        Link an existing bucket from another project.

        Creates a new bucket which contains the contents of a shared bucket in
        a source project. Linking a bucket from another project is only
        possible if it has been enabled in the project.
        """
        raise NotImplementedError

    def share(self, *args, **kwargs):
        """
        **Not implemented**

        Enable sharing of a bucket.

        The bucket will be shared to the entire organisation to which the
        project belongs. It may then be shared to any project of that
        organization. This operation is only available to administrator tokens.
        """
        raise NotImplementedError

    def unshare(self, *args, **kwargs):
        """
        **Not implemented**

        Stop sharing a bucket.

        The bucket must not be linked to other projects. To unshare an already
        linked bucket, the links must first be deleted - use ``delete`` on the
        bucket in the linking project. This operation is only available for
        administrator tokens.
        """
        raise NotImplementedError



================================================
FILE: kbcstorage/client.py
================================================
""""
Entry point for the Storage API client.
"""

from kbcstorage.buckets import Buckets
from kbcstorage.workspaces import Workspaces
from kbcstorage.jobs import Jobs
from kbcstorage.tables import Tables
from kbcstorage.files import Files


class Client:
    """
    Storage API Client.
    """

    def __init__(self, api_domain, token):
        """
        Initialise a client.

        Args:
            api_domain (str): The domain on which the API sits. eg.
                "https://connection.keboola.com".
            token (str): A storage API key.
        """
        self.root_url = api_domain
        self._token = token

        self.buckets = Buckets(self.root_url, self.token)
        self.files = Files(self.root_url, self.token)
        self.jobs = Jobs(self.root_url, self.token)
        self.tables = Tables(self.root_url, self.token)
        self.workspaces = Workspaces(self.root_url, self.token)

    @property
    def token(self):
        return self._token



================================================
FILE: kbcstorage/components.py
================================================
"""
Manages calls to the Storage API relating to components.

Full documentation `here`.

.. _here:
    http://docs.keboola.apiary.io/#reference/components/
"""

import json
import urllib

from kbcstorage.base import Endpoint


class Components(Endpoint):
    """

    """

    def __init__(self, root_url, token):
        """
        Create a Components endpoint.

        Args:
            root_url (:obj:`str`): The base url for the API.
            token (:obj:`str`): A storage API key.
        """
        super().__init__(root_url, 'components', token)

    def list(self):
        """
        List all components details.

        Returns:
            response_body: The json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        return self._get(self.base_url)

    def detail(self, component_id):
        """
        Retrieves information about a given component.

        Args:
            component_id (str or int): The id of the component.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}'.format(self.base_url, component_id)

        return self._get(url)

    def get_config_detail(self, component_id, configuration_id):
        """
        Retrieves component's configuration detail.

        Args:
            component_id (str or int): The id of the component.
            configuration_id (str or int): The id of configuration
        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}/configs/{}'.format(self.base_url, component_id, configuration_id)

        return self._get(url)

    def get_config_row_detail(self, component_id, configuration_id, row_id):
        """
        Retrieves component's configuration row detail.

        Args:
            component_id (str or int): The id of the component.
            configuration_id (str or int): The id of configuration
            row_id
        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}/configs/{}/rows/{}'.format(self.base_url, component_id, configuration_id, row_id)

        return self._get(url)

    def get_config_rows(self, component_id, configuration_id):
        """
        Retrieves component's configuration detail.

        Args:
            component_id (str or int): The id of the component.
            configuration_id (int): The id of configuration
        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}/configs/{}/rows'.format(self.base_url, component_id, configuration_id)

        return self._get(url)

    def update_configuration(self, component_id, configuration_id, configuration, name, state=None, description='',
                             changeDescription='', rowsSortOrder=None):
        """

        Args:
            name: required string Configuration name
            description: string Configuration description
            configuration: dict configuration JSON; the maximum allowed size is 4MB
            state: state dict configuration state JSON; the maximum allowed size is 4MB
            changeDescription: string Description of the configuration modification
            rowsSortOrder:

            param component_id:
            configuration_id:

        Returns:

        """
        parameters = {}
        url = '{}/configs/{}'.format(self.base_url, component_id, configuration_id)
        # convert objects to string
        parameters['configuration'] = json.dumps(configuration)
        parameters['name'] = name
        parameters['description'] = description
        parameters['changeDescription'] = changeDescription
        parameters['rowsSortOrder'] = rowsSortOrder
        if state:
            parameters['state'] = json.dumps(state)

        return self._put(url, params=parameters)

    def create(self, component_id, name, description, configuration, configurationId=None, state=None,
               changeDescription='', **kwargs):
        """
        Create a new table from CSV file.

        Args:
            component_id (str):
            name (str): The new table name (only alphanumeric and underscores)
            configuration (dict): configuration JSON; the maximum allowed size is 4MB
            state (dict): configuration JSON; the maximum allowed size is 4MB
            changeDescription (str): Escape character used in the CSV file.

        Returns:
            table_id (str): Id of the created table.

        Raises:
            requests.HTTPError: If the API request fails.
        """

        parameters = {}
        url = '{}/{}/configs'.format(self.base_url, component_id)
        # convert objects to string
        if configurationId:
            parameters['configurationId'] = configurationId
        parameters['configuration'] = json.dumps(configuration)
        parameters['name'] = name
        parameters['description'] = description
        parameters['changeDescription'] = changeDescription
        if state:
            parameters['state'] = json.dumps(state)
        header = {'Content-Type': 'application/x-www-form-urlencoded'}
        data = urllib.parse.urlencode(parameters)
        return self._post(url, data=data, headers=header)

    def create_config_row(self, component_id, configuration_id, name, configuration,
                          description='', rowId=None, state=None, changeDescription='', isDisabled=False, **kwargs):
        """
        Create a new table from CSV file.

        Args:
            component_id (str):
            name (str): The new table name (only alphanumeric and underscores)
            configuration (dict): configuration JSON; the maximum allowed size is 4MB
            state (dict): configuration JSON; the maximum allowed size is 4MB
            changeDescription (str): Escape character used in the CSV file.

        Returns:
            table_id (str): Id of the created table.

        Raises:
            requests.HTTPError: If the API request fails.
        """

        parameters = {}
        url = url = '{}/{}/configs/{}/rows'.format(self.base_url, component_id, configuration_id)
        # convert objects to string
        parameters['configuration'] = json.dumps(configuration)
        parameters['name'] = name
        parameters['description'] = description
        if rowId:
            parameters['rowId'] = rowId
        parameters['changeDescription'] = changeDescription
        parameters['isDisabled'] = isDisabled
        if state:
            parameters['state'] = json.dumps(state)

        header = {'Content-Type': 'application/x-www-form-urlencoded'}
        data = urllib.parse.urlencode(parameters)
        return self._post(url, data=data, headers=header)

    def update_config_row(self, component_id, configuration_id, row_id, name, configuration,
                          description='', rowId=None, state=None, changeDescription='', isDisabled=False, **kwargs):
        """
        Create a new table from CSV file.

        Args:
            component_id (str):
            name (str): The new table name (only alphanumeric and underscores)
            configuration (dict): configuration JSON; the maximum allowed size is 4MB
            state (dict): configuration JSON; the maximum allowed size is 4MB
            changeDescription (str): Escape character used in the CSV file.

        Returns:
            table_id (str): Id of the created table.

        Raises:
            requests.HTTPError: If the API request fails.
        """

        parameters = {}
        url = '{}/{}/configs/{}'.format(self.base_url, component_id, configuration_id)
        # convert objects to string
        parameters['configuration'] = json.dumps(configuration)
        parameters['name'] = name
        parameters['description'] = description
        if rowId:
            parameters['rowId'] = rowId
        parameters['changeDescription'] = changeDescription
        parameters['isDisabled'] = isDisabled
        if state:
            parameters['state'] = json.dumps(state)

        header = {'Content-Type': 'application/x-www-form-urlencoded'}
        data = urllib.parse.urlencode(parameters)
        return self._put(url, data=data, headers=header)



================================================
FILE: kbcstorage/files.py
================================================
"""
Manages calls to the Storage API relating to files

Full documentation `here`.

.. _here:
    http://docs.keboola.apiary.io/#reference/files/
"""
import os
import boto3
import requests

from kbcstorage.base import Endpoint


class Files(Endpoint):
    """
    Buckets Endpoint
    """
    def __init__(self, root_url, token):
        """
        Create a Files endpoint.

        Args:
            root_url (:obj:`str`): The base url for the API.
            token (:obj:`str`): A storage API key.
        """
        super().__init__(root_url, 'files', token)

    def detail(self, file_id, federation_token=False):
        """
        Retrieves information about a given file.

        Args:
            file_id (str): The id of the file.
            federation_token (bool): True to get AWS credentials
                for file download

        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}'.format(self.base_url, file_id)
        params = {}
        if federation_token:
            params['federationToken'] = 'true'
        return self._get(url, params=params)

    def upload_file(self, file_path, tags=None, is_public=False,
                    is_permanent=False, is_encrypted=True,
                    is_sliced=False, do_notify=False, compress=False):
        """
        Upload a file to storage

        Args:
            file_path (str): Local path to file to upload
            tags (list): Array of tags
            is_public (bool): File is public
            is_permanent (bool): File is permanent
            is_encrypted (bool): File is encrypted
            is_sliced (bool): File is sliced
            do_notify (bool): Notify members of project that file was uploaded

        Returns:
            file_id (str): Id of the created file

        Raises:
            requests.HTTPError: If the API request fails.
        """
        if not os.path.exists(file_path) or not os.path.isfile(file_path):
            raise ValueError("File " + file_path + " does not exist")
        if compress:
            import gzip
            import shutil
            with open(file_path, 'rb') as f_in, gzip.open(file_path + '.gz',
                                                          'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
            file_path = file_path + '.gz'
        file_name = os.path.basename(file_path)
        size = os.path.getsize(file_path)
        file_resource = self.prepare_upload(file_name, size, tags, is_public,
                                            is_permanent, is_encrypted,
                                            is_sliced, do_notify, True)
        upload_params = file_resource['uploadParams']
        key_id = upload_params['credentials']['AccessKeyId']
        key = upload_params['credentials']['SecretAccessKey']
        token = upload_params['credentials']['SessionToken']
        s3 = boto3.resource('s3', aws_access_key_id=key_id,
                            aws_secret_access_key=key,
                            aws_session_token=token,
                            region_name=file_resource['region'])

        s3_object = s3.Object(bucket_name=upload_params['bucket'],
                              key=upload_params['key'])
        disposition = 'attachment; filename={};'.format(file_resource['name'])
        with open(file_path, mode='rb') as file:
            if is_encrypted:
                encryption = upload_params['x-amz-server-side-encryption']
                s3_object.put(ACL=upload_params['acl'], Body=file,
                              ContentDisposition=disposition,
                              ServerSideEncryption=encryption)
            else:
                s3_object.put(ACL=upload_params['acl'], Body=file,
                              ContentDisposition=disposition)
        return file_resource['id']

    def prepare_upload(self, name, size_bytes=None, tags=None, is_public=False,
                       is_permanent=False, is_encrypted=True,
                       is_sliced=False, do_notify=False,
                       federation_token=True):
        """
        Prepare a file resource for a new file

        Args:
            name (str): The new file name (only alphanumeric and underscores)
            size_bytes (int): Size of the file
            tags (list): Array of tags
            is_public (bool): File is public
            is_permanent (bool): File is permanent
            is_encrypted (bool): File is encrypted
            is_sliced (bool): File is sliced
            do_notify (bool): Notify members of project that file was uploaded
            federation_token (bool): Obtain AWS federation token

        Returns:
            response_body: The parsed json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}'.format(self.base_url, 'prepare')
        body = {
            'isPublic': int(is_public),
            'isPermanent': int(is_permanent),
            'isEncrypted': int(is_encrypted),
            'isSliced': int(is_sliced),
            'notify': int(do_notify),
            'name': name
        }
        if tags is not None and isinstance(tags, list):
            body['tags[]'] = tags
        if size_bytes is not None:
            body['sizeBytes'] = size_bytes
        if federation_token is not None:
            body['federationToken'] = int(federation_token)
        return self._post(url, data=body)

    def delete(self, file_id):
        """
        Delete a bucket referenced by ``file_id``.

        Args:
            file_id (str): The id of the file to be deleted.
        """
        url = '{}/{}'.format(self.base_url, file_id)
        self._delete(url)

    def list(self, limit=100, offset=0, tags=None, q=None, run_id=None,
             since_id=None, max_id=None):
        """
        List files in project.

        Args:
            limit (int): Pagination size
            offset (int): Pagination start
            tags (list): List files with the tags
            q (str) Elastic query string
            run_id (str) Run Id
            since_id (str) List files with ID bigger than
            max_id (str) List files with ID smaller than

        Returns:
            response_body: The parsed json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        params = {
            'limit': int(limit),
            'offset': int(offset)
        }
        if tags is not None and isinstance(tags, list):
            params['tags[]'] = tags
        if q is not None:
            params['q'] = q
        if run_id is not None:
            params['sinceId'] = since_id
        if max_id is not None:
            params['maxId'] = max_id
        return self._get(self.base_url, params=params)

    def download(self, file_id, local_path):
        if not os.path.exists(local_path):
            os.mkdir(local_path)
        file_info = self.detail(file_id=file_id, federation_token=True)
        local_file = os.path.join(local_path, file_info['name'])
        s3 = boto3.resource(
            's3',
            aws_access_key_id=file_info['credentials']['AccessKeyId'],
            aws_secret_access_key=file_info['credentials']['SecretAccessKey'],
            aws_session_token=file_info['credentials']['SessionToken'],
            region_name=file_info['region']
        )
        if file_info['isSliced']:
            manifest = requests.get(url=file_info['url']).json()
            file_names = []
            for entry in manifest["entries"]:
                full_path = entry["url"]
                file_name = full_path.rsplit("/", 1)[1]
                file_names.append(file_name)
                splitted_path = full_path.split("/")
                file_key = "/".join(splitted_path[3:])
                bucket = s3.Bucket(file_info['s3Path']['bucket'])
                bucket.download_file(file_key, file_name)
            # merge the downloaded files
            with open(local_file, mode='wb') as out_file:
                for file_name in file_names:
                    with open(file_name, mode='rb') as in_file:
                        for line in in_file:
                            out_file.write(line)
                    os.remove(file_name)
        else:
            bucket = s3.Bucket(file_info["s3Path"]["bucket"])
            bucket.download_file(file_info["s3Path"]["key"], local_file)
        return local_file



================================================
FILE: kbcstorage/jobs.py
================================================
"""
Manages calls to the Storage API relating to jobs.

Full documentation `here`.

.. _here:
    http://docs.keboola.apiary.io/#reference/jobs/
"""
import time

from kbcstorage.base import Endpoint


class Jobs(Endpoint):
    """
    Jobs are objects that manage asynchronous tasks, these are all
    potentially long-running actions such as loading table data,
    snapshotting, table structure modifications. Jobs are created by
    actions on target resources.

    A job has four available statuses:

    ``waiting``
        The job is in the queue and is waiting for execution.

    ``processing``
        The job is being processed by a worker.

    ``success``
        The job is done with a success.

    ``error``
        The job is done with an error.
    """

    def __init__(self, root_url, token):
        """
        Create a Jobs endpoint.

        Args:
            root_url (:obj:`str`): The base url for the API.
            token (:obj:`str`): A storage API key.
        """
        super().__init__(root_url, 'jobs', token)

    def list(self):
        """
        List all jobs details.

        Returns:
            response_body: The json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        return self._get(self.base_url)

    def detail(self, job_id):
        """
        Retrieves information about a given job.

        Args:
            job_id (str or int): The id of the job.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}'.format(self.base_url, job_id)

        return self._get(url)

    def status(self, job_id):
        """
        Retrieve the status of a given job.

        Args:
            job_id (str or int): The id of the job.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        return self.detail(job_id)['status']

    def completed(self, job_id):
        """
        Check if a job is completed or not.

        Args:
            job_id (str or int): The id of the job.

        Returns:
            completed (bool): True if job is completed, else False.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        completed_statuses = ('error', 'success')
        return self.status(job_id) in completed_statuses

    def block_until_completed(self, job_id):
        """
        Poll the API until the job is completed.

        Args:
            job_id (str): The id of the job

        Returns:
            response_body: The parsed json from the HTTP response
                containing a storage Job.

        Raises:
            requests.HTTPError: If any API request fails.
        """
        retries = 1
        while True:
            job = self.detail(job_id)
            if job['status'] in ('error', 'success'):
                return job
            time.sleep(min(2 ** retries, 20))

    def block_for_success(self, job_id):
        """
        Poll the API until the job is completed, then return ``True`` if the
        job is successful, else ``False``.

        Args:
            job_id (str): The id of the job

        Returns:
            success (bool): True if the job status is success, else False.

        Raises:
            requests.HTTPError: If any API request fails.
        """
        job = self.block_until_completed(job_id)
        return job['status'] == 'success'



================================================
FILE: kbcstorage/tables.py
================================================
"""
Manages calls to the Storage API relating to tables

Full documentation `here`.

.. _here:
    http://docs.keboola.apiary.io/#reference/tables/
"""
import os
import tempfile

from kbcstorage.base import Endpoint
from kbcstorage.files import Files
from kbcstorage.jobs import Jobs


class Tables(Endpoint):
    """
    Buckets Endpoint
    """

    def __init__(self, root_url, token):
        """
        Create a Tables endpoint.

        Args:
            root_url (:obj:`str`): The base url for the API.
            token (:obj:`str`): A storage API key.
        """
        super().__init__(root_url, 'tables', token)

    def list(self, include=None):
        """
        List all tables accessible by token.

        Args:
            include (list): Properties to list (attributes, columns, buckets)
        Returns:
            response_body: The parsed json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        params = {'include': ','.join(include)} if include else {}
        return self._get(self.base_url, params=params)

    def detail(self, table_id):
        """
        Retrieves information about a given table.

        Args:
            table_id (str): The id of the table.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        if not isinstance(table_id, str) or table_id == '':
            raise ValueError("Invalid table_id '{}'.".format(table_id))
        url = '{}/{}'.format(self.base_url, table_id)
        return self._get(url)

    def delete(self, table_id):
        """
        Delete a table referenced by ``table_id``.

        Args:
            table_id (str): The id of the table to be deleted.
        """
        if not isinstance(table_id, str) or table_id == '':
            raise ValueError("Invalid table_id '{}'.".format(table_id))
        url = '{}/{}'.format(self.base_url, table_id)
        self._delete(url)

    def create(self, bucket_id, name, file_path, delimiter=',', enclosure='"',
               escaped_by='', primary_key=None, compress=False):
        """
        Create a new table from CSV file.

        Args:
            bucket_id (str): Bucket id where table is created
            name (str): The new table name (only alphanumeric and underscores)
            file_path (str): Path to local CSV file.
            delimiter (str): Field delimiter used in the CSV file.
            enclosure (str): Field enclosure used in the CSV file.
            escaped_by (str): Escape character used in the CSV file.
            primary_key (list): Primary key of a table.

        Returns:
            table_id (str): Id of the created table.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        files = Files(self.root_url, self.token)
        file_id = files.upload_file(file_path=file_path, tags=['file-import'],
                                    do_notify=False, is_public=False, compress=compress)
        job = self.create_raw(bucket_id=bucket_id, name=name,
                              data_file_id=file_id, delimiter=delimiter,
                              enclosure=enclosure, escaped_by=escaped_by,
                              primary_key=primary_key)
        jobs = Jobs(self.root_url, self.token)
        job = jobs.block_until_completed(job['id'])
        if job['status'] == 'error':
            raise RuntimeError(job['error']['message'])
        return job['results']['id']

    def create_raw(self, bucket_id, name, data_url=None, data_file_id=None,
                   snapshot_id=None, data_workspace_id=None,
                   data_table_name=None, delimiter=',', enclosure='"',
                   escaped_by='', primary_key=None):
        """
        Create a new table.

        Args:
            bucket_id (str): Bucket id where table is created
            name (str): The new table name (only alphanumeric and underscores)
            data_url (str): Publicly accessible url with a CSV file to import
            data_file_id (str): id of the file stored in File Uploads
            snapshot_id (str): id of a table snapshot -
                a table will be created from the snapshot.
            data_workspace_id (str): Load from the table workspace.
                Use with the dataTableName attribute.
            data_table_name (str): Load from a table in workspace.
            delimiter (str): Field delimiter used in the CSV file.
            enclosure (str): Field enclosure used in the CSV file.
            escaped_by (str): Escape character used in the CSV file.
            primary_key (list): Primary key of a table.

        Returns:
            response_body: The parsed json from the HTTP
                response containing a storage Job.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        if not isinstance(bucket_id, str) or bucket_id == '':
            raise ValueError("Invalid bucket_id '{}'.".format(bucket_id))
        if not isinstance(name, str) or name == '':
            raise ValueError("Invalid name_id '{}'.".format(name))
        body = {
            'name': name,
            'delimiter': delimiter,
            'enclosure': enclosure,
            'escapedBy': escaped_by
        }
        body.update(self.validate_data_source(data_url, data_file_id,
                                              snapshot_id, data_workspace_id,
                                              data_table_name))
        if enclosure is not '' and escaped_by is not '':
            raise ValueError("Only one of enclosure and escaped_by may be "
                             "specified.")
        if primary_key is not None and isinstance(primary_key, list):
            body['primaryKey'] = ",".join(primary_key)
        # todo solve this better
        url = '{}/v2/storage/buckets/{}/tables-async'.format(self.root_url,
                                                             bucket_id)
        return self._post(url, data=body)

    @staticmethod
    def validate_data_source(data_url, data_file_id, snapshot_id,
                             data_workspace_id, data_table_name):
        """
        Check that table data source is configured properly

        Args:
            data_url (str): Publicly accessible url with a CSV file to import
            data_file_id (str): id of the file stored in File Uploads
            snapshot_id (str): id of a table snapshot - a table will
                be created from the snapshot.
            data_workspace_id (str): Load from the table workspace.
                Use with the dataTableName attribute.
            data_table_name (str): Load from a table in workspace.

        Returns:
            body (dict): Request parameters
        """
        source = False
        body = {}
        if data_url is not None:
            body['dataUrl'] = data_url
            source = True
        if data_file_id is not None:
            if source:
                raise ValueError("Only one of data_url, data_file_id, "
                                 "snapshot_id, data_workspace_id may be "
                                 "specified.")
            body['dataFileId'] = data_file_id
            source = True
        if snapshot_id is not None:
            if source:
                raise ValueError("Only one of data_url, data_file_id, "
                                 "snapshot_id, data_workspace_id may be "
                                 "specified.")
            body['snapshotId'] = snapshot_id
            source = True
        if data_workspace_id is not None and data_table_name is not None:
            if source:
                raise ValueError("Only one of data_url, data_file_id, "
                                 "snapshot_id, data_workspace_id may be "
                                 "specified.")
            body['dataWorkspaceId'] = data_workspace_id
            body['dataTableName'] = data_table_name
            source = True
        if not source:
            raise ValueError("One of data_url, data_file_id, snapshot_id, "
                             "data_workspace_id must be specified.")
        return body

    def load(self, table_id, file_path, is_incremental=False, delimiter=',',
             enclosure='"', escaped_by='', columns=None,
             without_headers=False):
        """
        Load data into an existing table

        Args:
            table_id (str): Table id
            file_path (str): Path to local CSV file.
            is_incremental (bool): Load incrementally (do not truncate table).
            delimiter (str): Field delimiter used in the CSV file.
            enclosure (str): Field enclosure used in the CSV file.
            escaped_by (str): Escape character used in the CSV file.
            columns (list): List of columns
            without_headers (bool): CSV does not contain headers

        Returns:
            response_body: The parsed json from the HTTP response
                containing write results

        Raises:
            requests.HTTPError: If the API request fails.
        """
        files = Files(self.root_url, self.token)
        file_id = files.upload_file(file_path=file_path, tags=['file-import'],
                                    do_notify=False, is_public=False)
        job = self.load_raw(table_id=table_id, data_file_id=file_id,
                            delimiter=delimiter, enclosure=enclosure,
                            escaped_by=escaped_by,
                            is_incremental=is_incremental, columns=columns,
                            without_headers=without_headers)
        jobs = Jobs(self.root_url, self.token)
        job = jobs.block_until_completed(job['id'])
        if job['status'] == 'error':
            raise RuntimeError(job['error']['message'])
        return job['results']

    def load_raw(self, table_id, data_url=None, data_file_id=None,
                 snapshot_id=None, data_workspace_id=None,
                 data_table_name=None, is_incremental=False,
                 delimiter=',', enclosure='"', escaped_by='', columns=None,
                 without_headers=False):
        """
        Load data into an existing table

        Args:
            table_id (str): Table id
            data_url (str): Publicly accessible url with a CSV file to import
            data_file_id (str): id of the file stored in File Uploads
            snapshot_id (str): id of a table snapshot - a table will
                be created from the snapshot.
            data_workspace_id (str): Load from the table workspace.
                Use with the dataTableName attribute.
            data_table_name (str): Load from a table in workspace.
            is_incremental (bool): Load incrementally (do not truncate table).
            delimiter (str): Field delimiter used in the CSV file.
            enclosure (str): Field enclosure used in the CSV file.
            escaped_by (str): Escape character used in the CSV file.
            columns (list): List of columns
            without_headers (bool): CSV does not contain headers

        Returns:
            response_body: The parsed json from the HTTP response
                containing a storage Job.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        if not isinstance(table_id, str) or table_id == '':
            raise ValueError("Invalid file_id '{}'.".format(table_id))
        body = {
            'delimiter': delimiter,
            'enclosure': enclosure,
            'escapedBy': escaped_by,
            'incremental': int(is_incremental),
            'withoutHeaders': int(without_headers)
        }
        body.update(self.validate_data_source(data_url, data_file_id,
                                              snapshot_id, data_workspace_id,
                                              data_table_name))
        if enclosure is not '' and escaped_by is not '':
            raise ValueError("Only one of enclosure and escaped_by may be "
                             "specified.")
        if columns is not None and isinstance(columns, list):
            body['primaryKey[]'] = columns
        url = '{}/{}/import-async'.format(self.base_url, table_id)
        return self._post(url, data=body)

    @staticmethod
    def validate_filter(where_column, where_operator, where_values):
        params = {}
        if where_column is not None and where_values is not None:
            if not isinstance(where_column, str):
                raise ValueError("Invalid where_column '{}'.".
                                 format(where_column))
            if not isinstance(where_operator, str) or \
                    where_operator not in ('eq', 'neq'):
                raise ValueError("Invalid where_operator '{}'.".
                                 format(where_operator))
            if not isinstance(where_values, list):
                raise ValueError("Invalid where_values '{}'.".
                                 format(where_values))
            params['whereValues[]'] = where_values
            params['whereColumn'] = where_column
            params['whereOperator'] = where_operator
        return params

    def preview(self, table_id, changed_since=None, changed_until=None,
                columns=None, where_column=None, where_values=None,
                where_operator='eq'):
        """
        Export preview of a table.

        Args:
            table_id (str): Table id
            changed_until (str): Filtering by import date
                Both until and since values can be a unix timestamp or any
                date accepted by strtotime.
            changed_since (str): Filtering by import date
                Both until and since values can be a unix timestamp or any
                date accepted by strtotime.
            where_column (str): Column for exporting only matching rows
            where_operator (str): 'eq' or 'neq'
            where_values (list): Values for exporting only matching rows
            columns (list): List of columns to display

        Returns:
            response_body: Table data contents.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        params = {}
        if not isinstance(table_id, str) or table_id == '':
            raise ValueError("Invalid table_id '{}'.".format(table_id))
        if changed_since is not None:
            if not isinstance(changed_since, str):
                raise ValueError("Invalid changed_since '{}'.".
                                 format(changed_since))
            params['changedSince'] = changed_since
        if changed_until is not None:
            if not isinstance(changed_until, str):
                raise ValueError("Invalid changed_until '{}'.".
                                 format(changed_until))
            params['changedUntil'] = changed_until
        params.update(self.validate_filter(where_column, where_operator,
                                           where_values))
        if columns is not None and isinstance(columns, list):
            params['columns'] = ','.join(columns)
        url = '{}/{}/data-preview'.format(self.base_url, table_id)
        r = self._get_raw(url=url, params=params)
        return r.content.decode('utf-8')

    def export_to_file(self, table_id, path_name, limit=None,
                       file_format='rfc', changed_since=None,
                       changed_until=None, columns=None,
                       where_column=None, where_values=None,
                       where_operator='eq', is_gzip=True):
        """
        Export data from a table to a local file

        Args:
            table_id (str): Table id
            path_name (str): Destination path for file.
            limit (int): Number of rows to export.
            file_format (str): 'rfc', 'escaped' or 'raw'
            changed_until (str): Filtering by import date
                Both until and since values can be a unix timestamp or any
                date accepted by strtotime.
            changed_since (str): Filtering by import date
                Both until and since values can be a unix timestamp or any
                date accepted by strtotime.
            where_column (str): Column for exporting only matching rows
            where_operator (str): 'eq' or 'neq'
            where_values (list): Values for exporting only matching rows
            columns (list): List of columns to display
            is_gzip (bool): Result will be gzipped

        Returns:
            destination_file: Local file with exported data

        Raises:
            requests.HTTPError: If the API request fails.
        """

        table_detail = self.detail(table_id)
        job = self.export_raw(table_id=table_id, limit=limit,
                              file_format=file_format,
                              changed_since=changed_since,
                              changed_until=changed_until, columns=columns,
                              where_column=where_column,
                              where_values=where_values,
                              where_operator=where_operator, is_gzip=is_gzip)
        jobs = Jobs(self.root_url, self.token)
        job = jobs.block_until_completed(job['id'])
        if job['status'] == 'error':
            raise RuntimeError(job['error']['message'])
        files = Files(self.root_url, self.token)
        temp_path = tempfile.TemporaryDirectory()
        local_file = files.download(file_id=job['results']['file']['id'],
                                    local_path=temp_path.name)
        destination_file = os.path.join(path_name, table_detail['name'])
        # the file containing table export is always without headers (it is
        # always sliced on Snowflake and Redshift
        if is_gzip:
            import gzip
            import shutil
            with gzip.open(local_file, 'rb') as f_in, \
                    open(local_file + '.un', 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
            os.remove(local_file)
            local_file = local_file + '.un'

        with open(local_file, mode='rb') as in_file, \
                open(destination_file, mode='wb') as out_file:
            columns = table_detail['columns']
            columns = ['"{}"'.format(col) for col in columns]
            header = ",".join(columns) + '\n'
            out_file.write(header.encode('utf-8'))
            for line in in_file:
                out_file.write(line)
        return destination_file

    def export(self, table_id, limit=None, file_format='rfc',
               changed_since=None, changed_until=None, columns=None,
               where_column=None, where_values=None, where_operator='eq',
               is_gzip=False):
        """
        Export data from a table to a Storage file

        Args:
            table_id (str): Table id
            limit (int): Number of rows to export.
            file_format (str): 'rfc', 'escaped' or 'raw'
            changed_until (str): Filtering by import date
                Both until and since values can be a unix timestamp or any
                date accepted by strtotime.
            changed_since (str): Filtering by import date
                Both until and since values can be a unix timestamp or any
                date accepted by strtotime.
            where_column (str): Column for exporting only matching rows
            where_operator (str): 'eq' or 'neq'
            where_values (list): Values for exporting only matching rows
            columns (list): List of columns to display
            is_gzip (bool): Result will be gzipped

        Returns:
            response_body: File id of the table export

        Raises:
            requests.HTTPError: If the API request fails.
        """

        job = self.export_raw(table_id=table_id, limit=limit,
                              file_format=file_format,
                              changed_since=changed_since,
                              changed_until=changed_until, columns=columns,
                              where_column=where_column,
                              where_values=where_values,
                              where_operator=where_operator, is_gzip=is_gzip)
        jobs = Jobs(self.root_url, self.token)
        job = jobs.block_until_completed(job['id'])
        if job['status'] == 'error':
            raise RuntimeError(job['error']['message'])
        return job['results']['file']['id']

    def export_raw(self, table_id, limit=None, file_format='rfc',
                   changed_since=None, changed_until=None, columns=None,
                   where_column=None, where_values=None, where_operator='eq',
                   is_gzip=False):
        """
        Export data from a table

        Args:
            table_id (str): Table id
            limit (int): Number of rows to export.
            file_format (str): 'rfc', 'escaped' or 'raw'
            changed_until (str): Filtering by import date
                Both until and since values can be a unix timestamp or any
                date accepted by strtotime.
            changed_since (str): Filtering by import date
                Both until and since values can be a unix timestamp or any
                date accepted by strtotime.
            where_column (str): Column for exporting only matching rows
            where_operator (str): 'eq' or 'neq'
            where_values (list): Values for exporting only matching rows
            columns (list): List of columns to display
            is_gzip (bool): Result will be gzipped

        Returns:
            response_body: The parsed json from the HTTP response
                containing a storage Job.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        params = {
            'gzip': int(is_gzip)
        }
        if not isinstance(table_id, str) or table_id == '':
            raise ValueError("Invalid table_id '{}'.".format(table_id))
        if limit is not None and limit is not isinstance(table_id, int):
            raise ValueError("Invalid limit '{}'.".format(limit))
        if file_format not in ('rfc', 'escaped', 'raw'):
            raise ValueError("Invalid format '{}'.".format(file_format))
        if changed_since is not None:
            if not isinstance(changed_since, str):
                raise ValueError("Invalid changed_since '{}'.".
                                 format(changed_since))
            params['changedSince'] = changed_since
        if changed_until is not None:
            if not isinstance(changed_until, str):
                raise ValueError("Invalid changed_until '{}'.".
                                 format(changed_until))
            params['changedUntil'] = changed_until
        params.update(self.validate_filter(where_column, where_operator,
                                           where_values))
        if columns is not None and isinstance(columns, list):
            params['columns'] = ','.join(columns)
        url = '{}/{}/export-async'.format(self.base_url, table_id)
        return self._post(url, data=params)

    def optimize(self, table_id):
        """Optimize RedShift table size

        http://docs.keboola.apiary.io/#reference/tables/table-optimize/optimize-table

        Args:
            table_id (str): table id to optimize ("in.c-my-bucket.table666")

        Returns:
            json object with optimization statistics. The optimization
            happens asynchronously. You can use the 'id' parameter from the
            response body to poll the status of the job using methods
            implemented in Client.Jobs endpoint. (detail, block_for_success,
            block_until_completed, etc...)

        """
        url = '{}/{}/optimize'.format(self.base_url, table_id)
        return self._post(url)



================================================
FILE: kbcstorage/workspaces.py
================================================
"""
Manages calls to the Storage API relating to workspaces.

Full documentation `here`.

.. _here:
    http://docs.keboola.apiary.io/#reference/workspaces/
"""
from kbcstorage.base import Endpoint


def _make_body(mapping):
    """
    Given a dict mapping Keboola tables to aliases, construct the body of
    the HTTP request to load said tables.

    Args:
        mapping(:obj:`dict`): Keys contain the full names of the tables to
            be loaded (ie. 'in.c-bucker.table_name') and values contain the
            aliases to which they will be loaded (ie. 'table_name').
    """
    body = {}
    template = 'input[{0}][{1}]'
    for i, (k, v) in enumerate(mapping.items()):
        body[template.format(i, 'source')] = k
        body[template.format(i, 'destination')] = v

    return body


class Workspaces(Endpoint):
    """
    Workspaces Endpoint
    """
    def __init__(self, root_url, token):
        """
        Create a Workspaces endpoint.

        Args:
            root_url (:obj:`str`): The base url for the API.
            token (:obj:`str`): A storage API key.
        """
        super().__init__(root_url, 'workspaces', token)

    def list(self):
        """
        List the details of all workspaces in the project.

        Returns:
            response_body: The json from the HTTP response.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        return self._get(self.base_url)

    def detail(self, workspace_id):
        """
        Retrieves information about a given workspace.

        Note that the password to the workspace can only be retrieved when the
        workspace is created.

        Args:
            workspace_id (int or str): The id of the workspace.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}'.format(self.base_url, workspace_id)
        return self._get(url)

    def create(self, backend=None, timeout=None):
        """
        Create a new Workspace and return the credentials.

        Args:
            backend (:obj:`str`): The type of engine for the workspace.
                'redshift' or 'snowflake'. Default redshift.
            timeout (int): The timeout, in seconds, for SQL statements.
                Only supported by snowflake backends.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        body = {
            'backend': backend,
            'statementTimeoutSeconds': timeout
        }

        return self._post(self.base_url, data=body)

    def delete(self, workspace_id):
        """
        Deletes a workspace.

        This also irreversibly removes workspace content.

        Args:
            workspace_id (int or str): The id of the workspace to be deleted.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}'.format(self.base_url, workspace_id)

        self._delete(url)

    def reset_password(self, workspace_id):
        """
        Generate a new password for the workspace.

        Args:
            workspace_id (int or str): The id of the workspace for which the
                password should be reset.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        url = '{}/{}/password'.format(self.base_url, workspace_id)
        return self._post(url)

    def load_tables(self, workspace_id, table_mapping, preserve=None):
        """
        Load tabes from storage into a workspace.

        Args:
            workspace_id (int or str): The id of the workspace to which to load
                the tables.
            table_mapping (:obj:`dict`): Source table names mapped to
                destination table names.
            preserve (bool): If False, drop tables, else keep tables in
                workspace.

        Raises:
            requests.HTTPError: If the API request fails.

        Todo:
            * Column data types.
        """
        body = _make_body(table_mapping)
        body['preserve'] = preserve
        url = '{}/{}/load'.format(self.base_url, workspace_id)

        return self._post(url, data=body)


