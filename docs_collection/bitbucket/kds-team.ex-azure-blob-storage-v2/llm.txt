Directory structure:
└── kds_consulting_team-kds-team.ex-azure-blob-storage-v2/
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    ├── change_log.md
    ├── Dockerfile
    ├── flake8.cfg
    ├── src/
    │   ├── azure_oauth.py
    │   ├── blob_procedure.py
    │   └── component.py
    ├── LICENSE.md
    ├── docs/
    │   └── imgs/
    ├── requirements.txt
    ├── bitbucket-pipelines.yml
    ├── component_config/
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── stack_parameters.json
    │   ├── configuration_description.md
    │   ├── component_long_description.md
    │   ├── sample-config/
    │   │   ├── in/
    │   │   │   ├── tables/
    │   │   │   │   ├── test.csv
    │   │   │   │   └── test.csv.manifest
    │   │   │   ├── state.json
    │   │   │   └── files/
    │   │   │       └── order1.xml
    │   │   ├── out/
    │   │   │   ├── tables/
    │   │   │   │   └── test.csv
    │   │   │   └── files/
    │   │   │       └── order1.xml
    │   │   └── config.json
    │   └── configSchema_v2.json
    ├── deploy.sh
    ├── docker-compose.yml
    ├── scripts/
    │   ├── build_n_test.sh
    │   ├── update_dev_portal_properties.sh
    │   ├── build_n_run.ps1
    │   ├── run.bat
    │   └── run_kbc_tests.ps1
    └── README.md

================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: /change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog


================================================
File: /Dockerfile
================================================
FROM python:3.7.2-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential && apt-get install -y git

RUN pip install --upgrade pip

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]


================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /src/azure_oauth.py
================================================
'''
Template Component main class.

'''

import csv  # noqa
import logging
import os  # noqa
import re  # noqa
import sys
import json  # noqa
import requests
import datetime

import logging_gelf.formatters
import logging_gelf.handlers  # noqa
# from azure.storage.blob import BlockBlobService, PublicAccess  # noqa
from azure.identity import AuthorizationCodeCredential  # noqa
from azure.storage.blob import ContainerClient
from kbc.env_handler import KBCEnvHandler
from kbc.result import KBCTableDef  # noqa
from kbc.result import ResultWriter  # noqa
from msal import token_cache  # noqa

from blob_procedure import Blob_Procedure

# configuration variables
KEY_ACCOUNT_NAME = 'account_name'
KEY_ACCOUNT_KEY = '#account_key'
KEY_CONTAINER_NAME = 'container_name'
KEY_NEW_FILE_ONLY = 'new_file_only'
# KEY_FILES = 'files'
KEY_FILE = 'file'
KEY_DEBUG = 'debug'

MANDATORY_PARS = [
    KEY_ACCOUNT_NAME,
    KEY_ACCOUNT_KEY,
    KEY_CONTAINER_NAME,
    KEY_FILE
]
MANDATORY_IMAGE_PARS = []

APP_VERSION = '0.1.5'


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS)
        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        # Disabling list of libraries you want to output in the logger
        disable_libraries = [
            'azure.core.pipeline.policies.http_logging_policy'
        ]
        for library in disable_libraries:
            logging.getLogger(library).disabled = True

        try:
            self.validate_config()
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as e:
            logging.error(e)
            exit(1)

    def validate_config_params(self, params):
        '''
        Validating if input configuration contain everything needed
        '''

        # Credentials Conditions
        # Validate if config is blank
        if params == {}:
            logging.error(
                'Configurations are missing. Please configure your component.')
            sys.exit(1)

        # Validate if the configuration is empty
        empty_config = {
            'account_name': '',
            '#account_key': '',
            'container_name': ''
        }
        if params == empty_config:
            logging.error(
                'Configurations are missing. Please configure your component.')
            sys.exit(1)

        # Validating config parameters
        if params[KEY_ACCOUNT_NAME] == '' or params[KEY_ACCOUNT_KEY] == '':
            logging.error(
                "Credientials missing: Account Name, Access Key...")
            sys.exit(1)
        if params[KEY_CONTAINER_NAME] == '':
            logging.error(
                "Blob Container name is missing, check your configuration.")
            sys.exit(1)

    def refresh_token(self, refresh_token, client_id, client_secret):
        '''
        Refreshing Blob Account token
        '''

        logging.info('Authenticating...')
        request_url = 'https://login.microsoftonline.com/common/oauth2/token'
        request_body = {
            'grant_type': 'refresh_token',
            'refresh_token': refresh_token,
            'client_id': client_id,
            'client_secret': client_secret,
            'resource': 'https://storage.azure.com'
        }

        request = requests.post(url=request_url, data=request_body)

        if request.status_code in [200]:
            return request.json()['access_token']
        else:
            logging.error(
                f'Error in refreshing access token. Message: {request.json()}')

    def run(self):
        '''
        Main execution code
        '''

        params = self.cfg_params  # noqa
        self.validate_config_params(params)

        # Get proper list of tables
        now = int(datetime.datetime.utcnow().timestamp())
        account_name = params.get(KEY_ACCOUNT_NAME)
        account_key = params.get(KEY_ACCOUNT_KEY)  # noqa
        container_name = params.get(KEY_CONTAINER_NAME)
        file = params.get(KEY_FILE)
        new_file_only = params.get(KEY_FILE).get(KEY_NEW_FILE_ONLY)
        logging.debug(f'New File Only: {new_file_only}')
        account_url = '{}.blob.core.windows.net'.format(account_name)

        # Get state file
        state = self.get_state_file()  # noqa
        if state and new_file_only:
            last_run_timestamp = state.get('component').get('lastRunTimestamp')

        else:
            last_run_timestamp = None
            state = {}

        # OAuth Method
        authorization = self.get_authorization()
        auth_data = json.loads(authorization['#data'])
        # oauth_token = auth_data['access_token']
        refresh_token = auth_data['refresh_token'] if not state.get(
            '#refresh_token') else state.get('#refresh_token')
        active_directory_tenant_id = 'common'
        active_directory_application_id = authorization['appKey']
        active_directory_application_secret = authorization['#appSecret']

        access_token = self.refresh_token(
            refresh_token=refresh_token,
            client_id=active_directory_application_id,
            client_secret=active_directory_application_secret
        )

        # define scopes that are needed for azure app -> this is the same that was used to register the app
        # and the oAuth broker. It is needed for the refresh
        scope = ['https://storage.azure.com/.default']
        # create first synthetic event to store access_token and refresh_token from #data in Credentials cache
        # because this first step was done by the oAuth broker
        synth_first_event = {'response': {'access_token': access_token,
                                          'refresh_token': refresh_token},
                             'scope': scope}

        # add to cache
        pre_populated_cache = token_cache.TokenCache()
        pre_populated_cache.add(synth_first_event)

        # create auth_code credential with prepopulated cache -
        # we don't need auth_code and redirect url since its the first step
        token_credentials = AuthorizationCodeCredential(
            tenant_id=active_directory_tenant_id,
            client_id=active_directory_application_id,
            authorization_code='',
            redirect_uri='',
            client_secret=active_directory_application_secret,
            cache=pre_populated_cache
        )

        # Instantiate a BlobServiceClient using a token credential
        # from azure.storage.blob import BlobServiceClient
        # blob_service_client = BlobServiceClient(account_url="https://kbcblob.blob.core.windows.net",
        #                                         credential=token_credentials)
        # # [END create_blob_service_client_oauth]
        # # Get account information for the Blob Service
        # # account_info = blob_service_client.get_service_properties()

        # Initializing Blob Container client
        blob_container_client = ContainerClient(
            account_url=account_url,
            container_name=container_name,
            # credential=account_key
            credential=token_credentials
        )

        # Processing blobs
        Blob_Procedure(
            blob_obj=blob_container_client,
            file=file,
            current_timestamp=now,
            DEFAULT_TABLE_SOURCE=f'{self.tables_in_path}/',
            DEFAULT_TABLE_DESTINATION=f'{self.files_out_path}/',
            last_run_timestamp=last_run_timestamp
        )

        # Updating State file
        # OAuth Piece
        state['#refresh_token'] = refresh_token

        if new_file_only:
            state['component'] = {}
            state['component']['lastRunTimestamp'] = now
            self.write_state_file(state)
            logging.debug('Latest State: {}'.format(state))

        logging.info("Blob Storage Extraction finished")


"""
        Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug = sys.argv[1]
    else:
        debug = True
    comp = Component(debug)
    comp.run()


================================================
File: /src/blob_procedure.py
================================================
import datetime
import logging
import sys
import time
from pathlib import Path

import backoff
from azure.core.exceptions import ResourceModifiedError, HttpResponseError
from azure.storage.blob import ContainerClient
from azure.storage.blob._models import BlobPrefix
from fnmatch2 import fnmatch2


def _retry_on_race_condititon(details):
    logging.warning(f"Resource modified during extraction, retrying in {details['wait']}")


class BlobRequestError(Exception):
    pass


class Blob_Procedure():

    def __init__(self, blob_obj,
                 file,
                 current_timestamp,
                 DEFAULT_TABLE_SOURCE="/data/in/tables/",
                 DEFAULT_TABLE_DESTINATION="/data/out/files/",
                 last_run_timestamp=None):
        self.blob_obj: ContainerClient = blob_obj
        self.last_run_timestamp = last_run_timestamp
        self.current_timestamp = current_timestamp
        self.DEFAULT_TABLE_SOURCE = DEFAULT_TABLE_SOURCE
        self.DEFAULT_TABLE_DESTINATION = DEFAULT_TABLE_DESTINATION
        try:
            self.standard_process(file)
        except HttpResponseError as e:
            if e.status_code == 403:
                error = f'The credentials do not have permission to access the file {file}. ' \
                        'Check if the token has read & list permissions.'
            elif e.status_code == 401:
                error = 'The credentials are invalid! Check the SAS key / token'
            else:
                error = e.reason
            raise BlobRequestError(error)

    def list_all_blobs_legacy(self):
        return self.blob_obj.list_blobs(retry_total=0)

    def standard_process(self, file: dict):
        """
        Main process to download the blob file(s)
        :param file:
        :return:
        """

        logging.info("Listing available blobs")
        try:
            # fallback to legacy listing method for backward compatibility
            if "**" in file["file_name"]:
                all_files = self.list_all_blobs_legacy()
            else:
                all_files = self.list_blobs_hierarchical_listing(file)
        except Exception as err:
            # TODO: raise exception instead of exit
            logging.exception(f'Error occurred while listing blob files: {err}')
            logging.error('Please contact support.')
            sys.exit(1)

        logging.info('Processing [{}]...'.format(file['file_name']))
        start = time.time()

        blobs_to_download = self.qualify_files(
            file_pattern=file['file_name'], blob_files=all_files)

        end = time.time()
        logging.debug(f'List blobs: {end - start}')

        # To add timestamp into the file or not
        add_timestamp_bool = file['add_timestamp'] if 'add_timestamp' in file else False

        # When there is nothing to download
        if len(blobs_to_download) == 0:
            return

        start = time.time()
        for _index, blob in enumerate(blobs_to_download):

            blob_full_filename = blob['name']
            blob_created_timestamp = blob['created_timestamp']
            logging.info(f'Downloading [{blob_full_filename}]...')

            # Blob Paths and filenames
            # blob_file_full_path needs to be relative path, otherwise the join will fail
            if blob_full_filename.startswith('/'):
                blob_full_filename = blob_full_filename.split(
                    '/', maxsplit=1)[1]

            output_dir_path = Path(self.DEFAULT_TABLE_DESTINATION)
            # blob_file_full_path = Path(blob_full_filename)
            blob_file_full_path = blob_full_filename.replace('/', '_')

            output_full_file_path = output_dir_path.joinpath(
                blob_file_full_path)

            # append unique id to file, because there can be blob and folder of a same name in one folder
            output_full_file_path = output_full_file_path.parent.joinpath(
                f'{_index}_{output_full_file_path.name}')

            # Append timestamp
            if add_timestamp_bool:
                output_full_file_path = output_full_file_path.parent.joinpath(
                    f'{blob_created_timestamp}_{output_full_file_path.name}')

            # Creating directory
            output_full_file_path.parent.mkdir(parents=True, exist_ok=True)

            logging.debug(f'{blob_file_full_path} --> {output_full_file_path}')

            # download with retry rather than creating a lease
            self._download_into_file(output_full_file_path, blob_full_filename)

        end = time.time()
        logging.debug(f'Download blobs: {end - start}')

    def get_file_folders(self, file_name):
        file_paths = self.get_file_path(file_name)
        if len(file_paths) > 1:
            return file_paths[:-1]
        return []

    @staticmethod
    def get_file_path(file_name):
        return file_name.split("/")

    @staticmethod
    def get_file_name_from_path(file_name):
        file_paths = file_name.split("/")
        if len(file_paths) > 1:
            return file_paths[-1]
        return file_name

    def list_blobs_hierarchical_listing(self, file: dict):
        """
        List blobs by hierarchically walking the directories and filtering on path prefix.
        :param file: (dict) file defintion / pattern
        :return:
        """
        file_folders = self.get_file_folders(file['file_name'])
        longest_direct_path = self.get_longest_direct_path(file_folders)

        files = []

        def walk_blob_hierarchy(prefix=""):
            for item in self.blob_obj.walk_blobs(name_starts_with=prefix):
                if isinstance(item, BlobPrefix):
                    if fnmatch2(item.name, file["file_name"]):
                        walk_blob_hierarchy(prefix=item.name)
                    elif self.matches(item, file_folders):
                        walk_blob_hierarchy(prefix=item.name)
                    else:
                        logging.debug(f"ignoring folder {item.name}")
                        continue
                else:
                    if fnmatch2(item.name, file['file_name']):
                        files.append(item)

        walk_blob_hierarchy(prefix=longest_direct_path)
        return files

    @staticmethod
    def get_item_folders(item):
        item_folders = item.name.split("/")
        if not item_folders[-1]:
            item_folders = item_folders[0:-1]
        return item_folders

    @staticmethod
    def is_direct_path(path):
        if any(character in path for character in ["*", "[", "]", "?", "!"]):
            return False
        return True

    def get_longest_direct_path(self, path_list):
        longest_direct_path = []
        for path in path_list:
            if self.is_direct_path(path):
                longest_direct_path.append(path)
            else:
                break
        return "/".join(longest_direct_path)

    def matches(self, item, file_folders):
        item_folders = self.get_item_folders(item)
        if len(file_folders) < len(item_folders):
            return False
        for i, folder in enumerate(item_folders):
            if not fnmatch2(item_folders[i], file_folders[i]):
                return False
        return True

    @backoff.on_exception(backoff.expo,
                          ResourceModifiedError,
                          max_tries=3,
                          on_backoff=_retry_on_race_condititon
                          )
    def _download_into_file(self, output_full_file_path, blob_full_filename):
        with open(output_full_file_path, 'wb') as my_blob:
            blob_data = self.blob_obj.download_blob(
                blob=blob_full_filename)
            blob_data.readinto(my_blob)

    def qualify_files(self, file_pattern, blob_files=None):
        '''
        Fetching BLOBs matches with GLOB configuration
        '''
        qualified_files = []
        qualified_files_short = []

        for blob in blob_files:

            blob_name = blob['name']
            blob_last_modified = datetime.datetime.timestamp(
                blob['last_modified'])
            blob_creation_date = datetime.datetime.timestamp(
                blob['creation_time'])

            match_bool = fnmatch2(blob_name, file_pattern)
            logging.debug(
                f'Matched: {match_bool} | {file_pattern} <> {blob_name}')

            if match_bool is True:
                blob_obj = {}
                if self.last_run_timestamp:
                    if int(blob_last_modified) >= int(self.last_run_timestamp):
                        logging.debug('New File: True')
                        blob_obj['name'] = blob_name
                        blob_obj['created_timestamp'] = blob_creation_date
                        qualified_files.append(blob_obj)
                        qualified_files_short.append(blob_name)

                    else:
                        logging.debug('New File: False')

                else:
                    # qualified_files += [blob_name]
                    blob_obj['name'] = blob_name
                    blob_obj['created_timestamp'] = int(blob_creation_date)
                    qualified_files.append(blob_obj)
                    qualified_files_short.append(blob_name)

        logging.info(f'Number of qualified blob files: {len(qualified_files)}')
        logging.debug(f'Qualified Files: {qualified_files_short}')

        return qualified_files


================================================
File: /src/component.py
================================================
'''
Template Component main class.

'''

import datetime
import logging
import os
import sys
from typing import Dict

import logging_gelf.handlers  # noqa
from azure.core.exceptions import ResourceNotFoundError, HttpResponseError
# from azure.storage.blob import BlockBlobService, PublicAccess  # noqa
from azure.identity import AuthorizationCodeCredential  # noqa
from azure.storage.blob import ContainerClient
from azure.storage.blob._shared.authentication import AzureSigningError
from kbc.env_handler import KBCEnvHandler
from kbc.result import KBCTableDef  # noqa
from kbc.result import ResultWriter  # noqa
from kbcstorage.workspaces import Workspaces
from msal import token_cache  # noqa
from requests import HTTPError

from blob_procedure import Blob_Procedure, BlobRequestError

# configuration variables
KEY_AUTH_TYPE = "auth_type"

KEY_ACCOUNT_NAME = 'account_name'
KEY_ACCOUNT_KEY = '#account_key'

KEY_WORKSPACE_ID = "workspace_id"
KEY_STORAGE_TOKEN = "#storage_token"

KEY_CONTAINER_NAME = 'container_name'
KEY_NEW_FILE_ONLY = 'new_files_only'
KEY_FILE = 'file'
KEY_DEBUG = 'debug'
KEY_BLOB_DOMAIN = 'blob_domain'

WORKSPACE_AUTH_TYPE = "Workspace Credentials"
AZURE_AUTH_TYPE = "Azure Credentials"

MANDATORY_PARS = [
    KEY_ACCOUNT_NAME,
    KEY_ACCOUNT_KEY,
    KEY_CONTAINER_NAME,
    KEY_FILE
]
MANDATORY_IMAGE_PARS = []

DEFAULT_BLOB_DOMAIN = 'blob.core.windows.net'

APP_VERSION = '0.2.1'


class UserException(Exception):
    pass


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS)
        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        # Disabling list of libraries you want to output in the logger
        disable_libraries = [
            'azure.core.pipeline.policies.http_logging_policy'
        ]
        for library in disable_libraries:
            logging.getLogger(library).disabled = True

        try:
            self.validate_config()
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as e:
            logging.error(e)
            exit(1)

    @staticmethod
    def validate_config_params(params: Dict) -> None:
        """
        Validating if input configuration contain everything needed
        """

        # Credentials Conditions
        # Validate if config is blank
        if params == {}:
            raise UserException('Configurations are missing. Please configure your component.')

        # Validate if the configuration is empty
        empty_config = {
            'account_name': '',
            '#account_key': '',
            'container_name': ''
        }
        if params == empty_config:
            raise UserException('Configurations are missing. Please configure your component.')

        # Validating config parameters
        if not params.get(KEY_ACCOUNT_NAME):
            raise UserException("Credientials missing: Account Name")

        if params.get(KEY_AUTH_TYPE, AZURE_AUTH_TYPE) == AZURE_AUTH_TYPE and not params.get(KEY_ACCOUNT_KEY):
            raise UserException("Credientials missing: Access Key.")

        if not params.get(KEY_CONTAINER_NAME):
            raise UserException("Blob Container name is missing, check your configuration.")

    def run(self):
        """
        Main execution code
        """

        params = self.cfg_params  # noqa
        self.validate_config_params(params)

        # Get proper list of tables
        now = int(datetime.datetime.utcnow().timestamp())
        account_name = params.get(KEY_ACCOUNT_NAME)
        account_key = params.get(KEY_ACCOUNT_KEY)  # noqa
        container_name = params.get(KEY_CONTAINER_NAME)
        file = params.get(KEY_FILE)

        # Validating user input file configuration
        if not file:
            logging.error('File configuration is missing.')
            sys.exit(1)

        # New file parameter if file configuration exists
        new_file_only = file.get(KEY_NEW_FILE_ONLY)
        logging.info(f'New File Only: {new_file_only}')
        blob_domain = params.get(KEY_BLOB_DOMAIN, DEFAULT_BLOB_DOMAIN)
        account_url = f'{account_name}.{blob_domain}'

        # Get state file
        state = self.get_state_file()  # noqa
        if state and new_file_only:
            last_run_timestamp = state.get('lastRunTimestamp')
            logging.info(f'Extracting from: {last_run_timestamp}')

        else:
            last_run_timestamp = None
            state = {}

        if params.get(KEY_AUTH_TYPE, AZURE_AUTH_TYPE) == WORKSPACE_AUTH_TYPE:
            workspace_token = params.get(KEY_STORAGE_TOKEN)
            workspace_id = params.get(KEY_WORKSPACE_ID)
            workspace_client = Workspaces(f'https://{os.environ.get("KBC_STACKID")}', workspace_token)
            account_key = self._refresh_abs_container_token(workspace_client, workspace_id)

        # Initializing Blob Container client
        blob_container_client = ContainerClient(
            account_url=account_url,
            container_name=container_name,
            credential=account_key
        )

        # Validating Credentials
        # Exceptions Handling
        try:
            logging.info("Validating connection.")
            blob_container_client.get_account_information()

        except AzureSigningError:
            # Credentials and Account Name validation
            logging.error(
                'The specified credentials [Account Name] & [Account Key]/[SAS token] are invalid.')
            sys.exit(1)

        except ResourceNotFoundError as e:
            # Container validation
            logging.error(e)
            sys.exit(1)
        except HttpResponseError as e:
            logging.error(e)
            sys.exit(1)

        except Exception as e:
            # If there are any other errors, reach out support
            logging.error(e)
            logging.error('Please validate your [Account Name].')
            sys.exit(1)

        # Processing blobs
        Blob_Procedure(
            blob_obj=blob_container_client,
            file=file,
            current_timestamp=now,
            DEFAULT_TABLE_SOURCE=f'{self.tables_in_path}/',
            DEFAULT_TABLE_DESTINATION=f'{self.files_out_path}/',
            last_run_timestamp=last_run_timestamp
        )

        # Updating State file regardless
        state = {'lastRunTimestamp': now}
        self.write_state_file(state)
        logging.debug(f'Writing State: {state}')

        logging.info("Blob Storage Extraction finished")

    @staticmethod
    def _refresh_abs_container_token(workspace_client: Workspaces, workspace_id: str) -> str:
        try:
            ps = workspace_client.reset_password(workspace_id)
            return ps['connectionString'].split('SharedAccessSignature=')[1]
        except HTTPError as e:
            if e.response.status_code == 404:
                raise UserException(f"Workspace ID: {workspace_id} was not found in the destination project. "
                                    f"Please check the ID and verify the correct Storage token is entered.") from e
            elif e.response.status_code == 401:
                raise UserException("Failed to refresh the SAS token. Invalid Storage token.") from e
            elif 400 >= e.response.status_code < 500:
                raise UserException(e) from e
            elif 500 >= e.response.status_code:
                raise e


"""
        Main entrypoint
"""

if __name__ == "__main__":
    debug = sys.argv[1] if len(sys.argv) > 1 else True

    try:
        comp = Component(debug)
        comp.run()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except BlobRequestError as e:
        logging.error(e)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)


================================================
File: /LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.10.zip#egg=kbc
logging_gelf==0.0.18
mock~=4.0.3
freezegun~=1.1.0
pandas
azure-storage-blob==12.3.1
azure-identity==1.3.1
fnmatch2~=0.0.8
backoff~=1.11.1
urllib3
git+https://github.com/davidesner/sapi-python-client.git#egg=kbcstorage
kbc~=0.2.10
msal~=1.17.0
requests~=2.27.1

================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - echo 'Pushing test image to repo. [tag=test]'
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:test
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /component_config/component_short_description.md
================================================
Extracting files from Azure Blob Storage Container

================================================
File: /component_config/configSchema.json
================================================
{}

================================================
File: /component_config/stack_parameters.json
================================================
{}

================================================
File: /component_config/configuration_description.md
================================================
1. Account Name - `Required`
    - Azure Storage Account Name
2. Account Key - `Required`
    - Access keys to authentication when making requests to the Azure Storage Account
    - Access Keys can be found under the **Settings** of the Storage Account
    - **Settings** >> **Access keys** >> **Key** from key1 or key2
3. Container Name - `Required`
    - Azure Storage Container Name

================================================
File: /component_config/component_long_description.md
================================================
This component is to extract a single or multiple files from a single or nested folders from an Azure Blob Container and stores them in Storage.

By default, the component expects CSV files that are stored in [table storage](https://help.keboola.com/storage/tables/), but it can handle any file type or destination using [processors](https://components.keboola.com/components?type=processor).

## Configurations

1. Account Name - `Required`
    - Azure Storage Account Name
2. Account Key - `Required`
    - Access keys to authentication when making requests to the Azure Storage Account
    - Access Keys can be found under the **Settings** of the Storage Account
    - **Settings** >> **Access keys** >> **Key** from key1 or key2
3. Container Name - `Required`
    - Azure Storage Container Name

================================================
File: /component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: /component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}


================================================
File: /component_config/configSchema_v2.json
================================================
{
  "type": "object",
  "title": "Parameters",
  "required": [
    "auth_type",
    "container_name"
  ],
  "properties": {
    "auth_type": {
      "type": "string",
      "title": "Authorization type",
      "propertyOrder": 1,
      "enum": [
        "Azure Credentials",
        "Workspace Credentials"
      ],
      "default": "Azure Credentials"
    },
    "account_name": {
      "description": "Azure Storage Resources Account Name",
      "type": "string",
      "title": "Account Name",
      "minLength": 1,
      "propertyOrder": 10,
      "default": "STORAGE_NAME"
    },
    "#account_key": {
      "description": "Azure Storage Access Key",
      "type": "string",
      "title": "Access Key",
      "propertyOrder": 20,
      "options": {
        "dependencies": {
          "auth_type": "Azure Credentials"
        }
      }
    },
    "workspace_id": {
      "type": "string",
      "title": "Workspace ID",
      "description": "ID of the persistent workspace.",
      "propertyOrder": 40,
      "options": {
        "dependencies": {
          "auth_type": "Workspace Credentials"
        }
      }
    },
    "#storage_token": {
      "type": "string",
      "format": "password",
      "title": "Storage Token (Workspace)",
      "description": "Storage token to refresh the Workspace SAS key.",
      "propertyOrder": 50,
      "options": {
        "dependencies": {
          "auth_type": "Workspace Credentials"
        }
      }
    },
    "container_name": {
      "type": "string",
      "title": "Container Name",
      "description": "Azure Storage Container Name",
      "minLength": 1,
      "propertyOrder": 60
    }
  }
}


================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /README.md
================================================
# Azure Blob Storage Extractor

This component is to extract a single or multiple CSV files from a single or nested folders from a Azure Blob Container and stores them into Keboola.

## Configurations

- **Account Name** - `Required`
    - Azure Storage Account Name
- **Account Key (Or SAS Token)** - `Required`
    - Account Key can be found:
      ```
      [Your Storage Account Overview] > Settings > Access Keys
      ```
    - SAS token can be generated from the Azure portal. More info [here](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview):
      ```
      [Your Storage Account Overview] > Your Blob Container > Shared access tokens
      ```   
- **Container Name** - `Required`
    - Azure Storage Container Name
    
    
- **File name** - Name of a source file with its extension or wildcard.
`folder/subfolder/test.csv` will download "test" CSV file from "folder/subfolder" directory
`test_*` will download all CSV files with "test_" prefix
- **New files only** - Every job stores the timestamp of the last downloaded file and a subsequent job can pick up from there.


### Raw JSON configuration example

```json
{ "parameters":{
        "account_name": "ACCOUNTNAME",
        "#account_key": "XXXXX",
        "path": "folder/subfolder/test.csv",
        "container_name": "keboola-test",
        "file":
            {
                "file_name": "testing/*",
                "storage": "testing",
                "incremental": false,
                "primary_key": []
            },
        "debug": true
    }
}
```

