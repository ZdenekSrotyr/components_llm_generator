Directory structure:
└── kds_consulting_team-kds-team.wr-graphql/
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    ├── change_log.md
    ├── Dockerfile
    ├── flake8.cfg
    ├── src/
    │   └── component.py
    ├── LICENSE.md
    ├── docs/
    │   └── imgs/
    ├── requirements.txt
    ├── bitbucket-pipelines.yml
    ├── component_config/
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── stack_parameters.json
    │   ├── configuration_description.md
    │   ├── component_long_description.md
    │   ├── sample-config/
    │   │   ├── in/
    │   │   │   ├── tables/
    │   │   │   │   ├── test.csv
    │   │   │   │   └── test.csv.manifest
    │   │   │   ├── state.json
    │   │   │   └── files/
    │   │   │       └── order1.xml
    │   │   ├── out/
    │   │   │   ├── tables/
    │   │   │   │   └── test.csv
    │   │   │   └── files/
    │   │   │       └── order1.xml
    │   │   └── config.json
    │   └── configSchema_v2.json
    ├── deploy.sh
    ├── docker-compose.yml
    ├── scripts/
    │   ├── build_n_test.sh
    │   ├── update_dev_portal_properties.sh
    │   ├── build_n_run.ps1
    │   ├── run.bat
    │   └── run_kbc_tests.ps1
    └── README.md

================================================
File: /tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: /tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: /change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog


================================================
File: /Dockerfile
================================================
FROM python:3.7.2-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install --upgrade pip

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]


================================================
File: /flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: /src/component.py
================================================
'''
GraphQL Writer

'''

import logging
import logging_gelf.handlers
import logging_gelf.formatters  # noqa
import sys
import os  # noqa
import datetime  # noqa
import json
import pandas as pd
import requests
from nested_lookup import nested_lookup

from kbc.env_handler import KBCEnvHandler
from kbc.result import KBCTableDef  # noqa
from kbc.result import ResultWriter  # noqa


# configuration variables
KEY_URL = 'url'
KEY_REQUEST_TYPE = 'request_type'
KEY_USER_PARAMETERS = 'user_parameters'
KEY_HEADERS = 'headers'
KEY_PARAMS = 'params'
KEY_GRAPHQL_QUERY = 'graphql_query'
KEY_GRAPHQL_VARIABLES = 'graphql_variables'
KEY_DEBUG = 'debug'

MANDATORY_PARS = [
    KEY_URL,
    KEY_REQUEST_TYPE,
    KEY_USER_PARAMETERS,
    KEY_HEADERS,
    KEY_PARAMS,
    KEY_GRAPHQL_QUERY,
    KEY_GRAPHQL_VARIABLES,
]
MANDATORY_IMAGE_PARS = []

# Default Table Output Destination
DEFAULT_TABLE_SOURCE = "/data/in/tables/"
DEFAULT_TABLE_DESTINATION = "/data/out/tables/"
DEFAULT_FILE_DESTINATION = "/data/out/files/"
DEFAULT_FILE_SOURCE = "/data/in/files/"

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s : [line:%(lineno)3s] %(message)s',
    datefmt="%Y-%m-%d %H:%M:%S")

# Disabling list of libraries you want to output in the logger
disable_libraries = []
for library in disable_libraries:
    logging.getLogger(library).disabled = True

if 'KBC_LOGGER_ADDR' in os.environ and 'KBC_LOGGER_PORT' in os.environ:
    logger = logging.getLogger()
    logging_gelf_handler = logging_gelf.handlers.GELFTCPSocketHandler(
        host=os.getenv('KBC_LOGGER_ADDR'), port=int(os.getenv('KBC_LOGGER_PORT')))
    logging_gelf_handler.setFormatter(
        logging_gelf.formatters.GELFFormatter(null_character=True))
    logger.addHandler(logging_gelf_handler)
    # remove default logging to stdout
    logger.removeHandler(logger.handlers[0])

NOW = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

APP_VERSION = '0.0.4'


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS)
        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        if self.cfg_params.get(KEY_DEBUG, False) is True:
            logger = logging.getLogger()
            logger.setLevel(level='DEBUG')

        try:
            self.validate_config()
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as e:
            logging.error(e)
            exit(1)

    def get_tables(self, tables, mapping):
        """
        Evaluate input and output table names.
        Only taking the first one into consideration!
        mapping: input_mapping, output_mappings
        """
        # input file
        table_list = []
        for table in tables:
            if mapping == "input_mapping":
                destination = table["destination"]
            elif mapping == "output_mapping":
                destination = table["source"]
            table_list.append(destination)

        return table_list

    def _fill_in_user_params(self, config_obj, user_params):
        '''
        Replacing attr with user parameters
        '''

        steps_string = json.dumps(config_obj, separators=(',', ':'))
        for key in user_params:
            lookup_str = '{"attr":"' + key + '"}'
            steps_string = steps_string.replace(
                lookup_str, '"' + str(user_params[key]) + '"')

        new_steps = json.loads(steps_string)
        non_matched = nested_lookup('attr', new_steps)

        if non_matched:
            raise ValueError(
                'Some user attributes [{}] specified in configuration '
                'are not present in "user_parameters" field.'.format(non_matched))

        return new_steps

    def validate_json(self, input_type, str_input):
        '''
        Validating if the input is a JSON
        '''

        try:
            output = json.loads(str_input)
        except Exception:
            if str_input != '':
                logging.error(
                    'Please validate JSON format: [{}]'.format(input_type))
                sys.exit(1)
            else:
                output = {}

        return output

    def validate_columns(self, input_columns, config_var):
        '''
        Validate whether the config column names exist in file inputs
        '''

        for variable in config_var:
            if variable not in input_columns:
                logging.error(
                    '[{}] does not exist in the input file. Please validate GraphQL variable inputs.'.format(variable))
                # variable['name']))s
                sys.exit(1)

    def define_datatype(self, datatype, value):
        '''
        Defining the datatype needed for the value
        '''

        if datatype == 'STRING':
            return str(value)
        elif datatype == 'FLOAT':
            return float(value)
        elif datatype == 'INTEGER':
            return int(value)

    def construct_variable_params(self, row, config_var):
        '''
        Constructing configuration variable for request
        '''

        params_var = {}
        for var in config_var:
            # params_var[var] = self.define_datatype(config_var[var], row [var])
            params_var[var] = row[var]

        return params_var

    def update_request(self, request_method, url, headers, params, body):
        '''
        Sending update requests
        '''

        # Data request
        if request_method == 'POST':
            r = requests.post(
                url=url,
                headers=headers,
                params=params,
                data=body
            )
        elif request_method == 'PUT':
            r = requests.put(
                url=url,
                headers=headers,
                params=params,
                data=body
            )
        elif request_method == 'PATCH':
            r = requests.patch(
                url=url,
                headers=headers,
                params=params,
                data=body
            )

        else:
            logging.error(
                'Configured Request Method [{}] is not supported. Please contact support.'.format(request_method))
            sys.exit(1)

        logging.debug(f'Request Response: {r.json()}')

        response_text = json.dumps(r.json())

        # Failing the compmonent if request is not 'successful'
        if r.status_code not in [200, 201, 202]:
            logging.error(
                'Request issue. Please refer to the error: {}'.format(
                    r.raise_for_status())
            )
            response_text = json.dumps(r.raise_for_status())
            # sys.exit(1)

        return r.status_code, response_text

    def output_log(self, df):
        '''
        Outputting log messages
        '''

        log_filename = 'request_log.csv'
        log_file_destination = DEFAULT_TABLE_DESTINATION+log_filename
        log_headers = [
            "source_file",
            "request_datetime",
            "request_query",
            "request_variables",
            "request_status",
            "request_response"
        ]

        if not os.path.isfile(log_file_destination):
            with open(log_file_destination, 'a') as b:
                df.to_csv(b, index=False, columns=log_headers)
            b.close()

            # Manfiest
            manifest = {
                "incremental": True,
                "primary_key": [
                    "source_file",
                    "request_datetime",
                    "request_query",
                    "request_variables"
                ]
            }
            with open(f'{log_file_destination}.manifest', 'w') as m:
                json.dump(manifest, m)
        else:
            with open(log_file_destination, 'a') as b:
                df.to_csv(b, index=False, header=False, columns=log_headers)
            b.close()

    def run(self):
        '''
        Main execution code
        '''
        # Get proper list of tables
        in_tables = self.configuration.get_input_tables()
        in_table_names = self.get_tables(in_tables, 'input_mapping')
        logging.info("IN tables mapped: "+str(in_table_names))

        # Terminate code if no input tables are found
        if len(in_table_names) == 0:
            logging.error('Input Mapping is missing.')
            sys.exit(1)

        # Wrtier configuration parameters
        params = self.cfg_params  # noqa
        url = params.get(KEY_URL)
        request_type = params.get(KEY_REQUEST_TYPE) if params.get(
            KEY_REQUEST_TYPE) else 'POST'
        user_parameters = params.get(KEY_USER_PARAMETERS) if params.get(
            KEY_USER_PARAMETERS) else {}
        headers = params.get(KEY_HEADERS) if params.get(KEY_HEADERS) else {}
        r_params = params.get(KEY_PARAMS) if params.get(KEY_PARAMS) else {}
        graphql_query = params.get(KEY_GRAPHQL_QUERY)
        graphql_variables = params.get(KEY_GRAPHQL_VARIABLES)

        # Validate input params
        if not url or not graphql_query or not graphql_variables:
            raise ValueError(
                'Required Parameters are missing: url, graphql_query, graphql_variables')

        # Reconstruct graphql variables, easier to work with
        graphql_var = graphql_variables
        # for variable in graphql_variables:
        #    graphql_var[variable['name']] = variable['datatype']

        # Validating Input JSONs Parameters
        # headers_json = self.validate_json('Request Headers', headers)
        # params_json = self.validate_json('Request Parameters', r_params)
        # Filling in user parametrs
        headers_json = self._fill_in_user_params(headers, user_parameters)
        params_json = self._fill_in_user_params(r_params, user_parameters)

        if len(graphql_variables) == 0:
            logging.error('Please input GraphQL Variables.')
            sys.exit(1)

        # Forcing Header's Content-Type to GraphQL
        headers_json['Content-Type'] = 'application/graphql'

        logging.debug('Request Type: {}'.format(request_type))
        logging.debug('Request Headers: {}'.format(headers_json))
        logging.debug('Request Parameters: {}'.format(params_json))
        logging.debug('GraphQL Query: {}'.format(graphql_query))

        # Gauge the number of total requests
        total_requests = 0

        # Read Input tables
        for table in in_tables:
            logging.info('Fetching details in [{}]'.format(
                table['destination']))

            # Reading Input file in chunks
            for data_in in pd.read_csv(table['full_path'], chunksize=100):
                # Logging all request to the Server
                request_log = []

                table_columns = list(data_in.columns)
                self.validate_columns(table_columns, graphql_var)

                for index, row in data_in.iterrows():
                    request_params_variables = self.construct_variable_params(
                        row=row, config_var=graphql_var)
                    logging.debug('Request Parameters Variables: {}'.format(
                        request_params_variables))

                    # Request Parameters for this row
                    # row_params = params_json
                    # row_params['variables'] = json.dumps(
                    #     request_params_variables)
                    # print(graphql_query)

                    # Request Body for this row
                    row_body = {}
                    row_body['query'] = graphql_query
                    row_body['variables'] = request_params_variables

                    request_status, response = self.update_request(
                        request_method=request_type,
                        url=url,
                        headers=headers_json,
                        # params=row_params,
                        params=params_json,
                        # body=graphql_query
                        body=json.dumps(row_body)
                    )

                    # Logging all of the requests going through
                    request_log_row = {
                        "source_file": table['destination'],
                        "request_datetime": NOW,
                        "request_query": graphql_query,
                        "request_variables": request_params_variables,
                        "request_status": request_status,
                        "request_response": response
                    }
                    request_log.append(request_log_row)

                    total_requests += 1
                    if total_requests % 100 == 0:
                        logging.info(
                            'Number of requests pushed: {}'.format(total_requests))

                out_log = pd.DataFrame(request_log)
                self.output_log(out_log)

        logging.info('Total requests pushed: {}'.format(total_requests))

        logging.info("GraphQL Writer finished")


"""
        Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug = sys.argv[1]
    else:
        debug = True
    comp = Component(debug)
    comp.run()


================================================
File: /LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: /requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.0.zip#egg=kbc
logging_gelf==0.0.18
mock
freezegun
pandas
nested_lookup

================================================
File: /bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
#          - echo 'Pushing test image to repo. [tag=test]'
#          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#          - docker tag $APP_IMAGE:latest $REPOSITORY:test
#          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#          - docker push $REPOSITORY:test

  branches:
    master:
      - step:
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=$APP_IMAGE
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh

================================================
File: /component_config/component_short_description.md
================================================
Publishing or updating data points with a specified GraphQL query to the API server that is powered by GraphQL.

================================================
File: /component_config/configSchema.json
================================================
{}

================================================
File: /component_config/stack_parameters.json
================================================
{}

================================================
File: /component_config/configuration_description.md
================================================
### Support Attributes
1. url - `string` (`required`)
    - Input Type: `string`
2. request_type - `string`
    1. POST
    2. PUT
    3. PATCH
    - Default - `POST`
3. user_parameters - `json`
    - {"attribute_name": "attribute_value"}
    - Variables can be used in headers and params
    - For any variables need encryption, add `#` in the prefix of the variable's name
        - {"#attribute_name": "attribute_value"}
4. headers - `json`
    - to call variables defined in user_parameters
        - {"attr": "attribute_name"}
5. params - `json`
    - to call variables defined in user_parameters
        - {"attr": "attribute_name"}
6. graphql_query - `string` (`required`)
    - GraphQL query which will be used for the requests
    - The query needs to be `URL encoded`
7. graphql_variables - `list` (`required`)
    - GraphQL variables which be used for the requests
    - All variables configured in this property needs to exist in the tables from the Input Mapping

### Sample Configuration
```
{
    "url": "https://www.testing.com/graphql",
    "request_type": "POST",
    "user_parameters": {
        "#token": "12345",
        "paging": "1"
    },
    "headers": {
        "token_testing": {
            "attr": "#token"
        }
    },
    "params": {
        "page": {
            "attr": "paging"
        }
    },
    "graphql_query": "URL_ENCODED_GRAPHQL_QUERY",
    "graphql_variables": [
        "orderid",
        "order__external_id__c",
        "totalprice",
        "unitprice",
        "quantity"
    ]
}
```

================================================
File: /component_config/component_long_description.md
================================================
The purpose of GraphQL Writer is to publish or update data points with a specified GraphQL query to the API server that is powered by GraphQL.

## Configuration

Tables from the input mapping will be used as an input for the GraphQL Writer. Each row from the input tables will be converted into a `graphql_variables`and convert each of the row into a variables object. Details regarding how to send GraphQL Queries with Variables can be found [here](https://graphql.org/learn/queries/#variables). 

### Support Attributes
1. url - `string` (`required`)
    - Input Type: `string`
2. request_type - `string`
    1. POST
    2. PUT
    3. PATCH
    - Default - `POST`
3. user_parameters - `json`
    - {"attribute_name": "attribute_value"}
    - Variables can be used in headers and params
    - For any variables need encryption, add `#` in the prefix of the variable's name
        - {"#attribute_name": "attribute_value"}
4. headers - `json`
    - to call variables defined in user_parameters
        - {"attr": "attribute_name"}
5. params - `json`
    - to call variables defined in user_parameters
        - {"attr": "attribute_name"}
6. graphql_query - `string` (`required`)
    - GraphQL query which will be used for the requests
    - The query needs to be `URL encoded`
7. graphql_variables - `list` (`required`)
    - GraphQL variables which be used for the requests
    - All variables configured in this property needs to exist in the tables from the Input Mapping

### Sample Configuration
```
{
    "url": "https://www.testing.com/graphql",
    "request_type": "POST",
    "user_parameters": {
        "#token": "12345",
        "paging": "1"
    },
    "headers": {
        "token_testing": {
            "attr": "#token"
        }
    },
    "params": {
        "page": {
            "attr": "paging"
        }
    },
    "graphql_query": "URL_ENCODED_GRAPHQL_QUERY",
    "graphql_variables": [
        "orderid",
        "order__external_id__c",
        "totalprice",
        "unitprice",
        "quantity"
    ]
}
```

### CURL Request from above Sample Configuration
```
curl --location --request POST 'https://www.testing.com/graphql?page=1' \
--header 'token_testing: 12345' \
--header 'Content-Type: application/json' \
--data-raw '{"query":"URL_ENCODED_GRAPHQL_QUERY","variables":{"orderid":"1234","order__external_id__c":"1234","totalprice":"10.00","unitprice":"5","quantity":"2"}}'
```


================================================
File: /component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: /component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: /component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: /component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: /component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}


================================================
File: /component_config/configSchema_v2.json
================================================
{
    "type": "object",
    "title": "Parameters",
    "required": [
        "url",
        "request_type",
        "headers",
        "params",
        "graphql_query",
        "graphql_variables"
    ],
    "properties": {
        "url": {
            "type": "string",
            "title": "Request URL",
            "minLength": 8,
            "propertyOrder": 100,
            "format": "url",
            "default": "https://"
        },
        "request_type": {
            "type": "string",
            "title": "Request Method",
            "enum": [
                "POST",
                "PUT",
                "PATCH"
            ],
            "default": "POST",
            "propertyOrder": 200
        },
        "user_parameters": {
            "type": "string",
            "title": "Custom User Parameters",
            "description": "Custom user parameters needed for the request headers and parameters",
            "propertyOrder": 300

        },
        "headers": {
            "type": "string",
            "title": "Request Headers",
            "default": "{}",
            "propertyOrder": 500,
            "format": "json",
            "description": "Valid JSON format required."
        },
        "params": {
            "type": "string",
            "title": "Request Parameters",
            "default": "{}",
            "propertyOrder": 600,
            "format": "json",
            "description": "Valid JSON format required."
        },
        "graphql_query": {
            "type": "string",
            "title": "GraphQL Query",
            "propertyOrder": 700,
            "format": "textarea"
        },
        "graphql_variables": {
            "type": "array",
            "title": "GraphQL Variables",
            "descriptions": "Variables required for the GraphQL Query",
            "propertyOrder": 800,
            "format": "table",
            "uniqueItems": true,
            "minLength": 1,
            "items": {
                "type": "object",
                "title": "Variable",
                "properties": {
                    "name": {
                        "type": "string",
                        "title": "Name"
                    },
                    "datatype": {
                        "type": "string",
                        "title": "DataType",
                        "default": "STRING",
                        "enum": [
                            "STRING",
                            "INTEGER",
                            "FLOAT"
                        ]
                    }
                }
            }
        }
    }
}

================================================
File: /deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
File: /docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: /scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: /scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

================================================
File: /scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: /scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag

================================================
File: /scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: /README.md
================================================
# GraphQL Writer

The purpose of GraphQL Writer is to publish or update data points with a specified GraphQL query to the API server that is powered by GraphQL.

## Configuration

Tables from the input mapping will be used as an input for the GraphQL Writer. Each row from the input tables will be converted into a `graphql_variables`and convert each of the row into a variables object. Details regarding how to send GraphQL Queries with Variables can be found [here](https://graphql.org/learn/queries/#variables). 

### Supported Attributes
1. url - `string` (`required`)
    - Input Type: `string`
2. request_type - `string`
    1. POST
    2. PUT
    3. PATCH
    - Default - `POST`
3. user_parameters - `json`
    - {"attribute_name": "attribute_value"}
    - Variables can be used in headers and params
    - For any variables need encryption, add `#` in the prefix of the variable's name
        - {"#attribute_name": "attribute_value"}
4. headers - `json`
    - to call variables defined in user_parameters
        - {"attr": "attribute_name"}
5. params - `json`
    - to call variables defined in user_parameters
        - {"attr": "attribute_name"}
6. graphql_query - `string` (`required`)
    - GraphQL query which will be used for the requests
    - The query needs to be `URL encoded`
7. graphql_variables - `list` (`required`)
    - GraphQL variables which be used for the requests
    - All variables configured in this property needs to exist in the tables from the Input Mapping

### Sample Configuration
```
{
    "url": "https://www.testing.com/graphql",
    "request_type": "POST",
    "user_parameters": {
        "#token": "12345",
        "paging": "1"
    },
    "headers": {
        "token_testing": {
            "attr": "#token"
        }
    },
    "params": {
        "page": {
            "attr": "paging"
        }
    },
    "graphql_query": "URL_ENCODED_GRAPHQL_QUERY",
    "graphql_variables": [
        "orderid",
        "order__external_id__c",
        "totalprice",
        "unitprice",
        "quantity"
    ]
}
```

### CURL Request from above Sample Configuration
```
curl --location --request POST 'https://www.testing.com/graphql?page=1' \
--header 'token_testing: 12345' \
--header 'Content-Type: application/json' \
--data-raw '{"query":"URL_ENCODED_GRAPHQL_QUERY","variables":{"orderid":"1234","order__external_id__c":"1234","totalprice":"10.00","unitprice":"5","quantity":"2"}}'
```

