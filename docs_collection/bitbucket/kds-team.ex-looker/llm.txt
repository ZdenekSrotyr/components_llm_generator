Directory structure:
└── kds_consulting_team-kds-team.ex-looker/
    ├── README.md
    ├── bitbucket-pipelines.yml
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── .dockerignore
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   └── stack_parameters.json
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run_kbc_tests.ps1
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── configuration.py
    │   └── client/
    │       ├── __init__.py
    │       └── client.py
    └── tests/
        ├── __init__.py
        └── test_component.py

================================================
FILE: README.md
================================================
# Looker Extractor for KBC

Looker extractor for KBC is an extension of extractors, which allows user to download data from looks in Looker via Looker API. Currently, the extractor is in development phase. See [documentation](https://github.com/SMKozuch/ex-looker/blob/master/README.md) for more information.

### API Limitations
The extractor is limited by **limitations** of Looker API, which currently are:

* If a look has more than 5000 rows and contains pivot columns, these columns along with dimensions will be omitted. Only non-pivoted columns will be downloaded. 
* If a look has less than 5000 rows, everything will be downloaded, without any limitations to pivoting and/or dimensions.
* If a look does not contain any pivot columns, everything will be downloaded with no limitations to row limit. 

If a limit is specified as `limit=-1`, all the data will be downloaded, even though it might get truncated due to above limitations.

### API secret and key
Obtaining the API secret and key is rather simple. In order to obtain the client secret and id, an admin access is required.
You can simply follow this [document](https://discourse.looker.com/t/using-the-looker-api-3/2988)
or the following steps:

1. Login to the relevant looker instance.

2. Navigate to **Admin** section (top right) and then **Users** in bar on the left.

3. Click on the edit button next to the relevant user.

4. Click on *Edit keys* in **API3 Keys** section.

5. Two scenarios can happen:
	* If API keys were generated previously, client id and secret will be displayed on screen.
	* If API keys were not generated previously, simply click on *New API3 Key* to generate the credentials.

6. Use obtained client id and secret to authorize the extractor.

### Input

Configuration schema accepts following parameters:

* **Client ID** - Client ID obtained in the API section of Looker dashboard.
* **Client Secret** - Client Secret obtained in the API section of Looker dashboard.
* **API Endpoint** - API Endpoint via which requests are sent.
* **Look ID** - ID of a look, from which the data should be downloaded. The look is ran automatically.
* **Destination Table** - A table in KBC Storage, where data will be loaded. If left blank, data will be downloaded to `in.c-looker.looker_data_xx`, where `xx` is equal to ID of a look.
* **Incremental Load** - Marks, whether load should be incremental.
* **Primary Key** - Comma-separated columns, which are to be used as PK.
* **Limit** - Row limit for look. See API Limitations above.

For more info about Looker API, see [Looker API Documentation](https://docs.looker.com/reference/api-and-integration/api-getting-started).

### Output

Output of the extractor is a table in storage data for the given look. By default, table is stored in `c-looker` bucket, with ID denoting the table. This behavior can be overwritten by specifying correct and full KBC destination in configuration schema.


================================================
FILE: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG


  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - echo 'Pushing test image to repo. [tag=test]'
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $KBC_CONFIG_1 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
FILE: change_log.md
================================================
**0.5.5**
Changed deployment script ot fetch configs.

**0.5.4**
Added proper primary key column sanitization.

**0.5.3**
Changed the destination bucket handling. Will be stripped of whitespaces by default.

**0.5.2**
Further changes to variable naming, added comments to functions.

**0.5.1**
Changed naming of the variables to avoid collisions with in-built functions

**0.5.0**
This is a migration of original private Looker extractor.
Added LICENSE.md
Added parameters check


================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.11-slim
ENV PYTHONIOENCODING utf-8

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY requirements.txt /code/requirements.txt
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.



================================================
FILE: requirements.txt
================================================
keboola.component==1.4.3
keboola.http-client==1.0.0
keboola.utils==1.1.0
looker-sdk==23.10.0
mock==5.0.2
freezegun==1.2.2
dataconf==2.2.1





================================================
FILE: .dockerignore
================================================
vendor
.git



================================================
FILE: component_config/component_long_description.md
================================================
# Looker Extractor for KBC

Looker extractor for KBC is an extension of extractors, which allows user to download data from looks in Looker via Looker API. Currently, the extractor is in development phase. See [documentation](https://github.com/SMKozuch/ex-looker/blob/master/README.md) for more information.

### API Limitations
The extractor is limited by **limitations** of Looker API, which currently are:

* If a look has more than 5000 rows and contains pivot columns, these columns along with dimensions will be omitted. Only non-pivoted columns will be downloaded. 
* If a look has less than 5000 rows, everything will be downloaded, without any limitations to pivoting and/or dimensions.
* If a look does not contain any pivot columns, everything will be downloaded with no limitations to row limit. 

If a limit is specified as `limit=-1`, all the data will be downloaded, even though it might get truncated due to above limitations.

### API secret and key
Obtaining the API secret and key is rather simple. In order to obtain the client secret and id, an admin access is required.
You can simply follow this [document](https://discourse.looker.com/t/using-the-looker-api-3/2988)
or the following steps:

1. Login to the relevant looker instance.

2. Navigate to **Admin** section (top right) and then **Users** in bar on the left.

3. Click on the edit button next to the relevant user.

4. Click on *Edit keys* in **API3 Keys** section.

5. Two scenarios can happen:
	* If API keys were generated previously, client id and secret will be displayed on screen.
	* If API keys were not generated previously, simply click on *New API3 Key* to generate the credentials.

6. Use obtained client id and secret to authorize the extractor.

### Input

Configuration schema accepts following parameters:

* **Client ID** - Client ID obtained in the API section of Looker dashboard.
* **Client Secret** - Client Secret obtained in the API section of Looker dashboard.
* **API Endpoint** - API Endpoint via which requests are sent.
* **Look ID** - ID of a look, from which the data should be downloaded. The look is ran automatically.
* **Destination Table** - A table in KBC Storage, where data will be loaded. If left blank, data will be downloaded to `in.c-looker.looker_data_xx`, where `xx` is equal to ID of a look.
* **Incremental Load** - Marks, whether load should be incremental.
* **Primary Key** - Comma-separated columns, which are to be used as PK.
* **Limit** - Row limit for look. See API Limitations above.

For more info about Looker API, see [Looker API Documentation](https://docs.looker.com/reference/api-and-integration/api-getting-started).

### Output

Output of the extractor is a table in storage data for the given look. By default, table is stored in `c-looker` bucket, with ID denoting the table. This behavior can be overwritten by specifying correct and full KBC destination in configuration schema.


================================================
FILE: component_config/component_short_description.md
================================================
Extractor for extracting data from Looker looks.


================================================
FILE: component_config/configRowSchema.json
================================================
{}


================================================
FILE: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Parameters",
  "required": [
    "client_id",
    "#client_secret",
    "api_endpoint",
    "looker_objects"
  ],
  "properties": {
    "client_id": {
      "type": "string",
      "title": "Client ID",
      "default": "",
      "minLength": 1,
      "description": "Client ID for API calls.",
      "propertyOrder": 1
    },
    "api_endpoint": {
      "type": "string",
      "title": "API Endpoint.",
      "default": "https://company.looker.com:19999/api/3.0/",
      "description": "API endpoint for API calls. Usually in form 'https://company.looker.com:19999/api/3.0/'.",
      "propertyOrder": 3
    },
    "#client_secret": {
      "type": "string",
      "title": "Client Secret",
      "format": "password",
      "default": "",
      "minLength": 1,
      "description": "Client secret for API calls.",
      "propertyOrder": 2
    },
    "looker_objects": {
      "type": "array",
      "items": {
        "type": "object",
        "title": "Object",
        "properties": {
          "id": {
            "type": "string",
            "title": "Look ID",
            "propertyOrder": 1
          },
          "limit": {
            "type": "integer",
            "title": "Limit",
            "default": 5000,
            "propertyOrder": 5
          },
          "output": {
            "type": "string",
            "title": "Destination table",
            "default": "",
            "propertyOrder": 2
          },
          "incremental": {
            "enum": [
              false,
              true
            ],
            "type": "boolean",
            "title": "Incremental load",
            "default": false,
            "propertyOrder": 3
          },
          "primary_key": {
            "type": "string",
            "title": "Primary key",
            "propertyOrder": 4
          }
        }
      },
      "title": "Looker objects",
      "format": "table",
      "default": [
        {
          "id": "1",
          "limit": 5000,
          "output": "",
          "incremental": false,
          "primary_key": ""
        }
      ],
      "description": "Look ID - ID of a look to be downloaded. \n Destination table - full path to KBC storage. If left blank, data will be downloaded to in.c-looker.look_data_id, where xx is the ID. \n Incremental - load data incrementally or not. \n Primary key - comma-separated column names to be used as PKs.\n Limit - row limit of the download. See API limitations in documentation.",
      "uniqueItems": true,
      "propertyOrder": 4
    }
  }
}


================================================
FILE: component_config/configuration_description.md
================================================
## Configuration description

Below is the description of input parameters and expected output.

### API Limitations
The extractor is limited by **limitations** of Looker API, which currently are:
* Pivoted look > 5000 rows - only unpivotted part will be extracted.
* Pivoted look <= 5000 rows - everything will be extracted.
* Non-pivoted look - everything will be extracted, regardless of # of rows.

To specify number of rows to be extracted, use `limit`. To download all rows, use `limit=-1`, even though data might be truncated due to above limitations.

### API secret and key
Obtaining the API secret and key is rather simple. In order to obtain the client secret and id, an admin access is required.
You can simply follow this [document](https://discourse.looker.com/t/using-the-looker-api-3/2988)
or the following steps:

1. Login to the relevant looker instance.

2. Navigate to **Admin** section (top right) and then **Users** in bar on the left.

3. Click on the edit button next to the relevant user.

4. Click on *Edit keys* in **API3 Keys** section.

5. Two scenarios can happen:
	* If API keys were generated previously, client id and secret will be displayed on screen.
	* If API keys were not generated previously, simply click on *New API3 Key* to generate the credentials.

6. Use obtained client id and secret to authorize the extractor.

### Input

Configuration schema accepts following parameters:
* **Client ID** - Client ID obtained in the API section of Looker dashboard.
* **Client Secret** - Client Secret obtained in the API section of Looker dashboard.
* **API Endpoint** - API Endpoint via which requests are sent.
* **Look ID** - ID of a look, from which the data should be downloaded. The look is ran automatically.
* **Destination Table** - A table in KBC Storage, where data will be loaded. If left blank, data will be downloaded to `in.c-looker.looker_data_id`, where `id` is equal to ID of a look.
* **Incremental Load** - Marks, whether load should be incremental.
* **Primary Key** - Comma-separated columns, which are to be used as PK.
* **Limit** - Row limit for look. See API Limitations above.

For more info about Looker API, see [Looker API Documentation](https://docs.looker.com/reference/api-and-integration/api-getting-started).

### Output

Table in specified location in KBC storage.


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/stack_parameters.json
================================================
{}


================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
fi

echo "Updating logger settings"

value=`cat component_config/logger`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} logger --value="$value"
else
    echo "logger type is empty!"
fi

echo "Updating logger configuration"
value=`cat component_config/loggerConfiguration.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} loggerConfiguration --value="$value"
else
    echo "loggerConfiguration is empty!"
fi


================================================
FILE: src/component.py
================================================
import csv
import re
import logging

import looker_sdk.error
from keboola.component.base import ComponentBase
from keboola.component.exceptions import UserException
from keboola.utils.helpers import comma_separated_values_to_list

from client import LookerClient, LookerClientException
from configuration import Configuration, LookerObject


class Component(ComponentBase):

    def __init__(self):
        super().__init__()
        self._configuration: Configuration
        self.client: LookerClient

    def run(self):
        self._init_configuration()
        self._init_client()

        for looker_object in self._configuration.looker_objects:
            try:
                self.download_look_object(looker_object)
            except looker_sdk.error.SDKError:
                raise UserException(f"Unable to download Looker object {looker_object}, please check credentials.")

    def _init_configuration(self) -> None:
        self.validate_configuration_parameters(Configuration.get_dataclass_required_parameters())
        self._configuration: Configuration = Configuration.load_from_dict(self.configuration.parameters)

    def _init_client(self) -> None:
        base_url = f"https://{self.extract_subdomain(self._configuration.api_endpoint)}.looker.com:19999"
        self.client = LookerClient(base_url=base_url,
                                   client_id=self._configuration.client_id,
                                   client_secret=self._configuration.pswd_client_secret)

    @staticmethod
    def full_match_re(pattern, string):
        """
        Function to return whether the match was full or not
        """
        return bool(re.fullmatch(pattern, string))

    def is_valid_keboola_destination(self, destination_string):
        return self.full_match_re(r'^(in|out)\.(c-)\w*\.[\w\-]*', destination_string)

    def get_destination(self, looker_object: LookerObject):
        if self.is_valid_keboola_destination(looker_object.output):
            destination = looker_object.output
            logging.debug(f"The table with id {looker_object.id} will be saved to {destination}.")
        elif not self.is_valid_keboola_destination(looker_object.output) and len(looker_object.output) == 0:
            destination = f"in.c-looker.looker_data_{looker_object.id}"
            logging.debug(f"The table with id {looker_object.id} will be saved to {destination}.")
        else:
            raise UserException(f"The name of the table {looker_object.output} contains unsupported characters. "
                                f"Please provide a valid name with bucket and table name.")
        return destination

    def download_look(self, look_id, limit=5000):
        try:
            return self.client.download_look(look_id=look_id, limit=limit)
        except LookerClientException as e:
            raise UserException(f"Failed to download look {look_id}. Validate that it is a valid look ID") from e

    @staticmethod
    def extract_subdomain(url):
        start = len('https://')
        end = url.find('.looker.com')
        return url[start:end]

    @staticmethod
    def lowercase_csv_header(csv_string: str) -> str:
        """Convert the first row (header) to lowercase"""
        csv_list = list(csv.reader(csv_string.splitlines()))

        if csv_list:
            csv_list[0] = [header.lower() for header in csv_list[0]]

        csv_with_lowercase_header = "\n".join([",".join(row) for row in csv_list])

        return csv_with_lowercase_header

    def download_look_object(self, looker_object: LookerObject):
        primary_keys = comma_separated_values_to_list(looker_object.primary_key)
        table_definition = self.create_out_table_definition(f'looker_data_{looker_object.id}.csv',
                                                            destination=self.get_destination(looker_object),
                                                            incremental=looker_object.incremental,
                                                            primary_key=primary_keys)

        look_data = self.download_look(looker_object.id, limit=looker_object.limit)

        # make header lowercase to use the same header as v3 API
        look_data = (self.lowercase_csv_header(look_data))

        with open(table_definition.full_path, "w") as file:
            file.write(look_data)

        self.write_manifest(table_definition)


if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)



================================================
FILE: src/configuration.py
================================================
import dataclasses
import json
from dataclasses import dataclass
from typing import List

import dataconf


class ConfigurationBase:
    @staticmethod
    def _convert_private_value(value: str):
        return value.replace('"#', '"pswd_')

    @staticmethod
    def _convert_private_value_inv(value: str):
        if value and value.startswith("pswd_"):
            return value.replace("pswd_", "#", 1)
        else:
            return value

    @classmethod
    def load_from_dict(cls, configuration: dict):
        """
        Initialize the configuration dataclass object from dictionary.
        Args:
            configuration: Dictionary loaded from json configuration.

        Returns:

        """
        json_conf = json.dumps(configuration)
        json_conf = ConfigurationBase._convert_private_value(json_conf)
        return dataconf.loads(json_conf, cls, ignore_unexpected=True)

    @classmethod
    def get_dataclass_required_parameters(cls) -> List[str]:
        """
        Return list of required parameters based on the dataclass definition (no default value)
        Returns: List[str]

        """
        return [cls._convert_private_value_inv(f.name)
                for f in dataclasses.fields(cls)
                if f.default == dataclasses.MISSING
                and f.default_factory == dataclasses.MISSING]


@dataclass
class LookerObject(ConfigurationBase):
    id: str
    limit: int
    output: str
    incremental: bool
    primary_key: str


@dataclass
class Configuration(ConfigurationBase):
    client_id: str
    pswd_client_secret: str
    api_endpoint: str
    looker_objects: list[LookerObject]



================================================
FILE: src/client/__init__.py
================================================
from .client import LookerClient, LookerClientException  # noqa


================================================
FILE: src/client/client.py
================================================
import looker_sdk
from looker_sdk import api_settings


class LookerClientException(Exception):
    pass


class MyApiSettings(api_settings.ApiSettings):
    def __init__(self, base_url: str, client_id: str, client_secret: str, *args, **kwargs):
        self.base_url = base_url
        self.client_id = client_id
        self.client_secret = client_secret
        super().__init__(*args, **kwargs)

    def read_config(self):
        return {
            "base_url": self.base_url,
            "client_id": self.client_id,
            "client_secret": self.client_secret,
            "timeout": "1200"
        }


class LookerClient:
    def __init__(self, base_url, client_id, client_secret):
        self.client = looker_sdk.init40(config_settings=MyApiSettings(base_url, client_id, client_secret))

    def download_look(self, look_id, limit):
        return self.client.run_look(look_id, 'csv', limit=limit)



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
FILE: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


