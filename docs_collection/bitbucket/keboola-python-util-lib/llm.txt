Directory structure:
└── kds_consulting_team-keboola-python-util-lib/
    ├── README.md
    ├── bitbucket-pipelines.yml
    ├── change_log.md
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── requirements.txt
    ├── setup.py
    ├── docs/
    │   ├── client_base.md
    │   ├── csv_tools.md
    │   ├── env_handler.md
    │   └── result.md
    ├── kbc/
    │   ├── __init__.py
    │   ├── client_base.py
    │   ├── csv_tools.py
    │   ├── env_handler.py
    │   └── result.py
    └── tests/
        ├── __init__.py
        └── kbc/
            ├── __init__.py
            ├── test_client_base.py
            ├── test_csv_tools.py
            ├── test_env_handler.py
            └── data/
                └── config.json

================================================
FILE: README.md
================================================
Package containing libraries for KBC Python components creation. It contains two modules:

**env_handler.KBCEnvHandler**

Class handling standard tasks for KBC component manipulation i.e. config load, validation. 
It contains some useful methods helping with boilerplate tasks. More info in [EnvHandler docs](./docs/env_handler.md)

**client_base.HttpClientBase**

Basic HTTP client core that can serve as a base of REST clients. More info in [docs](./docs/client_base.md)

**result.KBCResult**

definition of a result that can be processed by other methods. More info in [docs](./docs/result.md)

**result.KBCTableDef**

definition of a KBC Storage table - column, pkeys, name and other metadata. More info in [docs](./docs/result.md)

**result.ResultWriter**

Class providing methods of generic parsing and writing JSON responses. Can be used to build more complex
 writer based on this class as a parent. More info in [docs](./docs/result.md)
 
 
**csv_tools.CachedOrthogonalDictWriter**

DictWriter, built on top of csv.DictWriter, that supports automatic extension of headers according to what data it receives.
    The result file always has a complete header (as defined by fieldnames) or it is extended if some new columns
     are introduced in the data. It always produces a valid CSV (missing columns are filled with blanks).
 More info in [docs](./docs/csv_utils.md)

Installation
===============

```
pip3 install git+https://bitbucket.org/kds_consulting_team/keboola-python-util-lib.git
```

To upgrade existing installation use:

```
pip3 install --upgrade --force-reinstall https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.0.zip
```

**NOTE:** Please note that if using PIP version prior v`19.0`, version below <`0.2.0` of this library must be used along with `pip` flag `--process-dependency-links` needs due to the dependencies with [Keboola docker application](https://github.com/keboola/python-docker-application/tree/master/doc)  
which is not present on PyPy. Otherwise you mind get error ```ModuleNotFoundError: No module named 'keboola'```. If you keep receiving this error one option is to install the `keboola` package (`https://github.com/keboola/python-docker-application/zipball/master#egg=keboola`) manually, either in your `requirements.txt` or using `pip install` 

Usage
============
### `KBCEnvHandler`
Basic `KBCEnvHandler` usage:
```python
from kbc.env_handler import KBCEnvHandler
import logging
from datetime import datetime, date, timedelta

KEY_API_TOKEN = '#api_token'
MANDATORY_PARS = [KEY_API_TOKEN]
MANDATORY_IMG_PARS =[]

hdlr = KBCEnvHandler(MANDATORY_PARS)
# override debug from config
hdlr.set_default_logger('DEBUG' if debug else 'INFO')

##validate configuration
try:
    hdlr.validate_config(MANDATORY_PARS)
    hdlr.validate_image_parameters(MANDATORY_IMG_PARS)
except ValueError as e:
    logging.error(e)
    exit(1)

token = hdlr.cfg_params[KEY_API_TOKEN]

# get input table
table = hdlr.get_input_table_by_name('in_table.csv')

#get period chunks in days
periods = list(hdlr.split_dates_to_chunks(
            date.today() - timedelta(7), datetime.now().date(), 0, strformat='%Y-%m-%d'))
```

More info in [EnvHandler docs](./docs/env_handler.md)


### `result.ResultWriter`
In this package there is a set of classes that simplify JSON response processing and its conversion to the result CSV. 
It enables basic json response flattening, fix headers, injecting user values and more. 
All this in a memory efficient way so you don’t need to use pandas anymore. Moreover, there are some standardized 
methods in the KBCEnvHandler class that can leverage result objects generated by the Result Writers so the 
final result processing like manifest generation is very convenient.

**Usage example:**

```python
from kbc.env_handler import KBCEnvHandler
from kbc.result import ResultWriter
from kbc.result import KBCTableDef

# init handler
hdlr = KBCEnvHandler([])

# table def
companies_table = KBCTableDef(name='companies', pk=['company_id'])

api_service = MyApiService()
 
# writer setup
with ResultWriter(result_dir_path='data/out/tables/', table_def=companies_table, fix_headers=True) as comp_writer:
    
    for response in api_service.get_companies():
        comp_writer(response)
  

csv_results = comp_writer.collect_results()

# generate manifests for the result files
KBCEnvHandler.process_results(csv_results, None, None)
```



================================================
FILE: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover

  tags:
    '*':
      - step:
          script:
          - export APP_IMAGE=$APP_IMAGE
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover


================================================
FILE: change_log.md
================================================
**0.0.3**

- src folder structure
- remove dependency on handler lib - import the code directly to modify

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  test:
    build: .
    volumes:
        - ./:/code
    command: python -m unittest discover


================================================
FILE: Dockerfile
================================================
FROM python:3.7.2-alpine
ENV PYTHONIOENCODING utf-8

COPY . /code/
RUN apk add --no-cache --virtual .build-deps gcc musl-dev
RUN apk add --update tzdata
ENV TZ=UTC
RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    mapping.py
max-line-length = 119

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting
ignore = F812,H101,H202,H233,H301,H306,H401,H403,H404,H405,H501


================================================
FILE: requirements.txt
================================================
requests
https://github.com/keboola/python-docker-application/zipball/master#egg=keboola
pytz
python-dateutil
mock
freezegun
dateparser
pygelf
deprecated



================================================
FILE: setup.py
================================================
from distutils.core import setup
import setuptools

setup(
    name='kbc',
    version='0.5.1',
    setup_requires=['setuptools_scm'],
    url='https://bitbucket.org/kds_consulting_team/keboola-python-util-lib',
    download_url='https://bitbucket.org/kds_consulting_team/keboola-python-util-lib',
    packages=setuptools.find_packages(),
    install_requires=[
        'pytz',
        'python-dateutil',
        'pygelf',
        'requests',
        'dateparser',
        'keboola @ https://github.com/keboola/python-docker-application/zipball/2.1.1#egg=keboola-2.1.1',
        'deprecated'
    ],
    test_suite='tests',
    license="MIT"
)



================================================
FILE: docs/client_base.md
================================================
# kbc.client_base

## HttpClientBase
```python
HttpClientBase(self, base_url, max_retries=10, backoff_factor=0.3, status_forcelist=(500, 502, 504), default_http_header=[], auth=None, default_params=None)
```

Base class for implementing a single Http client related to some REST API.


**Args:**

- `base_url:` The base URL for this endpoint. e.g. `https://exampleservice.com/api_v1/`    
- `max_retries:` Total number of retries to allow.
    
- `backoff_factor`:  A back-off factor to apply between attempts.    
- `status_forcelist`:  A set of HTTP status codes that we should force a retry on. e.g. `[500,502]`
- `default_http_header` (dict): Default header to be sent with each request
                    eg. 
                    ```json
                    {'Authorization': 'Bearer ' + token,
                            'Content-Type' : 'application/json',
                            'Accept' : 'application/json'}
                    ```
- `auth`: Default Authentication tuple or object to attach to (from  requests.Session().auth).
                  eg. auth = (user, password)
- `default_params` (dict): default parameters to be sent with each request eg. `{'param':'value'}`

### get_raw
```python
HttpClientBase.get_raw(self, url, params={}, **kwargs)
```

Construct a requests GET call with args and kwargs and process the
results.


Args:
    url (str): requested url
    params (dict): additional url params to be passed to the underlying
        requests.get
    **kwargs: Key word arguments to pass to the get requests.get

Returns:
    r (requests.Response): :class:`Response <Response>` object.

Raises:
    requests.HTTPError: If the API request fails.

### post_raw
```python
HttpClientBase.post_raw(self, *args, **kwargs)
```

Construct a requests POST call with args and kwargs and process the
results.

Args:
    *args: Positional arguments to pass to the post request.
       Accepts supported params in requests.sessions.Session#request
    **kwargs: Key word arguments to pass to the post request.
       Accepts supported params in requests.sessions.Session#request
       eg. params = {'locId':'1'}, header = {some additional header}
       parameters and headers are appended to the default ones

Returns:
    Response: Returns :class:`Response <Response>` object.

Raises:
    requests.HTTPError: If the API request fails.

### post
```python
HttpClientBase.post(self, *args, **kwargs)
```

Construct a requests POST call with args and kwargs and process the
results.

Args:
    *args: Positional arguments to pass to the post request.
       Accepts supported params in requests.sessions.Session#request
    **kwargs: Key word arguments to pass to the post request.
       Accepts supported params in requests.sessions.Session#request
       eg. params = {'locId':'1'}, header = {some additional header}
       parameters and headers are appended to the default ones


Returns:
    body: json reposonse json-encoded content of a response

Raises:
    requests.HTTPError: If the API request fails.




================================================
FILE: docs/csv_tools.md
================================================
# kbc.csv_tools

## CachedOrthogonalDictWriter
```python
CachedOrthogonalDictWriter(file_path, fieldnames, temp_directory=None, dialect="excel", *args, **kwds)
```

DictWriter, built on top of csv.DictWriter, that supports automatic extension of headers according to what data it receives.
The result file always has a complete header (as defined by fieldnames) or it is extended if some new columns
 are introduced in the data. It always produces a valid CSV (missing columns are filled with blanks).
 It uses a series of cached writers / files that are merged into a single one with final set of columns on close()

**NOTE:** If not using "with" statement, close() method must be called at the end of processing to get the result.

**NOTE:** The final column list is stored in `fieldnames` property:

```python
columns = writer.fieldnames
```

**NOTE:** Does not keep the order of rows added - the rows containing additional headers always come first:

### Example:

```python
from kbc.csv_tools import CachedOrthogonalDictWriter
file = '/test/test.csv'
wr = CachedOrthogonalDictWriter(file, ["a", "b" , "c"])
wr.writerow({"a":1,"b":2})
wr.writerow({"b":2, "d":4})
wr.close()
```

leads to CSV with following content:
   
|a  |b  |c  |d  |
|---|---|---|---|
|   |2  |   |4  |
|1  | 2 |   |   |

May be also used with `with` statement to automatically close once finished:

```python
from kbc.csv_tools import CachedOrthogonalDictWriter
file = '/test/test.csv'
with CachedOrthogonalDictWriter(file, ["a", "b" , "c"]) as wr:
    wr.writerow({"a":1,"b":2})
    wr.writerow({"b":2, "d":4})

# get final headers
final_header = wr.fieldnames
```

:param file_path: result file path
:param fieldnames: Minimal column list
:param temp_directory: Optional path to a temp directory for cached files. By default the same directory as the
output file is used. Temporary files/directory are deleted on close.
:param dialect: As in csv package
:param args: As in csv package
:param kwds: As in csv package

### writerow

```python
CachedOrthogonalDictWriter.writerow(self, row_dict: dict)
```
### writerows

```python
CachedOrthogonalDictWriter.writerows(self, row_dict: List[dict])
```
### close

```python
CachedOrthogonalDictWriter.close(self)
```
Close all output streams / files and build and move the final result file.
Has to be called before result processing.



================================================
FILE: docs/env_handler.md
================================================
# kbc

# kbc.env_handler

## KBCEnvHandler
```python
KBCEnvHandler(self, mandatory_params, data_path=None)
```

Class handling standard tasks for KBC component manipulation i.e. config load, validation

It contains some useful methods helping with boilerplate tasks.

### validate_config
```python
KBCEnvHandler.validate_config(self, mandatory_params)
```

Validates config parameters based on provided mandatory parameters.
All provided parameters must be present in config to pass.

**ex1.:**
```python
par1 = 'par1'
par2 = 'par2'
mandatory_params = [par1, par2]
KBCEnvHandler.validate_config(mandatory_params)
```
Validation will fail when one of the above parameters is not found.

**Two levels of nesting:**

Parameters can be grouped as arrays: `par3 = [groupPar1, groupPar2]`
=> at least one of the pars has to be present

**ex2.**
```python
par1 = 'par1'
par2 = 'par2'
par3 = 'par3'
groupPar1 = 'groupPar1'
groupPar2 = 'groupPar2'
group1 = [groupPar1, groupPar2]
group3 = [par3, group1]
mandatory_params = [par1, par2, group1]KBCEnvHandler.validate_config(mandatory_params)

```
Folowing logical expression is evaluated:
`par1 AND par2 AND (groupPar1 OR groupPar2)`

**ex3**
```python
par1 = 'par1'
par2 = 'par2'
par3 = 'par3'
groupPar1 = 'groupPar1'
groupPar2 = 'groupPar2'
group1 = [groupPar1, groupPar2]
group3 = [par3, group1]
mandatory_params = [par1, par2, group3]
mandatory_params = [par1, par2, group1]KBCEnvHandler.validate_config(mandatory_params)
```
Following logical expression is evaluated:
`par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))`

### validate_image_parameters
```python
KBCEnvHandler.validate_image_parameters(self, mandatory_params)
```

Validates `image_parameters` section based on provided mandatory parameters.
All provided parameters must be present in config to pass.

for details [see validate_config](###validate_config)

### set_default_logger
```python
KBCEnvHandler.set_default_logger(self, log_level='INFO')
```

Sets default console logger.

Args:
    log_level: logging level, default: 'INFO'

Returns: logging object


### get_state_file
```python
KBCEnvHandler.get_state_file(self)
```


Return dict representation of state file or nothing if not present

Returns:
    dict:


### write_state_file
```python
KBCEnvHandler.write_state_file(self, state_dict)
```

Stores state file.
Args:
    state_dict:

### create_sliced_tables
```python
KBCEnvHandler.create_sliced_tables(self, folder_name, pkey=None, incremental=False, src_delimiter=',', src_enclosure='"', dest_bucket=None)
```

Creates prepares sliced tables from all files in DATA_PATH/out/tables/{folder_name} - i.e. removes all headers
and creates single manifest file based on provided parameters.

**Args:**

- folder_name: folder name present in DATA_PATH directory that contains files for slices, the same name will be used as table name
- `pkey`: array of pkeys
- incremental: boolean
- src_delimiter: delimiter of the source file [,]
- src_enclosure: enclosure of the source file ["]
- dest_bucket: name of the destination bucket, eg. in.c-input (optional)


### create_manifests
```python
KBCEnvHandler.create_manifests(self, results:List[kbc.result.KBCResult], headless=False, incremen
tal=True)
```

Write manifest files for the results produced by kbc.results.ResultWriter
:param results: List of result objects
:param headless: Flag whether results contain sliced headless tables and hence the `.column` attr
ibute should be
used in manifest file.
:param incremental:
:return:


### get_and_remove_headers_in_all
```python
KBCEnvHandler.get_and_remove_headers_in_all(self, files, delimiter, enclosure)
```

Removes header from all specified files and return it as a list of strings

Throws error if there is some file with different header.


### get_past_date
```python
KBCEnvHandler.get_past_date(self, str_days_ago, to_date=None, tz=<UTC>)
```

```python
def get_past_date(self, str_days_ago: str, to_date: datetime = None, 
tz: pytz.tzinfo.BaseTzInfo = pytz.utc) -> object:
```
Returns date in specified timezone relative to today.


e.g.
`'5 hours ago'`,
`'yesterday'`,
`'3 days ago'`,
`'4 months ago'`,
`'2 years ago'`,
`'today'`

### split_dates_to_chunks
```python
KBCEnvHandler.split_dates_to_chunks(self, start_date, end_date, intv, strformat='%m%d%Y')
```

Splits dates in given period into chunks of specified max size.
 
**Params:**
start_date -- start_period [datetime]
end_date -- end_period [datetime]
intv -- max chunk size
strformat -- dateformat of result periods

**Usage example:**
```python
list(split_dates_to_chunks("2018-01-01", "2018-01-04", 2, "%Y-%m-%d"))
```
returns 
```python
[{start_date: "2018-01-01", "end_date":"2018-01-02"}
 {start_date: "2018-01-02", "end_date":"2018-01-04"}]
 ```




================================================
FILE: docs/result.md
================================================
# kbc.result

## ResultWriter
```python
ResultWriter(self, result_dir_path:str, table_def:kbc.result.KBCTableDef, fix_headers=False, exclude_fields=[], user_value_cols=[], flatten_objects=True, buffer_size=8192)
```

Writer for parsing and storing JSON responses. It allows basic flattening of json object and handling of Array objects.

The Writer object support `_enter_` and `_close_` methods so it can be used in `with` statement. Otherwise method
`close()` has to be called before result retrieval. The write method opens output stream which is kept during the whole
lifetime of an instance, until the `close()` or `_close_` method is called (`with` statement exited)

### write
```python
ResultWriter.write(self, data, file_name=None, user_values=None, object_from_arrays=False, write_header=True)
```

Write the Json result using the writer configuration. Keeps the output stream opened and every result is written to
the resulting file assuring the memory efficiency.

:param data:
:param file_name:
:param user_values: Optional dictionary of user values to be added to the root object. The keys must match the column
names specified during initialization in `user_value_cols` parameter, otherwise it will throw a ValueError.

If used with `fix_headers` option, `user_value` columns must be specified on the table def object.

:param object_from_arrays: Create additional tables from the array objects
The result tables are named like: `parent_array-colnam.csv' and generates PK values: [parent_key, row_number]

:param write_header: Optional flag specifying whether to write the header - only needed when creating sliced files.
:return:

### collect_results
```python
ResultWriter.collect_results(self)
```

Collect the results.

NOTE: If not called after a `with` statement, method `close()` needs to be called prior processing
any of the result files to close all the streams.

:return: List of KBCResult objects

### close
```python
ResultWriter.close(self)
```

Close all output streams / files. Has to be called at end of extraction, before result processing.

:return:




================================================
FILE: kbc/__init__.py
================================================
[Empty file]


================================================
FILE: kbc/client_base.py
================================================
"""
Created on 5. 10. 2018

@author: esner
"""
import logging

import requests
from deprecated import deprecated
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

METHOD_RETRY_WHITELIST = ('GET', 'POST', 'PATCH', 'UPDATE', 'PUT')


class HttpClientBase:
    """
    Base class for implementing a single Http client related to some REST API.

    """

    def __init__(self, base_url, max_retries=10, backoff_factor=0.3,
                 status_forcelist=(500, 502, 504), default_http_header=None, auth_header=None, auth=None,
                 default_params=None,
                 method_whitelist=METHOD_RETRY_WHITELIST):
        """
        Create an endpoint.

          Args:
            base_url: The base URL for this endpoint. e.g. https://exampleservice.com/api_v1/
            max_retries: Total number of retries to allow.
            backoff_factor:  A back-off factor to apply between attempts.
            status_forcelist:  A set of HTTP status codes that we should force a retry on. e.g. [500,502]
            default_http_header (dict): Default header to be sent with each request
                                        eg. {
                                                       'Content-Type' : 'application/json',
                                                       'Accept' : 'application/json'}
            auth_header (dict): Auth header to be sent with each request
                                        eg. {'Authorization': 'Bearer ' + token}
            auth: Default Authentication tuple or object to attach to (from  requests.Session().auth).
                  eg. auth = (user, password)
            default_params (dict): default parameters to be sent with each request eg. {'param':'value'}

        """
        if not base_url:
            raise ValueError("Base URL is required.")
        self.base_url = base_url
        self.max_retries = max_retries
        self.backoff_factor = backoff_factor
        self.status_forcelist = status_forcelist
        self._auth = auth
        self._auth_header = auth_header if auth_header else {}
        self._default_header = default_http_header if default_http_header else {}
        self._default_params = default_params
        self.method_whitelist = method_whitelist

    def requests_retry_session(self, session=None):
        session = session or requests.Session()
        retry = Retry(
            total=self.max_retries,
            read=self.max_retries,
            connect=self.max_retries,
            backoff_factor=self.backoff_factor,
            status_forcelist=self.status_forcelist,
            allowed_methods=self.method_whitelist
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        return session

    def get_raw(self, *args, **kwargs):
        """
        Construct a requests GET call with args and kwargs and process the
        results.


        Args:
            url (str): requested url
            params (dict): additional url params to be passed to the underlying
                requests.get
            **kwargs: Key word arguments to pass to the get requests.get
                      ignore_auth: True to skip authentication

        Returns:
            r (requests.Response): :class:`Response <Response>` object.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        method = 'GET'

        r = self._request_raw(method, *args, **kwargs)
        return r

    def get(self, url, params=None, ignore_auth=False, **kwargs):
        """
                Construct a requests GET call with args and kwargs and process the
                results.

                Args:
                    *args: Positional arguments to pass to the post request.
                       Accepts supported params in requests.sessions.Session#request
                    **kwargs: Key word arguments to pass to the post request.
                       Accepts supported params in requests.sessions.Session#request
                       eg. params = {'locId':'1'}, header = {some additional header}
                       parameters and headers are appended to the default ones
                    ignore_auth: True to skip authentication


                Returns:
                    body: json reposonse json-encoded content of a response

                Raises:
                    requests.HTTPError: If the API request fails.
                """
        kwargs['ignore_auth'] = ignore_auth
        if params:
            kwargs['params'] = params
        kwargs['url'] = url
        r = self.get_raw(**kwargs)
        return r.json()

    def post_raw(self, *args, **kwargs):
        """
        Construct a requests POST call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
            **kwargs: Key word arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
               eg. params = {'locId':'1'}, header = {some additional header}
               parameters and headers are appended to the default ones
               ignore_auth: True to skip authentication

        Returns:
            Response: Returns :class:`Response <Response>` object.

        """
        method = 'POST'

        r = self._request_raw(method, *args, **kwargs)
        return r

    def post(self, *args, ignore_auth=False, **kwargs):
        """
        Construct a requests POST call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
            **kwargs: Key word arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
               eg. params = {'locId':'1'}, header = {some additional header}
               parameters and headers are appended to the default ones
            ignore_auth: True to skip authentication


        Returns:
            body: json reposonse json-encoded content of a response

        Raises:
            requests.HTTPError: If the API request fails.
        """

        try:
            kwargs['ignore_auth'] = ignore_auth
            r = self.post_raw(*args, **kwargs)
            r.raise_for_status()
        except requests.HTTPError as e:
            logging.warning(e, exc_info=True)
            # Handle different error codes
            raise
        else:
            return r.json()

    def patch_raw(self, *args, **kwargs):
        """
        Construct a requests PATCH call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
            **kwargs: Key word arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
               eg. params = {'locId':'1'}, header = {some additional header}
               parameters and headers are appended to the default ones
               ignore_auth: True to skip authentication

        Returns:
            Response: Returns :class:`Response <Response>` object.

        """
        method = 'PATCH'

        r = self._request_raw(method, *args, **kwargs)
        return r

    def patch(self, *args, ignore_auth=False, **kwargs):
        """
        Construct a requests PATCH call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
            **kwargs: Key word arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
               eg. params = {'locId':'1'}, header = {some additional header}
               parameters and headers are appended to the default ones
            ignore_auth: True to skip authentication


        Returns:
            body: json reposonse json-encoded content of a response

        Raises:
            requests.HTTPError: If the API request fails.
        """

        try:
            kwargs['ignore_auth'] = ignore_auth
            r = self.patch_raw(*args, **kwargs)
            r.raise_for_status()
        except requests.HTTPError as e:
            logging.warning(e, exc_info=True)
            # Handle different error codes
            raise
        else:
            return r.json()

    def update_raw(self, *args, **kwargs):
        """
        Construct a requests UPDATE call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
            **kwargs: Key word arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
               eg. params = {'locId':'1'}, header = {some additional header}
               parameters and headers are appended to the default ones
               ignore_auth: True to skip authentication

        Returns:
            Response: Returns :class:`Response <Response>` object.

        """
        method = 'UPDATE'

        r = self._request_raw(method, *args, **kwargs)
        return r

    def update(self, *args, ignore_auth=False, **kwargs):
        """
        Construct a requests UPDATE call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
            **kwargs: Key word arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
               eg. params = {'locId':'1'}, header = {some additional header}
               parameters and headers are appended to the default ones
            ignore_auth: True to skip authentication


        Returns:
            body: json reposonse json-encoded content of a response

        Raises:
            requests.HTTPError: If the API request fails.
        """

        try:
            kwargs['ignore_auth'] = ignore_auth
            r = self.update_raw(*args, **kwargs)
            r.raise_for_status()
        except requests.HTTPError as e:
            logging.warning(e, exc_info=True)
            # Handle different error codes
            raise
        else:
            return r.json()

    @deprecated
    def _patch(self, *args, **kwargs):
        """
        Construct a requests POST call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
            **kwargs: Key word arguments to pass to the post request.

        Returns:
            body: Response body parsed from json.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        headers = kwargs.pop('headers', {})
        headers.update(self._auth_header)
        r = requests.patch(headers=headers, *args, **kwargs)
        try:
            r.raise_for_status()
        except requests.HTTPError as e:
            logging.warning(e, exc_info=True)
            # Handle different error codes
            raise
        else:
            return r

    def put_raw(self, *args, **kwargs):
        """
        Construct a requests PUT call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
            **kwargs: Key word arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
               eg. params = {'locId':'1'}, header = {some additional header}
               parameters and headers are appended to the default ones
                ignore_auth: True to skip authentication

        Returns:
            Response: Returns :class:`Response <Response>` object.

        Raises:
            requests.HTTPError: If the API request fails.
        """
        method = 'PUT'

        r = self._request_raw(method, *args, **kwargs)
        return r

    def put(self, *args, ignore_auth=False, **kwargs):
        """
        Construct a requests PUT call with args and kwargs and process the
        results.

        Args:
            *args: Positional arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
            ignore_auth: True to skip authentication
            **kwargs: Key word arguments to pass to the post request.
               Accepts supported params in requests.sessions.Session#request
               eg. params = {'locId':'1'}, header = {some additional header}
               parameters and headers are appended to the default ones


        Returns:
            body: json reposonse json-encoded content of a response

        Raises:
            requests.HTTPError: If the API request fails.
        """

        try:
            kwargs['ignore_auth'] = ignore_auth
            r = self.put_raw(*args, **kwargs)
            r.raise_for_status()
        except requests.HTTPError as e:
            logging.warning(e, exc_info=True)
            # Handle different error codes
            raise
        else:
            return r.json()

    def _request_raw(self, method, *args, **kwargs):
        """
                Construct a requests PUT call with args and kwargs and process the
                results.

                Args:
                    method: (PUT/POST/PATCH/GET)
                    *args: Positional arguments to pass to the post request.
                       Accepts supported params in requests.sessions.Session#request
                    **kwargs: Key word arguments to pass to the post request.
                       Accepts supported params in requests.sessions.Session#request
                       eg. params = {'locId':'1'}, header = {some additional header}
                       parameters and headers are appended to the default ones
                       ignore_auth  - True to skip authentication



                Returns:
                    Response: Returns :class:`Response <Response>` object.

                Raises:
                    requests.HTTPError: If the API request fails.
                """
        s = requests.Session()
        headers = kwargs.pop('headers', {})
        headers.update(self._default_header)
        if not kwargs.pop('ignore_auth', False):
            headers.update(self._auth_header)
            s.headers.update(headers)
            s.auth = self._auth
        s.headers.update(headers)

        # set default params
        params = kwargs.pop('params', {})

        if params is None:
            params = {}

        if self._default_params:

            all_pars = {**params, **self._default_params}
            kwargs.update({'params': all_pars})

        else:
            kwargs.update({'params': params})

        r = self.requests_retry_session(session=s).request(method, *args, **kwargs)
        return r



================================================
FILE: kbc/csv_tools.py
================================================
import hashlib
import io
import os
import shutil
from csv import DictWriter, DictReader
from typing import List


class CachedOrthogonalDictWriter:
    """
    DictWriter, built on top of csv.DictWriter, that supports automatic extension of headers
    according to what data it receives.

    The result file always has a complete header (as defined by fieldnames) or it is extended if some new columns
     are introduced in the data. It always produces a valid CSV (missing columns are filled with blanks).

     It uses a series of cached writers / files that are merged into a single one with final set of columns on close()

     NOTE: If not using "with" statement, close() method must be called at the end of processing to get the result.

     NOTE: Does not keep the order of rows added - the rows containing additional headers always come first:
    Example:
        wr = CachedDictWriter(file, ["a", "b" , "c"])
        wr.write({"a":1,"b":2"})
        wr.writer({"b":2", "d":4})
        wr.close()

        leads to CSV with following content:
        "a","b","c","d"
         ,2,,4
        1,2,,
    """

    def __init__(self, file_path, fieldnames, temp_directory=None, dialect="excel", buffering=io.DEFAULT_BUFFER_SIZE,
                 *args, **kwds):
        """

        :param file_path: result file path
        :param fieldnames: Minimal column list
        :param temp_directory: Optional path to a temp directory for cached files. By default the same directory as the
        output file is used. Temporary files/directory are deleted on close.
        :param dialect: As in csv package
        :param args: As in csv package
        :param kwds: As in csv package
        """

        self.result_path = file_path
        self.fieldnames = fieldnames
        # global writer properties
        self.dialect = dialect
        self.args = args
        self.kwds = kwds

        self.buffering = buffering
        self.encoding = kwds.get('encoding', 'utf-8')

        self._write_header = False

        if not temp_directory:
            temp_directory = os.path.join(os.path.dirname(file_path), 'csv_temp')

        os.makedirs(temp_directory, exist_ok=True)
        self.temp_directory = temp_directory
        # set initial key value of the complete writer
        self._complete_writer_key = None
        # hashing method
        self._hash_method = self._generate_hashed_header_key
        self._writer_cache = {}
        self._tmp_file_cache = {}
        self._get_or_add_cached_writer(fieldnames)

    def writeheader(self):
        self._write_header = True

    def writerow(self, row_dict: dict):
        cols = list(row_dict.keys())
        writer = self._get_or_add_cached_writer(cols)
        writer.writerow(row_dict)

    def writerows(self, row_dicts: List[dict]):
        for r in row_dicts:
            self.writerow(r)

    def _update_complete_header(self, columns):
        cols_to_add = set(columns).difference(set(self.fieldnames))
        self.fieldnames.extend(cols_to_add)
        return self.fieldnames

    def _get_or_add_cached_writer(self, columns):
        writer_key = self._build_writer_key(columns)
        wr = self._writer_cache.get(writer_key)
        if not wr:
            tmp_file = os.path.join(self.temp_directory, writer_key)
            t_file = open(tmp_file, 'wt+', newline='', buffering=self.buffering, encoding=self.encoding)
            self._tmp_file_cache[writer_key] = t_file
            wr = DictWriter(t_file, self.fieldnames.copy(), *self.args, **self.kwds)
            wr.writeheader()
            self._writer_cache[writer_key] = wr

        return wr

    def _build_writer_key(self, columns):
        """
        Returns a writer key / complete or extended one. Updates the complete header set.
        :param columns:
        :return:
        """
        if not set(columns).issubset(self.fieldnames) or not self._complete_writer_key:
            new_header = self._update_complete_header(columns)
            self._complete_writer_key = self._hash_method(list(new_header))

        return self._complete_writer_key

    def _generate_hashed_header_key(self, columns):
        # sort cols
        columns.sort()
        return hashlib.md5('_'.join(columns).encode('utf-8')).hexdigest()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def close(self):
        """
        Close all output streams / files and build and move the final result file.
        Has to be called before result processing.

        :return:
        """
        final_header = list(self.fieldnames)
        final_writer = self._get_or_add_cached_writer(final_header)
        final_writer_key = self._build_writer_key(final_header)
        if len(self._writer_cache) == 1:
            # headers were the same
            self._tmp_file_cache[final_writer_key].close()
        else:
            self._writer_cache.pop(final_writer_key)
            self._append_missing_rows_and_close(final_writer, self._writer_cache, final_writer_key)

        src_file = os.path.join(self.temp_directory, final_writer_key)

        # write the result and add header.
        with open(src_file, 'r', encoding=self.encoding) as source_file, open(self.result_path, 'w',
                                                                              buffering=self.buffering,
                                                                              encoding=self.encoding) as target_file:
            if not self._write_header:
                source_file.readline()
            # this will truncate the file, so need to use a different file name:
            shutil.copyfileobj(source_file, target_file)

        # cleanup
        shutil.rmtree(self.temp_directory)

    def _append_missing_rows_and_close(self, final_writer, writers: dict, final_writer_key):
        """
        Appends missing rows (with less columns) to a final writer
        :param final_writer: final writer with complete set of headers
        :param writers: writers with smaller header
        :return:
        """
        # close all writers and writer headers
        for wkey in writers:
            self._tmp_file_cache[wkey].close()
            file_path = os.path.join(self.temp_directory, wkey)
            self._append_data(final_writer, file_path)

        self._tmp_file_cache[final_writer_key].close()

    def _append_data(self, final_writer, partition_path):
        with open(partition_path, mode='rt', encoding='utf-8') as in_file:
            reader = DictReader(in_file)
            for r in reader:
                final_writer.writerow(r)



================================================
FILE: kbc/env_handler.py
================================================
# ==============================================================================
# KBC Env handler
# ==============================================================================


# ============================ Import libraries ==========================
import csv
import glob
import json
import logging
import math
import os
import sys
from _datetime import timedelta
from collections import Counter
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import List

import dateparser
import pytz
from dateutil.relativedelta import relativedelta
from keboola import docker
from pygelf import GelfUdpHandler, GelfTcpHandler

from kbc.result import KBCResult

DEFAULT_DEL = ','
DEFAULT_ENCLOSURE = '"'


@dataclass
class EnvironmentVariables:
    """
    Dataclass for variables available in the docker environment
    https://developers.keboola.com/extend/common-interface/environment/#environment-variables
    """
    data_dir: str
    run_id: str
    project_id: str
    stack_id: str
    config_id: str
    component_id: str
    project_name: str
    token_id: str
    token_desc: str
    token: str
    url: str
    logger_addr: str
    logger_port: str


@dataclass
class TableDef:
    full_path: str
    file_name: str
    is_sliced: bool = False
    manifest: dict = field(default_factory=dict)


class KBCEnvHandler:
    """
    Class handling standard tasks for KBC component manipulation i.e. config load, validation

    It contains some useful methods helping with boilerplate tasks.
    """

    LOGGING_TYPE_STD = 'std'
    LOGGING_TYPE_GELF = 'gelf'

    def __init__(self, mandatory_params, data_path=None, log_level=logging.INFO, logging_type=None):
        """

        Args:
            mandatory_params (array(str)): Array of parameter names that needs to be present in the config.json.
            May be nested, see :func:`KBCEnvHandler.validateConfig()` docs for more details.

            data_path (str): optional path to data folder - if not specified data folder fetched
                             from KBC_DATADIR if present, otherwise '/data' as default
            env variable by default.

            log_level (int): logging.INFO or logging.DEBUG
            logging_type (str): optional 'std' or 'gelf', if left empty determined automatically


        """
        # setup GELF if available
        # backward compatibility
        logging_type_inf = KBCEnvHandler.LOGGING_TYPE_GELF if os.getenv('KBC_LOGGER_ADDR',
                                                                        None) else KBCEnvHandler.LOGGING_TYPE_STD
        if not logging_type:
            logging_type = logging_type_inf

        if logging_type == KBCEnvHandler.LOGGING_TYPE_STD:
            self.set_default_logger(log_level)
        elif logging_type == KBCEnvHandler.LOGGING_TYPE_GELF:
            self.set_gelf_logger(log_level)

        if not data_path and os.environ.get('KBC_DATADIR'):
            data_path = os.environ.get('KBC_DATADIR')
        elif not data_path:
            data_path = '/data'

        self.kbc_config_id = os.environ.get('KBC_CONFIGID')

        self.data_path = data_path
        self.configuration = docker.Config(data_path)
        self.cfg_params = self.configuration.get_parameters()
        self.image_params = self.configuration.config_data["image_parameters"]
        self.tables_out_path = os.path.join(data_path, 'out', 'tables')
        self.tables_in_path = os.path.join(data_path, 'in', 'tables')
        self.files_in_path = os.path.join(data_path, 'in', 'files')
        self.files_out_path = os.path.join(data_path, 'out', 'files')
        self.environment_variables = self.init_environment_variables()

        self._mandatory_params = mandatory_params

    # ==============================================================================
    @staticmethod
    def init_environment_variables() -> EnvironmentVariables:
        """
        Initializes environment variables available in the docker environment
            https://developers.keboola.com/extend/common-interface/environment/#environment-variables

        Returns:
            EnvironmentVariables:
        """
        return EnvironmentVariables(data_dir=os.environ.get('KBC_DATADIR', None),
                                    run_id=os.environ.get('KBC_RUNID', None),
                                    project_id=os.environ.get('KBC_PROJECTID', None),
                                    stack_id=os.environ.get('KBC_STACKID', None),
                                    config_id=os.environ.get('KBC_CONFIGID', None),
                                    component_id=os.environ.get('KBC_COMPONENTID', None),
                                    project_name=os.environ.get('KBC_PROJECTNAME', None),
                                    token_id=os.environ.get('KBC_TOKENID', None),
                                    token_desc=os.environ.get('KBC_TOKENDESC', None),
                                    token=os.environ.get('KBC_TOKEN', None),
                                    url=os.environ.get('KBC_URL', None),
                                    logger_addr=os.environ.get('KBC_LOGGER_ADDR', None),
                                    logger_port=os.environ.get('KBC_LOGGER_PORT', None)
                                    )

    def validate_config(self, mandatory_params=None):
        """
                Validates config parameters based on provided mandatory parameters.
                All provided parameters must be present in config to pass.
                ex1.:
                par1 = 'par1'
                par2 = 'par2'
                mandatory_params = [par1, par2]
                Validation will fail when one of the above parameters is not found

                Two levels of nesting:
                Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
                => at least one of the pars has to be present
                ex2.
                par1 = 'par1'
                par2 = 'par2'
                par3 = 'par3'
                groupPar1 = 'groupPar1'
                groupPar2 = 'groupPar2'
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group1]

                Folowing logical expression is evaluated:
                Par1 AND Par2 AND (groupPar1 OR groupPar2)

                ex3
                par1 = 'par1'
                par2 = 'par2'
                par3 = 'par3'
                groupPar1 = 'groupPar1'
                groupPar2 = 'groupPar2'
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group3]

                Following logical expression is evaluated:
                par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
                """
        if not mandatory_params:
            mandatory_params = []
        return self.validate_parameters(self.cfg_params, mandatory_params, 'config parameters')

    def validate_image_parameters(self, mandatory_params):
        """
                Validates image parameters based on provided mandatory parameters.
                All provided parameters must be present in config to pass.
                ex1.:
                par1 = 'par1'
                par2 = 'par2'
                mandatory_params = [par1, par2]
                Validation will fail when one of the above parameters is not found

                Two levels of nesting:
                Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
                => at least one of the pars has to be present
                ex2.
                par1 = 'par1'
                par2 = 'par2'
                par3 = 'par3'
                groupPar1 = 'groupPar1'
                groupPar2 = 'groupPar2'
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group1]

                Folowing logical expression is evaluated:
                Par1 AND Par2 AND (groupPar1 OR groupPar2)

                ex3
                par1 = 'par1'
                par2 = 'par2'
                par3 = 'par3'
                groupPar1 = 'groupPar1'
                groupPar2 = 'groupPar2'
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group3]

                Following logical expression is evaluated:
                par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
                """
        return self.validate_parameters(self.image_params, mandatory_params, 'image/stack parameters')

    def validate_parameters(self, parameters, mandatory_params, _type):
        """
        Validates provided parameters based on provided mandatory parameters.
        All provided parameters must be present in config to pass.
        ex1.:
        par1 = 'par1'
        par2 = 'par2'
        mandatory_params = [par1, par2]
        Validation will fail when one of the above parameters is not found

        Two levels of nesting:
        Parameters can be grouped as arrays par3 = [groupPar1, groupPar2] => at least one of the pars has to be present
        ex2.
        par1 = 'par1'
        par2 = 'par2'
        par3 = 'par3'
        groupPar1 = 'groupPar1'
        groupPar2 = 'groupPar2'
        group1 = [groupPar1, groupPar2]
        group3 = [par3, group1]
        mandatory_params = [par1, par2, group1]

        Folowing logical expression is evaluated:
        Par1 AND Par2 AND (groupPar1 OR groupPar2)

        ex3
        par1 = 'par1'
        par2 = 'par2'
        par3 = 'par3'
        groupPar1 = 'groupPar1'
        groupPar2 = 'groupPar2'
        group1 = [groupPar1, groupPar2]
        group3 = [par3, group1]
        mandatory_params = [par1, par2, group3]

        Following logical expression is evaluated:
        par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
        """
        missing_fields = []
        for par in mandatory_params:
            if isinstance(par, list):
                missing_fields.extend(self._validate_par_group(par, parameters))
            elif not parameters.get(par):
                missing_fields.append(par)

        if missing_fields:
            raise ValueError(
                'Missing mandatory {} fields: [{}] '.format(_type, ', '.join(missing_fields)))

    def _validate_par_group(self, par_group, parameters):
        missing_fields = []
        is_present = False
        for par in par_group:
            if isinstance(par, list):
                missing_subset = self._get_par_missing_fields(par, parameters)
                missing_fields.extend(missing_subset)
                if not missing_subset:
                    is_present = True

            elif parameters.get(par):
                is_present = True
            else:
                missing_fields.append(par)
        if not is_present:
            return missing_fields
        else:
            return []

    def _get_par_missing_fields(self, mand_params, parameters):
        missing_fields = []
        for par in mand_params:
            if not parameters.get(par):
                missing_fields.append(par)
        return missing_fields

    def get_storage_token(self):
        try:
            return os.environ["KBC_TOKEN"]
        except Exception:
            logging.error("Storage token is missing in KBC_TOKEN env variable.")
            exit(2)

    def get_authorization(self):
        """
        Returns a dictionary containing the authentication part of the configuration file. If not present,
        an exception is raised.
        The dictionary returned has the following form:
        {
            "id": "main",
            "authorizedFor": "Myself",
            "creator": {
              "id": "1234",
              "description": "me@keboola.com"
            },
            "created": "2016-01-31 00:13:30",
            "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*$\"}",
            "oauthVersion": "2.0",
            "appKey": "000003423433C184A49",
            "#appSecret": "sdsadasdas-CiN"
        }
        """

        try:
            return self.configuration.config_data["authorization"]["oauth_api"]["credentials"]
        except KeyError:
            logging.error("Authorization is missing in configuration file.")
            exit(2)

    def get_input_table_by_name(self, table_name) -> TableDef:
        """
        Returns TableDef object by table name.

        Raises ValueError if table not found

        :raises ValueError

        :rtype: TableDef
        """
        tables = self.get_input_tables_definitions()

        table = [t for t in tables if t.file_name == table_name]
        if not table:
            raise ValueError(
                'Specified input mapping [{}] does not exist'.format(table_name))
        return table[0]

    def get_input_table_by_name_from_config(self, table_name):
        tables = self.configuration.get_input_tables()
        table = [t for t in tables if t.get('destination') == table_name]
        if not table:
            raise ValueError(
                'Specified input mapping [{}] does not exist'.format(table_name))
        return table[0]

    def get_input_tables_definitions(self):
        """

        :rtype: List[TableDef]
        """
        table_files = [f for f in glob.glob(self.tables_in_path + "/**", recursive=False) if
                       not f.endswith('.manifest')]
        table_defs = list()
        for t in table_files:
            is_sliced = False
            manifest = dict()
            p = Path(t)
            if Path(t + '.manifest').exists():
                manifest = json.load(open(t + '.manifest'))

            if p.is_dir() and manifest:
                is_sliced = True
            elif p.is_dir() and not manifest:
                # skip folders that do not have matching manifest
                logging.warning(f'Folder {t} does not have matching manifest, it will be ignored!')
                continue

            table_defs.append(TableDef(full_path=t, file_name=p.name, is_sliced=is_sliced, manifest=manifest))
        return table_defs

    def get_image_parameters(self):
        return self.configuration.config_data["image_parameters"]

    # ================================= Logging ==============================

    def set_default_logger(self, log_level=logging.INFO):  # noqa: E301
        """
        Sets default console logger.

        Args:
            log_level: logging level, default: 'INFO'

        Returns: logging object

        """

        class InfoFilter(logging.Filter):
            def filter(self, rec):
                return rec.levelno in (logging.DEBUG, logging.INFO)

        hd1 = logging.StreamHandler(sys.stdout)
        hd1.addFilter(InfoFilter())
        hd2 = logging.StreamHandler(sys.stderr)
        hd2.setLevel(logging.WARNING)

        logging.getLogger().setLevel(log_level)
        # remove default handler
        for h in logging.getLogger().handlers:
            logging.getLogger().removeHandler(h)
        logging.getLogger().addHandler(hd1)
        logging.getLogger().addHandler(hd2)

        logger = logging.getLogger()
        return logger

    def set_gelf_logger(self, log_level=logging.INFO, transport_layer='TCP',
                        stdout=False, **gelf_kwargs):  # noqa: E301
        """
        Sets gelf console logger. Handler for console output is not included by default,
        for testing in non-gelf environments use stdout=True.

        Args:
            log_level: logging level, default: 'INFO'
            transport_layer: 'TCP' or 'UDP', default:'UDP

        Returns: logging object

        """
        # remove existing handlers
        for h in logging.getLogger().handlers:
            logging.getLogger().removeHandler(h)
        if stdout:
            self.set_default_logger(log_level)

        # gelf handler setup
        if not gelf_kwargs.get('include_extra_fields'):
            gelf_kwargs['include_extra_fields'] = True

        host = os.getenv('KBC_LOGGER_ADDR', 'localhost')
        port = os.getenv('KBC_LOGGER_PORT', 12201)
        if transport_layer == 'TCP':
            gelf = GelfTcpHandler(host=host, port=port, **gelf_kwargs)
        elif transport_layer == 'UDP':
            gelf = GelfUdpHandler(host=host, port=port, **gelf_kwargs)
        else:
            raise ValueError(F'Unsupported gelf transport layer: {transport_layer}. Choose TCP or UDP')

        logging.getLogger().setLevel(log_level)
        logging.getLogger().addHandler(gelf)

        logger = logging.getLogger()
        return logger

    def get_state_file(self):
        """

        Return dict representation of state file or nothing if not present

        Returns:
            dict:

        """
        logging.getLogger().info('Loading state file..')
        state_file_path = os.path.join(self.data_path, 'in', 'state.json')
        if not os.path.isfile(state_file_path):
            logging.getLogger().info('State file not found. First run?')
            return
        try:
            with open(state_file_path, 'r') \
                    as state_file:
                return json.load(state_file)
        except (OSError, IOError):
            raise ValueError(
                "State file state.json unable to read "
            )

    def write_state_file(self, state_dict):
        """
        Stores state file.
        Args:
            state_dict:
        """
        if not isinstance(state_dict, dict):
            raise TypeError('Dictionary expected as a state file datatype!')

        with open(os.path.join(self.configuration.data_dir, 'out', 'state.json'), 'w+') as state_file:
            json.dump(state_dict, state_file)

    def create_sliced_tables(self, folder_name, pkey=None, incremental=False,
                             src_delimiter=DEFAULT_DEL, src_enclosure=DEFAULT_ENCLOSURE, dest_bucket=None):
        """
        Creates prepares sliced tables from all files in DATA_PATH/out/tables/{folder_name} - i.e. removes all headers
        and creates single manifest file based on provided parameters.

        Args:
            folder_name: folder name present in DATA_PATH directory that contains files for slices,
        the same name will be used as table name
            pkey: array of pkeys
            incremental: boolean
            src_delimiter: delimiter of the source file [,]
            src_enclosure: enclosure of the source file ["]
            dest_bucket: name of the destination bucket, eg. in.c-input (optional)


        """
        log = logging
        log.info('Creating sliced tables for [{}]..'.format(folder_name))

        folder_path = os.path.join(self.tables_out_path, folder_name)

        if not os.path.isdir(folder_path):
            raise ValueError("Specified folder ({}) does not exist in the data folder ({})".format(
                folder_name, self.data_path))

        # get files
        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(
            os.path.join(folder_path, f))]

        header = self.get_and_remove_headers_in_all(
            files, src_delimiter, src_enclosure)
        if dest_bucket:
            destination = dest_bucket + '.' + folder_name
        else:
            destination = folder_name

        log.info('Creating manifest file..')
        self.configuration.write_table_manifest(
            file_name=folder_path, destination=destination, primary_key=pkey, incremental=incremental, columns=header)

    def get_and_remove_headers_in_all(self, files, delimiter, enclosure):
        """
        Removes header from all specified files and return it as a list of strings

        Throws error if there is some file with different header.

        """
        first_run = True
        for file in files:
            curr_header = self._get_and_remove_headers(
                file, delimiter, enclosure)
            if first_run:
                header = curr_header
                first_file = file
                first_run = False
            # check whether header matches
            if Counter(header) != Counter(curr_header):
                raise Exception('Header in file {}:[{}] is different than header in file {}:[{}]'.format(
                    first_file, header, file, curr_header))
        return header

    def _get_and_remove_headers(self, file, delimiter, enclosure):
        """
        Removes header from specified file and return it as a list of strings.
        Creates new updated file 'upd_'+origFileName and deletes the original
        """
        head, tail = os.path.split(file)
        with open(file, "r") as input_file:
            with open(os.path.join(head, 'upd_' + tail), 'w+', newline='') as updated:
                reader = csv.DictReader(
                    input_file, delimiter=delimiter, quotechar=enclosure)
                header = reader.fieldnames
                writer = csv.DictWriter(
                    updated, fieldnames=header, delimiter=DEFAULT_DEL, quotechar=DEFAULT_ENCLOSURE)
                for row in reader:
                    # write row
                    writer.writerow(row)
        os.remove(file)
        return header

    def process_results(self, res_files, def_bucket_name, output_bucket):
        for res in res_files:
            dest_bucket = def_bucket_name + str(self.kbc_config_id)
            if output_bucket:
                suffix = '-' + output_bucket
            else:
                suffix = ''

            # build manifest
            self.configuration.write_table_manifest(
                file_name=res['full_path'],
                destination=dest_bucket + suffix + '.' + res['name'],
                primary_key=res['pkey'],
                incremental=True)

    def process_results_sliced(self, res_files):
        res_sliced_folders = {}
        for file in res_files:
            res_sliced_folders.update({file['name']: file['pkey']})

        for folder in res_sliced_folders:
            self.create_sliced_tables(folder, res_sliced_folders[folder], True)

    def create_manifests(self, results: List[KBCResult], headless=False, incremental=True):
        """
        Write manifest files for the results produced by kbc.results.ResultWriter
        :param results: List of result objects
        :param headless: Flag whether results contain sliced headless tables and hence
        the `.column` attribute should be
        used in manifest file.
        :param incremental:
        :return:
        """
        for r in results:
            if not headless:
                self.configuration.write_table_manifest(r.full_path, r.table_def.destination,
                                                        r.table_def.pk,
                                                        None, incremental, r.table_def.metadata,
                                                        r.table_def.column_metadata)
            else:
                self.configuration.write_table_manifest(r.full_path, r.table_def.destination,
                                                        r.table_def.pk,
                                                        r.table_def.columns, incremental,
                                                        r.table_def.metadata,
                                                        r.table_def.column_metadata)

    # ==============================================================================
    # == UTIL functions

    def get_date_period_converted(self, period_from, period_to):
        """
        Returns given period parameters in datetime format, or next step in back-fill mode
        along with generated last state for next iteration.

        :param period_from: str YYYY-MM-DD or relative string supported by date parser e.g. 5 days ago
        :param period_to: str YYYY-MM-DD or relative string supported by date parser e.g. 5 days ago

        :return: start_date: datetime, end_date: datetime
        """

        start_date_form = dateparser.parse(period_from)
        end_date_form = dateparser.parse(period_to)
        day_diff = (end_date_form - start_date_form).days
        if day_diff < 0:
            raise ValueError("start_date cannot exceed end_date.")

        return start_date_form, end_date_form

    def get_backfill_period(self, period_from, period_to, last_state):
        """
        Get backfill period, either specified period in datetime type or period based on a previous run (last_state)
        Continues iterating date periods based on the initial period size defined by from and to parameters.
        ex.:
        Run 1:
        _get_backfill_period("2018-01-01", "2018-01-03", None ) -> datetime("2018-01-01"),datetime("2018-01-03"),state)

        Run 2:
        _get_backfill_period("2018-01-01", "2018-01-03", last_state(from previous) )
                -> datetime("2018-01-03"), datetime("2018-01-05"), state)

        etc...

        :type last_state: dict
        - None or state file produced by backfill mode
        e.g. {"last_period" : {
                                "start_date": "2018-01-01",
                                "end_date": "2018-01-02"
                                }
            }

        :type period_to: str YYYY-MM-DD format or relative string supported by date parser e.g. 5 days ago
        :type period_from: str YYYY-MM-DD format or relative string supported by date parser e.g. 5 days ago
        :rtype: start_date: datetime, end_date: datetime, state_file: dict
        """
        if last_state and last_state.get('last_period'):
            last_start_date = datetime.strptime(
                last_state['last_period']['start_date'], '%Y-%m-%d')
            last_end_date = datetime.strptime(
                last_state['last_period']['end_date'], '%Y-%m-%d')

            diff = last_end_date - last_start_date
            # if period is a single day
            if diff.days == 0:
                diff = timedelta(days=1)

            start_date = last_end_date
            if (last_end_date.date() + diff) >= datetime.now(pytz.utc).date() + timedelta(days=1):
                end_date = datetime.now(pytz.utc)
            else:
                end_date = last_end_date + diff
        else:
            start_date = dateparser.parse(period_from)
            end_date = dateparser.parse(period_to)
        return start_date, end_date

    def get_past_date(self, str_days_ago: str, to_date: datetime = None,
                      tz: pytz.tzinfo.BaseTzInfo = pytz.utc) -> object:
        """
        Returns date in specified timezone relative to today.

        e.g.
        '5 hours ago',
        'yesterday',
        '3 days ago',
        '4 months ago',
        '2 years ago',
        'today'

        :param str_days_ago: (str)
        :param to_date: (datetime)
        :param tz: (pytz.tzinfo.BaseTzInfo)
        :return:
        """
        if to_date:
            TODAY = to_date
        else:
            TODAY = datetime.datetime.now(tz)

        try:
            today_diff = (datetime.datetime.now(tz) - TODAY).days
            past_date = dateparser.parse(str_days_ago)
            past_date.replace(tzinfo=tz)
            date = past_date - relativedelta(days=today_diff)
            return date
        except TypeError:
            raise ValueError(
                "Please enter valid date parameters. Some of the values (%s, %s)are not in supported format",
                str_days_ago)

    def split_dates_to_chunks(self, start_date, end_date, intv, strformat="%m%d%Y"):
        """
        Splits dates in given period into chunks of specified max size.

        Params:
        start_date -- start_period [datetime]
        end_date -- end_period [datetime]
        intv -- max chunk size
        strformat -- dateformat of result periods

        Usage example:
        list(split_dates_to_chunks("2018-01-01", "2018-01-04", 2, "%Y-%m-%d"))

            returns [{start_date: "2018-01-01", "end_date":"2018-01-02"}
                     {start_date: "2018-01-02", "end_date":"2018-01-04"}]
        """
        return list(self._split_dates_to_chunks_gen(start_date, end_date, intv, strformat))

    def _split_dates_to_chunks_gen(self, start_date, end_date, intv, strformat="%m%d%Y"):
        """
        Splits dates in given period into chunks of specified max size.

        Params:
        start_date -- start_period [datetime]
        end_date -- end_period [datetime]
        intv -- max chunk size
        strformat -- dateformat of result periods

        Usage example:
        list(split_dates_to_chunks("2018-01-01", "2018-01-04", 2, "%Y-%m-%d"))

            returns [{start_date: "2018-01-01", "end_date":"2018-01-02"}
                     {start_date: "2018-01-02", "end_date":"2018-01-04"}]
        """

        nr_days = (end_date - start_date).days

        if nr_days <= intv:
            yield {'start_date': start_date.strftime(strformat),
                   'end_date': end_date.strftime(strformat)}
        elif intv == 0:
            diff = timedelta(days=1)
            for i in range(nr_days):
                yield {'start_date': (start_date + diff * i).strftime(strformat),
                       'end_date': (start_date + diff * i).strftime(strformat)}
        else:
            nr_parts = math.ceil(nr_days / intv)
            diff = (end_date - start_date) / nr_parts
            for i in range(nr_parts):
                yield {'start_date': (start_date + diff * i).strftime(strformat),
                       'end_date': (start_date + diff * (i + 1)).strftime(strformat)}



================================================
FILE: kbc/result.py
================================================
import io
import os
from dataclasses import dataclass
from typing import List, Dict

from kbc.csv_tools import CachedOrthogonalDictWriter

"""
Set of classes for standardized result processing, without use of Pandas library - more memory efficient.

Defines three classes
- KBCResult - definition of a result that can be processed by other methods
- KBCTableDef - definition of a KBC Storage table - column, pkeys, name and other metadata
- ResultWriter - Class providing methods of generic parsing and writing JSON responses.
Can be used to build more complex
 writer based on this class as a parent.

"""


# TODO: Support for sliced tables. Flag on KBC result and modified full_path to contain just folder

@dataclass
class KBCTableDef:
    pk: List[str]
    columns: List[str]
    name: str
    destination: str
    metadata: Dict = None
    column_metadata: Dict = None

    def __getitem__(self, index):
        return getattr(self, index)


@dataclass
class KBCResult:
    file_name: str
    full_path: str
    table_def: KBCTableDef
    incremental: bool = False

    def __getitem__(self, index):
        return getattr(self, index)


class ResultWriter:
    """
    Writer for parsing and storing JSON responses. It allows basic flattening of json object a
    nd handling of Array objects.

    The Writer object support `_enter_` and `_close_` methods so it can be used in `with` statement. Otherwise method
    `close()` has to be called before result retrieval. The write method opens output stream which is kept during
    the whole lifetime of an instance, until the `close()` or `_close_` method is called (`with` statement exited)
    """

    def __init__(self, result_dir_path: str, table_def: KBCTableDef, fix_headers=False,
                 exclude_fields=None, user_value_cols=None, flatten_objects=True, child_separator: str = '.',
                 buffer_size=io.DEFAULT_BUFFER_SIZE):
        """

         :type child_separator: String that will separate child objects while flattening
         :param write_header: asddas
         :param write_header: object
         :param table_def: KBCTableDef
         :param result_dir_path: str
         :param fix_headers: flag whether to use minimal header specified in table definition
         or rather infer the header from the result itself.
         Columns need to be specified in the TableDef object. If set to true, result will
         always contain at least the specified header.

         :param flatten_objects: Specifies whether to flatten the parsed object or write them as are.
         If set to true all object hierarchies are flattened with `_` separator. Arrays are not parsed and stored as
         normal values.

         **Note:** If set to `True` and  used in combination with `fix_headers` field the columns specified include
         all flattened objects, hierarchy separated by `_` e.g. cols = ['single_col', 'parent_child'] otherwise these
         will not be included.

         :param exclude_fields: List of col names to be excluded from parsing.
         Usefull with `flatten_object` parameter on. This is applied before flattening.

         :param user_value_cols: List of column names that will be added to the root object. Use this in cocnlusion
         with `user_values` parameter in the write() method.

         :param buffer_size: Optional size of buffer used with Writer object.
         :type buffer_size: int

         :rtype: object
         """
        self.result_dir_path = result_dir_path
        self.res_file_path = os.path.join(self.result_dir_path, table_def.name + '.csv')
        self.table_def = table_def
        if user_value_cols:
            self.table_def.columns += user_value_cols
        self.results = {}
        self.fix_headers = fix_headers
        self.exclude_fields = exclude_fields if exclude_fields else []
        self.flatten_objects = flatten_objects
        self.child_separator = child_separator
        self.user_value_cols = user_value_cols if user_value_cols else []

        # Cache open files, to limit expensive open operation count
        self._csv_writer_cache = {}
        self._buffer_size = buffer_size

    def write_all(self, data_array, file_name=None, user_values=None, object_from_arrays=False, write_header=True):

        for data in data_array:
            self.write(data, file_name=file_name, user_values=user_values, object_from_arrays=object_from_arrays,
                       write_header=write_header)

    def write(self, data, file_name=None, user_values=None, object_from_arrays=False, write_header=True):
        """
        Write the Json result using the writer configuration. Keeps the output stream opened and every result is
        written to the resulting file assuring the memory efficiency.

        :param data:
        :param file_name:
        :param user_values: Optional dictionary of user values to be added to the root object. The keys must match
        the column names specified during initialization in `user_value_cols` parameter,
        otherwise it will throw a ValueError.

        If used with `fix_headers` option, `user_value` columns must be specified on the table def object.

        :param object_from_arrays: Create additional tables from the array objects
        The result tables are named like: `parent_array-colnam.csv' and generates PK values: [parent_key, row_number]

        :param write_header: Optional flag specifying whether to write the header
        - only needed when creating sliced files.
        :return:
        """
        if not data:
            return {}

        if file_name:
            self.res_file_path = os.path.join(self.result_dir_path, file_name)

        # exclude columns
        data = self._exclude_cols(data)

        # flatten objects
        if self.flatten_objects:
            data = self.flatten_json(data)

        # add user values
        data = self._add_user_values(data, user_values)

        # write array objects
        if object_from_arrays:
            res = self._write_array_object(data, write_header, user_values)
            self.results = {**self.results, **res}

        fieldnames = self._get_header(data)
        # update columns in result if not specified and sort (dictwriter depends on column list order)
        if not self.fix_headers:
            fieldnames.sort()
            self.table_def.columns = fieldnames

        self._write_data_to_csv(data, self.res_file_path, write_header, fieldnames)

        self.results[file_name] = (KBCResult(file_name, self.res_file_path, self.table_def))

    def collect_results(self):
        """
        Collect the results.

        NOTE: If not called after a `with` statement, method `close()` needs to be called prior processing
        any of the result files to close all the streams.

        :return: List of KBCResult objects
        """

        return [self.results[r] for r in self.results]

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def close(self):
        """
        Close all output streams / files. Has to be called at end of extraction, before result processing.

        :return:
        """
        for res in self._csv_writer_cache:
            self._csv_writer_cache[res].close()

        # set final columns
        if self._csv_writer_cache.get(self.res_file_path):
            self.table_def.columns = self._csv_writer_cache[self.res_file_path].fieldnames

    def _write_array_object(self, data, write_header, user_values):
        results = {}
        for arr in [(key, data[key]) for key in data.keys() if isinstance(data[key], list)]:
            res = [{"row_nr": idx, "parent_key": self._get_pkey_values(data, user_values), "value": val}
                   for idx, val in enumerate(arr[1])]
            tb_name = self.table_def.name + '_' + arr[0]
            filename = tb_name + '.csv'
            res_path = os.path.join(self.result_dir_path, filename)
            columns = ["row_nr", "parent_key", "value"]
            self._write_data_to_csv(res, res_path, write_header, columns)
            res = KBCResult(filename, res_path,
                            KBCTableDef(["row_nr", "parent_key"], columns, tb_name, ''))
            results[res.file_name] = res
            # remove field from source
            data.pop(arr[0])
        return results

    def _get_pkey_values(self, data, user_values):
        pkeys = []
        for k in self.table_def.pk:
            if data.get(k):
                pkeys.append(str(data[k]))
            elif user_values.get(k):
                pkeys.append(str(user_values[k]))

        return '|'.join(pkeys)

    def _write_data_to_csv(self, data, res_file_path, write_header, fieldnames):
        # append if exists
        if self._csv_writer_cache.get(res_file_path):
            writer = self._csv_writer_cache.get(res_file_path)
            write_header = False
        else:
            writer = CachedOrthogonalDictWriter(res_file_path, fieldnames, buffering=self._buffer_size,
                                                temp_directory=f'{res_file_path}_temp')
            self._csv_writer_cache[res_file_path] = writer

        # not needed since OrthogonalWriter is being used
        # # fix headers
        # data = self._fix_fieldnames(data, fieldnames)

        if write_header:
            writer.writeheader()
        if isinstance(data, list):
            writer.writerows(data)
        else:
            writer.writerow(data)

    def _get_header(self, data):
        if self.fix_headers:
            cols = self.table_def.columns
        else:
            cols = list(data.keys())
        cols.sort()
        return cols

    def flatten_json(self, x, out=None, name=''):
        if out is None:
            out = dict()
        if type(x) is dict:
            for a in x:
                self.flatten_json(x[a], out, name + a + self.child_separator)
        else:
            out[name[:-len(self.child_separator)]] = x

        return out

    def _exclude_cols(self, data):
        for col in self.exclude_fields:
            if col in data.keys():
                data.pop(col)
        return data

    def _add_user_values(self, data, user_values):
        if not user_values:
            return data

        # validate
        if len(user_values) > 0 and not all(elem in user_values.keys() for elem in self.user_value_cols):
            raise ValueError("Some user value keys (%s) were not set in user_value_cols (%s) during initialization!",
                             user_values.keys(), self.user_value_cols)

        data = {**data, **user_values}

        for key in user_values:
            data[key] = user_values[key]
        return data

    def _fix_fieldnames(self, data, column_names):
        """
        Fix columns in the dictionary. Adds empty ones if key not present
        :param data:
        :return:
        """
        if isinstance(data, list):
            result = []
            for row in data:
                result.append(self._fix_fieldnames(row, column_names))
        else:
            result = {}
            for key in column_names:
                result[key] = data.get(key, '')

        return result



================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/kbc/__init__.py
================================================
[Empty file]


================================================
FILE: tests/kbc/test_client_base.py
================================================
import unittest
from unittest.mock import patch

from kbc import client_base


class TestClientBase(unittest.TestCase):

    @patch.object(client_base.requests.Session, 'request')
    def test_post_raw_default_pars_with_none_custom_pars_passes(self, mock_post):
        test_def_par = {"default_par": "test"}
        cl = client_base.HttpClientBase('http://example.com', default_params=test_def_par)

        # post raw
        cl.post_raw()
        mock_post.assert_called_with('POST', params=test_def_par)

    @patch.object(client_base.requests.Session, 'request')
    def test_post_default_pars_with_none_custom_pars_passes(self, mock_post):
        test_def_par = {"default_par": "test"}
        cl = client_base.HttpClientBase('http://example.com', default_params=test_def_par)

        # post
        cl.post()
        mock_post.assert_called_with('POST', params=test_def_par)

    @patch.object(client_base.requests.Session, 'request')
    def test_post_raw_default_pars_with_custom_pars_passes(self, mock_post):
        test_def_par = {"default_par": "test"}
        cl = client_base.HttpClientBase('http://example.com', default_params=test_def_par)

        # post_raw
        cust_par = {"custom_par": "custom_par_value"}
        cl.post_raw(params=cust_par)

        test_cust_def_par = {**test_def_par, **cust_par}
        mock_post.assert_called_with('POST', params=test_cust_def_par)

    @patch.object(client_base.requests.Session, 'request')
    def test_post_default_pars_with_custom_pars_passes(self, mock_post):
        test_def_par = {"default_par": "test"}
        cl = client_base.HttpClientBase('http://example.com', default_params=test_def_par)

        # post
        cust_par = {"custom_par": "custom_par_value"}
        cl.post(params=cust_par)

        test_cust_def_par = {**test_def_par, **cust_par}
        mock_post.assert_called_with('POST', params=test_cust_def_par)

    @patch.object(client_base.requests.Session, 'request')
    def test_post_raw_default_pars_with_custom_pars_to_None_passes(self, mock_post):
        test_def_par = {"default_par": "test"}
        cl = client_base.HttpClientBase('http://example.com', default_params=test_def_par)

        # post_raw
        cust_par = None
        cl.post_raw(params=cust_par)

        # post_raw changes None to empty dict
        _cust_par_transformed = {}
        test_cust_def_par = {**test_def_par, **_cust_par_transformed}
        mock_post.assert_called_with('POST', params=test_cust_def_par)

    @patch.object(client_base.requests.Session, 'request')
    def test_post_default_pars_with_custom_pars_to_None_passes(self, mock_post):
        test_def_par = {"default_par": "test"}
        cl = client_base.HttpClientBase('http://example.com', default_params=test_def_par)

        # post_raw
        cust_par = None
        cl.post(params=cust_par)

        # post_raw changes None to empty dict
        _cust_par_transformed = {}
        test_cust_def_par = {**test_def_par, **_cust_par_transformed}
        mock_post.assert_called_with('POST', params=test_cust_def_par)

    @patch.object(client_base.requests.Session, 'request')
    def test_post_raw_with_custom_pars_passes(self, mock_post):
        cl = client_base.HttpClientBase('http://example.com')

        # post_raw
        cust_par = {"custom_par": "custom_par_value"}
        cl.post_raw(params=cust_par)

        mock_post.assert_called_with('POST', params=cust_par)

    @patch.object(client_base.requests.Session, 'request')
    def test_post_with_custom_pars_passes(self, mock_post):
        cl = client_base.HttpClientBase('http://example.com')

        # post_raw
        cust_par = {"custom_par": "custom_par_value"}
        cl.post(params=cust_par)

        mock_post.assert_called_with('POST', params=cust_par)

    @patch.object(client_base.requests.Session, 'request')
    def test_all_methods_requests_raw_with_custom_pars_passes(self, mock_post):
        cl = client_base.HttpClientBase('http://example.com')

        # post_raw
        cust_par = {"custom_par": "custom_par_value"}

        for met in ['GET', 'POST', 'PATCH', 'UPDATE', 'PUT']:
            cl._request_raw(method=met, ignore_auth=False, params=cust_par)
            mock_post.assert_called_with(met, params=cust_par)

        cl.requests_retry_session().close()

    @patch.object(client_base.HttpClientBase, '_request_raw')
    def test_all_methods_skip_auth(self, mock_post):
        cl = client_base.HttpClientBase('http://example.com')

        for m in ['GET', 'POST', 'PATCH', 'UPDATE', 'PUT']:
            method_to_call = getattr(cl, m.lower())
            method_to_call(url=cl.base_url, ignore_auth=True)
            mock_post.assert_called_with(m, ignore_auth=True, url='http://example.com')

    def test_request_skip_auth_header(self):
        cl = client_base.HttpClientBase('http://example.com', default_http_header={"defheader": "test"},
                                        auth_header={"Authorization": "test"})
        res = cl._request_raw(url=cl.base_url, method='POST', ignore_auth=True)
        cl.requests_retry_session().close()
        self.assertEqual(res.request.headers.get('defheader'), 'test')


if __name__ == '__main__':
    unittest.main()



================================================
FILE: tests/kbc/test_csv_tools.py
================================================
import csv
import os
import tempfile
import unittest

from kbc.csv_tools import CachedOrthogonalDictWriter


class TestCsvTools(unittest.TestCase):

    def test_returns_maintains_initial_header(self):
        fd, fname = tempfile.mkstemp(text=True)

        header = ['a', 'b', 'c', 'd']
        with CachedOrthogonalDictWriter(fname, header) as wr:
            wr.writeheader()
            wr.writerow({"a": 1})
            wr.writerow({"b": 2, "c": 3})

        rows = []
        expected_rows = [header, ['1', '', '', ''], ['', '2', '3', '']]
        with open(fname, 'r') as file:
            for r in csv.reader(file):
                rows.append(r)

        # cleanup
        os.close(fd)
        os.unlink(fname)

        print(rows)
        self.assertEqual(expected_rows, rows)

    def test_returns_maintains_initial_header_no_header(self):
        fd, fname = tempfile.mkstemp(text=True)

        header = ['a', 'b', 'c', 'd']
        with CachedOrthogonalDictWriter(fname, header) as wr:
            wr.writerow({"a": 1})
            wr.writerow({"b": 2, "c": 3})

        rows = []
        expected_rows = [['1', '', '', ''], ['', '2', '3', '']]
        with open(fname, 'r') as file:
            for r in csv.reader(file):
                rows.append(r)

        # cleanup
        os.close(fd)
        os.unlink(fname)

        print(rows)
        self.assertEqual(expected_rows, rows)

    def test_same_structure(self):
        fd, fname = tempfile.mkstemp(text=True)

        header = ['a', 'b']
        with CachedOrthogonalDictWriter(fname, header) as wr:
            wr.writeheader()
            wr.writerow({"a": 1, "b": 2})
            wr.writerow({"a": 1, "b": 2})

        rows = []
        expected_rows = [header, ['1', '2'], ['1', '2']]
        with open(fname, 'r') as file:
            for r in csv.reader(file):
                rows.append(r)

        # cleanup
        os.close(fd)
        os.unlink(fname)

        print(rows)
        self.assertEqual(expected_rows, rows)

    def test_returns_expands_initial_header(self):
        fd, fname = tempfile.mkstemp(text=True)

        header = ['a', 'b', 'c']
        with CachedOrthogonalDictWriter(fname, header) as wr:
            wr.writeheader()
            wr.writerow({"a": 1})
            wr.writerow({"b": 2, "c": 3, "d": 4})

        rows = []
        expected_rows = [['a', 'b', 'c', 'd'], ['', '2', '3', '4'], ['1', '', '', '']]
        with open(fname, 'r') as file:
            for r in csv.reader(file):
                rows.append(r)

        # cleanup
        os.close(fd)
        os.unlink(fname)

        print(rows)
        self.assertEqual(expected_rows, rows)

    def test_returns_expands_initial_header_no_with(self):
        fd, fname = tempfile.mkstemp(text=True)

        header = ['a', 'b', 'c']
        wr = CachedOrthogonalDictWriter(fname, header)
        wr.writeheader()
        wr.writerow({"a": 1})
        wr.writerow({"b": 2, "c": 3, "d": 4})
        wr.close()
        rows = []
        expected_rows = [['a', 'b', 'c', 'd'], ['', '2', '3', '4'], ['1', '', '', '']]
        with open(fname, 'r') as file:
            for r in csv.reader(file):
                rows.append(r)

        # cleanup
        os.close(fd)
        os.unlink(fname)

        print(rows)
        self.assertEqual(expected_rows, rows)



================================================
FILE: tests/kbc/test_env_handler.py
================================================
import os
import unittest
from datetime import datetime

from freezegun import freeze_time

from kbc import env_handler


class TestEnvHandler(unittest.TestCase):

    def setUp(self):
        path = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                            'data')
        os.environ["KBC_DATADIR"] = path

    def test_empty_required_params_pass(self):
        # set env
        hdlr = env_handler.KBCEnvHandler(mandatory_params=[])

        # tests
        try:
            hdlr.validate_config()
        except Exception:  # noeq
            self.fail("validateConfig() fails on empty Parameters!")

    def test_required_params_missing_fail(self):
        # set env - missing notbar
        hdlr = env_handler.KBCEnvHandler(mandatory_params=['fooBar', 'notbar'])

        with self.assertRaises(ValueError) as er:
            hdlr.validate_config(['fooBar', 'notbar'])

        self.assertEqual('Missing mandatory config parameters fields: [notbar] ', str(er.exception))

    # TEST backfill mode function
    @freeze_time("2010-10-10")
    def test_backfill_mode_period_no_state_first_step(self):
        # setup
        hdlr = env_handler.KBCEnvHandler(mandatory_params=['fooBar', 'notbar'])
        # override config params
        start_date = '2010-09-01'
        end_date = '2010-10-10'
        # expected res
        res_start = datetime(2010, 9, 1)
        res_end = datetime(2010, 10, 10)

        start_date, end_date = hdlr.get_backfill_period(start_date, end_date, None)
        self.assertEqual(start_date, res_start)
        self.assertEqual(res_end, res_end)

    @freeze_time("2010-10-10")
    def test_backfill_mode_period_state_last_step(self):
        # setup
        last_state = {"last_period": {"start_date": "2010-10-05", "end_date": "2010-10-10"}}
        hdlr = env_handler.KBCEnvHandler(mandatory_params=['fooBar', 'notbar'])
        # override config params
        start_date = '2010-09-01'
        end_date = '2010-10-10'
        # expected res
        res_start = datetime(2010, 10, 10)
        res_end = datetime(2010, 10, 10)

        start_date, end_date = hdlr.get_backfill_period(start_date, end_date, last_state)
        self.assertEqual(start_date, res_start)
        self.assertEqual(res_end, res_end)

    @freeze_time("2010-10-10")
    def test_backfill_mode_period_state_last_intermediate_step_full(self):
        # setup
        last_state = {"last_period": {"start_date": "2010-10-01", "end_date": "2010-10-05"}}
        hdlr = env_handler.KBCEnvHandler(mandatory_params=['fooBar', 'notbar'])
        # override config params / 4 day chunks
        start_date = '2010-09-01'
        end_date = '2010-09-04'
        # expected res - end_date + 4 days
        res_start = datetime(2010, 10, 5)
        res_end = datetime(2010, 10, 9)

        start_date, end_date = hdlr.get_backfill_period(start_date, end_date, last_state)
        self.assertEqual(start_date, res_start)
        self.assertEqual(res_end, res_end)

    @freeze_time("2010-10-10")
    def test_backfill_mode_period_state_last_intermediate_step_part(self):
        # setup
        last_state = {"last_period": {"start_date": "2010-10-05", "end_date": "2010-10-09"}}
        hdlr = env_handler.KBCEnvHandler(mandatory_params=['fooBar', 'notbar'])
        # override config params / 4 day chunks
        start_date = '2010-09-01'
        end_date = '2010-09-04'
        # expected res - end_date + 4 days
        # expected res - end_date + 1 days
        res_start = datetime(2010, 10, 9)
        res_end = datetime(2010, 10, 10)

        start_date, end_date = hdlr.get_backfill_period(start_date, end_date, last_state)
        self.assertEqual(start_date, res_start)
        self.assertEqual(res_end, res_end)

    """
    def test_remove_header_and_delete(self):
        # set env
        hdlr = env_handler.KBCEnvHandler(mandatory_params=[])
        
        # Create a temporary directory
        test_dir = tempfile.mkdtemp()
    """


if __name__ == '__main__':
    unittest.main()



================================================
FILE: tests/kbc/data/config.json
================================================
{
    "storage": {
        "input": {
            "files": [
                {
                    "tags": [
                        "dilbert"
                    ],
                    "processed_tags": []
                },
                {
                    "tags": [
                        "xkcd"
                    ],
                    "processed_tags": []
                }
            ],
            "tables": [
                {
                    "source": "in.c-main.test",
                    "destination": "sample.csv",
                    "columns": [],
                    "where_values": [],
                    "where_operator": "eq"
                },
                {
                    "source": "in.c-main.test2",
                    "destination": "fooBar",
                    "columns": [],
                    "where_values": [],
                    "where_operator": "eq"
                }
            ]
        },
        "output": {
            "tables": [
                {
                    "source": "results.csv",
                    "destination": "out.c-main.test",
                    "incremental": false,
                    "primary_key": [],
                    "delete_where_values": [],
                    "delete_where_operator": "eq"
                },
                {
                    "source": "results-new.csv",
                    "destination": "out.c-main.test2",
                    "incremental": false,
                    "primary_key": [],
                    "delete_where_values": [],
                    "delete_where_operator": "eq"
                }
            ],
            "files": [
                {
                    "source": "processed.png",
                    "tags": [
                        "processed-file"
                    ],
                    "is_public": false,
                    "is_permanent": false,
                    "is_encrypted": true,
                    "notify": false
                }
            ]
        }
    },
    "parameters": {
        "fooBar": {
            "foo": 42,
            "bar": 24
        },
        "baz": "bazBar"
    },
    "image_parameters": {},
    "action": "test",
    "authorization": {
        "oauth_api": {
            "id": "123456",
            "credentials": {
                "id": "main",
                "authorizedFor": "Myself",
                "creator": {
                    "id": "1234",
                    "description": "me@keboola.com"
                },
                "created": "2016-01-31 00:13:30",
                "oauthVersion": "2.0",
                "appKey": "myappkey",
                "#data": "{\"mykey\":\"myval\"}",
                "#appSecret": "myappsecret"
            }
        }
    }
}

