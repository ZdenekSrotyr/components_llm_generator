Directory structure:
└── kds_consulting_team-kds-team.ex-reviewtrackers/
    ├── README.md
    ├── bitbucket-pipelines.yml
    ├── change_log.md
    ├── deploy.sh
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configSchema_v1.json
    │   ├── configSchema_v2.json
    │   ├── configuration_description.md
    │   └── stack_parameters.json
    ├── deployment/
    │   ├── deploy.sh
    │   └── flake8.cfg
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run.bat
    │   ├── run_kbc_tests.ps1
    │   └── update_dev_portal_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── job_runner.py
    │   ├── kbc/
    │   │   └── env_handler.py
    │   └── service/
    │       ├── api_client.py
    │       └── parser.py
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── util-scripts/
        ├── build_n_run.ps1
        ├── run.bat
        └── run_kbc_tests.ps1

================================================
FILE: README.md
================================================
# Introduction
ReviewTrackers is an award-winning customer feedback software company that provides data and technology for tens of thousands of businesses looking to monitor their reviews, manage their business reputation, and drive insights into their customers feedback. Collecting reviews from over 100 sites, ReviewTrackers is an industry leader in helping companies transform the customer experience and convert every customer into a lifelong advocate. This extractor will import reviews from your ReviewTrackers account. Don’t have one? Start a trial [here](https://www.reviewtrackers.com/request-demo/?utm_source=keboola&utm_medium=affiliate&utm_campaign=trial_link). 

# Configuration
The extractor will create three tables in the destination bucket:

    1. Locations
    2. Reviews
    3. Responses

It will bring all the reviews accessible by the user whose credentials are used in the configuration. 

Every extraction run will produce a state file which contains the metadata of the last run. Due to the nature of the large data set, extractor will be running at a limited parameter and it will continue on where it left on from the last run with the parameters stored in the state file. With the option of `Clear State` enabled, the user will be able to reset the state file and start the extraction from scratch in cases of backfilling. Otherwise, please have this configuration as "false".


================================================
FILE: bitbucket-pipelines.yml
================================================
options:
  docker: true

pipelines:
  default:
    - step:
        caches:
          - docker
        script:
          - export APP_IMAGE=keboola-component
          - docker build . --tag=$APP_IMAGE
          - docker images
          - docker -v
          - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
          - echo "Running unit-tests..."
          - docker run $APP_IMAGE python -m unittest discover
          # push test image to ecr - uncomment for testing before deployment
          - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
          - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
          - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
          - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
          - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
          - docker push $REPOSITORY:$TEST_TAG


  branches:
    master:
      - step:
          caches:
            - docker
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker -v
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            # push test image to ecr - uncomment for testing before deployment
            - export TEST_TAG=${BITBUCKET_BRANCH//\//-}
            - echo "Pushing test image to repo. [tag=${TEST_TAG}]"
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:$TEST_TAG
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:$TEST_TAG
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - ./scripts/update_dev_portal_properties.sh
  tags:
    '*':
      - step:
          deployment: production
          script:
            - export APP_IMAGE=keboola-component
            - docker build . --tag=$APP_IMAGE
            - docker images
            - docker run $APP_IMAGE flake8 /code/ --config=/code/flake8.cfg
            - echo "Running unit-tests..."
            - docker run $APP_IMAGE python -m unittest discover
            - echo "Preparing KBC test image"
            - docker pull quay.io/keboola/developer-portal-cli-v2:latest
            # push test image to ECR - uncomment when initialised
            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            - docker tag $APP_IMAGE:latest $REPOSITORY:test
            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            - docker push $REPOSITORY:test
#            - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP 991513932 test
            - chmod +x ./scripts/update_dev_portal_properties.sh
            - chmod +x ./deploy.sh
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh


================================================
FILE: change_log.md
================================================



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi



================================================
FILE: Dockerfile
================================================
FROM python:3.12-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

RUN pip install flake8

RUN pip install -r /code/requirements.txt



WORKDIR /code/


# Execution
CMD ["python", "-u", "/code/src/component.py"]
#CMD python3 /code/src/component.py



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: requirements.txt
================================================
https://github.com/keboola/python-docker-application/zipball/master#egg=keboola
requests
pandas
logging_gelf


================================================
FILE: component_config/component_long_description.md
================================================
ReviewTracker is your go-to solution for streamlined online reputation management. Track and manage reviews across platforms, analyze sentiment, benchmark against competitors, and gain actionable insights. It's the one-stop tool for businesses to maintain a positive online presence and build customer trust.


================================================
FILE: component_config/component_short_description.md
================================================
ReviewTracker is your go-to solution for streamlined online reputation management. Track and manage reviews across platforms, analyze sentiment, benchmark against competitors, and gain actionable insights. It's the one-stop tool for businesses to maintain a positive online presence and build customer trust.


================================================
FILE: component_config/configSchema.json
================================================
{
    "title": "Parameters",
    "type": "object",
    "required":[
        "username",
        "#password",
        "clear_state"
    ],
    "properties":{
        "username":{
            "propertyOrder": 100,
            "title": "username",
            "type": "string",
            "minLength": 4
        },
        "#password":{
            "propertyOrder": 200,
            "title": "password",
            "format": "password",
            "type": "string",
            "minLength": 1
        },
        "clear_state":{
            "propertyOrder": 400,
            "title": "Clear State",
            "type": "string",
            "default": "false",
            "enum": [
                "true",
                "false"
            ],
            "description": "Enabling 'Clear State' will wipe out the current extractor state. Every endpoint will be starting from 1st page."
        }
    }
}



================================================
FILE: component_config/configSchema_v1.json
================================================
{
    "title": "Parameters",
    "type": "object",
    "required":[
        "username",
        "#password",
        "endpoints",
        "metrics"
    ],
    "properties":{
        "username":{
            "propertyOrder": 100,
            "title": "username",
            "type": "string",
            "minLength": 4
        },
        "#password":{
            "propertyOrder": 200,
            "title": "password",
            "format": "password",
            "type": "string",
            "minLength": 1
        },
        "metrics": {
          "title": "Metrics",
          "uniqueItems": true,
          "type": "array",
          "description": "Add metrics here. For each metric, please:<br><ol><li>Choose metric type from the dropdown</li><li>Specify the start & end of the metric period</li></ol>",
          "items": {
            "type": "object",
            "title": "Metrics",
            "properties":{
              "report_type":{
                "propertyOrder": 100,
                "title": "Report Type",
                "type": "string",
                "default": "Overview",
                "enum": [
                  "Overview",
                  "Monthly",
                  "Sources"
                ]
              },
              "month_after":{
                "propertyOrder": 200,
                "title": "Month After",
                "type": "string",
                "description": "Date Format <b>YYYY-MM-DD</b>. This is the start of your metric date range."
              },
              "month_before":{
                "propertyOrder": 300,
                "title": "Month Before",
                "type": "string",
                "description": "Date Format <b>YYYY-MM-DD</b>. This is the end of your metric date range."
              }
            },
            "format": "table",
            "default": {
              "report_type": "Overview",
              "month_after": "2018-08-01",
              "month_before": "2018-09-01"
            }
          }
        },
        "endpoints":{
            "propertyOrder": 400,
            "title": "Data Endpoints",
            "format": "select",
            "uniqueItems": false,
            "type": "array",
            "description": "Select <b>All</b> or the endpoints you want to extract",
            "default": "All",
            "items": {
                "default": "All",
                "type": "string",
                "enum": [
                            "All",
                            "accounts",
                            "alert_frequencies",
                            "alerts",
                            "alert_types",
                            "campaigns",
                            "competitors",
                            "contacts",
                            "groups",
                            "items",
                            "layouts",
                            "locations",
                            "notes",
                            "profiles",
                            "permissions",
                            "request_pages",
                            "requests",
                            "request_types",
                            "responses",
                            "reviews",
                            "review_status_labels",
                            "single_sign_ons",
                            "sources",
                            "templates",
                            "template_tags",
                            "urls",
                            "users",
                            "user_types",
                            "whitelabels"
                ]
            }
        }
    }
}



================================================
FILE: component_config/configSchema_v2.json
================================================
{
    "title": "Parameters",
    "type": "object",
    "required":[
        "username",
        "#password",
        "endpoints",
        "clear_state"
    ],
    "properties":{
        "username":{
            "propertyOrder": 100,
            "title": "username",
            "type": "string",
            "minLength": 4
        },
        "#password":{
            "propertyOrder": 200,
            "title": "password",
            "format": "password",
            "type": "string",
            "minLength": 1
        },
        "endpoints":{
            "propertyOrder": 300,
            "title": "Data Endpoints",
            "format": "select",
            "uniqueItems": false,
            "type": "array",
            "description": "Select <b>All</b> or the endpoints you want to extract",
            "default": "locations",
            "items": {
                "default": "All",
                "type": "string",
                "enum": [
                            "locations",
                            "responses",
                            "reviews"
                ]
            }
        },
        "clear_state":{
            "propertyOrder": 400,
            "title": "Clear State",
            "type": "string",
            "default": "false",
            "enum": [
                "true",
                "false"
            ],
            "description": "Enabling 'Clear State' will wipe out the current extractor state. Every endpoint will be starting from 1st page."
        }
    }
}



================================================
FILE: component_config/configuration_description.md
================================================
The extractor will create three tables in the destination bucket:

    1. Locations
    2. Reviews
    3. Responses

It will bring all the reviews accessible by the user whose credentials are used in the configuration. 

Every extraction run will produce a state file which contains the metadata of the last run. Due to the nature of the large data set, extractor will be running at a limited parameter and it will continue on where it left on from the last run with the parameters stored in the state file. With the option of `Clear State` enabled, the user will be able to reset the state file and start the extraction from scratch in cases of backfilling. Otherwise, please have this configuration as "false".


================================================
FILE: component_config/stack_parameters.json
================================================
{}


================================================
FILE: deployment/deploy.sh
================================================
#!/bin/sh
set -e

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
FILE: deployment/flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
FILE: src/component.py
================================================
from kbc.env_handler import KBCEnvHandler
import logging
import job_runner

MANDATORY_PARS = [
    'username',
    '#password',
]


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS)
        # override debug from config
        if self.cfg_params.get('debug'):
            debug = True

        self.set_default_logger('DEBUG' if debug else 'INFO')
        logging.info('Loading configuration...')

        try:
            self.validateConfig()
        except ValueError as e:
            logging.error(e)
            exit(1)

    def run(self):
        """
        Main execution code
        """
        params = self.cfg_params  # noqa
        username = params.get('username')
        password = params.get('#password')
        clear_state = params.get('clear_state')

        job_runner.run(username, password, clear_state)


"""
        Main entrypoint
"""
if __name__ == "__main__":
    comp = Component()
    comp.run()



================================================
FILE: src/job_runner.py
================================================
import json
import logging
import os
import sys
import warnings
import requests
from requests.auth import HTTPBasicAuth
from service.api_client import request_endpoint, request_reviews_v2, request_accounts

warnings.simplefilter(action='ignore', category=FutureWarning)


def _auth(username, password):
    """
    Basic Authorization to Server
    """
    url = 'https://api-gateway.reviewtrackers.com/auth'
    headers = {
        'Accept': "application/vnd.rtx.authorization.v2.hal+json;charset=utf-8",
        "Content-Type": "application/json"
    }

    res = requests.post(url=url, headers=headers,
                        auth=HTTPBasicAuth(username, password))

    auth_res = json.loads(res.text)
    # logging.info("Authorization Return: {0}".format(auth_res))
    if "error" in auth_res:
        logging.error("{0}: {1}".format(auth_res["error"], auth_res["status"]))
        sys.exit(1)

    return {
        'token': auth_res.get('token')
    }


def _read_state():
    """
    Return the last page Ex requested
    """

    if os.path.isfile("/data/in/state.json"):
        # Fetching refresh token from state file
        logging.info("Fetched State file...")
        with open("/data/in/state.json", 'r') as f:
            temp = json.load(f)
        logging.info("Extractor State: {0}".format(temp))

    else:
        temp = {}
        logging.info("No State file is found.")

    return temp


def _write_state(data_in):
    """
    Updating state file
    """

    logging.info("Outputting State file...")
    logging.info("Output State: {0}".format(data_in))
    with open("/data/out/state.json", "w") as f:
        json.dump(data_in, f)

    return


def run(ui_username, ui_password, ui_clear_state):
    """
    Main Executor for Job_runner
    """

    # Hardcoding the list of endpoints
    ui_endpoints = ["locations", "reviews", "responses"]

    # Authentication
    auth_res = _auth(username=ui_username, password=ui_password)
    token = auth_res.get('token')

    # last_update_time = _get_last_update_time(tables=ui_tables)

    # State File fetch
    logging.info("Clear State: {0}".format(ui_clear_state))
    if not ui_clear_state or ui_clear_state == 'false':
        ex_state = _read_state()
    else:
        logging.info("Clearing State File...")
        ex_state = {}

    accounts = request_accounts(ui_username, token)

    for endpoint in ui_endpoints:
        if endpoint not in ex_state:
            ex_state[endpoint] = {}

        for account in accounts:
            logging.info(f"fetching endpoint {endpoint} for account {account} ...")
            file_name = endpoint
            if endpoint == 'reviews':
                json_res, ex_state_new = request_reviews_v2(
                    ui_username, token, ex_state, endpoint, file_name, account)
            else:
                json_res, ex_state_new = request_endpoint(
                    ui_username, token, ex_state, endpoint, file_name, account)
            if json_res == 404:
                logging.warning(
                    "Endpoint [{}] not found, 404 Error".format(endpoint))
                continue

            ex_state[endpoint][account] = ex_state_new

        # State File Content after 1 Endpoint extraction
        logging.info("Extractor State: {0}".format(ex_state))

    # State File Out
    _write_state(ex_state)

    return



================================================
FILE: src/kbc/env_handler.py
================================================
# ==============================================================================
# KBC Env handler
# ==============================================================================

# ============================ Import libraries ==========================
import csv
import datetime
import json
import logging
import math
import os
import sys
from collections import Counter
import pytz
from dateutil.relativedelta import relativedelta
from _datetime import timedelta
from keboola import docker

DEFAULT_DEL = ','
DEFAULT_ENCLOSURE = '"'


class KBCEnvHandler:
    def __init__(self, mandatory_params, data_path=None):
        # fetch data folder from ENV by default
        if not data_path:
            data_path = os.environ.get('KBC_DATADIR')

        self.kbc_config_id = os.environ.get('KBC_CONFIGID')

        self.data_path = data_path
        self.configuration = docker.Config(data_path)
        self.cfg_params = self.configuration.get_parameters()
        # self.tables_out_path = os.path.join(data_path, 'out', 'tables')
        # self.tables_in_path = os.path.join(data_path, 'in', 'tables')
        self.tables_out_path = "out/tables/"
        self.tables_in_path = "in/tables/"

        self._mandatory_params = mandatory_params

# ==============================================================================

    def validateConfig(self):
        '''
        Validates config based on provided mandatory params.
        Parameters can be grouped as arrays [Par1,Par2] => at least one of the pars has to be present
        [par1,[par2,par3]] => either par1 OR both par2 and par3 needs to be present
        '''
        parameters = self.cfg_params
        missing_fields = []
        for field in self._mandatory_params:
            if isinstance(field, list):
                missing_fields.extend(self._validate_par_group(field))
            elif not parameters.get(field):
                missing_fields.append(field)

        if missing_fields:
            raise ValueError(
                'Missing mandatory configuration fields: [{}] '.format(', '.join(missing_fields)))

    def _validate_par_group(self, par_group):
        missing_fields = []
        is_present = False
        for par in par_group:
            if isinstance(par, list):
                missing_subset = self._get_par_missing_fields(par)
                missing_fields.extend(missing_subset)
                if not missing_subset:
                    is_present = True

            elif self.cfg_params.get(par):
                is_present = True
            else:
                missing_fields.append(par)
        if not is_present:
            return missing_fields
        else:
            return []

    def _get_par_missing_fields(self, mand_params):
        parameters = self.cfg_params
        missing_fields = []
        for field in mand_params:
            if not parameters.get(field):
                missing_fields.append(field)
        return missing_fields

    def get_input_table_by_name(self, table_name):
        tables = self.configuration.get_input_tables()
        table = [t for t in tables if t.get('destination') == table_name]
        if not table:
            raise ValueError(
                'Specified input mapping [{}] does not exist'.format(table_name))
        return table[0]


# ================================= Logging ==============================

    def set_default_logger(self, log_level='INFO'):  # noqa: E301
        """
        # This is a GELF logging:

        import logging_gelf.formatters
        import logging_gelf.handlers

        logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - %(message)s',
                datefmt="%Y-%m-%d %H:%M:%S"
                )

        logger = logging.getLogger('gelf')
        logging_gelf_handler = logging_gelf.handlers.GELFTCPSocketHandler(
            host=os.getenv('KBC_LOGGER_ADDR'),
            port=int(os.getenv('KBC_LOGGER_PORT'))
            )
        logging_gelf_handler.setFormatter(logging_gelf.formatters.GELFFormatter(null_character=True))
        logger.addHandler(logging_gelf_handler)

        # removes the initial stdout logging
        logger.removeHandler(logger.handlers[0])
        """

        hdl = logging.StreamHandler(sys.stdout)
        logging.basicConfig(
            level=log_level,
            format='%(levelname)s - %(message)s',
            handlers=[hdl])
        logger = logging.getLogger()

        return logger

    def get_state_file(self):
        logging.getLogger().info('Loading state file..')
        state_file_path = os.path.join(self.data_path, 'in', 'state.json')
        if not os.path.isfile(state_file_path):
            logging.getLogger().info('State file not found. First run?')
            return
        try:
            with open(state_file_path, 'r') \
                    as state_file:
                return json.load(state_file)
        except (OSError, IOError):
            raise ValueError(
                "State file state.json unable to read "
            )

    def write_state_file(self, state_dict):
        if not isinstance(state_dict, dict):
            raise TypeError('Dictionary expected as a state file datatype!')

        with open(os.path.join(self.configuration.data_dir, 'out', 'state.json'), 'w+') as state_file:
            json.dump(state_dict, state_file)

    def create_sliced_tables(self, folder_name, pkey=None, incremental=False,
                             src_delimiter=DEFAULT_DEL, src_enclosure=DEFAULT_ENCLOSURE, dest_bucket=None):
        """
        Creates prepares sliced tables from all files in DATA_PATH/out/tables/{folder_name} - i.e. removes all headers
        and creates single manifest file based on provided parameters.

        folder_name -- folder name in DATA_PATH directory that contains files for slices,
        the same name will be used as table name

        src_enclosure -- enclosure of the source file ["]
        src_delimiter -- delimiter of the source file [,]
        dest_bucket -- name of the destination bucket (optional)


        """
        log = logging
        log.info('Creating sliced tables for [{}]..'.format(folder_name))

        folder_path = os.path.join(self.tables_out_path, folder_name)

        if not os.path.isdir(folder_path):
            raise ValueError("Specified folder ({}) does not exist in the data folder ({})".format(
                folder_name, self.data_path))

        # get files
        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(
            os.path.join(folder_path, f))]

        header = self.get_and_remove_headers_in_all(
            files, src_delimiter, src_enclosure)
        if dest_bucket:
            destination = dest_bucket + '.' + folder_name
        else:
            destination = folder_name

        log.info('Creating manifest file..')
        self.configuration.write_table_manifest(
            file_name=folder_path, destination=destination, primary_key=pkey, incremental=incremental, columns=header)

    def get_and_remove_headers_in_all(self, files, delimiter, enclosure):
        """
        Removes header from all specified files and return it as a list of strings

        Throws error if there is some file with different header.

        """
        first_run = True
        for file in files:
            curr_header = self._get_and_remove_headers(
                file, delimiter, enclosure)
            if first_run:
                header = curr_header
                first_file = file
                first_run = False
            # check whether header matches
            if Counter(header) != Counter(curr_header):
                raise Exception('Header in file {}:[{}] is different than header in file {}:[{}]'.format(
                    first_file, header, file, curr_header))
        return header

    def _get_and_remove_headers(self, file, delimiter, enclosure):
        """
        Removes header from specified file and return it as a list of strings.
        Creates new updated file 'upd_'+origFileName and deletes the original
        """
        head, tail = os.path.split(file)
        with open(file, "r") as input_file:
            with open(os.path.join(head, 'upd_' + tail), 'w+', newline='') as updated:
                reader = csv.DictReader(
                    input_file, delimiter=delimiter, quotechar=enclosure)
                header = reader.fieldnames
                writer = csv.DictWriter(
                    updated, fieldnames=header, delimiter=DEFAULT_DEL, quotechar=DEFAULT_ENCLOSURE)
                for row in reader:
                    # write row
                    writer.writerow(row)
        os.remove(file)
        return header

    def process_results(self, res_files, def_bucket_name, output_bucket):
        for res in res_files:
            dest_bucket = def_bucket_name + str(self.kbc_config_id)
            if output_bucket:
                suffix = '-' + output_bucket
            else:
                suffix = ''

            # build manifest
            self.configuration.write_table_manifest(
                file_name=res['full_path'],
                destination=dest_bucket + suffix + '.' + res['name'],
                primary_key=res['pkey'],
                incremental=True)

    def process_results_sliced(self, res_files):
        res_sliced_folders = {}
        for file in res_files:
            res_sliced_folders.update({file['name']: file['pkey']})

        for folder in res_sliced_folders:
            self.create_sliced_tables(folder, res_sliced_folders[folder], True)

# ==============================================================================
# == UTIL functions

    def get_past_date(self, str_days_ago, to_date=None, tz=pytz.utc):
        '''
        Returns date in specified timezone relative to today.

        e.g.
        '5 hours ago',
        'yesterday',
        '3 days ago',
        '4 months ago',
        '2 years ago',
        'today'
        '''
        if to_date:
            TODAY = to_date
        else:
            TODAY = datetime.datetime.now(tz)
        splitted = str_days_ago.split()
        if len(splitted) == 1 and splitted[0].lower() == 'today':
            return TODAY
        elif len(splitted) == 1 and splitted[0].lower() == 'yesterday':
            date = TODAY - relativedelta(days=1)
            return date
        elif splitted[1].lower() in ['hour', 'hours', 'hr', 'hrs', 'h']:
            date = datetime.datetime.now() - \
                relativedelta(hours=int(splitted[0]))
            return date.date()
        elif splitted[1].lower() in ['day', 'days', 'd']:
            date = TODAY - relativedelta(days=int(splitted[0]))
            return date
        elif splitted[1].lower() in ['wk', 'wks', 'week', 'weeks', 'w']:
            date = TODAY - relativedelta(weeks=int(splitted[0]))
            return date
        elif splitted[1].lower() in ['mon', 'mons', 'month', 'months', 'm']:
            date = TODAY - relativedelta(months=int(splitted[0]))
            return date
        elif splitted[1].lower() in ['yrs', 'yr', 'years', 'year', 'y']:
            date = TODAY - relativedelta(years=int(splitted[0]))
            return date
        else:
            raise ValueError('Invalid relative period!')

    def split_dates_to_chunks(self, start_date, end_date, intv, strformat="%m%d%Y"):
        '''
        Splits dates in given period into chunks of specified max size.

        Params:
        start_date -- start_period [datetime]
        end_date -- end_period [datetime]
        intv -- max chunk size
        strformat -- dateformat of result periods

        Usage example:
        list(split_dates_to_chunks("2018-01-01", "2018-01-04", 2, "%Y-%m-%d"))

            returns [{start_date: "2018-01-01", "end_date":"2018-01-02"}
                     {start_date: "2018-01-02", "end_date":"2018-01-04"}]
        '''
        return list(self._split_dates_to_chunks_gen(start_date, end_date, intv, strformat))

    def _split_dates_to_chunks_gen(self, start_date, end_date, intv, strformat="%m%d%Y"):
        '''
        Splits dates in given period into chunks of specified max size.

        Params:
        start_date -- start_period [datetime]
        end_date -- end_period [datetime]
        intv -- max chunk size
        strformat -- dateformat of result periods

        Usage example:
        list(split_dates_to_chunks("2018-01-01", "2018-01-04", 2, "%Y-%m-%d"))

            returns [{start_date: "2018-01-01", "end_date":"2018-01-02"}
                     {start_date: "2018-01-02", "end_date":"2018-01-04"}]
        '''

        nr_days = (end_date - start_date).days

        if nr_days <= intv:
            yield {'start_date': start_date.strftime(strformat),
                   'end_date': end_date.strftime(strformat)}
        elif intv == 0:
            diff = timedelta(days=1)
            for i in range(nr_days):
                yield {'start_date': (start_date + diff * i).strftime(strformat),
                       'end_date': (start_date + diff * i).strftime(strformat)}
        else:
            nr_parts = math.ceil(nr_days / intv)
            diff = (end_date - start_date) / nr_parts
            for i in range(nr_parts):
                yield {'start_date': (start_date + diff * i).strftime(strformat),
                       'end_date': (start_date + diff * (i + 1)).strftime(strformat)}



================================================
FILE: src/service/api_client.py
================================================
import requests
import base64
import json
import logging
import copy
import sys

from service.parser import parse

BASE_URL = "https://api.reviewtrackers.com/"
DEFAULT_TABLE_SOURCE = "/data/in/tables/"
DEFAULT_TABLE_DESTINATION = "/data/out/tables/"


def _build_headers(username, token):
    auth_string = "{}:{}".format(username, token)
    encoded = base64.b64encode(auth_string.encode())
    encoded = encoded.decode("utf-8")
    return {
        "Authorization": "Basic {}".format(encoded),
        "Content-Type": "application/json; charset=utf-8",
        'Accept': "application/vnd.rtx.campaign.v2.hal+json;charset=utf-8",
    }


def request_reviews_v2(username, token, state_file, endpoint, file_name, account):
    """
    Request new review endpoint with provided parameters
    """

    entities = []
    headers = _build_headers(username, token)
    request_url = 'v2/{}'.format(endpoint)
    params = {'account_id': account, 'per_page': 500, 'sort[by]': 'published_at', 'sort[order]': 'ASC'}

    # Fetching last state
    next_cursor = state_file.get(endpoint, {}).get('last_cursor')
    if not next_cursor:
        next_cursor = state_file.get(endpoint, {}).get(account, {}).get('last_cursor')
    logging.info('[reviews] loaded last cursor from state: {}'.format(next_cursor))

    last_cursor = next_cursor

    while_loop = True
    while while_loop:
        if next_cursor:
            params['after'] = next_cursor

        res = requests.get(url=BASE_URL + request_url,
                           headers=headers, params=params)
        res_json = res.json()

        logging.info("Current Cursor: [{0}] @ [{1}] - Parsing".format(next_cursor, endpoint))

        try:
            parse(res_json['data'], file_name)
        except Exception as e:
            logging.error(res_json)
            logging.error("Error while parsing data: {}".format(str(e)))

        try:
            next_cursor = res_json['paging']['cursors']['after']
            if next_cursor is None:
                while_loop = False
            else:
                last_cursor = next_cursor
                logging.debug('[reviews] next paging cursor: {}'.format(last_cursor))
        except Exception:
            next_cursor = None
            while_loop = False

    endpoint_state = {'last_cursor': last_cursor}

    return entities, endpoint_state


def request_endpoint(username, token, state_file, endpoint, file_name, account):
    """
    Request endpoint with the provided pagination paramters
    """

    endpoint_state = {}

    entities = []
    headers = _build_headers(username, token)
    params = {'account_id': account, 'per_page': 500}

    res = requests.get(url=BASE_URL + endpoint, headers=headers, params=params)

    if res.status_code == 404:
        print(res.text)
        return 404
    res = json.loads(res.text)

    if 'metrics' in endpoint:
        entities.append(res)
    else:

        starting_page = state_file.get(endpoint, {}).get('last_page_fetched')
        if not starting_page:
            starting_page = state_file.get(endpoint, {}).get(account, {}).get('last_page_fetched')
        if not starting_page:
            starting_page = 1
            logging.info(f"Starting with page {starting_page} for endpoint {endpoint}")
        else:
            logging.info(f"Last fetched page from state for endpoint {endpoint} is {starting_page}")

        first_request_params = copy.deepcopy(params)
        first_request_params["page"] = starting_page
        first_request_params["sort[by]"] = "created_at"
        first_request_params["sort[order]"] = "ASC"

        # collect first page objects
        res = requests.get(url=BASE_URL + endpoint,
                           headers=headers, params=first_request_params)
        res = json.loads(res.text)

        # Captures error
        if "error" in res:
            logging.error("{0}: {1}".format(res["error"], res["status"]))
            sys.exit(1)

        total_pages = int(res.get('_total_pages'))

        logging.info("Endpoint: [{0}]; Total Pages: [{1}]".format(endpoint, total_pages))

        # First page processing
        logging.info("Current Page: [{0}] @ [{1}] - Parsing".format(starting_page, endpoint))
        entities_curr_page = res.get("_embedded").get(endpoint)
        entities += entities_curr_page
        parse(entities_curr_page, file_name)
        starting_page += 1

        while "next" in res["_links"]:
            next_url = res["_links"]["next"]["href"]
            logging.debug("Next Url: ...{0}".format(next_url[-60:]))

            res = requests.get(url=next_url, headers=headers, params=params)
            res = json.loads(res.text)
            logging.info("Current Page: [{0}] @ [{1}] - Parsing".format(starting_page, endpoint))

            entities_curr_page = res.get("_embedded").get(endpoint)
            entities += entities_curr_page

            # if there are no more records, stop at that page
            if len(entities_curr_page) == 0:
                logging.info("No records found on page [{0}] @ [{1}]".format(
                    starting_page, endpoint))
                logging.info("Stopping [{0}] @ page [{1}]".format(
                    endpoint, starting_page))
                total_pages = starting_page
                break
            else:
                parse(entities_curr_page, file_name)

            starting_page += 1

        # Update State file parameters
        # Prevent weird pagination output from ReviewTrackers
        if starting_page > total_pages:
            starting_page = total_pages
        endpoint_state = {
            "last_page_fetched": starting_page - 1,
            "total_pages": total_pages
        }

    return entities, endpoint_state


def request_accounts(username, token):
    """
    Request accounts
    """
    headers = _build_headers(username, token)
    res = requests.get(url=BASE_URL + "accounts", headers=headers)

    if res.status_code == 404:
        print(res.text)
        return 404
    res = json.loads(res.text)

    accounts = res.get("_embedded").get("accounts")
    accounts_ids = [account['id'] for account in accounts]
    return accounts_ids



================================================
FILE: src/service/parser.py
================================================
import os
import json
import logging
import pandas as pd

DEFAULT_TABLE_SOURCE = "/data/in/tables/"
DEFAULT_TABLE_DESTINATION = "/data/out/tables/"

review_header = [
    "account_id",
    "author",
    "business_response_url",
    "content",
    "created_at",
    "extra_text",
    "id",
    "location_id",
    "metadata_blank",
    "name",
    "permalink",
    "published_at",
    "rating",
    "respondable",
    "source_code",
    "source_name",
    "url_metadata_google_serp",
    "url_metadata_yelp",
    "origin_published_at",
    "updated_at"
]
location_header = [
    "account_id",
    "address",
    "city",
    "country",
    "country_id",
    "created_at",
    "deleted_at",
    "external_id",
    "feedback_url",
    "google_place_id",
    "has_issue",
    "id",
    "latitude",
    "longitude",
    "metadata_dealer_id",
    "metadata_import_id",
    "mute_issues",
    "name",
    "oid",
    "phone",
    "public_name",
    "request_page_id",
    "request_page_url",
    "resource",
    "state",
    "state_id",
    "updated_at",
    "url_id",
    "zipcode",
    "hours"
]
response_header = [
    "account_id",
    "content",
    "created_at",
    "created_by_user_id",
    "deleted_at",
    "id",
    "location_id",
    "published_at",
    "read_only",
    "reference_id",
    "resource",
    "response_template_id",
    "review_id",
    "source_id",
    "status",
    "updated_at"
]


def _review_parse(data_in):
    """
    Extracting Reviews from input data
    """

    data_out = []

    for entity in data_in:
        temp = {}
        for header in review_header:
            # if header in ["metadata_blank", "url_metadata_google_serp"]:
            if header == "metadata_blank" and "blank" in entity["metadata"]:
                temp["metadata_blank"] = entity["metadata"]["blank"]
            elif header == "url_metadata_google_serp" and "google_serp" in entity["url_metadata"]:
                temp["url_metadata_google_serp"] = entity["url_metadata"]["google_serp"]
            elif header in entity:
                temp[header] = entity[header]
            else:
                temp[header] = ""
        data_out.append(temp)

    return data_out, review_header


def _location_parse(data_in):
    """
    Extract Location Info from input data
    """

    data_out = []

    for entity in data_in:
        temp = {}
        for header in location_header:
            if header == "metadata_dealer_id" and "dealer_id" in entity["metadata"]:
                temp["metadata_dealer_id"] = entity["metadata"]["dealer_id"]
            elif header == "metadata_import_id" and "import_id" in entity["metadata"]:
                temp["metadata_import_id"] = entity["metadata"]["import_id"]
            elif header in entity:
                temp[header] = entity[header]
            else:
                temp[header] = ""
        data_out.append(temp)

    return data_out, location_header


def _response_parse(data_in):
    """
    Parsing JSON responses
    """

    data_out = []

    for entity in data_in:
        temp = {}
        for header in response_header:
            if header in entity:
                temp[header] = entity[header]
            else:
                temp[header] = ""
        data_out.append(temp)

    return data_out, response_header


def _produce_manifest(file_name, primary_key, columns):
    """
    Create manifest file
    """

    file = "/data/out/tables/" + str(file_name) + ".csv.manifest"

    manifest = {
        "incremental": True,
        "primary_key": primary_key,
        "columns": columns
    }
    logging.debug(manifest)
    try:
        with open(file, 'w') as file_out:
            json.dump(manifest, file_out)
            logging.info(
                "Output manifest file [{0}] produced.".format(file_name))
    except Exception as e:
        logging.error("Could not produce output file manifest.")
        logging.error(e)


def _output(filename, headers, data_in, primary_key):
    dest = DEFAULT_TABLE_DESTINATION + filename + ".csv"
    data = pd.DataFrame(data_in, columns=headers)

    # If file is empty
    '''
    if len(data) == 0:
        logging.info("[{0}] contains no data.".format(filename))
        pass
    else:
        if os.path.isfile(dest):
            with open(dest, 'a') as b:
                data.to_csv(b, index=False, header=False, columns=headers)
            b.close()
        else:
            with open(dest, 'w+') as b:
                data.to_csv(b, index=False, header=True, columns=headers)
            b.close()

            # Output Manifest
            _produce_manifest(filename, "id")
    '''
    # Outputting file even the file is empty
    if os.path.isfile(dest):
        with open(dest, 'a') as b:
            data.to_csv(b, index=False, header=False)  # , columns=headers)
        b.close()
    else:
        with open(dest, 'w+') as b:
            data.to_csv(b, index=False, header=False)  # , columns=headers)
        b.close()

        # Output Manifest
        _produce_manifest(filename, primary_key, headers)

    return


def parse(data_in, endpoint):
    """
    Different Parsing Method for different endpoint
    """

    if endpoint == "reviews":
        data_out, header = _review_parse(data_in)
        _output(endpoint, header, data_out, ["id", "updated_at"])
    elif endpoint == "locations":
        data_out, header = _location_parse(data_in)
        _output(endpoint, header, data_out, ["id"])
    elif endpoint == "responses":
        data_out, header = _response_parse(data_in)
        _output(endpoint, header, data_out, ["id"])

    return



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
FILE: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest

from component import Component


class TestComponent(unittest.TestCase):


    def testRunEmptyFails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()



================================================
FILE: util-scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -t $COMP_TAG ../

echo Running component...
$DATA_PATH = Read-Host -Prompt 'Input data folder path:'
Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: util-scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag


================================================
FILE: util-scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


