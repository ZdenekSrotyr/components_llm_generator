Directory structure:
└── keboola-component-kbc-project-metadata-v2/
    ├── README.md
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   ├── sourceCodeUrl.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_test.sh
    │   ├── update_dev_portal_properties.sh
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── client.py
    │   ├── component.py
    │   ├── parser.py
    │   ├── result.py
    │   └── table_definitions.py
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── .github/
        └── workflows/
            └── push.yml

================================================
FILE: README.md
================================================
# KBC Metadata extractor

**⚠ NOTE** This extractor is being decommissioned and replaced by the official [Telemetry Extractor](https://components.keboola.com/components/keboola.ex-telemetry-data) which provides the same data and is actively maintained.

The Keboola metadata extractor downloads information from Keboola's APIs about various objects, users, etc.

The extractor utilizes folllowing Keboola APIs:

1. [Storage API](https://keboola.docs.apiary.io/#),
2. [Management API](https://keboolamanagementapi.docs.apiary.io/#),
3. [Orchestrator API](https://keboolaorchestratorv2api.docs.apiary.io/#),
4. [Queue API](https://app.swaggerhub.com/apis-docs/keboola/job-queue-api/1.0.0#/);
5. [Notification API](https://app.swaggerhub.com/apis/odinuv/notifications-service/1.1.0);
6. [Scheduler API](https://app.swaggerhub.com/apis/odinuv/scheduler/1.0.0);

and allows to download information about:

1. tokens and their last events,
2. orchestrations,
3. waiting jobs,
4. tables and columns,
5. configurations,
6. transformations,
7. project and organization users.
8. Table events
9. Schedules 
10. Notifications

## Configuration

To configure the extractor, either [a management token](https://help.keboola.com/management/account/#tokens) is needed or an [array of storage tokens](https://help.keboola.com/management/project/tokens/) for all projects, for which metadata should be downloaded. Additionally, you'll need to be able to specify the ID of your organization and region, where your projects are located.

A sample of the configuration can be found in the [component repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-kbc-project-metadata-v2/src/master/component_config/sample-config/).

### Token specification

The application accepts either one of the two options:

1. one management token,
2. any number of storage tokens.

If both options are specified, management token is prioritized and storage tokens are disregarded.

#### Authenticating with management token

Specifying management token allows to download data for all projects in the specified organization. The user, whose access token is used in the configuration, **must be part of the organization**, for which the data should be downloaded. If user is not part of the organization, the extractor will not be able to access the organization data and hence won't be able to download necessary metadata.

The management token automatically accesses all projects within the organization and creates temporary storage tokens to download the necessary data. These tokens can be distinguished by their name, which follows `[PROJECT_NAME]_Telemetry_token` naming convention. All of the automatically created tokens have an expiration of 26 hours and are **re-used by the extractor**, if the extractor is ran multiple times a day.

Along with a management token, an ID of the organization must be provided as well as region, where the organization is located. The ID of the organization can be found in the URL of the organization page - follow our [help page article](https://help.keboola.com/management/organization/) on how to access it - e.g. in URL [https://connection.keboola.com/admin/organizations/1234](https://connection.keboola.com/admin/organizations/1234), `1234` is the ID of the organization.

#### Authenticating with storage tokens

Storage tokens provide direct access to the project, in which they were created. Extractor uses these tokens to directly download data from within the project. It's therefore crucial, that these tokens **do not have any limited functionality** (such as restricted to only some tables or components), otherwise the tokens will not be able to download all the necessary data.

### Available data

The metadata extractor allows to download an extended set of objects from the Keboola's APIs. All of the options and their requirements will be discussed here.

- Get Tokens (`get_tokens`)
    - **description**: the option downloads data about all storage tokens present in the project
    - **table(s)**: `tokens`
    - **requirements**: this option requires either a management token, a master storage token or a regular storage token with `canManageTokens` permissions
    - **use case**: monitor all tokens in the project to prevent security breaches
- Get Tokens Last Events (`get_tokens_last_events`)
    - **description**: the option downloads data about last one event for each of the tokens in the project
    - **table(s)**: `tokens-last-events`
    - **requirements**: the option requires the `get_tokens` option to be active, otherwise no events will be downloaded
    - **use case**: determine inactive tokens and remove them
- Get Orchestrations (`get_orchestrations`)
    - **description**: the option downloads information about all orchestrations in the project, along with orchestration tasks and notifications
    - **table(s)**: `orchestrations`, `orchestrations-tasks` and `orchestrations-notifications`
    - **requirements**: none
    - **use case**: find orchestrations without error notifications set up, find orchestrations with inactive tasks
- Get Orchestration Triggers (`get_triggers`)
    - **description**: downloads data about event triggers for event triggered orchestrations
    - **table(s)**: `triggers` and `triggers-tables`
    - **requirements**: none
    - **use case**: determine tables which are necessary to run your event triggered orchestrations
- Get Waiting Jobs (`get_waiting_jobs`)
    - **description**: downloads all currently waiting jobs in the project
    - **table(s)**: `waiting-jobs`
    - **requirements**: none
    - **use case**: monitor all waiting or processing jobs at any given time
- Get Tables (`get_tables`)
    - **description**: downloads information about all tables in storage as well as their metadata (descriptions, last updates, etc.)
    - **table(s)**: `tables` and `tables-metadata`
    - **requirements**: a storage token with no limited access to storage
    - **use case**: find tables without descriptions, find tables which were not updated in a certain period of time
- Get Columns (`get_columns`)
    - **description**: downloads information about all of the columns for each table
    - **table(s)**: `tables-columns` and `tables-columns-metadata`
    - **requirements**: `get_tables` option must be checked
    - **use case**: find columns without descriptions, find dropped columns with snapshotting
- Get All Configurations (`get_all_configurations`)
    - **description**: downloads information about all configurations present in the project
    - **table(s)** `configurations`
    - **requirements**: a storage token with unlimited access to all components
    - **use case**: find all configurations without descriptions, find all configurations that don't follow naming conventions
- Get Transformations and Their Details (`get_transformations`)
    - **description**: downloads information about transformations, input, outputs and queries used
    - **table(s)**: `transformations`, `transformations-buckets`, `transformations-inputs`, `transformations-inputs-metadata`, `transformations-outputs` and `transformations-queries`
    - **requirements**: a storage token with access to transformations
    - **use case**: analyze SQL queries for extra inputs, find transformations which are processing same output with full load
- Get Project Users (`get_project_users`)
    - **description**: downloads data about users and their access to projects
    - **table(s)**: `project-users`
    - **requirements**: a management access token
    - **use case**: monitor users' access to projects
- Get Organization Users (`get_organization_users`)
    - **description**: downloads data about users belonging to the specified organization
    - **table(s)**: `organization-users`
    - **requirements**: a management token
    - **use case**: monitor users' access to organization
- Get Tables Load Events (`get_table_load_events`)
    - **description**: downloads all import/export events of table. First load will backfill all available (6month back), each consecutive will bring in new ones.
    - **table(s)**: `table-load-events`
    - **requirements**:  a storage token with unlimited access to all buckets
    - **use case**: monitor table activity
- Get Schedules (`get_schedules`)
    - **description**: downloads data of all scheduling of components and orchestrations
    - **table(s)**: `schedules`
    - **requirements**:  a storage token with unlimited access to all components
    - **use case**: monitor scheduling of orchestration and components
- Get Notifications (`get_notifications`)
    - **description**: downloads all notifications for all components in QueueV2
    - **table(s)**: `notifications`
    - **requirements**: a storage token with read access to all components
    - **use case**: monitor notification

## Development

```
docker-compose build dev
docker-compose run --rm dev
```


================================================
FILE: change_log.md
================================================
**1.0.0**
Published version

**0.0.3**
Added support for obtaining transformation details.

**0.0.2**
Added option to download tables and configurations.
Fixed UI.

**0.0.1**
First testing version of the metadata extractor.


================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.8.11-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential
RUN pip install flake8
RUN pip install -r /code/requirements.txt
WORKDIR /code/

CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    venv/
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: requirements.txt
================================================
dataclasses
dateparser
mock
freezegun
keboola.component==1.3.6
keboola.http-client==1.0.0


================================================
FILE: component_config/component_long_description.md
================================================
**⚠ NOTE** This extractor is being decommissioned and replaced by the official [Telemetry Extractor](https://components.keboola.com/components/keboola.ex-telemetry-data) which provides the same data and is actively maintained.


The Keboola metadata extractor downloads information from Keboola's APIs about various objects, users, etc.

The extractor utilizes folllowing Keboola APIs:

1. [Storage API](https://keboola.docs.apiary.io/#),
2. [Management API](https://keboolamanagementapi.docs.apiary.io/#),
3. [Orchestrator API](https://keboolaorchestratorv2api.docs.apiary.io/#),
4. [Queue API](https://app.swaggerhub.com/apis-docs/keboola/job-queue-api/1.0.0#/);

and allows to download information about:

1. tokens and their last events,
2. orchestrations,
3. waiting jobs,
4. tables and columns,
5. configurations,
6. transformations,
7. project and organization users.

A sample of the configuration can be found in the [component repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-kbc-project-metadata-v2/src/master/component_config/sample-config/).


================================================
FILE: component_config/component_short_description.md
================================================
**⚠ NOTE** This extractor is being decommissioned and replaced by the official [Telemetry Extractor](https://components.keboola.com/components/keboola.ex-telemetry-data)!


================================================
FILE: component_config/configSchema.json
================================================
{
    "type": "object",
    "title": "App configuration",
    "required": [
        "tokens",
        "master_token",
        "datasets",
        "incremental_load"
    ],
    "properties": {
        "tokens": {
            "type": "array",
            "propertyOrder": 200,
            "title": "Storage Tokens",
            "description": "A list of Storage tokens from projects for which the metadata will be downloaded. <a href=\"https://help.keboola.com/management/project/tokens/\">Follow our guide</a> to create or manage storage tokens.",
            "items": {
                "format": "grid",
                "type": "object",
                "title": "Storage Token",
                "required": [
                    "#key",
                    "region"
                ],
                "properties": {
                    "#key": {
                        "type": "string",
                        "format": "password",
                        "title": "Storage Token",
                        "propertyOrder": 1000
                    },
                    "region": {
                        "enum": [
                            "eu-central-1",
                            "us-east-1",
                            "current"
                        ],
                        "options": {
                            "enum_titles": [
                                "EU",
                                "US",
                                "CURRENT_STACK"
                            ]
                        },
                        "type": "string",
                        "title": "Project Region",
                        "default": "EU",
                        "propertyOrder": 2000
                    }
                }
            }
        },
        "master_token": {
            "type": "array",
            "propertyOrder": 100,
            "title": "Management Token",
            "description": "If defined, using this management token, metadata  will be downloaded for all projects in the organization defined. Note that this overrides any tokens explicitly specified in the <strong>Storage Tokens</strong> section.<br>You can create a management token <a href=\"https://help.keboola.com/management/account/#tokens\">following the guide on our help page</a>.",
            "maxItems": 1,
            "items": {
                "format": "grid",
                "type": "object",
                "title": "Access Token",
                "required": [
                    "#token",
                    "org_id",
                    "region"
                ],
                "properties": {
                    "#token": {
                        "type": "string",
                        "format": "password",
                        "title": "Master Access Token",
                        "propertyOrder": 1000
                    },
                    "org_id": {
                        "type": "string",
                        "title": "Organization ID",
                        "propertyOrder": 1200
                    },
                    "region": {
                        "enum": [
                            "eu-central-1",
                            "us-east-1",
                            "current"
                        ],
                        "options": {
                            "enum_titles": [
                                "EU",
                                "US",
                                "CURRENT_STACK"
                            ]
                        },
                        "type": "string",
                        "title": "Org. Region",
                        "default": "EU",
                        "propertyOrder": 2000
                    }
                }
            }
        },
        "datasets": {
            "type": "object",
            "propertyOrder": 300,
            "required": [
                "get_tokens",
                "get_tokens_last_events",
                "get_orchestrations",
                "get_triggers",
                "get_waiting_jobs",
                "get_tables",
                "get_columns",
                "get_all_configurations",
                "get_transformations",
                "get_project_users",
                "get_organization_users",
                "get_workspace_load_events"
            ],
            "title": "Project data to download",
            "properties": {
                "get_tokens": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Tokens",
                    "propertyOrder": 2000,
                    "description": "Downloads information about all tokens in projects.</br>This option requires a management token, a master storage token or a general storage token with <strong><i>canManageTokens</i></strong> permissions."
                },
                "get_tokens_last_events": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Tokens Last Events",
                    "propertyOrder": 2100,
                    "description": "Downloads last event for each token.<br>This options requires <strong>Get Tokens</strong> option to be enabled."
                },
                "get_orchestrations": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Orchestrations",
                    "propertyOrder": 2500,
                    "description": "Downloads orchestrations, their tasks and notifications."
                },
                 "get_orchestrations_v2": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Orchestrations V2",
                    "propertyOrder": 2600,
                    "description": "Downloads orchestrations V2, their tasks and phases."
                },
                "get_triggers": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Orchestration Triggers",
                    "propertyOrder": 2750,
                    "description": "Downloads triggers and tables for event triggered orchestrations."
                },
                "get_waiting_jobs": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Waiting Jobs",
                    "propertyOrder": 3000,
                    "description": "Downloads all waiting and processing jobs."
                },
                "get_tables": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Tables",
                    "propertyOrder": 3500,
                    "description": "Downloads data and metadata about tables in storage."
                },
                "get_storage_buckets": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Storage Buckets",
                    "propertyOrder": 3600,
                    "description": "Downloads data and metadata about buckets in storage."
                },
                "get_columns": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Columns",
                    "propertyOrder": 3750,
                    "description": "Downloads data and metadata about columns in storage.<br>This option requires <strong>Get Tables</strong> option to be enabled."
                },
                "get_all_configurations": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get All Configurations",
                    "propertyOrder": 4000,
                    "description": "Downloads all configurations in a project."
                },
                "get_transformations": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Legacy Transformations and Their Details",
                    "description": "Downloads data to the transformation level, including input and output mappings, and queries.",
                    "propertyOrder": 4500
                },
                "get_workspace_load_events": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Load Events from Storage",
                    "description": "Downloads data about loading the tables from storage into transformation workspaces.",
                    "propertyOrder": 4700
                },
                "get_tables_load_events": {
                    "type": "boolean",
                    "format": "checkbox",
                    "required": true,
                    "title": "Get Tables Load Events",
                    "description": "Downloads information about all imports and exports of tables to workspaces, transformations or CSVs.",
                    "propertyOrder": 4750
                },
                "get_schedules": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Schedules",
                    "description": "Downloads information about all scheduling of components and orchestrations",
                    "propertyOrder": 4770
                },
                "get_transformations_v2": {
                    "type": "boolean",
                    "required": true,
                    "format": "checkbox",
                    "title": "Get Transformations v2 and Their Details",
                    "description": "Downloads data about new transformations, including mappings and codes.",
                    "propertyOrder": 4800
                },
                "get_project_users": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Project Users",
                    "propertyOrder": 5000,
                    "description": "Downloads information about project users.<br>The option requires a management access token."
                },
                "get_organization_users": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Organization Users",
                    "propertyOrder": 5500,
                    "description": "Downloads information about organization users.<br>The option requires a management access token."
                },
                "get_notifications": {
                    "type": "boolean",
                    "format": "checkbox",
                    "title": "Get Notifications",
                    "propertyOrder": 5600,
                    "description": "Downloads information about all notifications set on components in QueueV2"
                }
            }
        },
        "incremental_load": {
            "propertyOrder": 400,
            "type": "number",
            "enum": [
                1,
                0
            ],
            "title": "Load Type",
            "default": 1,
            "options": {
                "enum_titles": [
                    "Incremental Load",
                    "Full Load"
                ]
            }
        }
    }
}


================================================
FILE: component_config/configuration_description.md
================================================
**⚠ NOTE** This extractor is being decommissioned and replaced by the official [Telemetry Extractor](https://components.keboola.com/components/keboola.ex-telemetry-data) which provides the same data and is actively maintained.

To configure the extractor, either [a management token](https://help.keboola.com/management/account/#tokens) is needed or an [array of storage tokens](https://help.keboola.com/management/project/tokens/) for all projects, for which metadata should be downloaded. Additionally, you'll need to be able to specify the ID of your organization and region, where your projects are located.

A sample of the configuration can be found in the [component repository](https://bitbucket.org/kds_consulting_team/kds-team.ex-kbc-project-metadata-v2/src/master/component_config/sample-config/).

### Token specification

The application accepts either one of the two options:

1. one management token,
2. any number of storage tokens.

If both options are specified, management token is prioritized and storage tokens are disregarded.

#### Authenticating with management token

Specifying management token allows to download data for all projects in the specified organization. The user, whose access token is used in the configuration, **must be part of the organization**, for which the data should be downloaded. If user is not part of the organization, the extractor will not be able to access the organization data and hence won't be able to download necessary metadata.

The management token automatically accesses all projects within the organization and creates temporary storage tokens to download the necessary data. These tokens can be distinguished by their name, which follows `[PROJECT_NAME]_Telemetry_token` naming convention. All of the automatically created tokens have an expiration of 26 hours and are **re-used by the extractor**, if the extractor is ran multiple times a day.

Along with a management token, an ID of the organization must be provided as well as region, where the organization is located. The ID of the organization can be found in the URL of the organization page - follow our [help page article](https://help.keboola.com/management/organization/) on how to access it - e.g. in URL [https://connection.keboola.com/admin/organizations/1234](https://connection.keboola.com/admin/organizations/1234), `1234` is the ID of the organization.

#### Authenticating with storage tokens

Storage tokens provide direct access to the project, in which they were created. Extractor uses these tokens to directly download data from within the project. It's therefore crucial, that these tokens **do not have any limited functionality** (such as restricted to only some tables or components), otherwise the tokens will not be able to download all the necessary data.

### Available data

The metadata extractor allows to download an extended set of objects from the Keboola's APIs. All of the options and their requirements will be discussed here.

- Get Tokens (`get_tokens`)
    - **description**: the option downloads data about all storage tokens present in the project
    - **table(s)**: `tokens`
    - **requirements**: this option requires either a management token, a master storage token or a regular storage token with `canManageTokens` permissions
    - **use case**: monitor all tokens in the project to prevent security breaches
- Get Tokens Last Events (`get_tokens_last_events`)
    - **description**: the option downloads data about last one event for each of the tokens in the project
    - **table(s)**: `tokens-last-events`
    - **requirements**: the option requires the `get_tokens` option to be active, otherwise no events will be downloaded
    - **use case**: determine inactive tokens and remove them
- Get Orchestrations (`get_orchestrations`)
    - **description**: the option downloads information about all orchestrations in the project, along with orchestration tasks and notifications
    - **table(s)**: `orchestrations`, `orchestrations-tasks` and `orchestrations-notifications`
    - **requirements**: none
    - **use case**: find orchestrations without error notifications set up, find orchestrations with inactive tasks
- Get Orchestration Triggers (`get_triggers`)
    - **description**: downloads data about event triggers for event triggered orchestrations
    - **table(s)**: `triggers` and `triggers-tables`
    - **requirements**: none
    - **use case**: determine tables which are necessary to run your event triggered orchestrations
- Get Waiting Jobs (`get_waiting_jobs`)
    - **description**: downloads all currently waiting jobs in the project
    - **table(s)**: `waiting-jobs`
    - **requirements**: none
    - **use case**: monitor all waiting or processing jobs at any given time
- Get Tables (`get_tables`)
    - **description**: downloads information about all tables in storage as well as their metadata (descriptions, last updates, etc.)
    - **table(s)**: `tables` and `tables-metadata`
    - **requirements**: a storage token with no limited access to storage
    - **use case**: find tables without descriptions, find tables which were not updated in a certain period of time
- Get Columns (`get_columns`)
    - **description**: downloads information about all of the columns for each table
    - **table(s)**: `tables-columns` and `tables-columns-metadata`
    - **requirements**: `get_tables` option must be checked
    - **use case**: find columns without descriptions, find dropped columns with snapshotting
- Get All Configurations (`get_all_configurations`)
    - **description**: downloads information about all configurations present in the project
    - **table(s)** `configurations`
    - **requirements**: a storage token with unlimited access to all components
    - **use case**: find all configurations without descriptions, find all configurations that don't follow naming conventions
- Get Transformations and Their Details (`get_transformations`)
    - **description**: downloads information about transformations, input, outputs and queries used
    - **table(s)**: `transformations`, `transformations-buckets`, `transformations-inputs`, `transformations-inputs-metadata`, `transformations-outputs` and `transformations-queries`
    - **requirements**: a storage token with access to transformations
    - **use case**: analyze SQL queries for extra inputs, find transformations which are processing same output with full load
- Get Project Users (`get_project_users`)
    - **description**: downloads data about users and their access to projects
    - **table(s)**: `project-users`
    - **requirements**: a management access token
    - **use case**: monitor users' access to projects
- Get Organization Users (`get_organization_users`)
    - **description**: downloads data about users belonging to the specified organization
    - **table(s)**: `organization-users`
    - **requirements**: a management token
    - **use case**: monitor users' access to organization
- Get Tables Load Events (`get_table_load_events`)
    - **description**: downloads all import/export events of table. First load will backfill all available (6month back), each consecutive will bring in new ones.
    - **table(s)**: `table-load-events`
    - **requirements**:  a storage token with unlimited access to all buckets
    - **use case**: monitor table activity
- Get Notifications (`get_notifications`)
    - **description**: downloads all notifications for all components in QueueV2
    - **table(s)**: `notifications`
    - **requirements**: a storage token with read access to all components
    - **use case**: monitor notification
- Get Schedules (`get_schedules`)
    - **description**: downloads data of all scheduling of components and orchestrations
    - **table(s)**: `schedules`
    - **requirements**:  a storage token with unlimited access to all components
    - **use case**: monitor scheduling of orchestration and components


================================================
FILE: component_config/documentationUrl.md
================================================



================================================
FILE: component_config/licenseUrl.md
================================================



================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sourceCodeUrl.md
================================================



================================================
FILE: component_config/stack_parameters.json
================================================
{}


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {},
  "parameters": {
    "debug": true,
    "master_token": [
      {
        "#token": "<MASTER_ACCESS_TOKEN>",
        "org_id": "271",
        "region": "us-east-1"
      }
    ],
    "tokens": [
      {
        "region": "us-east-1",
        "#key": "Access token"
      },
      {
        "region": "us-east-1",
        "#key": "<STORAGE_TOKEN_2>"
      },
      {
        "region": "eu-central-1",
        "#key": "<STORAGE_TOKEN_3>"
      }
    ],
    "datasets": {
      "get_transformations": true,
      "get_tokens": true,
      "get_all_configurations": true,
      "get_triggers": true,
      "get_organization_users": true,
      "get_tokens_last_events": true,
      "get_tables": true,
      "get_columns": true,
      "get_waiting_jobs": true,
      "get_project_users": false,
      "get_orchestrations": true
    },
    "incremental_load": 0
  },
  "image_parameters": {},
  "authorization": {}
}


================================================
FILE: component_config/sample-config/in/state.json
================================================
{"us_east_1|5150": {"id": "287853", "#token": "<TOKEN_TO_ACCESS_PROJECT>", "expires": 1590058267}}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
FILE: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi


================================================
FILE: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"


================================================
FILE: src/client.py
================================================
import logging
import sys
from dataclasses import dataclass
from json import JSONDecodeError

import requests
from keboola.http_client import HttpClient

DEFAULT_TOKEN_EXPIRATION = 26 * 60 * 60  # Default token expiration set to 26 hours

KEBOOLA_API_URLS = {
    'syrup': 'https://syrup.{REGION}',
    'queue': 'https://queue.{REGION}',
    'scheduler': 'https://scheduler.{REGION}',
    'storage': 'https://connection.{REGION}/v2/storage',
    'notification': 'https://notification.{REGION}',
    'management': 'https://connection.{REGION}/manage',
}


@dataclass
class SAPIParameters:
    token: str
    region: str
    project: str


@dataclass
class ManAPIParameters:
    token: str
    region: str
    organization: str


def response_splitter(response: requests.models.Response):
    if not isinstance(response, requests.models.Response):
        raise TypeError(f"Expecting type requests.models.Response, received type {type(response)}")

    else:
        try:
            return response.status_code, response.json()
        except JSONDecodeError as e:
            logging.error(f"Cannot parse response : {response.text} from: {response.request.path_url}")
            raise e


class StorageClient(HttpClient):
    LIMIT = 100

    def __init__(self, region: str, token: str, project: str):

        _default_header = {'x-storageapi-token': token}
        _url = KEBOOLA_API_URLS['storage'].format(REGION=region)

        logging.debug(f"Storage URL set to: {_url}")

        super().__init__(base_url=_url, default_http_header=_default_header)
        self.parameters = SAPIParameters(token, region, project)

    def verify_storage_token(self) -> bool:

        rsp_verify = self.get_raw('tokens/verify')
        sc_verify, js_verify = response_splitter(rsp_verify)

        if sc_verify == 200:
            return True

        elif sc_verify in (400, 401):
            return False

        else:
            logging.exception(f"Could not verify storage token validity.\nReceived: {sc_verify} - {js_verify}.")
            sys.exit(1)

    def get_tokens(self) -> list:

        rsp_tokens = self.get_raw('tokens')
        sc_tokens, js_tokens = response_splitter(rsp_tokens)

        if sc_tokens == 200:
            return js_tokens

        else:
            logging.error(f"Could not download tokens for project {self.parameters.project} for stack "
                          f"{self.parameters.region}.\nReceived: {sc_tokens} - {js_tokens}.")
            sys.exit(1)

    def get_component_configurations(self, component_id: str) -> list:

        rsp_configs = self.get_raw(f'components/{component_id}/configs')
        sc_configs, js_configs = response_splitter(rsp_configs)

        if sc_configs == 200:
            return js_configs
        else:
            logging.error(f"Could not download configurations of component {component_id} for project "
                          f"{self.parameters.project} in"
                          f"stack {self.parameters.region}.\nReceived: {sc_configs} - {js_configs}.")
            sys.exit(1)

    def get_all_configurations(self) -> list:

        par_configs = {'include': 'configuration,rows'}

        rsp_configs = self.get_raw('components', params=par_configs)
        sc_configs, js_configs = response_splitter(rsp_configs)

        if sc_configs == 200:
            return js_configs

        else:
            logging.error(f"Could not download configurations for project {self.parameters.project} in"
                          f"stack {self.parameters.region}.\nReceived: {sc_configs} - {js_configs}.")
            sys.exit(1)

    def get_orchestrations(self) -> list:

        return self.get_component_configurations('orchestrator')

    def get_transformations_v1(self) -> list:

        return self.get_component_configurations('transformation')

    def get_storage_buckets(self) -> list:

        rsp_buckets = self.get_raw('buckets')
        sc_buckets, js_buckets = response_splitter(rsp_buckets)

        if sc_buckets == 200:
            return js_buckets

        else:
            logging.error(f"Could not download storage buckets for project {self.parameters.project} in stack "
                          f"{self.parameters.region}.\nReceived: {sc_buckets} - {js_buckets}.")
            sys.exit(1)

    def get_all_tables(self, include: bool = True) -> list:

        if include:
            par_tables = {'include': 'metadata,buckets,columns,columnMetadata'}
        else:
            par_tables = {}

        rsp_tables = self.get_raw('tables', params=par_tables)
        sc_tables, js_tables = response_splitter(rsp_tables)

        if sc_tables == 200:
            return js_tables

        else:
            logging.error(f"Could not download storage buckets for project {self.parameters.project} in stack "
                          f"{self.parameters.region}.\nReceived: {sc_tables} - {js_tables}.")
            sys.exit(1)

    def get_triggers(self) -> list:

        rsp_triggers = self.get_raw('triggers')
        sc_triggers, js_triggers = response_splitter(rsp_triggers)

        if sc_triggers == 200:
            return js_triggers

        else:
            logging.error(f"Could not download triggers for project {self.parameters.project} "
                          f"in stack {self.parameters.region}.\nReceived: {sc_triggers} - {js_triggers}.")
            sys.exit(1)

    def get_tokens_last_events(self, token_id: str) -> list:

        par_events = {'limit': 1, 'forceUuid': 'true'}

        rsp_events = self.get_raw(f'tokens/{token_id}/events', params=par_events)
        sc_events, js_events = response_splitter(rsp_events)

        if sc_events == 200:
            return self._ensure_event_id(js_events)

        else:
            logging.error(f"Could not download last token event for token {token_id} in project "
                          f"{self.parameters.project} in stack {self.parameters.region}.\n"
                          f"Received: {sc_events} - {js_events}.")
            return []

    def get_workspace_load_events(self, **kwargs):

        kwargs['component'] = 'storage'
        kwargs['q'] = 'event:storage.workspaceLoaded'

        return self._get_paged_events('events', **kwargs)

    def get_table_load_events(self, table_id: str, date: str, **kwargs):

        TABLE_LOAD_EVENTS = ['storage.tableExported', 'storage.tableImportError', 'storage.tableImportStarted',
                             'storage.tableImportDone', 'storage.workspaceLoaded', 'storage.workspaceTableCloned']

        kwargs['component'] = 'storage'
        kwargs['q'] = f"({' OR '.join([f'event:{e}' for e in TABLE_LOAD_EVENTS])}) AND created:>={date}"

        return self._get_paged_events(f'tables/{table_id}/events', **kwargs)

    @staticmethod
    def _ensure_event_id(events: list) -> list:
        """Ensure each event in the list has an 'id' field, using 'uuid' if necessary."""
        for event in events:
            if 'id' not in event or event['id'] is None:
                event['id'] = event['uuid']
        return events

    def _get_paged_events(self, url: str, **kwargs):

        par_events = kwargs
        par_events['limit'] = 1000
        # Always force UUID usage
        par_events['forceUuid'] = 'true'

        is_complete = False
        all_events = []
        max_id = None  # uses uuid

        while not is_complete:
            par_events['offset'] = 0
            if max_id:
                par_events.pop('offset', None)
                par_events['maxId'] = max_id

            rsp_events = self.get_raw(url, params=par_events)
            sc_events, js_events = response_splitter(rsp_events)

            if sc_events == 200:
                all_events += js_events
                if js_events and max_id != js_events[-1:][0]['uuid']:
                    max_id = js_events[-1:][0]['uuid']

                if not js_events or max_id == js_events[-1:][0]['uuid']:
                    is_complete = True
                    return self._ensure_event_id(all_events)

            else:
                logging.error(f"Could not download events for url {url} in project {self.parameters.project} "
                              f"in stack {self.parameters.region}.\nReceived: {sc_events} - {js_events}.")
                sys.exit(1)
        return None


class SyrupClient(HttpClient):
    LIMIT = 1000

    def __init__(self, region: str, token: str, project: str):

        _default_header = {'x-storageapi-token': token}
        _url = KEBOOLA_API_URLS['syrup'].format(REGION=region)

        logging.debug(f"Syrup URL set to: {_url}")

        super().__init__(base_url=_url, default_http_header=_default_header, status_forcelist=(500, 502, 504),
                         max_retries=2)
        self.parameters = SAPIParameters(token, region, project)

    def get_waiting_and_processing_jobs(self) -> list:

        par_jobs = {'q': 'status:waiting OR status:processing'}
        return self._get_paged_jobs(**par_jobs)

    def get_transformation_jobs(self, last_job_id: str = None, **kwargs) -> list:

        q = '(component:transformation OR params.component:transformation)'
        # ' AND -(status:processing OR status:waiting OR status:terminating)'

        if last_job_id is not None:
            q += f' AND id:>{last_job_id}'
            logging.debug(f"Downloading transformations jobs since last job id {last_job_id}.")
        else:
            q += ' AND createdTime:>now-7d'
            logging.debug("Downloading transformations jobs created in the last 7 days.")

        kwargs['q'] = q

        return self._get_paged_jobs(**kwargs)

    def _get_paged_jobs(self, **kwargs) -> list:

        par_jobs = kwargs
        par_jobs['limit'] = self.LIMIT

        offset = 0
        is_complete = False
        all_jobs = []

        while is_complete is False:
            par_jobs['offset'] = offset

            rsp_jobs = self.get_raw('queue/jobs', params=par_jobs)
            sc_jobs, js_jobs = response_splitter(rsp_jobs)

            if sc_jobs == 200:
                all_jobs += js_jobs

                if len(js_jobs) < self.LIMIT:
                    is_complete = True
                    return all_jobs

                else:
                    offset += self.LIMIT

            else:
                logging.error(f"Could not download jobs for project {self.parameters.project} in stack "
                              f"{self.parameters.region}.\nReceived: {sc_jobs} - {js_jobs}.")
                sys.exit(1)

    def get_orchestrations(self) -> list:

        rsp_orch = self.get_raw('orchestrator/orchestrations')
        sc_orch, js_orch = response_splitter(rsp_orch)

        if sc_orch == 200:
            return js_orch

        else:
            logging.error(f"Could not list orchestrations for project {self.parameters.project} in stack "
                          f"{self.parameters.region}.\nReceived: {sc_orch} - {js_orch}.")
            sys.exit(1)

    def get_orchestration_tasks(self, orchestration_id: str) -> list:

        rsp_tasks = self.get_raw(f'orchestrator/orchestrations/{orchestration_id}/tasks')
        sc_tasks, js_tasks = response_splitter(rsp_tasks)

        if sc_tasks == 200:
            return js_tasks

        else:
            logging.error(f"Could not list orchestration tasks for orchestration {orchestration_id} in "
                          f"project {self.parameters.project} in stack {self.parameters.region}.\n"
                          f"Received: {sc_tasks} - {js_tasks}.")
            sys.exit(1)


class NotificationClient(HttpClient):
    LIMIT = 1000

    def __init__(self, region: str, token: str, project: str):

        _default_header = {'x-storageapi-token': token}
        _url = KEBOOLA_API_URLS['notification'].format(REGION=region)

        logging.debug(f"Notification URL set to: {_url}")

        super().__init__(base_url=_url, default_http_header=_default_header)
        self.parameters = SAPIParameters(token, region, project)

    def get_notifications(self, **kwargs) -> list:
        return self._get_paged_notification(**kwargs)

    def _get_paged_notification(self, **kwargs) -> list:

        par_notifications = kwargs
        par_notifications['limit'] = self.LIMIT

        offset = 0
        is_complete = False
        all_notifications = []

        while is_complete is False:
            par_notifications['offset'] = offset

            rsp_notifications = self.get_raw('project-subscriptions', params=par_notifications)
            sc_notifications, js_notifications = response_splitter(rsp_notifications)

            if sc_notifications == 200:
                all_notifications += js_notifications

                if len(js_notifications) < self.LIMIT:
                    is_complete = True
                    return all_notifications

                else:
                    offset += self.LIMIT

            else:
                logging.error(f"Could not download subscriptions for project {self.parameters.project} in stack "
                              f"{self.parameters.region}.\nReceived: {all_notifications} - {js_notifications}.")
                sys.exit(1)


class QueueClient(HttpClient):
    LIMIT = 1000

    def __init__(self, region: str, token: str, project: str):

        _default_header = {'x-storageapi-token': token}
        _url = KEBOOLA_API_URLS['queue'].format(REGION=region)

        logging.debug(f"Queue URL set to: {_url}")

        super().__init__(base_url=_url, default_http_header=_default_header)
        self.parameters = SAPIParameters(token, region, project)

    def get_waiting_and_processing_jobs(self) -> list:

        par_jobs = {'q': 'status:waiting OR status:processing'}
        return self._get_paged_jobs(**par_jobs)

    def get_transformation_jobs(self, last_job_id: str = None, **kwargs) -> list:

        q = '(component:transformation OR params.component:transformation)'
        # ' AND -(status:processing OR status:waiting OR status:terminating)'

        if last_job_id is not None:
            q += f' AND id:>{last_job_id}'
            logging.debug(f"Downloading transformations jobs since last job id {last_job_id}.")
        else:
            q += ' AND createdTime:>now-7d'
            logging.debug("Downloading transformations jobs created in the last 7 days.")

        kwargs['q'] = q

        return self._get_paged_jobs(**kwargs)

    def _get_paged_jobs(self, **kwargs) -> list:

        par_jobs = kwargs
        par_jobs['limit'] = self.LIMIT

        offset = 0
        is_complete = False
        all_jobs = []

        while is_complete is False:
            par_jobs['offset'] = offset

            rsp_jobs = self.get_raw('jobs', params=par_jobs)
            sc_jobs, js_jobs = response_splitter(rsp_jobs)

            if sc_jobs == 200:
                all_jobs += js_jobs

                if len(js_jobs) < self.LIMIT:
                    is_complete = True
                    return all_jobs

                else:
                    offset += self.LIMIT

            else:
                logging.error(f"Could not download jobs for project {self.parameters.project} in stack "
                              f"{self.parameters.region}.\nReceived: {sc_jobs} - {js_jobs}.")
                sys.exit(1)


class ManagementClient(HttpClient):

    def __init__(self, region: str, token: str, organization: str):

        _default_header = {'X-KBC-ManageApiToken': token}
        _url = KEBOOLA_API_URLS['management'].format(REGION=region)

        super().__init__(base_url=_url, default_http_header=_default_header)
        self.parameters = ManAPIParameters(token, region, organization)
        self.verify_token()

    def verify_token(self) -> None:

        rsp_verify = self.get_raw('tokens/verify')
        sc_verify, js_verify = response_splitter(rsp_verify)

        if sc_verify == 200:
            return

        else:
            logging.error(f"Provided management token could not be verified.\nReceived: {sc_verify} - {js_verify}.")
            sys.exit(1)

    def get_organization(self) -> dict:

        rsp_org = self.get_raw(f'organizations/{self.parameters.organization}')
        sc_org, js_org = response_splitter(rsp_org)

        if sc_org == 200:
            return js_org

        elif sc_org == 403:
            logging.error(f"User does not have access to organization {self.parameters.organization} ",
                          f"in stack {self.parameters.region}.")
            sys.exit(1)

        else:
            logging.error(f"Could not access organization {self.parameters.organization} in stack "
                          f"{self.parameters.region}.\nReceived: {sc_org} - {js_org}.")
            sys.exit(1)

    def get_project_users(self, project_id: str) -> list:

        rsp_users = self.get_raw(f'projects/{project_id}/users')
        sc_users, js_users = response_splitter(rsp_users)

        if sc_users == 200:
            return js_users

        else:
            logging.error(f"Could not download users for project {project_id} in stack {self.parameters.region}.\n"
                          f"Received: {sc_users} - {js_users}.")
            sys.exit(1)

    def create_storage_token(self, project_id: str, description: str,
                             expiration: int = DEFAULT_TOKEN_EXPIRATION) -> dict:

        hdr_token = {
            'content-type': 'application/json',
            'accept': 'application/json'
        }

        data_token = {
            'description': description,
            'expiresIn': expiration,
            'canManageBuckets': True,
            'canReadAllFileUploads': False,
            'canPurgeTrash': False,
            'canManageTokens': True,
            'componentAccess': ['componentAccess'],
            'bucketPermissions': {"*": "read"}
        }

        rsp_token = self.post_raw(f'projects/{project_id}/tokens', headers=hdr_token, json=data_token)
        sc_token, js_token = response_splitter(rsp_token)

        if sc_token == 201:
            return js_token

        else:
            logging.error(f"Unable to create storage token in project {project_id} in stack {self.parameters.region}."
                          f"\nReceived: {sc_token} - {js_token}.")
            sys.exit(1)

    def get_organization_users(self) -> list:

        rsp_org_users = self.get_raw(f'organizations/{self.parameters.organization}/users')
        sc_org_users, js_org_users = response_splitter(rsp_org_users)

        if sc_org_users == 200:
            return js_org_users

        else:
            logging.error(f"Could not download organization users for organization {self.parameters.organization} "
                          f"in stack {self.parameters.region}.\nReceived: {sc_org_users} - {js_org_users}.")
            sys.exit(1)


class SchedulerClient(HttpClient):
    LIMIT = 1000

    def __init__(self, region: str, token: str, project: str):

        _default_header = {'x-storageapi-token': token}
        _url = KEBOOLA_API_URLS['scheduler'].format(REGION=region)

        logging.debug(f"Scheduler URL set to: {_url}")

        super().__init__(base_url=_url, default_http_header=_default_header)
        self.parameters = SAPIParameters(token, region, project)

    def get_schedules(self, **kwargs) -> list:
        return self._get_paged_schedules(**kwargs)

    def _get_paged_schedules(self, **kwargs) -> list:

        par_schedules = kwargs
        par_schedules['limit'] = self.LIMIT

        offset = 0
        is_complete = False
        all_jobs = []

        while is_complete is False:
            par_schedules['offset'] = offset

            rsp_schedules = self.get_raw('schedules', params=par_schedules)
            sc_schedules, js_schedules = response_splitter(rsp_schedules)

            if sc_schedules == 200:
                all_jobs += js_schedules

                if len(js_schedules) < self.LIMIT:
                    is_complete = True
                    return all_jobs

                else:
                    offset += self.LIMIT

            else:
                logging.error(f"Could not download jobs for project {self.parameters.project} in stack "
                              f"{self.parameters.region}.\nReceived: {sc_schedules} - {js_schedules}.")
                sys.exit(1)


class Client:

    def __init__(self):
        self.management = None
        self.syrup = None
        self.storage = None
        self.notification = None
        self.queue = None
        self.schedule = None

    def init_storage_and_syrup_clients(self, region, token, project):
        self.storage = StorageClient(region, token, project)
        self.syrup = SyrupClient(region, token, project)
        self.notification = NotificationClient(region, token, project)
        self.queue = QueueClient(region, token, project)
        self.schedule = SchedulerClient(region, token, project)

    def init_management_client(self, region, token, organization):
        self.management = ManagementClient(region, token, organization)



================================================
FILE: src/component.py
================================================
import csv
import logging
import sys
import time
from contextlib import nullcontext
from dataclasses import dataclass
from hashlib import md5
from pathlib import Path

import dateparser
import dateutil.parser
import requests
from keboola.component import CommonInterface

from client import Client, StorageClient
from parser import FlattenJsonParser
from result import Writer
from table_definitions import *  # noqa

# Key for current stack selection
STORAGE_BUCKET_COLUMNS = ["project_id", "region", "uri",
                          "id",
                          "name",
                          "displayName",
                          "stage",
                          "description",
                          "tables",
                          "created",
                          "lastChangeDate",
                          "isReadOnly",
                          "dataSizeBytes",
                          "rowsCount",
                          "isMaintenance",
                          "backend",
                          "sharing",
                          "directAccessEnabled",
                          "directAccessSchemaName",
                          "sourceBucket__id",
                          "sourceBucket__name",
                          "sourceBucket__displayName",
                          "sourceBucket__stage",
                          "sourceBucket__description",
                          "sourceBucket__sharing",
                          "sourceBucket__created",
                          "sourceBucket__lastChangeDate",
                          "sourceBucket__dataSizeBytes",
                          "sourceBucket__rowsCount",
                          "sourceBucket__backend",
                          "sharingParameters",
                          "sharedBy__id",
                          "sharedBy__name",
                          "sharedBy__date",
                          "attributes"]
KEY_CURRENT = 'current'

APP_VERSION = '2.0.3'
TOKEN_SUFFIX = '_Telemetry_token'
TOKEN_EXPIRATION_CUSHION = 30 * 60  # 30 minutes

KEY_TOKENS = 'tokens'
KEY_MASTERTOKEN = 'master_token'
KEY_DATASETS = 'datasets'
KEY_INCREMENTAL = 'incremental_load'

MANDATORY_PARAMS = [[KEY_TOKENS, KEY_MASTERTOKEN], KEY_DATASETS]

KEY_GET_ALL_CONFIGURATIONS = 'get_all_configurations'
KEY_GET_TOKENS = 'get_tokens'
KEY_GET_TOKENS_LAST_EVENTS = 'get_tokens_last_events'
KEY_GET_ORCHESTRATIONS = 'get_orchestrations'
KEY_GET_ORCHESTRATIONS_V2 = 'get_orchestrations_v2'
KEY_GET_WAITING_JOBS = 'get_waiting_jobs'
KEY_GET_TABLES = 'get_tables'
KEY_GET_TRANSFORMATIONS = 'get_transformations'
KEY_GET_TRANSFORMATIONS_V2 = 'get_transformations_v2'
KEY_GET_PROJECT_USERS = 'get_project_users'
KEY_GET_ORGANIZATION_USERS = 'get_organization_users'
KEY_GET_TRIGGERS = 'get_triggers'
KEY_GET_COLUMNS = 'get_columns'
KEY_GET_WORKSPACE_LOAD_EVENTS = 'get_workspace_load_events'
KEY_GET_TABLES_LOAD_EVENTS = 'get_tables_load_events'
KEY_GET_SCHEDULES = "get_schedules"
KEY_GET_NOTIFICATIONS = 'get_notifications'

# Token keys
KEY_MAN_TOKEN = '#token'
KEY_SAP_TOKEN = '#key'
KEY_REGION = 'region'
KEY_ORGANIZATION_ID = 'org_id'

TR_V2_CMP_ID = ['keboola.snowflake-transformation', 'keboola.python-transformation-v2',
                'keboola.synapse-transformation', 'keboola.redshift-transformation',
                'keboola.r-transformation-v2', 'keboola.openrefine-transformation',
                'keboola.oracle-transformation', 'keboola.csas-python-transformation-v2',
                'keboola.databricks-transformation', 'keboola.exasol-transformation',
                'keboola.python-mlflow-transformation']

STORAGE_ENDPOINTS = [KEY_GET_ALL_CONFIGURATIONS, KEY_GET_TOKENS, KEY_GET_ORCHESTRATIONS, KEY_GET_WAITING_JOBS,
                     KEY_GET_TABLES, KEY_GET_TRANSFORMATIONS, KEY_GET_TRIGGERS, KEY_GET_WORKSPACE_LOAD_EVENTS,
                     KEY_GET_TRANSFORMATIONS_V2, KEY_GET_TABLES_LOAD_EVENTS, KEY_GET_ORCHESTRATIONS_V2]
MANAGEMENT_ENDPOINTS = [KEY_GET_PROJECT_USERS, KEY_GET_ORGANIZATION_USERS]


class ComponentWriters:
    pass


@dataclass
class Parameters:
    tokens: list
    master_token: list
    datasets: list
    incremental: bool
    current_stack: str


@dataclass
class ManagementToken:
    token: str
    organization_id: str
    region: str


class Component(CommonInterface):

    def __init__(self):

        super().__init__(log_level=logging.INFO)
        self.validate_configuration_parameters(MANDATORY_PARAMS)

        logging.info(f"Running component version {APP_VERSION}...")

        _par = self.configuration.parameters

        if _par.get('debug', False) is True:
            sys.tracebacklimit = 3
            self.set_default_logger(logging.DEBUG)

        self.parameters = Parameters(_par.get(KEY_TOKENS, []), _par.get(KEY_MASTERTOKEN, []),
                                     _par[KEY_DATASETS], bool(_par.get(KEY_INCREMENTAL, False)),
                                     self.environment_variables.stack_id)

        self.client = Client()
        self.writers = ComponentWriters

        self.parameters.client_to_use = self.determine_token()
        self.check_token_permissions()
        # self.createWriters()

        state = self.get_state_file()
        if state is None:
            state = {}

        self.previous_tokens = state.get('tokens', {})
        if isinstance(self.previous_tokens, list):
            self.previous_tokens = {}
        self.new_tokens = {}

        self.last_processed_transformations = state.get('tr_last_processed_id', {})
        if isinstance(self.last_processed_transformations, list):
            self.last_processed_transformations = {}

        self.latest_date = state.get('date', dateparser.parse("7 months ago").strftime("%Y-%m-%d"))
        self.table_definitions = {}

        logging.debug(f"Using {self.parameters.client_to_use} token.")

    def check_token_permissions(self):

        if self.parameters.client_to_use == 'management':
            return

        else:
            config_bool = [self.parameters.datasets.get(k, False) for k in MANAGEMENT_ENDPOINTS]
            if any(config_bool) is True:
                logging.error(f"Management token required to obtain the following options: {MANAGEMENT_ENDPOINTS}.")
                sys.exit(1)

            else:
                pass

    def determine_token(self):

        if len_master := len(self.parameters.master_token) != 0:

            if len_master > 1:
                logging.warning("More than 1 master token specified. Only first will be used.")

            _master_token = self.parameters.master_token[0]

            if ('region' not in _master_token or 'org_id' not in _master_token or '#token' not in _master_token):
                logging.exception("Missing mandatory fields from master token specification.")
                sys.exit(1)

            elif (_master_token['region'].strip() == '' or _master_token['#token'].strip() == ''
                  or _master_token['org_id'].strip() == ''):
                logging.error("Missing parameter specification in master token.")
                sys.exit(1)

            else:
                return 'management'

        elif len(self.parameters.tokens) != 0:

            for token in self.parameters.tokens:

                if 'region' not in token or '#key' not in token:
                    logging.exception("Missing mandatory fields for storage token specification.")
                    sys.exit(1)

                elif token['region'].strip() == '' or token['#key'].strip() == '':
                    logging.error("Missing parameters specification for one of the storage tokens.")
                    sys.exit(1)

                else:
                    return 'storage'

        else:
            logging.exception("Neither master, nor storage token specified.")
            sys.exit(1)

    @staticmethod
    def _get_object_from_list(object_list: list, search_key: str, search_key_value) -> dict:

        _eval_list = [obj[search_key] == search_key_value for obj in object_list]
        _idx = _eval_list.index(True)

        return object_list[_idx]

    @staticmethod
    def convert_iso_format_to_epoch_timestamp(iso_dt_string: str) -> int:

        if iso_dt_string == '' or iso_dt_string is None:
            return None

        else:
            return int(dateutil.parser.parse(iso_dt_string).timestamp())

    @staticmethod
    def is_token_in_treshold(token_expiration: int) -> bool:
        if token_expiration is None:
            return True

        else:
            current = int(time.time())
            expiration = token_expiration - current

            if expiration >= TOKEN_EXPIRATION_CUSHION:
                return True
            else:
                return False

    def is_token_valid(self, token: str, token_expiration: int, region: str, project: str) -> bool:

        is_token_expired = self.is_token_in_treshold(token_expiration)
        is_token_valid = StorageClient(region=region, project=project, token=token).verify_storage_token()

        if all([is_token_valid, is_token_expired]):
            return True
        else:
            return False

    def determine_stack(self, region: str):

        if region == 'us-east-1':
            return 'keboola.com'
        elif region == 'eu-central-1':
            return 'eu-central-1.keboola.com'
        elif region == KEY_CURRENT:
            return self.parameters.current_stack.replace('connection.', '')
        else:
            return region

    def build_table_definition(self, table_name: str):

        if table_name in self.table_definitions:
            return self.table_definitions[table_name]

        else:
            raw_cols = eval(f'FIELDS_{table_name.upper().replace("-", "_")}')
            kbc_cols = eval(f'FIELDS_R_{table_name.upper().replace("-", "_")}')
            json_cols = eval(f'JSON_{table_name.upper().replace("-", "_")}')
            pk = eval(f'PK_{table_name.upper().replace("-", "_")}')

            tdf = self.create_out_table_definition(name=table_name, primary_key=pk, columns=kbc_cols,
                                                   incremental=self.parameters.incremental)
            tdf.writer_columns = raw_cols
            tdf.json_columns = json_cols

            self.table_definitions[table_name] = tdf

            return tdf

    def download_organization_data(self, project_ids: list):

        if self.parameters.datasets.get(KEY_GET_ORGANIZATION_USERS):
            logging.info(f"Downloading organization users for organization {self.management_token.organization_id} in "
                         f"stack {self.parameters.region}.")
            _org_pdict = {
                'organization_id': self.management_token.organization_id,
                'region': self.parameters.region
            }
            _org_users_tdf = self.build_table_definition('organization-users')

            with Writer(_org_users_tdf) as wrt:
                org_users = self.client.management.get_organization_users()
                wrt.write_rows(org_users, parent_dict=_org_pdict)

        if self.parameters.datasets.get(KEY_GET_PROJECT_USERS):

            logging.info(f"Downloading project users for organization {self.management_token.organization_id} in "
                         f"stack {self.parameters.region}.")

            _prj_users_tdf = self.build_table_definition('project-users')

            with Writer(_prj_users_tdf) as wrt:
                for prj_id in project_ids:
                    _pdict = {'project_id': prj_id, 'region': self.parameters.region}
                    users = self.client.management.get_project_users(prj_id)
                    wrt.write_rows(users, _pdict)

    def get_waiting_jobs(self, parent_dict: dict):

        _waiting_jobs_tdf = self.build_table_definition('waiting-jobs')
        try:
            jobs = self.client.syrup.get_waiting_and_processing_jobs()
        except requests.exceptions.ConnectionError:
            jobs = self.client.queue.get_waiting_and_processing_jobs()

        with Writer(_waiting_jobs_tdf) as wrt:
            wrt.write_rows(jobs, parent_dict)

    def get_tokens_and_events(self, parent_dict: dict):

        _tokens_tdf = self.build_table_definition('tokens')
        tokens = self.client.storage.get_tokens()

        with Writer(_tokens_tdf) as wrt:
            wrt.write_rows(tokens, parent_dict)

        if self.parameters.datasets.get(KEY_GET_TOKENS_LAST_EVENTS):

            _tokens_le_tdf = self.build_table_definition('tokens-last-events')
            with Writer(_tokens_le_tdf) as wrt:
                for token in tokens:
                    token_id = token['id']
                    _last_event = self.client.storage.get_tokens_last_events(token_id)

                    if _last_event != []:
                        wrt.write_rows(_last_event, {**parent_dict, **{'token_id': token_id}})

    def get_all_configurations(self, parent_dict: dict):

        _all_configs_tdf = self.build_table_definition('configurations')
        configs = self.client.storage.get_all_configurations()

        with Writer(_all_configs_tdf) as wrt:
            for component in configs:
                comp = {}
                comp['component_id'] = component['id']
                comp['component_type'] = component['type']
                comp['component_name'] = component['name']

                comp = {**comp, **parent_dict}

                wrt.write_rows(component['configurations'], comp)

    def get_tables(self, parent_dict: dict):

        tables = self.client.storage.get_all_tables()

        _tables_tdf = self.build_table_definition('tables')
        _tables_md_tdf = self.build_table_definition('tables-metadata')
        wrt_tables = Writer(_tables_tdf)
        wrt_tables_md = Writer(_tables_md_tdf)

        write_column_data = self.parameters.datasets.get(KEY_GET_COLUMNS)

        if write_column_data:
            _columns_tdf = self.build_table_definition('tables-columns')
            _columns_md_tdf = self.build_table_definition('tables-columns-metadata')
            wrt_columns = Writer(_columns_tdf)
            wrt_columns_md = Writer(_columns_md_tdf)
        else:
            wrt_columns = nullcontext()
            wrt_columns_md = nullcontext()

        with wrt_tables, wrt_tables_md, wrt_columns, wrt_columns_md:

            for t in tables:
                t['primaryKey'] = ','.join(t['primaryKey'])
                cfg = {}
                cfg['table_id'] = t['id']
                cfg = {**cfg, **parent_dict}

                wrt_tables_md.write_rows(t['metadata'], parent_dict=cfg)

                if write_column_data:

                    _cols = [{'column': col} for col in t['columns']]
                    wrt_columns.write_rows(_cols, parent_dict=cfg)

                    for col in t['columnMetadata']:
                        col_cfg = {**cfg, **{'column': col}}
                        wrt_columns_md.write_rows(t['columnMetadata'][col], col_cfg)

            wrt_tables.write_rows(tables, parent_dict)

    def get_buckets(self, parent_dict: dict):

        buckets = self.client.storage.get_storage_buckets()
        res_table = self.create_out_table_definition('storage_buckets.csv', primary_key=['id', 'project_id', 'region'],
                                                     columns=STORAGE_BUCKET_COLUMNS)
        self.table_definitions[res_table.name] = res_table
        parser = FlattenJsonParser(child_separator='__', keys_to_ignore=['tables', 'project'], flatten_lists=False)

        if Path(res_table.full_path).exists():
            mode = 'a'
        else:
            mode = 'w+'

        with open(res_table.full_path, mode) as out:
            writer = csv.DictWriter(out, fieldnames=STORAGE_BUCKET_COLUMNS, extrasaction='ignore')
            for t in buckets:
                res = {**t, **parent_dict}
                writer.writerow(parser.parse_row(res))

    def get_schedules(self, parent_dict: dict):
        _table_events_tdf = self.build_table_definition('schedules')
        wrt = Writer(_table_events_tdf)

        parser = FlattenJsonParser(child_separator='__', flatten_lists=False)

        with wrt:
            schedules = self.client.schedule.get_schedules()
            for schedule in schedules:
                parsed_data = parser.parse_row(schedule)
                res = {**parsed_data, **parent_dict}
                wrt.write_row(parser.parse_row(res))

    def get_orchestrations_v2(self, parent_dict: dict):

        _orchestrations_tdf = self.build_table_definition('orchestrations_v2')
        _orchestrations_phases_tdf = self.build_table_definition('orchestrations_v2_phases')
        _orchestrations_tasks_tdf = self.build_table_definition('orchestrations_v2_tasks')
        orchestrations = self.client.storage.get_component_configurations("keboola.orchestrator")

        with Writer(_orchestrations_tdf) as orch_wrt, \
                Writer(_orchestrations_phases_tdf) as phase_wrt, \
                Writer(_orchestrations_tasks_tdf) as task_wrt:
            for orchestration in orchestrations:
                orchestration_config = {
                    'id': orchestration.get("id"),
                    'region': parent_dict.get("region"),
                    'project_id': parent_dict.get("project_id"),
                    'name': orchestration.get("name"),
                    'description': orchestration.get("description"),
                    'createdTime': orchestration.get("created"),
                    'token_id': orchestration.get("creatorToken", {}).get("id"),
                    'token_description': orchestration.get("creatorToken", {}).get("description"),
                    'version': orchestration.get("version"),
                    'isDisabled': orchestration.get("isDisabled"),
                    'isDeleted': orchestration.get("isDeleted")
                }
                orch_wrt.write_row(orchestration_config)
                for idx, phase in enumerate(orchestration.get("configuration").get("phases", [])):
                    phase_wrt.write_row({"id": phase.get("id"),
                                         'region': parent_dict.get("region"),
                                         'project_id': parent_dict.get("project_id"),
                                         "orchestration_id": orchestration.get("id"),
                                         "name": phase.get("name"),
                                         "dependsOn": phase.get("dependsOn"),
                                         "phase_index": idx})
                for task in orchestration.get("configuration").get("tasks", []):
                    task_wrt.write_row({"id": task.get("id"),
                                        'region': parent_dict.get("region"),
                                        'project_id': parent_dict.get("project_id"),
                                        "orchestration_id": orchestration.get("id"),
                                        "name": task.get("name"),
                                        "phase": task.get("phase"),
                                        "component_id": task.get("task").get("componentId"),
                                        "config_id": task.get("task").get("configId"),
                                        "mode": task.get("task").get("mode"),
                                        "continueOnFailure": task.get("continueOnFailure"),
                                        "enabled": task.get("enabled")})

    def get_orchestrations(self, parent_dict: dict):

        _orch_tdf = self.build_table_definition('orchestrations')
        try:
            orchestrations = self.client.syrup.get_orchestrations()
        except requests.exceptions.ConnectionError:
            logging.exception("Orchestrations are not available in the project you are extracting metadata from, "
                              "extract the Orchestrations V2 instead.")
            sys.exit(1)
        orchestrations_sapi = self.client.storage.get_orchestrations()

        with Writer(_orch_tdf) as wrt:
            wrt.write_rows(orchestrations, parent_dict)

        _orch_notif_tdf = self.build_table_definition('orchestrations-notifications')
        _orch_tasks_tdf = self.build_table_definition('orchestrations-tasks')

        with Writer(_orch_notif_tdf) as wrt_notif, Writer(_orch_tasks_tdf) as wrt_tasks:
            for orch in orchestrations:
                orch_id = orch['id']
                _orch_pdict = {**{'orchestration_id': orch_id}, **parent_dict}

                wrt_notif.write_rows(orch['notifications'], _orch_pdict)

                sapi_config = self._get_object_from_list(orchestrations_sapi, 'id', str(orch_id))
                orch_tasks = sapi_config['configuration']['tasks']

                for idx, task in enumerate(orch_tasks):
                    wrt_tasks.write_row({**task, **{'api_index': idx}}, _orch_pdict)

    def get_triggers(self, parent_dict: dict):

        _triggers_tdf = self.build_table_definition('triggers')
        _triggers_tables_tdf = self.build_table_definition('triggers-tables')
        triggers = self.client.storage.get_triggers()

        wrt_triggers = Writer(_triggers_tdf)
        wrt_triggers_tables = Writer(_triggers_tables_tdf)

        with wrt_triggers, wrt_triggers_tables:
            wrt_triggers.write_rows(triggers, parent_dict)

            for trigger in triggers:
                _trigg_pdict = {**{'trigger_id': trigger['id']}, **parent_dict}
                wrt_triggers_tables.write_rows(trigger.get('tables', []), _trigg_pdict)

    def get_workspace_load_events(self, parent_dict: dict, project_key: str):

        _ws_load_events_tdf = self.build_table_definition('workspace-table-loads')
        last_processed_job_id = self.last_processed_transformations.get(project_key)
        try:
            transformation_jobs = self.client.syrup.get_transformation_jobs(last_processed_job_id)
        except requests.exceptions.ConnectionError:
            transformation_jobs = self.client.queue.get_transformation_jobs(last_processed_job_id)
        transformation_jobs.reverse()
        encountered_processing = False

        with Writer(_ws_load_events_tdf) as wrt:
            for job in transformation_jobs:
                if job['status'] in ('processing', 'waiting', 'terminating'):
                    encountered_processing = True

                if encountered_processing is False:
                    last_processed_job_id = job['id']

                run_id = job['runId']
                storage_events = self.client.storage.get_workspace_load_events(runId=run_id)
                wrt.write_rows(storage_events, parent_dict)

        self.last_processed_transformations[project_key] = last_processed_job_id

    def get_transformations_v1(self, parent_dict: dict):

        _tr_tdf = self.build_table_definition('transformations')
        _tr_buckets_tdf = self.build_table_definition('transformations-buckets')
        _tr_inputs_tdf = self.build_table_definition('transformations-inputs')
        _tr_inputs_md_tdf = self.build_table_definition('transformations-inputs-metadata')
        _tr_outputs_tdf = self.build_table_definition('transformations-outputs')
        _tr_queries_tdf = self.build_table_definition('transformations-queries')

        buckets = self.client.storage.get_transformations_v1()

        with Writer(_tr_buckets_tdf) as wrt_buckets:
            wrt_buckets.write_rows(buckets, parent_dict)

        wrt_tr = Writer(_tr_tdf)
        wrt_inputs = Writer(_tr_inputs_tdf)
        wrt_inputs_md = Writer(_tr_inputs_md_tdf)
        wrt_outputs = Writer(_tr_outputs_tdf)
        wrt_queries = Writer(_tr_queries_tdf)

        with wrt_tr, wrt_inputs, wrt_inputs_md, wrt_outputs, wrt_queries:

            for bucket in buckets:
                _bucket = {}
                _bucket['bucket_id'] = bucket['id']
                _bucket_parent = {**_bucket, **parent_dict}

                for transformation in bucket['rows']:
                    if transformation['configuration'].get('backend') == 'mysql':
                        continue

                    transformation['configuration']['packages'] = ','.join(transformation['configuration']
                                                                           .get('packages', []))
                    transformation['configuration']['requires'] = ','.join(transformation['configuration']
                                                                           .get('requires', []))

                    _tr_hash = md5('|'.join([transformation['id'], bucket['id']]).encode()).hexdigest()
                    transformation['id_md5'] = _tr_hash
                    wrt_tr.write_row(transformation, _bucket_parent)

                    _tr_parent = {**{'transformation_id': _tr_hash}, **parent_dict}

                    for table_input in transformation['configuration'].get('input', []):
                        table_input['columns'] = ','.join(table_input.get('columns', []))
                        table_input['whereValues'] = ','.join([str(x) for x in table_input.get('whereValues', [])])
                        table_input['loadType'] = table_input.get('loadType', 'copy')

                        wrt_inputs.write_row(table_input, _tr_parent)

                        if transformation['configuration'].get('backend') != 'redshift':
                            for column in table_input.get('datatypes', []):
                                _dt = table_input['datatypes'][column]
                                if _dt is None:
                                    continue
                                _metadata = {**_dt, **{'source': table_input['source'],
                                                       'destination': table_input['destination']}}

                                wrt_inputs_md.write_row(_metadata, _tr_parent)

                    for table_output in transformation['configuration'].get('output', []):
                        table_output['primaryKey'] = ','.join(table_output.get('primaryKey', []))
                        table_output['incremental'] = table_output.get('incremental', False)
                        table_output['deleteWhereValues'] = ','.join(table_output.get('deleteWhereValues', []))

                        wrt_outputs.write_row(table_output, _tr_parent)

                    _queries = [{'query_index': idx, 'query': q} for idx, q in
                                enumerate(transformation['configuration'].get('queries', []))]
                    wrt_queries.write_rows(_queries, _tr_parent)

    def get_transformations_v2(self, parent_dict: dict):

        _tr_tdf = self.build_table_definition('transformations-v2')
        _tr_inputs_tdf = self.build_table_definition('transformations-v2-inputs')
        _tr_inputs_md_tdf = self.build_table_definition('transformations-v2-inputs-metadata')
        _tr_outputs_tdf = self.build_table_definition('transformations-v2-outputs')
        _tr_codes_tdf = self.build_table_definition('transformations-v2-codes')
        wrt_tr = Writer(_tr_tdf)
        wrt_tr_inputs = Writer(_tr_inputs_tdf)
        wrt_tr_inputs_md = Writer(_tr_inputs_md_tdf)
        wrt_tr_outputs = Writer(_tr_outputs_tdf)
        wrt_tr_codes = Writer(_tr_codes_tdf)

        with wrt_tr, wrt_tr_inputs, wrt_tr_inputs_md, wrt_tr_outputs, wrt_tr_codes:

            for tr_cmp_id in TR_V2_CMP_ID:
                tr_configs = self.client.storage.get_component_configurations(tr_cmp_id)

                _cmp_pdict = {**{'component_id': tr_cmp_id}, **parent_dict}

                for tr in tr_configs:
                    tr_id = tr['id']
                    tr['packages'] = ','.join(tr['configuration'].get('parameters', {}).get('packages', []))
                    tr['variables_id'] = tr['configuration'].get('variables_id', '')
                    tr['variables_values_id'] = tr['configuration'].get('variables_values_id', '')

                    wrt_tr.write_row(tr, _cmp_pdict)

                    _tr_pdict = {**{'transformation_id': tr_id}, **_cmp_pdict}
                    io = tr['configuration'].get('storage', {})

                    for ti in io.get('input', {}).get('tables', []):
                        wrt_tr_inputs.write_row(ti, _tr_pdict)

                        source = ti.get('source') or ti.get('source_search')
                        _ti_pdict = {**{'table_source': source,
                                        'table_destination': ti.get('destination', '')},
                                     **_tr_pdict}

                        wrt_tr_inputs_md.write_rows(ti.get('column_types', []), _ti_pdict)

                    for to in io.get('output', {}).get('tables', []):
                        wrt_tr_outputs.write_row(to, _tr_pdict)

                    code_blocks = tr['configuration'].get('parameters', {}).get('blocks', [])

                    for cb_idx, cb in enumerate(code_blocks):
                        _cb_parent = {**{'block_name': cb['name'], 'block_index': cb_idx}, **_tr_pdict}

                        for c_idx, c in enumerate(cb.get('codes', [])):
                            _c_parent = {**_cb_parent, **{'code_name': c['name'], 'code_index': c_idx}}

                            for sc_idx, sc in enumerate(c.get('script', [])):
                                wrt_tr_codes.write_row({'script': sc, 'script_index': sc_idx}, _c_parent)

    def get_table_load_events(self, parent_dict: dict):

        _table_events_tdf = self.build_table_definition('tables-load-events')
        wrt = Writer(_table_events_tdf)

        table_ids = [t['id'] for t in self.client.storage.get_all_tables(include=False)]

        with wrt:
            for table in table_ids:
                load_events = self.client.storage.get_table_load_events(table, self.latest_date)
                wrt.write_rows(load_events, parent_dict)

    def get_notifications(self, parent_dict: dict):
        _notifications_tdf = self.build_table_definition('notifications')
        wrt = Writer(_notifications_tdf)

        parser = FlattenJsonParser(child_separator='__', flatten_lists=False)

        with wrt:
            notifications = self.client.notification.get_notifications()
            for notification in notifications:
                parsed_data = parser.parse_row(notification)
                component_id_dict = self._get_component_id_from_notification(parsed_data)
                config_id_dict = self._get_configuration_id_from_notification(parsed_data)
                phase_id_dict = self._get_configuration_id_from_notification(parsed_data)
                res = {**parsed_data, **parent_dict, **component_id_dict, **config_id_dict, **phase_id_dict}
                wrt.write_row(parser.parse_row(res))

    @staticmethod
    def _get_component_id_from_notification(notification_data: dict) -> dict:
        component_id = None
        for notification_filter in notification_data.get("filters"):
            if notification_filter.get("field") == "job.component.id":
                component_id = notification_filter.get("value")
        return {"component_id": component_id}

    @staticmethod
    def _get_configuration_id_from_notification(notification_data: dict) -> dict:
        configuration_id = None
        for notification_filter in notification_data.get("filters"):
            if notification_filter.get("field") == "job.configuration.id":
                configuration_id = notification_filter.get("value")
        return {"configuration_id": configuration_id}

    @staticmethod
    def _get_phase_id_id_from_notification(notification_data: dict) -> dict:
        phase_id = None
        for notification_filter in notification_data.get("filters"):
            if notification_filter.get("field") == "phase.id":
                phase_id = notification_filter.get("value")
        return {"phase_id": phase_id}

    def get_project_data(self, project_id: str, project_token: str, project_key: str):

        self.client.init_storage_and_syrup_clients(self.parameters.region, project_token, project_id)
        _p_dict = {'region': self.parameters.region, 'project_id': project_id}

        if self.parameters.datasets.get(KEY_GET_ORCHESTRATIONS):
            logging.info("Fetching metadata of Orchestrations")
            self.get_orchestrations(_p_dict)

        if self.parameters.datasets.get(KEY_GET_WAITING_JOBS):
            logging.info("Fetching metadata of waiting jobs")
            self.get_waiting_jobs(_p_dict)

        if self.parameters.datasets.get(KEY_GET_TOKENS):
            logging.info("Fetching metadata of Tokens")
            self.get_tokens_and_events(_p_dict)

        if self.parameters.datasets.get(KEY_GET_ALL_CONFIGURATIONS):
            logging.info("Fetching metadata of All Configurations")
            self.get_all_configurations(_p_dict)

        if self.parameters.datasets.get(KEY_GET_TABLES):
            logging.info("Fetching metadata of Tables")
            self.get_tables(_p_dict)

        if self.parameters.datasets.get(KEY_GET_ORCHESTRATIONS_V2):
            logging.info("Fetching metadata of Orchestrations V2")
            self.get_orchestrations_v2(_p_dict)

        if self.parameters.datasets.get(KEY_GET_TRIGGERS):
            logging.info("Fetching metadata of Triggers")
            self.get_triggers(_p_dict)

        if self.parameters.datasets.get(KEY_GET_WORKSPACE_LOAD_EVENTS):
            logging.info("Fetching metadata of Workspace Load Events")
            self.get_workspace_load_events(_p_dict, project_key)

        if self.parameters.datasets.get(KEY_GET_TRANSFORMATIONS):
            logging.info("Fetching metadata of Transformations")
            self.get_transformations_v1(_p_dict)

        if self.parameters.datasets.get(KEY_GET_TRANSFORMATIONS_V2):
            logging.info("Fetching metadata of Transformations V2")
            self.get_transformations_v2(_p_dict)

        if self.parameters.datasets.get(KEY_GET_TABLES_LOAD_EVENTS):
            logging.info("Fetching metadata of Table Load Events")
            self.get_table_load_events(_p_dict)

        if self.parameters.datasets.get(KEY_GET_NOTIFICATIONS):
            logging.info("Fetching metadata of Notifications")
            self.get_notifications(_p_dict)

        if self.parameters.datasets.get('get_storage_buckets'):
            logging.info("Fetching metadata of Storage Buckets")
            self.get_buckets(_p_dict)

        if self.parameters.datasets.get(KEY_GET_SCHEDULES):
            logging.info("Fetching schedules of configurations")
            self.get_schedules(_p_dict)

    def run(self):

        if self.parameters.client_to_use == 'management':

            _man_token = self.parameters.master_token[0]
            self.management_token = ManagementToken(_man_token[KEY_MAN_TOKEN], _man_token[KEY_ORGANIZATION_ID],
                                                    _man_token[KEY_REGION])

            self.parameters.region = self.determine_stack(self.management_token.region)

            self.client.init_management_client(self.parameters.region, self.management_token.token,
                                               self.management_token.organization_id)

            all_projects = self.client.management.get_organization()['projects']
            all_project_ids = [prj['id'] for prj in all_projects]

            self.download_organization_data(all_project_ids)

            storage_data_bool = [self.parameters.datasets.get(key, False) for key in STORAGE_ENDPOINTS]
            if any(storage_data_bool):

                for prj in all_projects:

                    prj_id = str(prj['id'])
                    prj_name = prj['name']
                    prj_region = self.parameters.region
                    prj_token_description = prj_name + TOKEN_SUFFIX
                    prj_token_key = '|'.join([prj_region.replace('-', '_'), prj_id])

                    prj_token_old = self.previous_tokens.get(prj_token_key)

                    if not prj_token_old:
                        logging.debug(f"Creating new storage token for project {prj_id} in stack {prj_region}.")
                        prj_token_new = self.client.management.create_storage_token(prj_id, prj_token_description)
                        prj_token = {
                            'id': prj_token_new['id'],
                            '#token': prj_token_new['token'],
                            'expires': self.convert_iso_format_to_epoch_timestamp(prj_token_new['expires'])
                        }

                    else:

                        valid = self.is_token_valid(prj_token_old['#token'], prj_token_old['expires'],
                                                    prj_region, prj_id)

                        if valid:
                            logging.debug(f"Using token {prj_token_old['id']} from state for project {prj_id} in "
                                          f"stack {prj_region}.")
                            prj_token = prj_token_old

                        else:
                            logging.debug(f"Creating new storage token for project {prj_id} in stack {prj_region}.")
                            prj_token_new = self.client.management.create_storage_token(prj_id, prj_token_description)

                            prj_token = {
                                'id': prj_token_new['id'],
                                '#token': prj_token_new['token'],
                                'expires': self.convert_iso_format_to_epoch_timestamp(prj_token_new['expires'])
                            }

                    logging.info(f"Downloading data for project {prj_name} in stack {prj_region}.")
                    self.get_project_data(prj_id, prj_token['#token'], prj_token_key)
                    self.new_tokens[prj_token_key] = prj_token

        else:
            for idx, prj in enumerate(self.parameters.tokens):

                prj_token = prj[KEY_SAP_TOKEN]
                self.parameters.region = self.determine_stack(prj[KEY_REGION])
                prj_id = prj_token.split('-')[0]
                prj_token_key = '|'.join([self.parameters.region.replace('-', '_'), prj_id])

                if prj_token.strip() == '':
                    logging.error(f"Token as position {idx} is empty. Skipping.")
                    continue

                logging.info(f"Downloading data for project {prj_id} in stack {self.parameters.region}.")
                self.get_project_data(prj_id, prj_token, prj_token_key)

        new_state = {
            'tokens': self.new_tokens,
            'tr_last_processed_id': self.last_processed_transformations,
            'date': dateparser.parse('today').strftime('%Y-%m-%d')
        }

        self.write_state_file(new_state)
        self.write_manifests(self.table_definitions.values())


if __name__ == '__main__':
    m = Component()
    m.run()



================================================
FILE: src/parser.py
================================================
class FlattenJsonParser:
    def __init__(self, child_separator: str = '_', exclude_fields=None, flatten_lists=False, keys_to_ignore=None):
        self.child_separator = child_separator
        self.exclude_fields = exclude_fields
        self.flatten_lists = flatten_lists
        self.keys_to_ignore = keys_to_ignore
        if self.keys_to_ignore is None:
            self.keys_to_ignore = set()

    def parse_data(self, data):
        for i, row in enumerate(data):
            data[i] = self._flatten_row(row)
        return data

    def parse_row(self, row: dict):
        return self._flatten_row(row)

    @staticmethod
    def _construct_key(parent_key, separator, child_key):
        if parent_key:
            return "".join([parent_key, separator, child_key])
        else:
            return child_key

    def _flatten_row(self, nested_dict):
        if len(nested_dict) == 0:
            return {}

        flattened_dict = dict()

        def _flatten(dict_object, key_name=None, name_with_parent=''):
            if isinstance(dict_object, dict):
                for key in dict_object:
                    if key not in self.keys_to_ignore:
                        new_parent_name = self._construct_key(name_with_parent, self.child_separator, key)
                        _flatten(dict_object[key], key_name=key, name_with_parent=new_parent_name)
                    else:
                        flattened_dict[key] = dict_object[key]
            elif isinstance(dict_object, (list, set, tuple)):
                if self.flatten_lists:
                    for index, item in enumerate(dict_object):
                        new_key_name = self._construct_key(name_with_parent, self.child_separator, str(index))
                        _flatten(item, key_name=new_key_name)
                else:
                    flattened_dict[name_with_parent] = dict_object
            else:
                flattened_dict[name_with_parent] = dict_object

        _flatten(nested_dict, None)
        return flattened_dict



================================================
FILE: src/result.py
================================================
import csv
import json

from keboola.component.dao import TableDefinition


class Writer:

    def __init__(self, table_definition: TableDefinition):

        self.tdf = table_definition

    def create_manifest(self):

        template = {
            'incremental': self.tdf.incremental,
            'primary_key': self.tdf.primary_key,
            'columns': self.tdf.columns
        }

        path = self.tdf.full_path + '.manifest'

        with open(path, 'w') as manifest:
            json.dump(template, manifest)

    def __enter__(self):
        self.io = open(self.tdf.full_path, 'a')
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.io.close()

    def create_writer(self):
        self.writer = csv.DictWriter(self.io, fieldnames=self.tdf.writer_columns,
                                     restval='', extrasaction='ignore', quotechar='\"', quoting=csv.QUOTE_ALL)

    def write_row(self, row, parent_dict=None):

        if hasattr(self, 'writer') is False:
            self.create_writer()

        save_aside = {}
        for field in self.tdf.json_columns:
            if field not in row:
                continue
            save_aside[field] = json.dumps(row[field])
            del row[field]

        row_f = {**self.flatten_json(x=row), **save_aside}
        _dict_to_write = {}

        for key, value in row_f.items():

            if key in self.tdf.writer_columns:
                _dict_to_write[key] = value

            else:
                continue

        if parent_dict is not None:
            _dict_to_write = {**_dict_to_write, **parent_dict}

        self.writer.writerow(_dict_to_write)

    def write_rows(self, list_to_write, parent_dict=None):

        for row in list_to_write:
            self.write_row(row, parent_dict)

    def flatten_json(self, x, out=None, name=''):
        if out is None:
            out = dict()

        if type(x) is dict:
            for a in x:
                self.flatten_json(x[a], out, name + a + '_')
        else:
            out[name[:-1]] = x

        return out



================================================
FILE: src/table_definitions.py
================================================
FIELDS_ORCHESTRATIONS = ['id', 'region', 'project_id', 'name', 'crontabRecord', 'crontabTimezone', 'createdTime',
                         'lastScheduledTime', 'nextScheduledTime', 'token_id', 'token_description', 'active',
                         'lastExecutedJob_id', 'lastExecutedJob_status', 'lastExecutedJob_createdTime',
                         'lastExecutedJob_startTime', 'lastExecutedJob_endTime']
FIELDS_R_ORCHESTRATIONS = ['id', 'region', 'project_id', 'name', 'crontab_record', 'crontab_timezone', 'created_time',
                           'last_scheduled_time', 'next_scheduled_time', 'token_id', 'token_description', 'active',
                           'last_executed_job_id', 'last_executed_job_status', 'last_executed_job_created_time',
                           'last_executed_job_start_time', 'last_executed_job_end_time']

PK_ORCHESTRATIONS = ['id', 'region']
JSON_ORCHESTRATIONS = []

FIELDS_ORCHESTRATIONS_TASKS = ['id', 'orchestration_id', 'region', 'component', 'action', 'actionParameters',
                               'timeoutMinutes', 'active', 'continueOnFailure', 'phase', 'api_index']
FIELDS_R_ORCHESTRATIONS_TASKS = ['id', 'orchestration_id', 'region', 'component_id', 'action', 'action_parameters',
                                 'timeout_minutes', 'is_active', 'continue_on_failure', 'phase', 'api_index']
PK_ORCHESTRATIONS_TASKS = ['id', 'region']
JSON_ORCHESTRATIONS_TASKS = ['actionParameters']

FIELDS_ORCHESTRATIONS_NOTIFICATIONS = ['orchestration_id', 'region', 'email', 'channel', 'parameters']
FIELDS_R_ORCHESTRATIONS_NOTIFICATIONS = FIELDS_ORCHESTRATIONS_NOTIFICATIONS
PK_ORCHESTRATIONS_NOTIFICATIONS = ['orchestration_id', 'region', 'email', 'channel']
JSON_ORCHESTRATIONS_NOTIFICATIONS = ['parameters']

FIELDS_ORCHESTRATIONS_V2 = ['id', 'region', 'project_id', 'name', 'description', 'createdTime', 'token_id',
                            'token_description', 'version', 'isDisabled', 'isDeleted']
FIELDS_R_ORCHESTRATIONS_V2 = ['id', 'region', 'project_id', 'name', 'description', 'created_time', 'token_id',
                              'token_description', 'version', 'is_disabled', 'is_deleted']

PK_ORCHESTRATIONS_V2 = ['id', 'region', 'project_id']
JSON_ORCHESTRATIONS_V2 = []

FIELDS_ORCHESTRATIONS_V2_TASKS = ['id', 'orchestration_id', 'region', 'project_id', 'name', 'phase', 'component_id',
                                  'config_id', 'mode', 'continueOnFailure', 'enabled']
FIELDS_R_ORCHESTRATIONS_V2_TASKS = ['id', 'orchestration_id', 'region', 'project_id', 'name', 'phase', 'component_id',
                                    'config_id', 'mode', 'continue_on_failure', 'enabled']
PK_ORCHESTRATIONS_V2_TASKS = ['id', 'orchestration_id', 'region', 'project_id']
JSON_ORCHESTRATIONS_V2_TASKS = []

FIELDS_ORCHESTRATIONS_V2_PHASES = ['id', 'orchestration_id', 'region', 'project_id', 'name', 'dependsOn', 'phase_index']
FIELDS_R_ORCHESTRATIONS_V2_PHASES = ['id', 'orchestration_id', 'region', 'project_id', 'name', 'depends_on',
                                     'phase_index']
PK_ORCHESTRATIONS_V2_PHASES = ['id', 'orchestration_id', 'region', 'project_id']
JSON_ORCHESTRATIONS_V2_PHASES = []

FIELDS_WAITING_JOBS = ['id', 'region', 'runId', 'project_id', 'project_name', 'token_id', 'token_description',
                       'component', 'status', 'createdTime', 'startTime', 'endTime', 'params_config',
                       'params_configBucketId']
FIELDS_R_WAITING_JOBS = ['id', 'region', 'run_id', 'project_id', 'project_name', 'token_id', 'token_description',
                         'component', 'status', 'created_time', 'start_time', 'end_time', 'params_configuration_id',
                         'params_configuration_bucket_id']
PK_WAITING_JOBS = ['id', 'region']
JSON_WAITING_JOBS = []

FIELDS_TOKENS = ['id', 'region', 'project_id', 'created', 'refreshed', 'description', 'isMasterToken',
                 'canManageBuckets', 'canManageTokens', 'canReadAllFileUploads', 'canPurgeTrash', 'expires',
                 'isExpired', 'isDisabled', 'dailyCapacity', 'creatorToken_id', 'creatorToken_description',
                 'admin_id', 'admin_name']
FIELDS_R_TOKENS = ['id', 'region', 'project_id', 'created', 'refreshed', 'description', 'is_master_token',
                   'can_manage_buckets', 'can_manage_tokens', 'can_read_all_file_uploads', 'can_purge_trash', 'expires',
                   'is_expired', 'is_disabled', 'daily_capacity', 'creator_token_id', 'creator_token_description',
                   'admin_id', 'admin_name']
PK_TOKENS = ['id', 'region']
JSON_TOKENS = []

FIELDS_TOKENS_LAST_EVENTS = ['token_id', 'region', 'project_id', 'id', 'event', 'component', 'message',
                             'description', 'type', 'created', 'configurationId', 'objectId', 'objectName',
                             'objectType', 'uri']
FIELDS_R_TOKENS_LAST_EVENTS = ['token_id', 'region', 'project_id', 'event_id', 'event', 'component', 'message',
                               'description', 'type', 'event_created', 'configuration_id', 'object_id',
                               'object_name', 'object_type', 'uri']
PK_TOKENS_LAST_EVENTS = ['token_id', 'region', 'project_id']
JSON_TOKENS_LAST_EVENTS = []

FIELDS_CONFIGURATIONS = ['id', 'region', 'project_id', 'name', 'created', 'creatorToken_id', 'creatorToken_description',
                         'component_id', 'component_name', 'component_type',
                         'version', 'isDeleted', 'currentVersion_created', 'currentVersion_creatorToken_id',
                         'currentVersion_creatorToken_description', 'currentVersion_changeDescription', 'description',
                         'configuration', 'rows']
FIELDS_R_CONFIGURATIONS = ['id', 'region', 'project_id', 'name', 'created', 'creator_token_id',
                           'creator_token_description', 'component_id', 'component_name', 'component_type',
                           'version', 'is_deleted', 'current_version_created',
                           'current_version_creator_token_id', 'current_version_creator_token_description',
                           'current_version_change_description', 'description', 'configuration', 'rows']
PK_CONFIGURATIONS = ['id', 'region']
JSON_CONFIGURATIONS = ['configuration', 'rows']

FIELDS_TABLES = ['id', 'region', 'project_id', 'name', 'primaryKey', 'created', 'lastImportDate', 'lastChangeDate',
                 'rowsCount', 'dataSizeBytes', 'isAlias', 'isAliasable', 'bucket_id', 'bucket_name', 'bucket_stage',
                 'bucket_created', 'bucket_lastChangeDate', 'bucket_isReadOnly', 'bucket_sharing',
                 'bucket_sharedBy_id', 'bucket_sharedBy_name', 'bucket_sharedBy_date', 'sourceTable_id',
                 'sourceTable_project_id']
FIELDS_R_TABLES = ['id', 'region', 'project_id', 'name', 'primary_key', 'created', 'last_import_date',
                   'last_change_date', 'rows_count', 'data_size_bytes', 'is_alias', 'is_aliasable', 'bucket_id',
                   'bucket_name', 'bucket_stage', 'bucket_created', 'bucket_last_change_date', 'bucket_is_read_only',
                   'sharing', 'shared_by_id', 'shared_by_name', 'shared_by_date', 'source_table_id',
                   'source_table_project_id']
PK_TABLES = ['id', 'region', 'project_id']
JSON_TABLES = []

FIELDS_TABLES_METADATA = ['table_id', 'region', 'project_id', 'id', 'key', 'value', 'provider', 'timestamp']
FIELDS_R_TABLES_METADATA = FIELDS_TABLES_METADATA
PK_TABLES_METADATA = ['id', 'table_id', 'region', 'project_id']
JSON_TABLES_METADATA = []

FIELDS_TABLES_COLUMNS = ['table_id', 'region', 'project_id', 'column']
FIELDS_R_TABLES_COLUMNS = FIELDS_TABLES_COLUMNS
PK_TABLES_COLUMNS = ['table_id', 'region', 'project_id', 'column']
JSON_TABLES_COLUMNS = []

FIELDS_TABLES_COLUMNS_METADATA = ['table_id', 'region', 'project_id', 'column', 'id', 'key',
                                  'value', 'provider', 'timestamp']
FIELDS_R_TABLES_COLUMNS_METADATA = FIELDS_TABLES_COLUMNS_METADATA
PK_TABLES_COLUMNS_METADATA = ['id']
JSON_TABLES_COLUMNS_METADATA = []

FIELDS_TRANSFORMATIONS_BUCKETS = ['id', 'region', 'project_id', 'name', 'description', 'version', 'created',
                                  'creatorToken_id', 'creatorToken_description', 'changeDescription',
                                  'currentVersion_created', 'currentVersion_creatorToken_id',
                                  'currentVersion_creatorToken_description']
FIELDS_R_TRANSFORMATIONS_BUCKETS = ['id', 'region', 'project_id', 'name', 'description', 'version', 'created',
                                    'creator_token_id', 'creator_token_description', 'change_description',
                                    'current_version_created', 'current_version_creator_token_id',
                                    'current_version_creator_token_description']
PK_TRANSFORMATIONS_BUCKETS = ['id', 'region', 'project_id']
JSON_TRANSFORMATIONS_BUCKETS = []

FIELDS_TRANSFORMATIONS = ['id_md5', 'id', 'region', 'project_id', 'bucket_id', 'name', 'description',
                          'configuration_packages', 'configuration_requires', 'configuration_backend',
                          'configuration_type', 'configuration_phase', 'configuration_disabled', 'version', 'created',
                          'creatorToken_id', 'creatorToken_description', 'changeDescription']
FIELDS_R_TRANSFORMATIONS = ['id', 'number', 'region', 'project_id', 'bucket_id', 'name', 'description',
                            'packages', 'requires', 'backend', 'type', 'phase', 'disabled', 'version', 'created',
                            'creator_token_id', 'creator_token_description', 'change_description']
PK_TRANSFORMATIONS = ['id', 'region', 'project_id']
JSON_TRANSFORMATIONS = []

FIELDS_TRANSFORMATIONS_INPUTS = ['transformation_id', 'region', 'source', 'destination', 'loadType',
                                 'whereColumn', 'whereValues', 'whereOperator', 'changedSince', 'columns']
FIELDS_R_TRANSFORMATIONS_INPUTS = ['transformation_id', 'region', 'source', 'destination', 'load_type',
                                   'filter_where_column', 'filter_where_values', 'filter_where_operator',
                                   'filter_changed_since', 'input_columns']
PK_TRANSFORMATIONS_INPUTS = ['transformation_id', 'region', 'source', 'destination']
JSON_TRANSFORMATIONS_INPUTS = []

FIELDS_TRANSFORMATIONS_INPUTS_METADATA = ['transformation_id', 'region', 'source', 'destination', 'column', 'type',
                                          'length', 'convertEmptyValuesToNull']
FIELDS_R_TRANSFORMATIONS_INPUTS_METADATA = ['transformation_id', 'region', 'source', 'destination', 'column',
                                            'datatype', 'length', 'convert_empty_values_to_null']
PK_TRANSFORMATIONS_INPUTS_METADATA = ['transformation_id', 'region', 'source', 'destination', 'column']
JSON_TRANSFORMATIONS_INPUTS_METADATA = []

FIELDS_TRANSFORMATIONS_OUTPUTS = ['transformation_id', 'region', 'destination', 'source', 'primaryKey',
                                  'incremental', 'deleteWhereColumn', 'deleteWhereOperator', 'deleteWhereValues']
FIELDS_R_TRANSFORMATIONS_OUTPUTS = ['transformation_id', 'region', 'destination', 'source', 'primary_key',
                                    'incremental_load', 'delete_where_column', 'delete_where_operator',
                                    'delete_where_values']
PK_TRANSFORMATIONS_OUTPUTS = ['transformation_id', 'region', 'destination', 'source']
JSON_TRANSFORMATIONS_OUTPUTS = []

FIELDS_PROJECT_USERS = ['id', 'region', 'project_id', 'name', 'email', 'mfaEnabled', 'canAccessLogs', 'isSuperAdmin',
                        'expires', 'created', 'reason', 'role', 'status', 'invitor_id', 'invitor_name', 'invitor_email',
                        'approver_id', 'approver_name', 'approver_email']
FIELDS_R_PROJECT_USERS = ['id', 'region', 'project_id', 'name', 'email', 'mfa_enabled', 'can_access_logs',
                          'is_super_admin', 'expires', 'created', 'reason', 'role', 'status', 'invitor_id',
                          'invitor_name', 'invitor_email', 'approver_id', 'approver_name', 'approver_email']
PK_PROJECT_USERS = ['id', 'region', 'project_id']
JSON_PROJECT_USERS = []

FIELDS_ORGANIZATION_USERS = ['id', 'region', 'organization_id', 'name', 'email', 'mfaEnabled', 'canAccessLogs',
                             'isSuperAdmin', 'created', 'invitor_id', 'invitor_name', 'invitor_email']
FIELDS_R_ORGANIZATION_USERS = ['id', 'region', 'organization_id', 'name', 'email', 'mfa_enabled', 'can_access_logs',
                               'is_super_admin', 'created', 'invitor_id', 'invitor_name', 'invitor_email']
PK_ORGANIZATION_USERS = ['id', 'region', 'organization_id']
JSON_ORGANIZATION_USERS = []

FIELDS_TRANSFORMATIONS_QUERIES = ['transformation_id', 'region', 'query_index', 'query', 'bucket_id']
FIELDS_R_TRANSFORMATIONS_QUERIES = FIELDS_TRANSFORMATIONS_QUERIES
PK_TRANSFORMATIONS_QUERIES = ['transformation_id', 'region', 'query_index']
JSON_TRANSFORMATIONS_QUERIES = []

FIELDS_TRIGGERS = ['id', 'region', 'project_id', 'runWithTokenId', 'component', 'configurationId', 'lastRun',
                   'creatorToken_id', 'creatorToken_description', 'coolDownPeriodMinutes']
FIELDS_R_TRIGGERS = ['id', 'region', 'project_id', 'run_with_token_id', 'component', 'configuration_id', 'last_run',
                     'creator_token_id', 'creator_token_description', 'cooldown_period_minutes']
PK_TRIGGERS = ['id', 'region', 'project_id']
JSON_TRIGGERS = []

FIELDS_TRIGGERS_TABLES = ['trigger_id', 'region', 'project_id', 'tableId']
FIELDS_R_TRIGGERS_TABLES = ['trigger_id', 'region', 'project_id', 'table_id']
PK_TRIGGERS_TABLES = ['trigger_id', 'region', 'project_id', 'table_id']
JSON_TRIGGERS_TABLES = []

FIELDS_WORKSPACE_TABLE_LOADS = ['id', 'region', 'project_id', 'event', 'component', 'message', 'runId', 'created',
                                'configurationId', 'objectId', 'objectName', 'objectType', 'context', 'params',
                                'results', 'performance', 'token_id', 'token_name', 'uri']
FIELDS_R_WORKSPACE_TABLE_LOADS = ['id', 'region', 'project_id', 'event', 'component', 'message', 'run_id', 'created',
                                  'configuration_id', 'object_id', 'object_name', 'object_type', 'context', 'params',
                                  'results', 'performance', 'token_id', 'token_name', 'uri']
PK_WORKSPACE_TABLE_LOADS = ['id', 'region', 'project_id']
JSON_WORKSPACE_TABLE_LOADS = ['context', 'params', 'results', 'performance']

FIELDS_TRANSFORMATIONS_V2 = ['id', 'region', 'project_id', 'component_id', 'name', 'description', 'version', 'created',
                             'creatorToken_id', 'creatorToken_description', 'changeDescription', 'packages',
                             'variables_id', 'variables_values_id', 'currentVersion_created',
                             'currentVersion_creatorToken_id', 'currentVersion_creatorToken_description']
FIELDS_R_TRANSFORMATIONS_V2 = ['id', 'region', 'project_id', 'component_id', 'name', 'description', 'version',
                               'created', 'creator_token_id', 'creator_token_description', 'change_description',
                               'packages', 'variables_id', 'variables_values_id', 'current_version_created',
                               'current_version_creator_token_id', 'current_version_creator_token_description']
PK_TRANSFORMATIONS_V2 = ['id', 'region', 'project_id', 'component_id']
JSON_TRANSFORMATIONS_V2 = []

FIELDS_TRANSFORMATIONS_V2_INPUTS = ['transformation_id', 'region', 'project_id', 'component_id', 'source',
                                    'destination', 'where_column', 'where_values', 'where_operator', 'columns',
                                    'changed_since']
FIELDS_R_TRANSFORMATIONS_V2_INPUTS = FIELDS_TRANSFORMATIONS_V2_INPUTS
PK_TRANSFORMATIONS_V2_INPUTS = ['transformation_id', 'region', 'project_id', 'component_id', 'source', 'destination']
JSON_TRANSFORMATIONS_V2_INPUTS = ['where_values', 'columns']

FIELDS_TRANSFORMATIONS_V2_INPUTS_METADATA = ['transformation_id', 'region', 'project_id', 'component_id',
                                             'table_source', 'table_destination', 'source', 'type', 'length',
                                             'nullable', 'convert_empty_values_to_null']
FIELDS_R_TRANSFORMATIONS_V2_INPUTS_METADATA = ['transformation_id', 'region', 'project_id', 'component_id',
                                               'table_source', 'table_destination', 'column', 'type', 'length',
                                               'nullable', 'convert_empty_values_to_null']
PK_TRANSFORMATIONS_V2_INPUTS_METADATA = ['transformation_id', 'region', 'project_id', 'component_id',
                                         'table_source', 'table_destination', 'column']
JSON_TRANSFORMATIONS_V2_INPUTS_METADATA = []

FIELDS_TRANSFORMATIONS_V2_OUTPUTS = ['transformation_id', 'region', 'project_id', 'component_id', 'source',
                                     'destination', 'incremental', 'delete_where_column', 'delete_where_operator',
                                     'delete_where_values', 'primary_key', 'where_column', 'where_values',
                                     'where_operator', 'columns', 'changed_since']
FIELDS_R_TRANSFORMATIONS_V2_OUTPUTS = FIELDS_TRANSFORMATIONS_V2_OUTPUTS
PK_TRANSFORMATIONS_V2_OUTPUTS = ['transformation_id', 'region', 'project_id', 'component_id', 'source', 'destination']
JSON_TRANSFORMATIONS_V2_OUTPUTS = ['delete_where_values', 'primary_key']

FIELDS_TRANSFORMATIONS_V2_CODES = ['transformation_id', 'region', 'project_id', 'component_id', 'block_name',
                                   'block_index', 'code_name', 'code_index', 'script', 'script_index']
FIELDS_R_TRANSFORMATIONS_V2_CODES = FIELDS_TRANSFORMATIONS_V2_CODES
PK_TRANSFORMATIONS_V2_CODES = ['transformation_id', 'region', 'project_id', 'component_id', 'block_index',
                               'code_index', 'script_index']
JSON_TRANSFORMATIONS_V2_CODES = []

FIELDS_TABLES_LOAD_EVENTS = FIELDS_WORKSPACE_TABLE_LOADS
FIELDS_R_TABLES_LOAD_EVENTS = FIELDS_R_WORKSPACE_TABLE_LOADS
PK_TABLES_LOAD_EVENTS = PK_WORKSPACE_TABLE_LOADS
JSON_TABLES_LOAD_EVENTS = JSON_WORKSPACE_TABLE_LOADS

FIELDS_SCHEDULES = ["id", "tokenId", "configurationId", "configurationVersionId", 'region', 'project_id',
                    "schedule__cronTab", "schedule__timezone", "schedule__state", "target__componentId",
                    "target__configurationId", "target__configurationRowIds", "target__mode", "target__tag",
                    "executions"]
FIELDS_R_SCHEDULES = ["id", "token_id", "configuration_id", "configuration_version_id", 'region', 'project_id',
                      "schedule__cronTab", "schedule__timezone", "schedule__state", "target__component_id",
                      "target__configuration_id", "target__configuration_row_ids", "target__mode", "target__tag",
                      "executions"]
JSON_SCHEDULES = []
PK_SCHEDULES = ["id", 'region', 'project_id']


FIELDS_NOTIFICATIONS = ["id", "component_id", "configuration_id", 'region', 'project_id', "event", "phase_id",
                        'recipient__channel',
                        'recipient__address', 'filters']
FIELDS_R_NOTIFICATIONS = ["id", "component_id", "configuration_id", 'region', 'project_id', "event", "phase_id",
                          'recipient__channel',
                          'recipient__address', 'filters']
JSON_NOTIFICATIONS = []
PK_NOTIFICATIONS = ["id", 'region', 'project_id']



================================================
FILE: tests/__init__.py
================================================



================================================
FILE: tests/test_component.py
================================================



================================================
FILE: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on: [push]

concurrency: ci-${{ github.ref }} # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: "kds-team.ex-kbc-project-metadata-v2" # replace with your component id
  KBC_DEVELOPERPORTAL_VENDOR: "kds-team" # replace with your vendor
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: "kds-team+github"

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }} # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (Optional) Test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: "" # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }} # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo $raw | sed "s/.*origin\///" | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh

