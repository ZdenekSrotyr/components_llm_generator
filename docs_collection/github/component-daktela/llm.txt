Directory structure:
└── keboola-component-daktela/
    ├── README.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── pyproject.toml
    ├── uv.lock
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   ├── sourceCodeUrl.md
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_test.sh
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── configuration.py
    │   ├── daktela_client.py
    │   ├── extractor.py
    │   └── transformer.py
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── .github/
        └── workflows/
            └── push.yml

================================================
FILE: README.md
================================================
# Daktela Extractor

Keboola component for extracting data from Daktela CRM/Contact Center API v6.

## Functionality

This component extracts data from Daktela API v6 endpoints and loads them into Keboola Storage tables. It supports:

- ✅ **Multiple endpoint extraction** - Configure multiple endpoints with individual field selection
- ✅ **Incremental loading** - Load only new/updated data based on timestamps
- ✅ **Date range filtering** - Filter data by date ranges with flexible date formats
- ✅ **Parallel extraction** - Concurrent requests and endpoint processing for performance
- ✅ **Field discovery** - Sync action to discover available fields for each endpoint
- ✅ **Automatic schema management** - Tracks and persists table schemas across runs
- ✅ **Memory-efficient streaming** - Processes large datasets without loading everything into memory

### Supported Endpoints

- `accounts`, `activities`, `activitiesCall`, `activitiesChat`, `activitiesEmail`
- `campaignsRecords`, `contacts`, `crmRecords`, `groups`, `pauses`
- `queues`, `statuses`, `templates`, `tickets`, `users`

## Configuration

### 1. Connection

```json
{
  "connection": {
    "url": "https://yourcompany.daktela.com",
    "username": "your_username",
    "#password": "your_password",
    "verify_ssl": true
  }
}
```

- **url**: Daktela instance URL
- **username**: API username
- **#password**: API password (automatically encrypted by Keboola)
- **verify_ssl**: SSL certificate verification (default: `true`)

### 2. Data Selection

```json
{
  "data_selection": {
    "date_from": "7 days ago",
    "date_to": "today",
    "endpoints": [
      {
        "endpoint": "contacts",
        "fields": ["name", "title", "email", "phone"]
      },
      {
        "endpoint": "tickets",
        "fields": []
      },
      {
        "endpoint": "users"
      }
    ]
  }
}
```

- **date_from**: Start date for extraction
  - Formats: `today`, `yesterday`, `5 hours ago`, `3 days ago`, `4 months ago`, `2 years ago`
- **date_to**: End date for extraction (same formats as `date_from`)
- **endpoints**: Array of endpoint configurations
  - **endpoint**: Name of the API endpoint to extract
  - **fields**: (Optional) Array of field names to extract
    - If empty or omitted, all available fields will be extracted

#### Field Discovery

Use the **"Discover Available Fields"** sync action to see all available fields for your configured endpoints:

1. Configure your endpoints in the UI
2. Click "Discover Available Fields" button
3. The component will return a JSON object with all available fields per endpoint:
   ```json
   {
     "contacts": ["name", "title", "email", "phone", "created", "modified"],
     "tickets": ["title", "status", "priority", "created", "modified"]
   }
   ```
4. Copy the field names you need into the `fields` array for each endpoint

### 3. Destination

```json
{
  "destination": {
    "incremental": false
  }
}
```

- **incremental**: Enable incremental loading (appends data instead of replacing)
  - `false` (default): Replace existing tables with new data
  - `true`: Append new data to existing tables

### 4. Advanced Settings

```json
{
  "advanced": {
    "batch_size": 1000,
    "max_concurrent_requests": 10,
    "max_concurrent_endpoints": 3
  }
}
```

- **batch_size**: Number of records per API page and CSV flush (100-1000, default: 1000)
  - Note: Daktela API max is 1000, higher values are automatically capped
- **max_concurrent_requests**: Max concurrent API requests across all endpoints (1-50, default: 10)
- **max_concurrent_endpoints**: Max endpoints to extract simultaneously (1-20, default: 3)
  - Lower values reduce memory usage but take longer

### 5. Debug Mode

```json
{
  "debug": true
}
```

- **debug**: Enable detailed debug logging (default: `false`)

## Complete Configuration Example

```json
{
  "parameters": {
    "connection": {
      "url": "https://democz.daktela.com",
      "username": "agent_420407",
      "#password": "your_password",
      "verify_ssl": true
    },
    "data_selection": {
      "date_from": "7 days ago",
      "date_to": "today",
      "endpoints": [
        {
          "endpoint": "contacts",
          "fields": ["name", "title", "email", "phone"]
        },
        {
          "endpoint": "tickets",
          "fields": ["title", "status", "priority", "created"]
        },
        {
          "endpoint": "users"
        }
      ]
    },
    "destination": {
      "incremental": false
    },
    "advanced": {
      "batch_size": 1000,
      "max_concurrent_requests": 10,
      "max_concurrent_endpoints": 3
    },
    "debug": false
  }
}
```

## Output Tables

The component creates one output table per endpoint:

- Table name: `{endpoint}.csv` (e.g., `contacts.csv`, `tickets.csv`)
- Primary key: `name` (or `id_call` for `activitiesCall` endpoint)
- Incremental mode: Supported via primary key

## Development

### Local Setup

```bash
# Clone repository
git clone https://github.com/keboola/component-daktela
cd component-daktela

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies with uv
pip install uv
uv sync

# Run locally
python src/component.py
```

### Testing

```bash
# Run tests
uv run pytest

# Run with coverage
uv run pytest --cov=src
```

### Docker

```bash
# Build image
docker-compose build

# Run in dev mode
docker-compose run --rm dev

# Run tests
docker-compose run --rm test
```

## Architecture

### Component Structure

```
src/
├── component.py          # Main component orchestration
├── configuration.py      # Pydantic configuration models
├── daktela_client.py     # Async API client with rate limiting
├── extractor.py          # Extraction logic with streaming
└── __main__.py          # Entry point
```

### Key Features

1. **Async Architecture**
   - Uses `asyncio` for concurrent API requests
   - Rate limiting with `aiolimiter`
   - Connection pooling with `httpx`

2. **Memory-Efficient Streaming**
   - Processes records in batches
   - Streams directly to CSV without loading full datasets
   - Configurable batch sizes

3. **Schema State Management**
   - Tracks table schemas across runs
   - Persists schemas in state file
   - Handles schema evolution gracefully

4. **Robust Error Handling**
   - Retries with exponential backoff
   - Detailed error logging
   - User-friendly error messages

## Integration

For details about deployment and integration with Keboola Connection, refer to the [Keboola Developer Documentation](https://developers.keboola.com/extend/component/deployment/).

## License

MIT License - see [LICENSE.md](LICENSE.md)

## Support

For issues and questions, please use the [GitHub Issues](https://github.com/keboola/component-daktela/issues).



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh



================================================
FILE: Dockerfile
================================================
FROM python:3.13-slim
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

# uncomment the following line should you have any troubles installing certain packages which require C/C++ extensions
# to be compiled during installation, eg. numpy, psycopg2, …
# RUN apt-get update && apt-get install -y build-essential

WORKDIR /code/

COPY pyproject.toml .
COPY uv.lock .

ENV UV_PROJECT_ENVIRONMENT="/usr/local/"
RUN uv sync --all-groups --frozen

COPY src/ src
COPY tests/ tests
COPY scripts/ scripts
COPY flake8.cfg .
COPY deploy.sh .

CMD ["python", "-u", "src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    __pycache__,
    .git,
    .venv,
    venv
ignore = E203,W503
max-line-length = 120



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: pyproject.toml
================================================
[project]
name = "daktela"
dynamic = ["version"]
readme = "README.md"
requires-python = "~=3.13"
dependencies = [
    "flake8>=7.2.0",
    "freezegun>=1.5.1",
    "keboola-component>=1.6.10",
    "keboola-http-client>=1.2.0",
    "keboola-utils>=1.1.0",
    "mock>=5.2.0",
    "pydantic>=2.11.3",
    "requests>=2.31.0",
    "ruff>=0.11.5",
]

[dependency-groups]
dev = [
    "flask>=3.1.0",
    "flask-cors>=5.0.0",
    "pytest>=9.0.2",
    "pytest-asyncio>=1.3.0",
]



================================================
FILE: uv.lock
================================================
version = 1
requires-python = ">=3.13, <4"

[[package]]
name = "aiolimiter"
version = "1.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/23/b52debf471f7a1e42e362d959a3982bdcb4fe13a5d46e63d28868807a79c/aiolimiter-1.2.1.tar.gz", hash = "sha256:e02a37ea1a855d9e832252a105420ad4d15011505512a1a1d814647451b5cca9", size = 7185 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f3/ba/df6e8e1045aebc4778d19b8a3a9bc1808adb1619ba94ca354d9ba17d86c3/aiolimiter-1.2.1-py3-none-any.whl", hash = "sha256:d3f249e9059a20badcb56b61601a83556133655c11d1eb3dd3e04ff069e5f3c7", size = 6711 },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anyio"
version = "4.12.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/16/ce/8a777047513153587e5434fd752e89334ac33e379aa3497db860eeb60377/anyio-4.12.0.tar.gz", hash = "sha256:73c693b567b0c55130c104d0b43a9baf3aa6a31fc6110116509f27bf75e21ec0", size = 228266 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7f/9c/36c5c37947ebfb8c7f22e0eb6e4d188ee2d53aa3880f3f2744fb894f0cb1/anyio-4.12.0-py3-none-any.whl", hash = "sha256:dad2376a628f98eeca4881fc56cd06affd18f659b17a747d3ff0307ced94b1bb", size = 113362 },
]

[[package]]
name = "blinker"
version = "1.9.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/21/28/9b3f50ce0e048515135495f198351908d99540d69bfdc8c1d15b73dc55ce/blinker-1.9.0.tar.gz", hash = "sha256:b4ce2265a7abece45e7cc896e98dbebe6cead56bcf805a3d23136d145f5445bf", size = 22460 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/10/cb/f2ad4230dc2eb1a74edf38f1a38b9b52277f75bef262d8908e60d957e13c/blinker-1.9.0-py3-none-any.whl", hash = "sha256:ba0efaa9080b619ff2f3459d1d500c57bddea4a6b424b60a91141db6fd2f08bc", size = 8458 },
]

[[package]]
name = "certifi"
version = "2025.1.31"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1c/ab/c9f1e32b7b1bf505bf26f0ef697775960db7932abeb7b516de930ba2705f/certifi-2025.1.31.tar.gz", hash = "sha256:3d5da6925056f6f18f119200434a4780a94263f10d1c21d032a6f6b2baa20651", size = 167577 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/38/fc/bce832fd4fd99766c04d1ee0eead6b0ec6486fb100ae5e74c1d91292b982/certifi-2025.1.31-py3-none-any.whl", hash = "sha256:ca78db4565a652026a4db2bcdf68f2fb589ea80d0be70e03929ed730746b84fe", size = 166393 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/16/b0/572805e227f01586461c80e0fd25d65a2115599cc9dad142fee4b747c357/charset_normalizer-3.4.1.tar.gz", hash = "sha256:44251f18cd68a75b56585dd00dae26183e102cd5e0f9f1466e6df5da2ed64ea3", size = 123188 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/38/94/ce8e6f63d18049672c76d07d119304e1e2d7c6098f0841b51c666e9f44a0/charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:aabfa34badd18f1da5ec1bc2715cadc8dca465868a4e73a0173466b688f29dda", size = 195698 },
    { url = "https://files.pythonhosted.org/packages/24/2e/dfdd9770664aae179a96561cc6952ff08f9a8cd09a908f259a9dfa063568/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:22e14b5d70560b8dd51ec22863f370d1e595ac3d024cb8ad7d308b4cd95f8313", size = 140162 },
    { url = "https://files.pythonhosted.org/packages/24/4e/f646b9093cff8fc86f2d60af2de4dc17c759de9d554f130b140ea4738ca6/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8436c508b408b82d87dc5f62496973a1805cd46727c34440b0d29d8a2f50a6c9", size = 150263 },
    { url = "https://files.pythonhosted.org/packages/5e/67/2937f8d548c3ef6e2f9aab0f6e21001056f692d43282b165e7c56023e6dd/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2d074908e1aecee37a7635990b2c6d504cd4766c7bc9fc86d63f9c09af3fa11b", size = 142966 },
    { url = "https://files.pythonhosted.org/packages/52/ed/b7f4f07de100bdb95c1756d3a4d17b90c1a3c53715c1a476f8738058e0fa/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:955f8851919303c92343d2f66165294848d57e9bba6cf6e3625485a70a038d11", size = 144992 },
    { url = "https://files.pythonhosted.org/packages/96/2c/d49710a6dbcd3776265f4c923bb73ebe83933dfbaa841c5da850fe0fd20b/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:44ecbf16649486d4aebafeaa7ec4c9fed8b88101f4dd612dcaf65d5e815f837f", size = 147162 },
    { url = "https://files.pythonhosted.org/packages/b4/41/35ff1f9a6bd380303dea55e44c4933b4cc3c4850988927d4082ada230273/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:0924e81d3d5e70f8126529951dac65c1010cdf117bb75eb02dd12339b57749dd", size = 140972 },
    { url = "https://files.pythonhosted.org/packages/fb/43/c6a0b685fe6910d08ba971f62cd9c3e862a85770395ba5d9cad4fede33ab/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:2967f74ad52c3b98de4c3b32e1a44e32975e008a9cd2a8cc8966d6a5218c5cb2", size = 149095 },
    { url = "https://files.pythonhosted.org/packages/4c/ff/a9a504662452e2d2878512115638966e75633519ec11f25fca3d2049a94a/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:c75cb2a3e389853835e84a2d8fb2b81a10645b503eca9bcb98df6b5a43eb8886", size = 152668 },
    { url = "https://files.pythonhosted.org/packages/6c/71/189996b6d9a4b932564701628af5cee6716733e9165af1d5e1b285c530ed/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:09b26ae6b1abf0d27570633b2b078a2a20419c99d66fb2823173d73f188ce601", size = 150073 },
    { url = "https://files.pythonhosted.org/packages/e4/93/946a86ce20790e11312c87c75ba68d5f6ad2208cfb52b2d6a2c32840d922/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:fa88b843d6e211393a37219e6a1c1df99d35e8fd90446f1118f4216e307e48cd", size = 145732 },
    { url = "https://files.pythonhosted.org/packages/cd/e5/131d2fb1b0dddafc37be4f3a2fa79aa4c037368be9423061dccadfd90091/charset_normalizer-3.4.1-cp313-cp313-win32.whl", hash = "sha256:eb8178fe3dba6450a3e024e95ac49ed3400e506fd4e9e5c32d30adda88cbd407", size = 95391 },
    { url = "https://files.pythonhosted.org/packages/27/f2/4f9a69cc7712b9b5ad8fdb87039fd89abba997ad5cbe690d1835d40405b0/charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl", hash = "sha256:b1ac5992a838106edb89654e0aebfc24f5848ae2547d22c2c3f66454daa11971", size = 102702 },
    { url = "https://files.pythonhosted.org/packages/0e/f6/65ecc6878a89bb1c23a086ea335ad4bf21a588990c3f535a227b9eea9108/charset_normalizer-3.4.1-py3-none-any.whl", hash = "sha256:d98b1668f06378c6dbefec3b92299716b931cd4e6061f3c875a71ced1780ab85", size = 49767 },
]

[[package]]
name = "click"
version = "8.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/3d/fa/656b739db8587d7b5dfa22e22ed02566950fbfbcdc20311993483657a5c0/click-8.3.1.tar.gz", hash = "sha256:12ff4785d337a1bb490bb7e9c2b1ee5da3112e94a8622f26a6c77f5d2fc6842a", size = 295065 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/98/78/01c019cdb5d6498122777c1a43056ebb3ebfeef2076d9d026bfe15583b2b/click-8.3.1-py3-none-any.whl", hash = "sha256:981153a64e25f12d547d3426c367a4857371575ee7ad18df2a6183ab0545b2a6", size = 108274 },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "daktela"
source = { virtual = "." }
dependencies = [
    { name = "flake8" },
    { name = "freezegun" },
    { name = "keboola-component" },
    { name = "keboola-http-client" },
    { name = "keboola-utils" },
    { name = "mock" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "ruff" },
]

[package.dev-dependencies]
dev = [
    { name = "flask" },
    { name = "flask-cors" },
    { name = "pytest" },
    { name = "pytest-asyncio" },
]

[package.metadata]
requires-dist = [
    { name = "flake8", specifier = ">=7.2.0" },
    { name = "freezegun", specifier = ">=1.5.1" },
    { name = "keboola-component", specifier = ">=1.6.10" },
    { name = "keboola-http-client", specifier = ">=1.2.0" },
    { name = "keboola-utils", specifier = ">=1.1.0" },
    { name = "mock", specifier = ">=5.2.0" },
    { name = "pydantic", specifier = ">=2.11.3" },
    { name = "requests", specifier = ">=2.31.0" },
    { name = "ruff", specifier = ">=0.11.5" },
]

[package.metadata.requires-dev]
dev = [
    { name = "flask", specifier = ">=3.1.0" },
    { name = "flask-cors", specifier = ">=5.0.0" },
    { name = "pytest", specifier = ">=9.0.2" },
    { name = "pytest-asyncio", specifier = ">=1.3.0" },
]

[[package]]
name = "dateparser"
version = "1.2.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "python-dateutil" },
    { name = "pytz" },
    { name = "regex" },
    { name = "tzlocal" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bd/3f/d3207a05f5b6a78c66d86631e60bfba5af163738a599a5b9aa2c2737a09e/dateparser-1.2.1.tar.gz", hash = "sha256:7e4919aeb48481dbfc01ac9683c8e20bfe95bb715a38c1e9f6af889f4f30ccc3", size = 309924 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cf/0a/981c438c4cd84147c781e4e96c1d72df03775deb1bc76c5a6ee8afa89c62/dateparser-1.2.1-py3-none-any.whl", hash = "sha256:bdcac262a467e6260030040748ad7c10d6bacd4f3b9cdb4cfd2251939174508c", size = 295658 },
]

[[package]]
name = "deprecated"
version = "1.2.18"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wrapt" },
]
sdist = { url = "https://files.pythonhosted.org/packages/98/97/06afe62762c9a8a86af0cfb7bfdab22a43ad17138b07af5b1a58442690a2/deprecated-1.2.18.tar.gz", hash = "sha256:422b6f6d859da6f2ef57857761bfb392480502a64c3028ca9bbe86085d72115d", size = 2928744 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6e/c6/ac0b6c1e2d138f1002bcf799d330bd6d85084fece321e662a14223794041/Deprecated-1.2.18-py2.py3-none-any.whl", hash = "sha256:bd5011788200372a32418f888e326a09ff80d0214bd961147cfed01b5c018eec", size = 9998 },
]

[[package]]
name = "flake8"
version = "7.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mccabe" },
    { name = "pycodestyle" },
    { name = "pyflakes" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e7/c4/5842fc9fc94584c455543540af62fd9900faade32511fab650e9891ec225/flake8-7.2.0.tar.gz", hash = "sha256:fa558ae3f6f7dbf2b4f22663e5343b6b6023620461f8d4ff2019ef4b5ee70426", size = 48177 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/83/5c/0627be4c9976d56b1217cb5187b7504e7fd7d3503f8bfd312a04077bd4f7/flake8-7.2.0-py2.py3-none-any.whl", hash = "sha256:93b92ba5bdb60754a6da14fa3b93a9361fd00a59632ada61fd7b130436c40343", size = 57786 },
]

[[package]]
name = "flask"
version = "3.1.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "blinker" },
    { name = "click" },
    { name = "itsdangerous" },
    { name = "jinja2" },
    { name = "markupsafe" },
    { name = "werkzeug" },
]
sdist = { url = "https://files.pythonhosted.org/packages/dc/6d/cfe3c0fcc5e477df242b98bfe186a4c34357b4847e87ecaef04507332dab/flask-3.1.2.tar.gz", hash = "sha256:bf656c15c80190ed628ad08cdfd3aaa35beb087855e2f494910aa3774cc4fd87", size = 720160 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/f9/7f9263c5695f4bd0023734af91bedb2ff8209e8de6ead162f35d8dc762fd/flask-3.1.2-py3-none-any.whl", hash = "sha256:ca1d8112ec8a6158cc29ea4858963350011b5c846a414cdb7a954aa9e967d03c", size = 103308 },
]

[[package]]
name = "flask-cors"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "flask" },
    { name = "werkzeug" },
]
sdist = { url = "https://files.pythonhosted.org/packages/70/74/0fc0fa68d62f21daef41017dafab19ef4b36551521260987eb3a5394c7ba/flask_cors-6.0.2.tar.gz", hash = "sha256:6e118f3698249ae33e429760db98ce032a8bf9913638d085ca0f4c5534ad2423", size = 13472 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4f/af/72ad54402e599152de6d067324c46fe6a4f531c7c65baf7e96c63db55eaf/flask_cors-6.0.2-py3-none-any.whl", hash = "sha256:e57544d415dfd7da89a9564e1e3a9e515042df76e12130641ca6f3f2f03b699a", size = 13257 },
]

[[package]]
name = "freezegun"
version = "1.5.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "python-dateutil" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2c/ef/722b8d71ddf4d48f25f6d78aa2533d505bf3eec000a7cacb8ccc8de61f2f/freezegun-1.5.1.tar.gz", hash = "sha256:b29dedfcda6d5e8e083ce71b2b542753ad48cfec44037b3fc79702e2980a89e9", size = 33697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/51/0b/0d7fee5919bccc1fdc1c2a7528b98f65c6f69b223a3fd8f809918c142c36/freezegun-1.5.1-py3-none-any.whl", hash = "sha256:bf111d7138a8abe55ab48a71755673dbaa4ab87f4cff5634a4442dfec34c15f1", size = 17569 },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515 },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "iniconfig"
version = "2.3.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/72/34/14ca021ce8e5dfedc35312d08ba8bf51fdd999c576889fc2c24cb97f4f10/iniconfig-2.3.0.tar.gz", hash = "sha256:c76315c77db068650d49c5b56314774a7804df16fee4402c1f19d6d15d8c4730", size = 20503 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cb/b1/3846dd7f199d53cb17f49cba7e651e9ce294d8497c8c150530ed11865bb8/iniconfig-2.3.0-py3-none-any.whl", hash = "sha256:f631c04d2c48c52b84d0d0549c99ff3859c98df65b3101406327ecc7d53fbf12", size = 7484 },
]

[[package]]
name = "itsdangerous"
version = "2.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9c/cb/8ac0172223afbccb63986cc25049b154ecfb5e85932587206f42317be31d/itsdangerous-2.2.0.tar.gz", hash = "sha256:e0050c0b7da1eea53ffaf149c0cfbb5c6e2e2b69c4bef22c81fa6eb73e5f6173", size = 54410 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/96/92447566d16df59b2a776c0fb82dbc4d9e07cd95062562af01e408583fc4/itsdangerous-2.2.0-py3-none-any.whl", hash = "sha256:c6242fc49e35958c8b15141343aa660db5fc54d4f13a1db01a3f5891b98700ef", size = 16234 },
]

[[package]]
name = "jinja2"
version = "3.1.6"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markupsafe" },
]
sdist = { url = "https://files.pythonhosted.org/packages/df/bf/f7da0350254c0ed7c72f3e33cef02e048281fec7ecec5f032d4aac52226b/jinja2-3.1.6.tar.gz", hash = "sha256:0137fb05990d35f1275a587e9aee6d56da821fc83491a0fb838183be43f66d6d", size = 245115 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl", hash = "sha256:85ece4451f492d0c13c5dd7c13a64681a86afae63a5f347908daf103ce6d2f67", size = 134899 },
]

[[package]]
name = "keboola-component"
version = "1.6.10"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "deprecated" },
    { name = "pygelf" },
    { name = "pytz" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ef/dd/391f1e6eaae5e925f56f30788c40e38c8bce3a80ee898512e79ced2eabab/keboola.component-1.6.10.tar.gz", hash = "sha256:5f4c347e8e96bb4dff1fe1254217e88754a4edea8a1b147815552335dcfba941", size = 47336 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4c/34/301082c106bff256e2871a5c5db54d691fdad77a9d8a89a52eef30cd18ed/keboola.component-1.6.10-py3-none-any.whl", hash = "sha256:9a13b73beb71373d9a2b456eb44f902cfcfc07747c084bfbfad761b5eaaa4d93", size = 42243 },
]

[[package]]
name = "keboola-http-client"
version = "1.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiolimiter" },
    { name = "httpx" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0f/b9/8e43e2b7c1f2667a9bc40b96a0098dcdd5d77b8d937fa1312ebd99ad9561/keboola_http_client-1.2.0.tar.gz", hash = "sha256:b3a3bcdc096ab84cff19ffa65d2ee303032c73d2d8d8b8aa93a82fb5ba6da484", size = 18369 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/09/7d/1d2b64896f9fff44de82783194a0def1e683dbfe7a8491db6d88d9a403c9/keboola_http_client-1.2.0-py3-none-any.whl", hash = "sha256:de80f5866d4d0aafc3a67492dc3e2d31d6c7e53f79406d7130620c952a4da493", size = 12632 },
]

[[package]]
name = "keboola-utils"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "dateparser" },
    { name = "pytz" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a7/b8/ccfddc2eb510f7a6ab878ab8a6249a23494194780a436676da6c2f5d23c7/keboola.utils-1.1.0.tar.gz", hash = "sha256:e943dbda932d945bcd5edd51283eea8f7035249c9dac769d3e96d2f507b52f60", size = 9830 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/f4/6697a0c2ff512baa7b84413972e51d5449a0a145f68dc750f05a8b1da39d/keboola.utils-1.1.0-py3-none-any.whl", hash = "sha256:8c73faa4a81f371a2eecd8465b08a51b3f7608969dd91d38d5b3bcfad7ef0da5", size = 10131 },
]

[[package]]
name = "markupsafe"
version = "3.0.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7e/99/7690b6d4034fffd95959cbe0c02de8deb3098cc577c67bb6a24fe5d7caa7/markupsafe-3.0.3.tar.gz", hash = "sha256:722695808f4b6457b320fdc131280796bdceb04ab50fe1795cd540799ebe1698", size = 80313 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/38/2f/907b9c7bbba283e68f20259574b13d005c121a0fa4c175f9bed27c4597ff/markupsafe-3.0.3-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:e1cf1972137e83c5d4c136c43ced9ac51d0e124706ee1c8aa8532c1287fa8795", size = 11622 },
    { url = "https://files.pythonhosted.org/packages/9c/d9/5f7756922cdd676869eca1c4e3c0cd0df60ed30199ffd775e319089cb3ed/markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:116bb52f642a37c115f517494ea5feb03889e04df47eeff5b130b1808ce7c219", size = 12029 },
    { url = "https://files.pythonhosted.org/packages/00/07/575a68c754943058c78f30db02ee03a64b3c638586fba6a6dd56830b30a3/markupsafe-3.0.3-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:133a43e73a802c5562be9bbcd03d090aa5a1fe899db609c29e8c8d815c5f6de6", size = 24374 },
    { url = "https://files.pythonhosted.org/packages/a9/21/9b05698b46f218fc0e118e1f8168395c65c8a2c750ae2bab54fc4bd4e0e8/markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:ccfcd093f13f0f0b7fdd0f198b90053bf7b2f02a3927a30e63f3ccc9df56b676", size = 22980 },
    { url = "https://files.pythonhosted.org/packages/7f/71/544260864f893f18b6827315b988c146b559391e6e7e8f7252839b1b846a/markupsafe-3.0.3-cp313-cp313-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:509fa21c6deb7a7a273d629cf5ec029bc209d1a51178615ddf718f5918992ab9", size = 21990 },
    { url = "https://files.pythonhosted.org/packages/c2/28/b50fc2f74d1ad761af2f5dcce7492648b983d00a65b8c0e0cb457c82ebbe/markupsafe-3.0.3-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a4afe79fb3de0b7097d81da19090f4df4f8d3a2b3adaa8764138aac2e44f3af1", size = 23784 },
    { url = "https://files.pythonhosted.org/packages/ed/76/104b2aa106a208da8b17a2fb72e033a5a9d7073c68f7e508b94916ed47a9/markupsafe-3.0.3-cp313-cp313-musllinux_1_2_riscv64.whl", hash = "sha256:795e7751525cae078558e679d646ae45574b47ed6e7771863fcc079a6171a0fc", size = 21588 },
    { url = "https://files.pythonhosted.org/packages/b5/99/16a5eb2d140087ebd97180d95249b00a03aa87e29cc224056274f2e45fd6/markupsafe-3.0.3-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:8485f406a96febb5140bfeca44a73e3ce5116b2501ac54fe953e488fb1d03b12", size = 23041 },
    { url = "https://files.pythonhosted.org/packages/19/bc/e7140ed90c5d61d77cea142eed9f9c303f4c4806f60a1044c13e3f1471d0/markupsafe-3.0.3-cp313-cp313-win32.whl", hash = "sha256:bdd37121970bfd8be76c5fb069c7751683bdf373db1ed6c010162b2a130248ed", size = 14543 },
    { url = "https://files.pythonhosted.org/packages/05/73/c4abe620b841b6b791f2edc248f556900667a5a1cf023a6646967ae98335/markupsafe-3.0.3-cp313-cp313-win_amd64.whl", hash = "sha256:9a1abfdc021a164803f4d485104931fb8f8c1efd55bc6b748d2f5774e78b62c5", size = 15113 },
    { url = "https://files.pythonhosted.org/packages/f0/3a/fa34a0f7cfef23cf9500d68cb7c32dd64ffd58a12b09225fb03dd37d5b80/markupsafe-3.0.3-cp313-cp313-win_arm64.whl", hash = "sha256:7e68f88e5b8799aa49c85cd116c932a1ac15caaa3f5db09087854d218359e485", size = 13911 },
    { url = "https://files.pythonhosted.org/packages/e4/d7/e05cd7efe43a88a17a37b3ae96e79a19e846f3f456fe79c57ca61356ef01/markupsafe-3.0.3-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:218551f6df4868a8d527e3062d0fb968682fe92054e89978594c28e642c43a73", size = 11658 },
    { url = "https://files.pythonhosted.org/packages/99/9e/e412117548182ce2148bdeacdda3bb494260c0b0184360fe0d56389b523b/markupsafe-3.0.3-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:3524b778fe5cfb3452a09d31e7b5adefeea8c5be1d43c4f810ba09f2ceb29d37", size = 12066 },
    { url = "https://files.pythonhosted.org/packages/bc/e6/fa0ffcda717ef64a5108eaa7b4f5ed28d56122c9a6d70ab8b72f9f715c80/markupsafe-3.0.3-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:4e885a3d1efa2eadc93c894a21770e4bc67899e3543680313b09f139e149ab19", size = 25639 },
    { url = "https://files.pythonhosted.org/packages/96/ec/2102e881fe9d25fc16cb4b25d5f5cde50970967ffa5dddafdb771237062d/markupsafe-3.0.3-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:8709b08f4a89aa7586de0aadc8da56180242ee0ada3999749b183aa23df95025", size = 23569 },
    { url = "https://files.pythonhosted.org/packages/4b/30/6f2fce1f1f205fc9323255b216ca8a235b15860c34b6798f810f05828e32/markupsafe-3.0.3-cp313-cp313t-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:b8512a91625c9b3da6f127803b166b629725e68af71f8184ae7e7d54686a56d6", size = 23284 },
    { url = "https://files.pythonhosted.org/packages/58/47/4a0ccea4ab9f5dcb6f79c0236d954acb382202721e704223a8aafa38b5c8/markupsafe-3.0.3-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:9b79b7a16f7fedff2495d684f2b59b0457c3b493778c9eed31111be64d58279f", size = 24801 },
    { url = "https://files.pythonhosted.org/packages/6a/70/3780e9b72180b6fecb83a4814d84c3bf4b4ae4bf0b19c27196104149734c/markupsafe-3.0.3-cp313-cp313t-musllinux_1_2_riscv64.whl", hash = "sha256:12c63dfb4a98206f045aa9563db46507995f7ef6d83b2f68eda65c307c6829eb", size = 22769 },
    { url = "https://files.pythonhosted.org/packages/98/c5/c03c7f4125180fc215220c035beac6b9cb684bc7a067c84fc69414d315f5/markupsafe-3.0.3-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:8f71bc33915be5186016f675cd83a1e08523649b0e33efdb898db577ef5bb009", size = 23642 },
    { url = "https://files.pythonhosted.org/packages/80/d6/2d1b89f6ca4bff1036499b1e29a1d02d282259f3681540e16563f27ebc23/markupsafe-3.0.3-cp313-cp313t-win32.whl", hash = "sha256:69c0b73548bc525c8cb9a251cddf1931d1db4d2258e9599c28c07ef3580ef354", size = 14612 },
    { url = "https://files.pythonhosted.org/packages/2b/98/e48a4bfba0a0ffcf9925fe2d69240bfaa19c6f7507b8cd09c70684a53c1e/markupsafe-3.0.3-cp313-cp313t-win_amd64.whl", hash = "sha256:1b4b79e8ebf6b55351f0d91fe80f893b4743f104bff22e90697db1590e47a218", size = 15200 },
    { url = "https://files.pythonhosted.org/packages/0e/72/e3cc540f351f316e9ed0f092757459afbc595824ca724cbc5a5d4263713f/markupsafe-3.0.3-cp313-cp313t-win_arm64.whl", hash = "sha256:ad2cf8aa28b8c020ab2fc8287b0f823d0a7d8630784c31e9ee5edea20f406287", size = 13973 },
    { url = "https://files.pythonhosted.org/packages/33/8a/8e42d4838cd89b7dde187011e97fe6c3af66d8c044997d2183fbd6d31352/markupsafe-3.0.3-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:eaa9599de571d72e2daf60164784109f19978b327a3910d3e9de8c97b5b70cfe", size = 11619 },
    { url = "https://files.pythonhosted.org/packages/b5/64/7660f8a4a8e53c924d0fa05dc3a55c9cee10bbd82b11c5afb27d44b096ce/markupsafe-3.0.3-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:c47a551199eb8eb2121d4f0f15ae0f923d31350ab9280078d1e5f12b249e0026", size = 12029 },
    { url = "https://files.pythonhosted.org/packages/da/ef/e648bfd021127bef5fa12e1720ffed0c6cbb8310c8d9bea7266337ff06de/markupsafe-3.0.3-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:f34c41761022dd093b4b6896d4810782ffbabe30f2d443ff5f083e0cbbb8c737", size = 24408 },
    { url = "https://files.pythonhosted.org/packages/41/3c/a36c2450754618e62008bf7435ccb0f88053e07592e6028a34776213d877/markupsafe-3.0.3-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:457a69a9577064c05a97c41f4e65148652db078a3a509039e64d3467b9e7ef97", size = 23005 },
    { url = "https://files.pythonhosted.org/packages/bc/20/b7fdf89a8456b099837cd1dc21974632a02a999ec9bf7ca3e490aacd98e7/markupsafe-3.0.3-cp314-cp314-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:e8afc3f2ccfa24215f8cb28dcf43f0113ac3c37c2f0f0806d8c70e4228c5cf4d", size = 22048 },
    { url = "https://files.pythonhosted.org/packages/9a/a7/591f592afdc734f47db08a75793a55d7fbcc6902a723ae4cfbab61010cc5/markupsafe-3.0.3-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:ec15a59cf5af7be74194f7ab02d0f59a62bdcf1a537677ce67a2537c9b87fcda", size = 23821 },
    { url = "https://files.pythonhosted.org/packages/7d/33/45b24e4f44195b26521bc6f1a82197118f74df348556594bd2262bda1038/markupsafe-3.0.3-cp314-cp314-musllinux_1_2_riscv64.whl", hash = "sha256:0eb9ff8191e8498cca014656ae6b8d61f39da5f95b488805da4bb029cccbfbaf", size = 21606 },
    { url = "https://files.pythonhosted.org/packages/ff/0e/53dfaca23a69fbfbbf17a4b64072090e70717344c52eaaaa9c5ddff1e5f0/markupsafe-3.0.3-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:2713baf880df847f2bece4230d4d094280f4e67b1e813eec43b4c0e144a34ffe", size = 23043 },
    { url = "https://files.pythonhosted.org/packages/46/11/f333a06fc16236d5238bfe74daccbca41459dcd8d1fa952e8fbd5dccfb70/markupsafe-3.0.3-cp314-cp314-win32.whl", hash = "sha256:729586769a26dbceff69f7a7dbbf59ab6572b99d94576a5592625d5b411576b9", size = 14747 },
    { url = "https://files.pythonhosted.org/packages/28/52/182836104b33b444e400b14f797212f720cbc9ed6ba34c800639d154e821/markupsafe-3.0.3-cp314-cp314-win_amd64.whl", hash = "sha256:bdc919ead48f234740ad807933cdf545180bfbe9342c2bb451556db2ed958581", size = 15341 },
    { url = "https://files.pythonhosted.org/packages/6f/18/acf23e91bd94fd7b3031558b1f013adfa21a8e407a3fdb32745538730382/markupsafe-3.0.3-cp314-cp314-win_arm64.whl", hash = "sha256:5a7d5dc5140555cf21a6fefbdbf8723f06fcd2f63ef108f2854de715e4422cb4", size = 14073 },
    { url = "https://files.pythonhosted.org/packages/3c/f0/57689aa4076e1b43b15fdfa646b04653969d50cf30c32a102762be2485da/markupsafe-3.0.3-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:1353ef0c1b138e1907ae78e2f6c63ff67501122006b0f9abad68fda5f4ffc6ab", size = 11661 },
    { url = "https://files.pythonhosted.org/packages/89/c3/2e67a7ca217c6912985ec766c6393b636fb0c2344443ff9d91404dc4c79f/markupsafe-3.0.3-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:1085e7fbddd3be5f89cc898938f42c0b3c711fdcb37d75221de2666af647c175", size = 12069 },
    { url = "https://files.pythonhosted.org/packages/f0/00/be561dce4e6ca66b15276e184ce4b8aec61fe83662cce2f7d72bd3249d28/markupsafe-3.0.3-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:1b52b4fb9df4eb9ae465f8d0c228a00624de2334f216f178a995ccdcf82c4634", size = 25670 },
    { url = "https://files.pythonhosted.org/packages/50/09/c419f6f5a92e5fadde27efd190eca90f05e1261b10dbd8cbcb39cd8ea1dc/markupsafe-3.0.3-cp314-cp314t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:fed51ac40f757d41b7c48425901843666a6677e3e8eb0abcff09e4ba6e664f50", size = 23598 },
    { url = "https://files.pythonhosted.org/packages/22/44/a0681611106e0b2921b3033fc19bc53323e0b50bc70cffdd19f7d679bb66/markupsafe-3.0.3-cp314-cp314t-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:f190daf01f13c72eac4efd5c430a8de82489d9cff23c364c3ea822545032993e", size = 23261 },
    { url = "https://files.pythonhosted.org/packages/5f/57/1b0b3f100259dc9fffe780cfb60d4be71375510e435efec3d116b6436d43/markupsafe-3.0.3-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:e56b7d45a839a697b5eb268c82a71bd8c7f6c94d6fd50c3d577fa39a9f1409f5", size = 24835 },
    { url = "https://files.pythonhosted.org/packages/26/6a/4bf6d0c97c4920f1597cc14dd720705eca0bf7c787aebc6bb4d1bead5388/markupsafe-3.0.3-cp314-cp314t-musllinux_1_2_riscv64.whl", hash = "sha256:f3e98bb3798ead92273dc0e5fd0f31ade220f59a266ffd8a4f6065e0a3ce0523", size = 22733 },
    { url = "https://files.pythonhosted.org/packages/14/c7/ca723101509b518797fedc2fdf79ba57f886b4aca8a7d31857ba3ee8281f/markupsafe-3.0.3-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:5678211cb9333a6468fb8d8be0305520aa073f50d17f089b5b4b477ea6e67fdc", size = 23672 },
    { url = "https://files.pythonhosted.org/packages/fb/df/5bd7a48c256faecd1d36edc13133e51397e41b73bb77e1a69deab746ebac/markupsafe-3.0.3-cp314-cp314t-win32.whl", hash = "sha256:915c04ba3851909ce68ccc2b8e2cd691618c4dc4c4232fb7982bca3f41fd8c3d", size = 14819 },
    { url = "https://files.pythonhosted.org/packages/1a/8a/0402ba61a2f16038b48b39bccca271134be00c5c9f0f623208399333c448/markupsafe-3.0.3-cp314-cp314t-win_amd64.whl", hash = "sha256:4faffd047e07c38848ce017e8725090413cd80cbc23d86e55c587bf979e579c9", size = 15426 },
    { url = "https://files.pythonhosted.org/packages/70/bc/6f1c2f612465f5fa89b95bead1f44dcb607670fd42891d8fdcd5d039f4f4/markupsafe-3.0.3-cp314-cp314t-win_arm64.whl", hash = "sha256:32001d6a8fc98c8cb5c947787c5d08b0a50663d139f1305bac5885d98d9b40fa", size = 14146 },
]

[[package]]
name = "mccabe"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e7/ff/0ffefdcac38932a54d2b5eed4e0ba8a408f215002cd178ad1df0f2806ff8/mccabe-0.7.0.tar.gz", hash = "sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325", size = 9658 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/27/1a/1f68f9ba0c207934b35b86a8ca3aad8395a3d6dd7921c0686e23853ff5a9/mccabe-0.7.0-py2.py3-none-any.whl", hash = "sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e", size = 7350 },
]

[[package]]
name = "mock"
version = "5.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/8c/14c2ae915e5f9dca5a22edd68b35be94400719ccfa068a03e0fb63d0f6f6/mock-5.2.0.tar.gz", hash = "sha256:4e460e818629b4b173f32d08bf30d3af8123afbb8e04bb5707a1fd4799e503f0", size = 92796 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/d9/617e6af809bf3a1d468e0d58c3997b1dc219a9a9202e650d30c2fc85d481/mock-5.2.0-py3-none-any.whl", hash = "sha256:7ba87f72ca0e915175596069dbbcc7c75af7b5e9b9bc107ad6349ede0819982f", size = 31617 },
]

[[package]]
name = "packaging"
version = "25.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/d4/1fc4078c65507b51b96ca8f8c3ba19e6a61c8253c72794544580a7b6c24d/packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f", size = 165727 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484", size = 66469 },
]

[[package]]
name = "pluggy"
version = "1.6.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f9/e2/3e91f31a7d2b083fe6ef3fa267035b518369d9511ffab804f839851d2779/pluggy-1.6.0.tar.gz", hash = "sha256:7dcc130b76258d33b90f61b658791dede3486c3e6bfb003ee5c9bfb396dd22f3", size = 69412 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/54/20/4d324d65cc6d9205fabedc306948156824eb9f0ee1633355a8f7ec5c66bf/pluggy-1.6.0-py3-none-any.whl", hash = "sha256:e920276dd6813095e9377c0bc5566d94c932c33b27a3e3945d8389c374dd4746", size = 20538 },
]

[[package]]
name = "pycodestyle"
version = "2.13.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/04/6e/1f4a62078e4d95d82367f24e685aef3a672abfd27d1a868068fed4ed2254/pycodestyle-2.13.0.tar.gz", hash = "sha256:c8415bf09abe81d9c7f872502a6eee881fbe85d8763dd5b9924bb0a01d67efae", size = 39312 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/07/be/b00116df1bfb3e0bb5b45e29d604799f7b91dd861637e4d448b4e09e6a3e/pycodestyle-2.13.0-py2.py3-none-any.whl", hash = "sha256:35863c5974a271c7a726ed228a14a4f6daf49df369d8c50cd9a6f58a5e143ba9", size = 31424 },
]

[[package]]
name = "pydantic"
version = "2.11.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/10/2e/ca897f093ee6c5f3b0bee123ee4465c50e75431c3d5b6a3b44a47134e891/pydantic-2.11.3.tar.gz", hash = "sha256:7471657138c16adad9322fe3070c0116dd6c3ad8d649300e3cbdfe91f4db4ec3", size = 785513 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b0/1d/407b29780a289868ed696d1616f4aad49d6388e5a77f567dcd2629dcd7b8/pydantic-2.11.3-py3-none-any.whl", hash = "sha256:a082753436a07f9ba1289c6ffa01cd93db3548776088aa917cc43b63f68fa60f", size = 443591 },
]

[[package]]
name = "pydantic-core"
version = "2.33.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/17/19/ed6a078a5287aea7922de6841ef4c06157931622c89c2a47940837b5eecd/pydantic_core-2.33.1.tar.gz", hash = "sha256:bcc9c6fdb0ced789245b02b7d6603e17d1563064ddcfc36f046b61c0c05dd9df", size = 434395 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/24/eed3466a4308d79155f1cdd5c7432c80ddcc4530ba8623b79d5ced021641/pydantic_core-2.33.1-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:70af6a21237b53d1fe7b9325b20e65cbf2f0a848cf77bed492b029139701e66a", size = 2033551 },
    { url = "https://files.pythonhosted.org/packages/ab/14/df54b1a0bc9b6ded9b758b73139d2c11b4e8eb43e8ab9c5847c0a2913ada/pydantic_core-2.33.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:282b3fe1bbbe5ae35224a0dbd05aed9ccabccd241e8e6b60370484234b456266", size = 1852785 },
    { url = "https://files.pythonhosted.org/packages/fa/96/e275f15ff3d34bb04b0125d9bc8848bf69f25d784d92a63676112451bfb9/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4b315e596282bbb5822d0c7ee9d255595bd7506d1cb20c2911a4da0b970187d3", size = 1897758 },
    { url = "https://files.pythonhosted.org/packages/b7/d8/96bc536e975b69e3a924b507d2a19aedbf50b24e08c80fb00e35f9baaed8/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:1dfae24cf9921875ca0ca6a8ecb4bb2f13c855794ed0d468d6abbec6e6dcd44a", size = 1986109 },
    { url = "https://files.pythonhosted.org/packages/90/72/ab58e43ce7e900b88cb571ed057b2fcd0e95b708a2e0bed475b10130393e/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:6dd8ecfde08d8bfadaea669e83c63939af76f4cf5538a72597016edfa3fad516", size = 2129159 },
    { url = "https://files.pythonhosted.org/packages/dc/3f/52d85781406886c6870ac995ec0ba7ccc028b530b0798c9080531b409fdb/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2f593494876eae852dc98c43c6f260f45abdbfeec9e4324e31a481d948214764", size = 2680222 },
    { url = "https://files.pythonhosted.org/packages/f4/56/6e2ef42f363a0eec0fd92f74a91e0ac48cd2e49b695aac1509ad81eee86a/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:948b73114f47fd7016088e5186d13faf5e1b2fe83f5e320e371f035557fd264d", size = 2006980 },
    { url = "https://files.pythonhosted.org/packages/4c/c0/604536c4379cc78359f9ee0aa319f4aedf6b652ec2854953f5a14fc38c5a/pydantic_core-2.33.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:e11f3864eb516af21b01e25fac915a82e9ddad3bb0fb9e95a246067398b435a4", size = 2120840 },
    { url = "https://files.pythonhosted.org/packages/1f/46/9eb764814f508f0edfb291a0f75d10854d78113fa13900ce13729aaec3ae/pydantic_core-2.33.1-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:549150be302428b56fdad0c23c2741dcdb5572413776826c965619a25d9c6bde", size = 2072518 },
    { url = "https://files.pythonhosted.org/packages/42/e3/fb6b2a732b82d1666fa6bf53e3627867ea3131c5f39f98ce92141e3e3dc1/pydantic_core-2.33.1-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:495bc156026efafd9ef2d82372bd38afce78ddd82bf28ef5276c469e57c0c83e", size = 2248025 },
    { url = "https://files.pythonhosted.org/packages/5c/9d/fbe8fe9d1aa4dac88723f10a921bc7418bd3378a567cb5e21193a3c48b43/pydantic_core-2.33.1-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:ec79de2a8680b1a67a07490bddf9636d5c2fab609ba8c57597e855fa5fa4dacd", size = 2254991 },
    { url = "https://files.pythonhosted.org/packages/aa/99/07e2237b8a66438d9b26482332cda99a9acccb58d284af7bc7c946a42fd3/pydantic_core-2.33.1-cp313-cp313-win32.whl", hash = "sha256:ee12a7be1742f81b8a65b36c6921022301d466b82d80315d215c4c691724986f", size = 1915262 },
    { url = "https://files.pythonhosted.org/packages/8a/f4/e457a7849beeed1e5defbcf5051c6f7b3c91a0624dd31543a64fc9adcf52/pydantic_core-2.33.1-cp313-cp313-win_amd64.whl", hash = "sha256:ede9b407e39949d2afc46385ce6bd6e11588660c26f80576c11c958e6647bc40", size = 1956626 },
    { url = "https://files.pythonhosted.org/packages/20/d0/e8d567a7cff7b04e017ae164d98011f1e1894269fe8e90ea187a3cbfb562/pydantic_core-2.33.1-cp313-cp313-win_arm64.whl", hash = "sha256:aa687a23d4b7871a00e03ca96a09cad0f28f443690d300500603bd0adba4b523", size = 1909590 },
    { url = "https://files.pythonhosted.org/packages/ef/fd/24ea4302d7a527d672c5be06e17df16aabfb4e9fdc6e0b345c21580f3d2a/pydantic_core-2.33.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:401d7b76e1000d0dd5538e6381d28febdcacb097c8d340dde7d7fc6e13e9f95d", size = 1812963 },
    { url = "https://files.pythonhosted.org/packages/5f/95/4fbc2ecdeb5c1c53f1175a32d870250194eb2fdf6291b795ab08c8646d5d/pydantic_core-2.33.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7aeb055a42d734c0255c9e489ac67e75397d59c6fbe60d155851e9782f276a9c", size = 1986896 },
    { url = "https://files.pythonhosted.org/packages/71/ae/fe31e7f4a62431222d8f65a3bd02e3fa7e6026d154a00818e6d30520ea77/pydantic_core-2.33.1-cp313-cp313t-win_amd64.whl", hash = "sha256:338ea9b73e6e109f15ab439e62cb3b78aa752c7fd9536794112e14bee02c8d18", size = 1931810 },
]

[[package]]
name = "pyflakes"
version = "3.3.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/af/cc/1df338bd7ed1fa7c317081dcf29bf2f01266603b301e6858856d346a12b3/pyflakes-3.3.2.tar.gz", hash = "sha256:6dfd61d87b97fba5dcfaaf781171ac16be16453be6d816147989e7f6e6a9576b", size = 64175 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/15/40/b293a4fa769f3b02ab9e387c707c4cbdc34f073f945de0386107d4e669e6/pyflakes-3.3.2-py2.py3-none-any.whl", hash = "sha256:5039c8339cbb1944045f4ee5466908906180f13cc99cc9949348d10f82a5c32a", size = 63164 },
]

[[package]]
name = "pygelf"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fe/d3/73d1fe74a156f9a0e519bedc87815ed309e64af19c73b94352e4c0959ddb/pygelf-0.4.2.tar.gz", hash = "sha256:d0bb8f45ff648a9a187713f4a05c09f685fcb8add7b04bb7471f20071bd11aad", size = 11991 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/03/cd/4afdddbc73f54ddf31d16137ef81c3d47192d75754b3115d925926081fd6/pygelf-0.4.2-py3-none-any.whl", hash = "sha256:ab57d1b26bffa014e29ae645ee51d2aa2f0c0cb419c522f2d24a237090b894a1", size = 8714 },
]

[[package]]
name = "pygments"
version = "2.19.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/b0/77/a5b8c569bf593b0140bde72ea885a803b82086995367bf2037de0159d924/pygments-2.19.2.tar.gz", hash = "sha256:636cb2477cec7f8952536970bc533bc43743542f70392ae026374600add5b887", size = 4968631 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c7/21/705964c7812476f378728bdf590ca4b771ec72385c533964653c68e86bdc/pygments-2.19.2-py3-none-any.whl", hash = "sha256:86540386c03d588bb81d44bc3928634ff26449851e99741617ecb9037ee5ec0b", size = 1225217 },
]

[[package]]
name = "pytest"
version = "9.0.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "iniconfig" },
    { name = "packaging" },
    { name = "pluggy" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/d1/db/7ef3487e0fb0049ddb5ce41d3a49c235bf9ad299b6a25d5780a89f19230f/pytest-9.0.2.tar.gz", hash = "sha256:75186651a92bd89611d1d9fc20f0b4345fd827c41ccd5c299a868a05d70edf11", size = 1568901 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/3b/ab/b3226f0bd7cdcf710fbede2b3548584366da3b19b5021e74f5bde2a8fa3f/pytest-9.0.2-py3-none-any.whl", hash = "sha256:711ffd45bf766d5264d487b917733b453d917afd2b0ad65223959f59089f875b", size = 374801 },
]

[[package]]
name = "pytest-asyncio"
version = "1.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pytest" },
]
sdist = { url = "https://files.pythonhosted.org/packages/90/2c/8af215c0f776415f3590cac4f9086ccefd6fd463befeae41cd4d3f193e5a/pytest_asyncio-1.3.0.tar.gz", hash = "sha256:d7f52f36d231b80ee124cd216ffb19369aa168fc10095013c6b014a34d3ee9e5", size = 50087 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e5/35/f8b19922b6a25bc0880171a2f1a003eaeb93657475193ab516fd87cac9da/pytest_asyncio-1.3.0-py3-none-any.whl", hash = "sha256:611e26147c7f77640e6d0a92a38ed17c3e9848063698d5c93d5aa7aa11cebff5", size = 15075 },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892 },
]

[[package]]
name = "pytz"
version = "2025.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f8/bf/abbd3cdfb8fbc7fb3d4d38d320f2441b1e7cbe29be4f23797b4a2b5d8aac/pytz-2025.2.tar.gz", hash = "sha256:360b9e3dbb49a209c21ad61809c7fb453643e048b38924c765813546746e81c3", size = 320884 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl", hash = "sha256:5ddf76296dd8c44c26eb8f4b6f35488f3ccbf6fbbd7adee0b7262d43f0ec2f00", size = 509225 },
]

[[package]]
name = "regex"
version = "2024.11.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8e/5f/bd69653fbfb76cf8604468d3b4ec4c403197144c7bfe0e6a5fc9e02a07cb/regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519", size = 399494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/90/73/bcb0e36614601016552fa9344544a3a2ae1809dc1401b100eab02e772e1f/regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84", size = 483525 },
    { url = "https://files.pythonhosted.org/packages/0f/3f/f1a082a46b31e25291d830b369b6b0c5576a6f7fb89d3053a354c24b8a83/regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4", size = 288324 },
    { url = "https://files.pythonhosted.org/packages/09/c9/4e68181a4a652fb3ef5099e077faf4fd2a694ea6e0f806a7737aff9e758a/regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0", size = 284617 },
    { url = "https://files.pythonhosted.org/packages/fc/fd/37868b75eaf63843165f1d2122ca6cb94bfc0271e4428cf58c0616786dce/regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0", size = 795023 },
    { url = "https://files.pythonhosted.org/packages/c4/7c/d4cd9c528502a3dedb5c13c146e7a7a539a3853dc20209c8e75d9ba9d1b2/regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7", size = 833072 },
    { url = "https://files.pythonhosted.org/packages/4f/db/46f563a08f969159c5a0f0e722260568425363bea43bb7ae370becb66a67/regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7", size = 823130 },
    { url = "https://files.pythonhosted.org/packages/db/60/1eeca2074f5b87df394fccaa432ae3fc06c9c9bfa97c5051aed70e6e00c2/regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c", size = 796857 },
    { url = "https://files.pythonhosted.org/packages/10/db/ac718a08fcee981554d2f7bb8402f1faa7e868c1345c16ab1ebec54b0d7b/regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3", size = 784006 },
    { url = "https://files.pythonhosted.org/packages/c2/41/7da3fe70216cea93144bf12da2b87367590bcf07db97604edeea55dac9ad/regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07", size = 781650 },
    { url = "https://files.pythonhosted.org/packages/a7/d5/880921ee4eec393a4752e6ab9f0fe28009435417c3102fc413f3fe81c4e5/regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e", size = 789545 },
    { url = "https://files.pythonhosted.org/packages/dc/96/53770115e507081122beca8899ab7f5ae28ae790bfcc82b5e38976df6a77/regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6", size = 853045 },
    { url = "https://files.pythonhosted.org/packages/31/d3/1372add5251cc2d44b451bd94f43b2ec78e15a6e82bff6a290ef9fd8f00a/regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4", size = 860182 },
    { url = "https://files.pythonhosted.org/packages/ed/e3/c446a64984ea9f69982ba1a69d4658d5014bc7a0ea468a07e1a1265db6e2/regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d", size = 787733 },
    { url = "https://files.pythonhosted.org/packages/2b/f1/e40c8373e3480e4f29f2692bd21b3e05f296d3afebc7e5dcf21b9756ca1c/regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff", size = 262122 },
    { url = "https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a", size = 273545 },
]

[[package]]
name = "requests"
version = "2.32.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/c9/74/b3ff8e6c8446842c3f5c837e9c3dfcfe2018ea6ecef224c710c85ef728f4/requests-2.32.5.tar.gz", hash = "sha256:dbba0bac56e100853db0ea71b82b4dfd5fe2bf6d3754a8893c3af500cec7d7cf", size = 134517 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/db/4254e3eabe8020b458f1a747140d32277ec7a271daf1d235b70dc0b4e6e3/requests-2.32.5-py3-none-any.whl", hash = "sha256:2462f94637a34fd532264295e186976db0f5d453d1cdd31473c85a6a161affb6", size = 64738 },
]

[[package]]
name = "ruff"
version = "0.11.5"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/45/71/5759b2a6b2279bb77fe15b1435b89473631c2cd6374d45ccdb6b785810be/ruff-0.11.5.tar.gz", hash = "sha256:cae2e2439cb88853e421901ec040a758960b576126dab520fa08e9de431d1bef", size = 3976488 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/23/db/6efda6381778eec7f35875b5cbefd194904832a1153d68d36d6b269d81a8/ruff-0.11.5-py3-none-linux_armv6l.whl", hash = "sha256:2561294e108eb648e50f210671cc56aee590fb6167b594144401532138c66c7b", size = 10103150 },
    { url = "https://files.pythonhosted.org/packages/44/f2/06cd9006077a8db61956768bc200a8e52515bf33a8f9b671ee527bb10d77/ruff-0.11.5-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:ac12884b9e005c12d0bd121f56ccf8033e1614f736f766c118ad60780882a077", size = 10898637 },
    { url = "https://files.pythonhosted.org/packages/18/f5/af390a013c56022fe6f72b95c86eb7b2585c89cc25d63882d3bfe411ecf1/ruff-0.11.5-py3-none-macosx_11_0_arm64.whl", hash = "sha256:4bfd80a6ec559a5eeb96c33f832418bf0fb96752de0539905cf7b0cc1d31d779", size = 10236012 },
    { url = "https://files.pythonhosted.org/packages/b8/ca/b9bf954cfed165e1a0c24b86305d5c8ea75def256707f2448439ac5e0d8b/ruff-0.11.5-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0947c0a1afa75dcb5db4b34b070ec2bccee869d40e6cc8ab25aca11a7d527794", size = 10415338 },
    { url = "https://files.pythonhosted.org/packages/d9/4d/2522dde4e790f1b59885283f8786ab0046958dfd39959c81acc75d347467/ruff-0.11.5-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:ad871ff74b5ec9caa66cb725b85d4ef89b53f8170f47c3406e32ef040400b038", size = 9965277 },
    { url = "https://files.pythonhosted.org/packages/e5/7a/749f56f150eef71ce2f626a2f6988446c620af2f9ba2a7804295ca450397/ruff-0.11.5-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e6cf918390cfe46d240732d4d72fa6e18e528ca1f60e318a10835cf2fa3dc19f", size = 11541614 },
    { url = "https://files.pythonhosted.org/packages/89/b2/7d9b8435222485b6aac627d9c29793ba89be40b5de11584ca604b829e960/ruff-0.11.5-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:56145ee1478582f61c08f21076dc59153310d606ad663acc00ea3ab5b2125f82", size = 12198873 },
    { url = "https://files.pythonhosted.org/packages/00/e0/a1a69ef5ffb5c5f9c31554b27e030a9c468fc6f57055886d27d316dfbabd/ruff-0.11.5-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e5f66f8f1e8c9fc594cbd66fbc5f246a8d91f916cb9667e80208663ec3728304", size = 11670190 },
    { url = "https://files.pythonhosted.org/packages/05/61/c1c16df6e92975072c07f8b20dad35cd858e8462b8865bc856fe5d6ccb63/ruff-0.11.5-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:80b4df4d335a80315ab9afc81ed1cff62be112bd165e162b5eed8ac55bfc8470", size = 13902301 },
    { url = "https://files.pythonhosted.org/packages/79/89/0af10c8af4363304fd8cb833bd407a2850c760b71edf742c18d5a87bb3ad/ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3068befab73620b8a0cc2431bd46b3cd619bc17d6f7695a3e1bb166b652c382a", size = 11350132 },
    { url = "https://files.pythonhosted.org/packages/b9/e1/ecb4c687cbf15164dd00e38cf62cbab238cad05dd8b6b0fc68b0c2785e15/ruff-0.11.5-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:f5da2e710a9641828e09aa98b92c9ebbc60518fdf3921241326ca3e8f8e55b8b", size = 10312937 },
    { url = "https://files.pythonhosted.org/packages/cf/4f/0e53fe5e500b65934500949361e3cd290c5ba60f0324ed59d15f46479c06/ruff-0.11.5-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:ef39f19cb8ec98cbc762344921e216f3857a06c47412030374fffd413fb8fd3a", size = 9936683 },
    { url = "https://files.pythonhosted.org/packages/04/a8/8183c4da6d35794ae7f76f96261ef5960853cd3f899c2671961f97a27d8e/ruff-0.11.5-py3-none-musllinux_1_2_i686.whl", hash = "sha256:b2a7cedf47244f431fd11aa5a7e2806dda2e0c365873bda7834e8f7d785ae159", size = 10950217 },
    { url = "https://files.pythonhosted.org/packages/26/88/9b85a5a8af21e46a0639b107fcf9bfc31da4f1d263f2fc7fbe7199b47f0a/ruff-0.11.5-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:81be52e7519f3d1a0beadcf8e974715b2dfc808ae8ec729ecfc79bddf8dbb783", size = 11404521 },
    { url = "https://files.pythonhosted.org/packages/fc/52/047f35d3b20fd1ae9ccfe28791ef0f3ca0ef0b3e6c1a58badd97d450131b/ruff-0.11.5-py3-none-win32.whl", hash = "sha256:e268da7b40f56e3eca571508a7e567e794f9bfcc0f412c4b607931d3af9c4afe", size = 10320697 },
    { url = "https://files.pythonhosted.org/packages/b9/fe/00c78010e3332a6e92762424cf4c1919065707e962232797d0b57fd8267e/ruff-0.11.5-py3-none-win_amd64.whl", hash = "sha256:6c6dc38af3cfe2863213ea25b6dc616d679205732dc0fb673356c2d69608f800", size = 11378665 },
    { url = "https://files.pythonhosted.org/packages/43/7c/c83fe5cbb70ff017612ff36654edfebec4b1ef79b558b8e5fd933bab836b/ruff-0.11.5-py3-none-win_arm64.whl", hash = "sha256:67e241b4314f4eacf14a601d586026a962f4002a475aa702c69980a38087aa4e", size = 10460287 },
]

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050 },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "typing-inspection"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz", hash = "sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122", size = 76222 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl", hash = "sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f", size = 14125 },
]

[[package]]
name = "tzdata"
version = "2025.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/95/32/1a225d6164441be760d75c2c42e2780dc0873fe382da3e98a2e1e48361e5/tzdata-2025.2.tar.gz", hash = "sha256:b60a638fcc0daffadf82fe0f57e53d06bdec2f36c4df66280ae79bce6bd6f2b9", size = 196380 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl", hash = "sha256:1a403fada01ff9221ca8044d701868fa132215d84beb92242d9acd2147f667a8", size = 347839 },
]

[[package]]
name = "tzlocal"
version = "5.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "tzdata", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/8b/2e/c14812d3d4d9cd1773c6be938f89e5735a1f11a9f184ac3639b93cef35d5/tzlocal-5.3.1.tar.gz", hash = "sha256:cceffc7edecefea1f595541dbd6e990cb1ea3d19bf01b2809f362a03dd7921fd", size = 30761 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c2/14/e2a54fabd4f08cd7af1c07030603c3356b74da07f7cc056e600436edfa17/tzlocal-5.3.1-py3-none-any.whl", hash = "sha256:eb1a66c3ef5847adf7a834f1be0800581b683b5608e74f86ecbcef8ab91bb85d", size = 18026 },
]

[[package]]
name = "urllib3"
version = "2.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8a/78/16493d9c386d8e60e442a35feac5e00f0913c0f4b7c217c11e8ec2ff53e0/urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466", size = 390672 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813", size = 128680 },
]

[[package]]
name = "werkzeug"
version = "3.1.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markupsafe" },
]
sdist = { url = "https://files.pythonhosted.org/packages/45/ea/b0f8eeb287f8df9066e56e831c7824ac6bab645dd6c7a8f4b2d767944f9b/werkzeug-3.1.4.tar.gz", hash = "sha256:cd3cd98b1b92dc3b7b3995038826c68097dcb16f9baa63abe35f20eafeb9fe5e", size = 864687 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2f/f9/9e082990c2585c744734f85bec79b5dae5df9c974ffee58fe421652c8e91/werkzeug-3.1.4-py3-none-any.whl", hash = "sha256:2ad50fb9ed09cc3af22c54698351027ace879a0b60a3b5edf5730b2f7d876905", size = 224960 },
]

[[package]]
name = "wrapt"
version = "1.17.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c3/fc/e91cc220803d7bc4db93fb02facd8461c37364151b8494762cc88b0fbcef/wrapt-1.17.2.tar.gz", hash = "sha256:41388e9d4d1522446fe79d3213196bd9e3b301a336965b9e27ca2788ebd122f3", size = 55531 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/b9/0ffd557a92f3b11d4c5d5e0c5e4ad057bd9eb8586615cdaf901409920b14/wrapt-1.17.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:6ed6ffac43aecfe6d86ec5b74b06a5be33d5bb9243d055141e8cabb12aa08125", size = 53800 },
    { url = "https://files.pythonhosted.org/packages/c0/ef/8be90a0b7e73c32e550c73cfb2fa09db62234227ece47b0e80a05073b375/wrapt-1.17.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:35621ae4c00e056adb0009f8e86e28eb4a41a4bfa8f9bfa9fca7d343fe94f998", size = 38824 },
    { url = "https://files.pythonhosted.org/packages/36/89/0aae34c10fe524cce30fe5fc433210376bce94cf74d05b0d68344c8ba46e/wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a604bf7a053f8362d27eb9fefd2097f82600b856d5abe996d623babd067b1ab5", size = 38920 },
    { url = "https://files.pythonhosted.org/packages/3b/24/11c4510de906d77e0cfb5197f1b1445d4fec42c9a39ea853d482698ac681/wrapt-1.17.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5cbabee4f083b6b4cd282f5b817a867cf0b1028c54d445b7ec7cfe6505057cf8", size = 88690 },
    { url = "https://files.pythonhosted.org/packages/71/d7/cfcf842291267bf455b3e266c0c29dcb675b5540ee8b50ba1699abf3af45/wrapt-1.17.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:49703ce2ddc220df165bd2962f8e03b84c89fee2d65e1c24a7defff6f988f4d6", size = 80861 },
    { url = "https://files.pythonhosted.org/packages/d5/66/5d973e9f3e7370fd686fb47a9af3319418ed925c27d72ce16b791231576d/wrapt-1.17.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8112e52c5822fc4253f3901b676c55ddf288614dc7011634e2719718eaa187dc", size = 89174 },
    { url = "https://files.pythonhosted.org/packages/a7/d3/8e17bb70f6ae25dabc1aaf990f86824e4fd98ee9cadf197054e068500d27/wrapt-1.17.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9fee687dce376205d9a494e9c121e27183b2a3df18037f89d69bd7b35bcf59e2", size = 86721 },
    { url = "https://files.pythonhosted.org/packages/6f/54/f170dfb278fe1c30d0ff864513cff526d624ab8de3254b20abb9cffedc24/wrapt-1.17.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:18983c537e04d11cf027fbb60a1e8dfd5190e2b60cc27bc0808e653e7b218d1b", size = 79763 },
    { url = "https://files.pythonhosted.org/packages/4a/98/de07243751f1c4a9b15c76019250210dd3486ce098c3d80d5f729cba029c/wrapt-1.17.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:703919b1633412ab54bcf920ab388735832fdcb9f9a00ae49387f0fe67dad504", size = 87585 },
    { url = "https://files.pythonhosted.org/packages/f9/f0/13925f4bd6548013038cdeb11ee2cbd4e37c30f8bfd5db9e5a2a370d6e20/wrapt-1.17.2-cp313-cp313-win32.whl", hash = "sha256:abbb9e76177c35d4e8568e58650aa6926040d6a9f6f03435b7a522bf1c487f9a", size = 36676 },
    { url = "https://files.pythonhosted.org/packages/bf/ae/743f16ef8c2e3628df3ddfd652b7d4c555d12c84b53f3d8218498f4ade9b/wrapt-1.17.2-cp313-cp313-win_amd64.whl", hash = "sha256:69606d7bb691b50a4240ce6b22ebb319c1cfb164e5f6569835058196e0f3a845", size = 38871 },
    { url = "https://files.pythonhosted.org/packages/3d/bc/30f903f891a82d402ffb5fda27ec1d621cc97cb74c16fea0b6141f1d4e87/wrapt-1.17.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:4a721d3c943dae44f8e243b380cb645a709ba5bd35d3ad27bc2ed947e9c68192", size = 56312 },
    { url = "https://files.pythonhosted.org/packages/8a/04/c97273eb491b5f1c918857cd26f314b74fc9b29224521f5b83f872253725/wrapt-1.17.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:766d8bbefcb9e00c3ac3b000d9acc51f1b399513f44d77dfe0eb026ad7c9a19b", size = 40062 },
    { url = "https://files.pythonhosted.org/packages/4e/ca/3b7afa1eae3a9e7fefe499db9b96813f41828b9fdb016ee836c4c379dadb/wrapt-1.17.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:e496a8ce2c256da1eb98bd15803a79bee00fc351f5dfb9ea82594a3f058309e0", size = 40155 },
    { url = "https://files.pythonhosted.org/packages/89/be/7c1baed43290775cb9030c774bc53c860db140397047cc49aedaf0a15477/wrapt-1.17.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d615e4fe22f4ad3528448c193b218e077656ca9ccb22ce2cb20db730f8d306", size = 113471 },
    { url = "https://files.pythonhosted.org/packages/32/98/4ed894cf012b6d6aae5f5cc974006bdeb92f0241775addad3f8cd6ab71c8/wrapt-1.17.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a5aaeff38654462bc4b09023918b7f21790efb807f54c000a39d41d69cf552cb", size = 101208 },
    { url = "https://files.pythonhosted.org/packages/ea/fd/0c30f2301ca94e655e5e057012e83284ce8c545df7661a78d8bfca2fac7a/wrapt-1.17.2-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9a7d15bbd2bc99e92e39f49a04653062ee6085c0e18b3b7512a4f2fe91f2d681", size = 109339 },
    { url = "https://files.pythonhosted.org/packages/75/56/05d000de894c4cfcb84bcd6b1df6214297b8089a7bd324c21a4765e49b14/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:e3890b508a23299083e065f435a492b5435eba6e304a7114d2f919d400888cc6", size = 110232 },
    { url = "https://files.pythonhosted.org/packages/53/f8/c3f6b2cf9b9277fb0813418e1503e68414cd036b3b099c823379c9575e6d/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:8c8b293cd65ad716d13d8dd3624e42e5a19cc2a2f1acc74b30c2c13f15cb61a6", size = 100476 },
    { url = "https://files.pythonhosted.org/packages/a7/b1/0bb11e29aa5139d90b770ebbfa167267b1fc548d2302c30c8f7572851738/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:4c82b8785d98cdd9fed4cac84d765d234ed3251bd6afe34cb7ac523cb93e8b4f", size = 106377 },
    { url = "https://files.pythonhosted.org/packages/6a/e1/0122853035b40b3f333bbb25f1939fc1045e21dd518f7f0922b60c156f7c/wrapt-1.17.2-cp313-cp313t-win32.whl", hash = "sha256:13e6afb7fe71fe7485a4550a8844cc9ffbe263c0f1a1eea569bc7091d4898555", size = 37986 },
    { url = "https://files.pythonhosted.org/packages/09/5e/1655cf481e079c1f22d0cabdd4e51733679932718dc23bf2db175f329b76/wrapt-1.17.2-cp313-cp313t-win_amd64.whl", hash = "sha256:eaf675418ed6b3b31c7a989fd007fa7c3be66ce14e5c3b27336383604c9da85c", size = 40750 },
    { url = "https://files.pythonhosted.org/packages/2d/82/f56956041adef78f849db6b289b282e72b55ab8045a75abad81898c28d19/wrapt-1.17.2-py3-none-any.whl", hash = "sha256:b18f2d1533a71f069c7f82d524a52599053d4c7166e9dd374ae2136b7f40f7c8", size = 23594 },
]



================================================
FILE: component_config/component_long_description.md
================================================
This component extracts data from the Daktela CRM/Contact Center API v6 and loads it into Keboola Storage tables.

### Supported Endpoints

The component can extract data from the following Daktela API endpoints:

- **accounts** - Account information
- **activities** - General activity records
- **activitiesCall** - Phone call activities
- **activitiesChat** - Chat conversation activities
- **activitiesEmail** - Email activities
- **campaignsRecords** - Campaign records
- **contacts** - Customer and contact information
- **crmRecords** - CRM records
- **groups** - User groups
- **pauses** - Pause records
- **queues** - Queue information
- **statuses** - Status definitions
- **templates** - Template configurations
- **tickets** - Support tickets and cases
- **users** - User accounts

### Key Features

- **Incremental loading** with automatic state management for efficient data synchronization
- **Date range filtering** to extract data for specific time periods
- **Parallel extraction** of multiple endpoints with configurable concurrency limits


================================================
FILE: component_config/component_short_description.md
================================================
Daktela CRM and Contact Center platform


================================================
FILE: component_config/configRowSchema.json
================================================
{
  "type": "object",
  "title": "Endpoint Extraction Configuration",
  "required": [
    "endpoint",
    "date_from",
    "date_to"
  ],
  "properties": {
    "endpoint": {
      "type": "string",
      "title": "Endpoint",
      "description": "Select the API endpoint to extract",
      "enum": [
        "accounts",
        "activities",
        "activitiesCall",
        "activitiesChat",
        "activitiesEmail",
        "campaignsRecords",
        "contacts",
        "crmRecords",
        "queues",
        "statuses",
        "tickets",
        "users",
        "groups",
        "pauses",
        "templates"
      ],
      "propertyOrder": 100
    },
    "date_from": {
      "type": "string",
      "title": "Date From",
      "description": "Start date for extraction. Supported formats: 'today', 'yesterday', '5 hours ago', '3 days ago', '4 months ago', '2 years ago'. Used as filter for tickets, contacts, and activities endpoints.",
      "default": "7 days ago",
      "propertyOrder": 200
    },
    "date_to": {
      "type": "string",
      "title": "Date To",
      "description": "End date for extraction. Supported formats: 'today', 'yesterday', '5 hours ago', '3 days ago', '4 months ago', '2 years ago'. Used as filter for tickets, contacts, and activities endpoints.",
      "default": "today",
      "propertyOrder": 300
    },
    "discover_fields": {
      "type": "button",
      "format": "sync-action",
      "title": "Discover Available Fields",
      "options": {
        "async": {
          "label": "Discover Available Fields",
          "action": "listFields",
          "description": "Click to discover all available fields for this endpoint. The result will help you fill in the 'Fields to Extract' below."
        }
      },
      "propertyOrder": 400
    },
    "fields": {
      "type": "array",
      "title": "Fields to Extract",
      "description": "Specify which fields to extract for this endpoint (leave empty to extract all fields). You can type field names manually or use the 'Discover Available Fields' button above.",
      "format": "select",
      "uniqueItems": true,
      "items": {
        "type": "string"
      },
      "options": {
        "tags": true
      },
      "propertyOrder": 500
    },
    "destination": {
      "type": "object",
      "title": "Destination",
      "format": "group",
      "propertyOrder": 600,
      "properties": {
        "load_type": {
          "type": "string",
          "title": "Load Type",
          "enum": ["full_load", "incremental_load"],
          "default": "full_load",
          "description": "Full load replaces existing data. Incremental load appends new data.",
          "propertyOrder": 1
        },
        "primary_key": {
          "type": "array",
          "title": "Primary Key",
          "description": "Column(s) used as primary key for deduplication. Required for incremental load.",
          "format": "select",
          "uniqueItems": true,
          "items": {
            "type": "string"
          },
          "options": {
            "tags": true,
            "dependencies": {
              "load_type": "incremental_load"
            }
          },
          "propertyOrder": 2
        }
      }
    }
  }
}



================================================
FILE: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Daktela Extractor Configuration",
  "required": [
    "connection"
  ],
  "properties": {
    "connection": {
      "type": "object",
      "title": "Connection",
      "required": [
        "url",
        "username",
        "#password"
      ],
      "properties": {
        "url": {
          "type": "string",
          "title": "URL",
          "description": "Daktela base URL (e.g., 'https://mycompany.daktela.com')",
          "propertyOrder": 1
        },
        "username": {
          "type": "string",
          "title": "Username",
          "description": "Daktela account username",
          "propertyOrder": 2
        },
        "#password": {
          "type": "string",
          "title": "Password",
          "description": "Daktela account password (encrypted)",
          "format": "password",
          "propertyOrder": 3
        },
        "verify_ssl": {
          "type": "boolean",
          "title": "Verify SSL Certificates",
          "description": "Enable SSL certificate verification. Disable only for testing with self-signed certificates. WARNING: Disabling SSL verification is insecure.",
          "format": "checkbox",
          "default": true,
          "propertyOrder": 4
        }
      },
      "propertyOrder": 100
    },
    "advanced": {
      "type": "object",
      "title": "Advanced Settings",
      "description": "Advanced performance settings. Adjust these if you experience timeouts or memory issues.",
      "options": {
        "collapsed": true
      },
      "properties": {
        "batch_size": {
          "type": "integer",
          "title": "Batch Size",
          "description": "Number of records to request per API page and to flush to CSV in one chunk. Note: Daktela API has a maximum page size of 1000, so values above 1000 will be automatically capped.",
          "default": 1000,
          "minimum": 100,
          "maximum": 1000,
          "propertyOrder": 1
        },
        "max_concurrent_requests": {
          "type": "integer",
          "title": "Max Concurrent Requests",
          "description": "Maximum number of concurrent API requests. Controls how many HTTP calls can run in parallel. Increase for faster extraction, decrease if experiencing rate limiting.",
          "default": 10,
          "minimum": 1,
          "maximum": 50,
          "propertyOrder": 2
        }
      },
      "propertyOrder": 200
    }
  }
}



================================================
FILE: component_config/configuration_description.md
================================================
Configuration description.


================================================
FILE: component_config/documentationUrl.md
================================================
https://github.com/keboola/component-daktela/blob/master/README.md


================================================
FILE: component_config/licenseUrl.md
================================================
https://github.com/keboola/component-daktela/blob/master/LICENSE.md


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sourceCodeUrl.md
================================================
https://github.com/keboola/component-daktela


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": []
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "connection": {
      "url": "https://demo.daktela.com",
      "username": "test_user",
      "#password": "test_password",
      "verify_ssl": true
    },
    "destination": {
      "incremental": false
    },
    "advanced": {
      "batch_size": 1000,
      "max_concurrent_requests": 10,
      "max_concurrent_endpoints": 3
    },
    "debug": true
  },
  "image_parameters": [
    {
      "endpoint": "contacts",
      "date_from": "7 days ago",
      "date_to": "today",
      "fields": ["name", "email", "phone"]
    },
    {
      "endpoint": "activities",
      "date_from": "3 days ago",
      "date_to": "today"
    },
    {
      "endpoint": "tickets",
      "date_from": "7 days ago",
      "date_to": "today",
      "fields": ["title", "status", "priority", "created"]
    }
  ]
}



================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover



================================================
FILE: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi


================================================
FILE: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"


================================================
FILE: src/component.py
================================================
"""
Daktela Extractor Component main class.
"""

import asyncio
import csv
import logging
import sys
import traceback
from datetime import datetime, timezone
from typing import Any

import keboola.utils
from keboola.component.base import ComponentBase, sync_action
from keboola.component.exceptions import UserException

from configuration import Configuration
from daktela_client import DaktelaApiClient
from extractor import DaktelaExtractor


class Component(ComponentBase):
    """
    Daktela Extractor Component.

    Extracts data from Daktela CRM/Contact Center API v6 and produces CSV outputs
    compatible with Keboola storage.
    """

    def __init__(self) -> None:
        super().__init__()
        self.config: Configuration | None = None
        self._table_definitions: dict[str, Any] = {}
        self._schema_state: dict[str, Any] = {}

    def run(self) -> None:
        """Main execution - orchestrates the component workflow."""
        try:
            # Load and validate merged configuration (root + row merged by platform)
            self.config = self._load_configuration()

            # Load schema state from previous runs
            self._load_schema_state()

            # Run async extraction
            asyncio.run(self._run_async_extraction())

            # Save updated schema state
            self._save_schema_state()

            logging.info("Daktela extraction completed successfully")

        except UserException as err:
            logging.error(f"Configuration/API error: {err}")
            print(err, file=sys.stderr)
            sys.exit(1)

        except Exception:
            logging.exception("Unhandled error in component execution")
            traceback.print_exc(file=sys.stderr)
            sys.exit(2)

    def _load_schema_state(self) -> None:
        """Load schema state from previous runs."""
        state = self.get_state_file()
        self._schema_state = state.get("schema", {})
        if self._schema_state:
            logging.info(f"Loaded schema state for {len(self._schema_state)} endpoints")

    def _save_schema_state(self) -> None:
        """Save schema state for future runs."""
        state = self.get_state_file()
        state["schema"] = self._schema_state
        state["last_updated"] = datetime.now(timezone.utc).isoformat()
        self.write_state_file(state)
        logging.info(f"Saved schema state for {len(self._schema_state)} endpoints")

    def get_schema_for_endpoint(self, endpoint: str) -> list[str] | None:
        """Get stored schema (columns) for an endpoint."""
        endpoint_schema = self._schema_state.get(endpoint)
        if endpoint_schema:
            return endpoint_schema.get("columns")
        return None

    def update_schema_for_endpoint(self, endpoint: str, columns: list[str]) -> None:
        """Update stored schema for an endpoint."""
        self._schema_state[endpoint] = {
            "columns": columns,
            "last_updated": datetime.now(timezone.utc).isoformat(),
        }

    @sync_action("listFields")
    def list_fields(self) -> dict[str, Any]:
        """
        Sync action to list available fields for the current row's endpoint.

        Platform passes merged config (root + row) in configuration.parameters.
        Returns a dictionary with the endpoint name and its available fields.
        """
        # Load merged configuration (root + row fields)
        try:
            config = Configuration.from_dict(self.configuration.parameters)
        except Exception as e:
            logging.error(f"Failed to load configuration for sync action: {e}")
            return {"error": f"Invalid configuration: {e}"}

        # Store for use by API client initialization
        self.config = config

        logging.info(f"Running listFields sync action for endpoint: {config.endpoint}")

        # Run async field discovery for this endpoint
        result = asyncio.run(self._discover_fields_async(config.endpoint))

        logging.info(f"Discovered {len(result)} fields for {config.endpoint}")
        return {config.endpoint: result}

    async def _discover_fields_async(self, endpoint: str) -> list[str]:
        """Discover available fields for a single endpoint."""
        async with self._initialize_api_client() as api_client:
            try:
                fields = await self._get_endpoint_fields(api_client, endpoint)
                logging.info(f"Discovered {len(fields)} fields for {endpoint}")
                return fields
            except Exception as e:
                logging.warning(f"Failed to discover fields for {endpoint}: {e}")
                return []

    async def _get_endpoint_fields(
        self, api_client: DaktelaApiClient, endpoint: str
    ) -> list[str]:
        """Get available fields for a single endpoint by fetching a sample record."""
        # Fetch just one record to discover fields
        async for page in api_client.fetch_table_data_batched(
            table_name=endpoint,
            endpoint=endpoint,
            batch_size=1,
        ):
            if page and len(page) > 0:
                # Extract field names from the first record
                return sorted(page[0].keys())
            break

        return []

    async def _run_async_extraction(self) -> None:
        """Run the async extraction process."""
        if not self.config:
            raise UserException("No configuration available for extraction")

        # Use async context manager for API client (auth happens in __init__)
        async with self._initialize_api_client() as api_client:
            logging.info(f"Processing endpoint: {self.config.endpoint}")
            extractor = self._create_extractor(api_client)
            await extractor.extract_all()

    def _load_configuration(self) -> Configuration:
        """
        Load and validate merged configuration.

        Platform merges root config (from configSchema.json) with row config
        (from configRowSchema.json) into self.configuration.parameters before
        running the component.
        """
        config = Configuration.from_dict(self.configuration.parameters)

        logging.info(
            f"Starting Daktela extraction from {config.connection.url}, "
            f"endpoint={config.endpoint}, "
            f"date_from={config.date_from}, date_to={config.date_to}, "
            f"load_type={config.destination.load_type}"
        )

        return config

    def _initialize_api_client(self) -> DaktelaApiClient:
        """Initialize and return configured API client (authenticates during init)."""
        config = self._require_config()
        return DaktelaApiClient(
            url=config.connection.url,
            username=config.connection.username,
            password=config.connection.password,
            max_concurrent=config.advanced.max_concurrent_requests,
            verify_ssl=config.connection.verify_ssl,
        )

    def _create_extractor(
        self,
        api_client: DaktelaApiClient,
    ) -> DaktelaExtractor:
        """Create and configure the extractor."""
        config = self._require_config()

        # Parse dates using keboola utils
        from_datetime = keboola.utils.get_past_date(
            config.date_from
        ).strftime("%Y-%m-%d %H:%M:%S")
        to_datetime = keboola.utils.get_past_date(
            config.date_to
        ).strftime("%Y-%m-%d %H:%M:%S")

        # Build table config for this endpoint
        endpoint = config.endpoint
        table_configs = {}

        # Use primary_key from config if set, otherwise use defaults
        if config.destination.primary_key:
            primary_keys = config.destination.primary_key
        elif endpoint == "activitiesCall":
            primary_keys = ["id_call"]
        else:
            primary_keys = ["name"]

        table_configs[endpoint] = {"primary_keys": primary_keys}

        # Prepare configured fields dict (only for this endpoint)
        configured_fields = {}
        if config.fields:
            configured_fields[endpoint] = config.fields

        return DaktelaExtractor(
            api_client=api_client,
            table_configs=table_configs,
            component=self,
            url=config.connection.url,
            requested_endpoints=[endpoint],
            batch_size=config.advanced.batch_size,
            date_from=from_datetime,
            date_to=to_datetime,
            incremental=config.destination.incremental,
            configured_fields=configured_fields if configured_fields else None,
        )

    def write_table_data(
        self,
        table_name: str,
        records: list[dict[str, Any]],
        table_config: dict[str, Any],
        columns: list[str],
        incremental: bool = False,
    ) -> None:
        """
        Write table data using create_out_table_definition and write_manifest pattern.

        Args:
            table_name: Name of the output table (e.g., "server_tablename.csv")
            records: List of records to write
            table_config: Table configuration dict
            incremental: Whether to use incremental mode
            columns: List of column names
        """
        table_definitions = self._get_table_definitions()

        # Create table definition on first write
        if table_name not in table_definitions:
            out_table = self.create_out_table_definition(
                table_name,
                columns=columns,
                primary_key=table_config.get("primary_keys"),
                incremental=incremental,
                has_header=True,
            )

            table_definitions[table_name] = out_table

            # Write header
            with open(out_table.full_path, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=columns)
                writer.writeheader()

            logging.info(
                f"Created table definition for {table_name} with {len(columns)} columns"
            )

        # Get table definition
        out_table = table_definitions.get(table_name)

        if not out_table:
            raise UserException(
                f"Table definition not found for {table_name}. This should not happen."
            )

        # Append records
        if records:
            with open(out_table.full_path, "a", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=columns, extrasaction="ignore")
                for record in records:
                    # Ensure all columns are present
                    row = {col: record.get(col) for col in columns}
                    writer.writerow(row)

            logging.info(f"Wrote {len(records)} records to {table_name}")

    def finalize_table(self, table_name: str) -> None:
        """
        Finalize table by writing manifest.

        Args:
            table_name: Name of the output table
        """
        out_table = self._get_table_definitions().get(table_name)

        if out_table:
            self.write_manifest(out_table)
            logging.info(f"Wrote manifest for {table_name}")
        else:
            logging.warning(
                f"No table definition found for {table_name}, skipping manifest"
            )

    def _get_table_definitions(self) -> dict[str, Any]:
        """Return initialized table definitions container."""
        if not hasattr(self, "_table_definitions"):
            self._table_definitions = {}
        return self._table_definitions

    def _require_config(self) -> Configuration:
        """Return initialized configuration or raise if missing."""
        if not self.config:
            raise UserException("Component configuration is not initialized.")
        return self.config


"""
Main entrypoint
"""
if __name__ == "__main__":
    comp = Component()
    # this triggers the run method by default and is controlled by the configuration.action parameter
    # Error handling is done in the run() method
    comp.execute_action()



================================================
FILE: src/configuration.py
================================================
import logging

from keboola.component.exceptions import UserException
from pydantic import BaseModel, ConfigDict, Field, ValidationError, field_validator, model_validator

DEFAULT_MAX_CONCURRENT_REQUESTS = (
    10  # Default maximum number of concurrent API requests
)
DEFAULT_BATCH_SIZE = (
    1000  # Default batch size for processing records before writing to CSV
)


class Connection(BaseModel):
    """Connection configuration."""

    url: str
    username: str
    password: str = Field(alias="#password")
    verify_ssl: bool = True


class Destination(BaseModel):
    """Destination configuration for load type and primary key."""

    load_type: str = "full_load"  # "full_load" or "incremental_load"
    primary_key: list[str] | None = None

    @property
    def incremental(self) -> bool:
        """Convert load_type to boolean for backward compatibility."""
        return self.load_type == "incremental_load"


class RowConfiguration(BaseModel):
    """Row configuration for a single endpoint extraction."""

    endpoint: str
    date_from: str
    date_to: str
    fields: list[str] | None = None
    destination: Destination = Field(default_factory=Destination)

    @classmethod
    def from_dict(cls, data: dict) -> "RowConfiguration":
        """Create RowConfiguration from dict with user-friendly error messages."""
        try:
            return cls(**data)
        except ValidationError as e:
            error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
            raise UserException(f"Row validation error: {', '.join(error_messages)}")


class Advanced(BaseModel):
    """Advanced performance configuration."""

    batch_size: int = DEFAULT_BATCH_SIZE
    max_concurrent_requests: int = DEFAULT_MAX_CONCURRENT_REQUESTS

    @field_validator("batch_size")
    @classmethod
    def validate_batch_size(cls, v: int) -> int:
        """Validate batch size is positive."""
        if v <= 0:
            raise ValueError("Batch size must be a positive integer.")
        return v


class Configuration(BaseModel):
    """
    Merged configuration combining root config and row config.

    Platform merges root config (from configSchema.json) with row config
    (from configRowSchema.json) into self.configuration.parameters.

    This class reads from the merged parameters and includes both:
    - Root config fields: connection, advanced, debug
    - Row config fields: endpoint, date_from, date_to, fields, destination
    """

    model_config = ConfigDict(extra="ignore")  # Ignore unknown fields

    # Root config fields (from configSchema.json)
    connection: Connection
    advanced: Advanced = Field(default_factory=Advanced)
    debug: bool = False

    # Row config fields (from configRowSchema.json)
    endpoint: str
    date_from: str
    date_to: str
    fields: list[str] | None = None
    destination: Destination = Field(default_factory=Destination)

    @model_validator(mode="after")
    def log_debug_mode(self) -> "Configuration":
        """Log if debug mode is enabled."""
        if self.debug:
            logging.debug("Component will run in Debug mode")
        return self

    @classmethod
    def from_dict(cls, data: dict) -> "Configuration":
        """Create Configuration from dict with user-friendly error messages."""
        try:
            return cls(**data)
        except ValidationError as e:
            error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
            raise UserException(f"Validation Error: {', '.join(error_messages)}")



================================================
FILE: src/daktela_client.py
================================================
"""
Async HTTP client for Daktela API with authentication and pagination.
Retry logic is handled by the Keboola AsyncHttpClient.
"""

import asyncio
import logging
import warnings
from collections.abc import AsyncIterator
from typing import Any

import httpx
import requests
from keboola.component.exceptions import UserException
from keboola.http_client import AsyncHttpClient

from configuration import DEFAULT_BATCH_SIZE, DEFAULT_MAX_CONCURRENT_REQUESTS

# API Client constants
DEFAULT_PAGE_LIMIT = 1000
"""Default number of records to fetch per API request."""

AUTH_TIMEOUT_SECONDS = 30
"""Timeout for authentication requests."""

MAX_AUTH_RETRIES = 2
"""Maximum number of authentication retry attempts."""

# Endpoints that support date filtering via filter[field]=edited
FILTER_PAGINATED_ENDPOINTS = {"tickets", "contacts"}
"""Endpoints that support filtering on the 'edited' field."""

# Activities endpoints and their time-like filter fields
ACTIVITIES_FILTER_FIELDS = {
    "activities": "time",
    "activitiesCall": "call_time",
    "activitiesChat": "time",
    "activitiesEmail": "time",
}
"""Endpoints that should be filtered on a time field, mapped per endpoint."""


class DaktelaApiClient:
    """Async HTTP client for Daktela API with built-in authentication and pagination."""

    def __init__(
        self,
        url: str,
        username: str,
        password: str,
        max_concurrent: int = DEFAULT_MAX_CONCURRENT_REQUESTS,
        verify_ssl: bool = True,
    ):
        """
        Initialize API client and authenticate.

        Args:
            url: Base URL for Daktela API
            username: Daktela account username
            password: Daktela account password
            max_concurrent: Maximum concurrent requests
            verify_ssl: Whether to verify SSL certificates (default: True)
        """
        self.url = url
        self.username = username
        self.password = password
        self.max_concurrent = max_concurrent
        self.verify_ssl = verify_ssl
        self.client = None  # Will be initialized in __aenter__
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self._token_lock = asyncio.Lock()  # Lock for thread-safe token refresh
        self._token_version = 0  # Track token version to prevent redundant refreshes

        # Authenticate synchronously during initialization
        self.access_token = self._authenticate()

    def _authenticate(self) -> str:
        """
        Authenticate with Daktela API and retrieve access token.

        Returns:
            str: Access token for subsequent API requests

        Raises:
            UserException: If authentication fails or connection error occurs
        """
        login_url = f"{self.url}/api/v6/login.json"
        params = {"username": self.username, "password": self.password, "only_token": 1}

        try:
            logging.info(f"Attempting to authenticate with Daktela API at {self.url}")
            if not self.verify_ssl:
                warnings.filterwarnings("ignore", message="Unverified HTTPS request")
                logging.warning(
                    "SSL verification is disabled for authentication. This is insecure."
                )

            response = requests.post(
                login_url,
                params=params,
                verify=self.verify_ssl,
                timeout=AUTH_TIMEOUT_SECONDS,
            )

            # Check for successful response
            if response.status_code != 200:
                raise UserException(
                    f"Invalid response from Daktela API. Status code: {response.status_code}. "
                    f"Response: {response.text[:200]}"
                )

            # Parse response
            try:
                result = response.json()
            except Exception as e:
                raise UserException(
                    f"Failed to parse authentication response: {str(e)}"
                )

            # Extract access token
            if "result" not in result or not result["result"]:
                raise UserException(
                    f"Invalid token in authentication response. Response: {response.text[:200]}"
                )

            # Extract just the accessToken string from the result object
            token_data = result["result"]
            if isinstance(token_data, dict) and "accessToken" in token_data:
                access_token = token_data["accessToken"]
            else:
                # Fallback for older API versions that might return token directly
                access_token = token_data

            logging.info("Successfully authenticated with Daktela API")
            self._token_version += 1  # Increment version on successful auth

            return access_token

        except requests.exceptions.ConnectionError as e:
            raise UserException(
                f"Server not responding. Failed to connect to {self.url}: {str(e)}"
            )
        except requests.exceptions.Timeout as e:
            raise UserException(
                f"Connection timeout when connecting to {self.url}: {str(e)}"
            )
        except requests.exceptions.RequestException as e:
            raise UserException(f"Request failed: {str(e)}")

    async def _refresh_token(self, old_version: int) -> None:
        """
        Refresh the access token with double-check locking.

        Args:
            old_version: Token version that was invalid (for double-check)
        """
        async with self._token_lock:
            # Double-check: if another request already refreshed the token, skip
            if self._token_version > old_version:
                logging.debug("Token already refreshed by another request")
                return

            logging.info("Refreshing access token...")
            self.access_token = self._authenticate()
            logging.info("Access token refreshed successfully")

    async def __aenter__(self):
        """Async context manager entry."""
        self.client = AsyncHttpClient(self.url, verify_ssl=self.verify_ssl)
        await self.client.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.client:
            await self.client.__aexit__(exc_type, exc_val, exc_tb)

    def _prepare_endpoint(self, endpoint: str) -> str:
        """Ensure endpoint includes api prefix and .json suffix."""
        cleaned = endpoint.lstrip("/")
        if not cleaned.endswith(".json"):
            cleaned = f"{cleaned}.json"
        if not cleaned.startswith("api/"):
            cleaned = f"api/v6/{cleaned}"
        return cleaned

    async def fetch_table_data_batched(
        self,
        table_name: str,
        date_from: str | None = None,
        date_to: str | None = None,
        limit: int = DEFAULT_PAGE_LIMIT,
        batch_size: int = DEFAULT_BATCH_SIZE,
        endpoint: str | None = None,
        fields: list[str] | None = None,
    ) -> AsyncIterator[list[dict[str, Any]]]:
        """
        Fetch data for a table in pages (generator for memory efficiency).

        For endpoints that support filtering (tickets, contacts, activities),
        applies date range filter on 'edited' field.

        Filtering modes:
        - One date only: Simple format filter[field]=edited&filter[operator]=gte&filter[value]=date
        - Both dates: Complex JSON format {"logic":"and","filters":[...]}

        Complex filter example structure:
        {
            "filter": {
                "logic": "or",
                "filters": [
                    {"field": "firstname", "operator": "eq", "value": "John"},
                    {"field": "firstname", "operator": "eq", "value": "James"},
                    {
                        "logic": "and",
                        "filters": [
                            {"field": "firstname", "operator": "eq", "value": "David"},
                            {"field": "lastname", "operator": "eq", "value": "Smith"}
                        ]
                    }
                ]
            }
        }

        Args:
            table_name: Name of the table to fetch
            date_from: Start date (edited >= date_from)
            date_to: End date (edited <= date_to)
            limit: Number of records per page (default: 1000). Kept for backward compatibility.
            batch_size: Configured batch size, used as API page size when provided.
            endpoint: Optional endpoint override
            fields: Optional list of field names to fetch from the API

        Yields:
            Pages of records from the API (up to 'page_limit' records per page)
        """
        base_limit = batch_size or limit or DEFAULT_PAGE_LIMIT
        if base_limit <= 0:
            raise UserException("Batch size must be a positive integer.")

        page_limit = min(base_limit, DEFAULT_PAGE_LIMIT)
        if page_limit != base_limit:
            logging.info(
                f"Batch size {base_limit} exceeds API page limit {DEFAULT_PAGE_LIMIT}; capping to {page_limit}"
            )

        endpoint_path = self._prepare_endpoint(endpoint or table_name)
        params = {"accessToken": self.access_token}

        # Add fields parameter if specified
        if fields:
            params["fields"] = ",".join(fields)

        # Apply date filtering for supported endpoints
        if (
            table_name in FILTER_PAGINATED_ENDPOINTS
            or table_name in ACTIVITIES_FILTER_FIELDS
        ):
            filters = []
            if table_name in FILTER_PAGINATED_ENDPOINTS:
                filter_field = "edited"
            else:
                filter_field = ACTIVITIES_FILTER_FIELDS[table_name]

            if date_from:
                filters.append(
                    {"field": filter_field, "operator": "gte", "value": date_from}
                )

            if date_to:
                filters.append(
                    {"field": filter_field, "operator": "lte", "value": date_to}
                )

            if len(filters) == 2:
                # Both dates: use URL parameter format (not JSON format)
                # Format: filter[0][field]=edited&filter[0][operator]=gte&filter[0][value]=...
                #         filter[1][field]=edited&filter[1][operator]=lte&filter[1][value]=...
                logging.info(f"Date filter for {table_name}: {date_from} to {date_to}")
                for i, f in enumerate(filters):
                    params[f"filter[{i}][field]"] = f["field"]
                    params[f"filter[{i}][operator]"] = f["operator"]
                    params[f"filter[{i}][value]"] = f["value"]
            elif len(filters) == 1:
                # Single date: use simple format
                f = filters[0]
                logging.info(
                    f"Date filter for {table_name}: {f['field']} {f['operator']} {f['value']}"
                )
                params["filter[field]"] = f["field"]
                params["filter[operator]"] = f["operator"]
                params["filter[value]"] = f["value"]

        # First, get total count
        params_count = params.copy()
        params_count["skip"] = 0
        params_count["take"] = 1

        logging.info(f"Fetching total count for table: {table_name}")
        try:
            first_response = await self.client.get(endpoint_path, params=params_count)
        except httpx.HTTPStatusError as exc:
            # Some activities sub-endpoints reject filters; retry without filters
            if (
                exc.response is not None
                and exc.response.status_code == 400
                and table_name in ACTIVITIES_FILTER_FIELDS
                and any(key.startswith("filter[") for key in params_count)
            ):
                logging.warning(
                    "API rejected date filter for %s (status 400). Retrying without filters for this endpoint.",
                    table_name,
                )
                params = {"accessToken": self.access_token}
                params_count = {"accessToken": self.access_token, "skip": 0, "take": 1}
                # Preserve fields parameter if it was set
                if fields:
                    params["fields"] = ",".join(fields)
                    params_count["fields"] = ",".join(fields)
                first_response = await self.client.get(
                    endpoint_path, params=params_count
                )
            else:
                raise

        if not first_response or "result" not in first_response:
            logging.warning(f"No data found for table: {table_name}")
            return

        total = first_response["result"].get("total", 0)
        logging.info(
            f"Table {table_name}: Total entries: {total}, Batches: {(total + page_limit - 1) // page_limit}"
        )

        if total == 0:
            return

        # Fetch and yield pages directly without accumulating
        for offset in range(0, total, page_limit):
            params_page = params.copy()
            params_page["skip"] = offset
            params_page["take"] = page_limit

            records = await self._fetch_page(
                endpoint_path, params_page, table_name, offset
            )

            if records:
                logging.debug(f"Yielding page of {len(records)} records")
                yield records

    async def fetch_table_data(
        self,
        table_name: str,
        filters: dict[str, Any],
        limit: int = DEFAULT_PAGE_LIMIT,
        endpoint: str | None = None,
    ) -> list[dict[str, Any]]:
        """
        Fetch all data for a table with pagination (loads all into memory).

        Note: For large datasets, consider using fetch_table_data_batched() instead.

        Args:
            table_name: Name of the table to fetch
            filters: Dictionary of filters to apply
            limit: Number of records per page

        Returns:
            List of records from the API
        """
        # Build endpoint (relative to base URL)
        endpoint = self._prepare_endpoint(endpoint or table_name)

        # Build query parameters
        params = {"accessToken": self.access_token}

        # Add filters
        params.update(filters)

        # First, get total count
        params_count = params.copy()
        params_count["skip"] = 0
        params_count["take"] = 1

        logging.info(f"Fetching total count for table: {table_name}")
        first_response = await self.client.get(endpoint, params=params_count)

        if not first_response or "result" not in first_response:
            logging.warning(f"No data found for table: {table_name}")
            return []

        total = first_response["result"].get("total", 0)
        logging.info(
            f"Table {table_name}: Total entries: {total}, Batches: {(total + limit - 1) // limit}"
        )

        if total == 0:
            return []

        # Fetch all pages
        all_records = []
        tasks = []

        for offset in range(0, total, limit):
            params_page = params.copy()
            params_page["skip"] = offset
            params_page["take"] = limit
            tasks.append(self._fetch_page(endpoint, params_page, table_name, offset))

        # Execute all requests concurrently
        results = await asyncio.gather(*tasks)

        # Combine all results
        for records in results:
            if records:
                all_records.extend(records)

        logging.info(f"Table {table_name}: Fetched {len(all_records)} records")
        return all_records

    async def _fetch_page(
        self, endpoint: str, params: dict[str, Any], table_name: str, offset: int
    ) -> list[dict[str, Any]]:
        """
        Fetch a single page of data with concurrency limiting.

        Args:
            endpoint: API endpoint (relative to base URL)
            params: Query parameters
            table_name: Name of table (for logging)
            offset: Offset for this page

        Returns:
            List of records from this page
        """
        async with self.semaphore:
            return await self._fetch_page_direct(endpoint, params, table_name, offset)

    async def _fetch_page_direct(
        self, endpoint: str, params: dict[str, Any], table_name: str, offset: int
    ) -> list[dict[str, Any]]:
        """
        Fetch a single page of data without concurrency limiting.

        Automatically refreshes token and retries on 401 errors.

        Args:
            endpoint: API endpoint (relative to base URL)
            params: Query parameters
            table_name: Name of table (for logging)
            offset: Offset for this page

        Returns:
            List of records from this page
        """
        for attempt in range(MAX_AUTH_RETRIES):
            # Capture current token version for double-check locking
            token_version = self._token_version

            # Build params with current token
            current_params = params.copy()
            current_params["accessToken"] = self.access_token

            try:
                logging.debug(f"Fetching {table_name} page at offset {offset}")
                response = await self.client.get(endpoint, params=current_params)

                if not response or "result" not in response:
                    return []

                data = response["result"].get("data", [])
                return data if isinstance(data, list) else []

            except httpx.HTTPStatusError as e:
                if e.response.status_code == 401 and attempt < MAX_AUTH_RETRIES - 1:
                    logging.warning(
                        f"Token expired for {table_name} at offset {offset}, refreshing..."
                    )
                    await self._refresh_token(token_version)
                    continue
                raise

            except Exception as e:
                logging.error(f"Error fetching {table_name} at offset {offset}: {e}")
                raise

        return []



================================================
FILE: src/extractor.py
================================================
"""Main extractor module for Daktela data extraction."""

import logging
from typing import TYPE_CHECKING, Any

from keboola.component.exceptions import UserException

from configuration import DEFAULT_BATCH_SIZE
from daktela_client import DaktelaApiClient
from transformer import DataTransformer

if TYPE_CHECKING:
    from component import Component


class DaktelaExtractor:
    """Main extractor class that orchestrates data extraction."""

    def __init__(
        self,
        api_client: DaktelaApiClient,
        table_configs: dict[str, Any],
        component: "Component",
        url: str,
        requested_endpoints: list[str],
        batch_size: int = DEFAULT_BATCH_SIZE,
        date_from: str | None = None,
        date_to: str | None = None,
        incremental: bool = False,
        configured_fields: dict[str, list[str]] | None = None,
    ):
        """
        Initialize extractor.

        Args:
            api_client: Configured API client
            table_configs: Dictionary of table configurations
            component: Component instance for writing tables
            url: Base URL (e.g., https://customer.daktela.com)
            requested_endpoints: List of endpoint names to extract (typically one per job)
            batch_size: Number of records to process in each batch (default: 1000)
            date_from: Start date for filtering (for supported endpoints)
            date_to: End date for filtering (for supported endpoints)
            incremental: Whether to use incremental mode
            configured_fields: User-configured fields per endpoint (optional)
        """
        self.api_client = api_client
        self.table_configs = table_configs
        self.component = component
        self.url = url
        self.requested_endpoints = requested_endpoints
        self.batch_size = batch_size
        self.date_from = date_from
        self.date_to = date_to
        self.incremental = incremental
        self.configured_fields = configured_fields or {}
        self._table_columns: dict[str, list[str]] = {}

    async def extract_all(self):
        """
        Extract all requested endpoints asynchronously.

        Note: In row-based configuration mode, typically only one endpoint
        is requested per job execution (platform executes one row per job).
        """
        logging.info(
            f"Starting extraction for {len(self.requested_endpoints)} endpoint(s)"
        )

        if not self.requested_endpoints:
            raise UserException("No endpoints specified for extraction")

        # Extract each endpoint (typically just one per job)
        for endpoint in self.requested_endpoints:
            await self._extract_table(endpoint)

        logging.info("Extraction completed successfully")

    def _get_table_endpoint(self, table_name: str, table_config: dict[str, Any]) -> str:
        """Return endpoint override for table if configured."""
        return table_config.get("endpoint", table_name)

    def _get_fields_for_endpoint(self, table_name: str) -> list[str] | None:
        """
        Determine which fields to extract for an endpoint.

        Precedence:
        1. User-configured fields (from configuration)
        2. Schema from state (from previous runs)
        3. None (fetch all fields from API)

        Args:
            table_name: Name of the endpoint/table

        Returns:
            List of field names or None to fetch all fields
        """
        # 1. User-configured fields take highest priority
        if table_name in self.configured_fields:
            fields = self.configured_fields[table_name]
            if fields:
                logging.info(
                    f"Using user-configured fields for {table_name}: {len(fields)} fields"
                )
                return fields

        # 2. Schema from state (previous runs)
        state_fields = self.component.get_schema_for_endpoint(table_name)
        if state_fields:
            logging.info(
                f"Using schema state fields for {table_name}: {len(state_fields)} fields"
            )
            return state_fields

        # 3. No fields specified - will fetch all from API
        logging.info(
            f"No field configuration for {table_name}, will fetch all fields from API"
        )
        return None

    async def _extract_table(self, table_name: str):
        """
        Extract a single table using batched processing for memory efficiency.

        Args:
            table_name: Name of table to extract
        """
        logging.info(f"Extracting table: {table_name}")

        table_config = self.table_configs[table_name]
        write_batch_size = max(1, self.batch_size)

        # Endpoint override support
        endpoint = self._get_table_endpoint(table_name, table_config)

        # Initialize transformer
        transformer = DataTransformer(table_name, table_config)

        # Table output name
        output_table_name = f"{table_name}.csv"

        # Get fields to fetch using precedence logic
        fields = self._get_fields_for_endpoint(table_name)

        # Fetch and process data in pages
        total_records = 0
        async for page in self.api_client.fetch_table_data_batched(
            table_name=table_name,
            endpoint=endpoint,
            date_from=self.date_from,
            date_to=self.date_to,
            batch_size=self.batch_size,
            fields=fields,
        ):
            if not page:
                continue

            # Transform page records one by one and write in small batches
            write_batch = []
            for transformed_record in transformer.transform_records(page):
                write_batch.append(transformed_record)

                # Write in configurable batches to reduce memory footprint
                if len(write_batch) >= write_batch_size:
                    total_records += self._write_records(
                        output_table_name, table_config, write_batch, table_name
                    )
                    write_batch = []

            # Write remaining records from this page
            if write_batch:
                total_records += self._write_records(
                    output_table_name, table_config, write_batch, table_name
                )

        # Finalize table (write manifest)
        if total_records > 0:
            self.component.finalize_table(output_table_name)
            logging.info(
                f"Completed extraction for table: {table_name} ({total_records} records)"
            )
        else:
            logging.warning(f"No data found for table: {table_name}")

    def _get_columns(self, sample_record: dict[str, Any]) -> list[str]:
        """
        Get ordered list of columns for output.

        Args:
            sample_record: Sample record to extract columns from

        Returns:
            Ordered list of column names
        """
        # Start with id
        columns = ["id"]

        # Add all other columns from sample record
        for key in sample_record.keys():
            if key not in columns:
                columns.append(key)

        return columns

    def _write_records(
        self,
        output_table_name: str,
        table_config: dict[str, Any],
        records: list[dict[str, Any]],
        table_name: str,
    ) -> int:
        """Write a batch of records via the component and return written count."""
        if not records:
            return 0

        if output_table_name not in self._table_columns:
            self._table_columns[output_table_name] = self._get_columns(records[0])
            # Update schema state with discovered columns
            self.component.update_schema_for_endpoint(
                table_name, self._table_columns[output_table_name]
            )

        self.component.write_table_data(
            table_name=output_table_name,
            records=records,
            table_config=table_config,
            columns=self._table_columns[output_table_name],
            incremental=self.incremental,
        )

        return len(records)



================================================
FILE: src/transformer.py
================================================
"""
Data transformation module for Daktela extractor.

This module transforms raw API responses into CSV-ready format through a series of steps:
1. Flatten nested JSON structures (up to 2 levels)
2. Clean HTML tags from string values
3. Handle list columns and list-of-dicts columns
4. Sanitize column names for Keboola compatibility
5. Add required output columns (id)

The transformation pipeline ensures data consistency and compatibility with
Keboola Storage tables.
"""

import logging
import re
from typing import Any

from keboola.utils.header_normalizer import DefaultHeaderNormalizer


class DataTransformer:
    """Transforms raw API data into structured CSV-ready format."""

    def __init__(self, table_name: str, table_config: dict[str, Any]):
        """
        Initialize data transformer.

        Args:
            table_name: Name of the table
            table_config: Configuration dict for the table being transformed
        """
        self.table_name = table_name
        self.primary_keys = table_config.get("primary_keys", [])
        self.secondary_keys = table_config.get("secondary_keys", [])
        self.list_columns = table_config.get("list_columns", [])
        self.list_of_dicts_columns = table_config.get("list_of_dicts_columns", [])
        self.header_normalizer = DefaultHeaderNormalizer()

    def transform_records(self, records: list[dict[str, Any]]):
        """
        Transform list of API records into CSV-ready format (generator).

        This method applies a transformation pipeline with the following steps:
        1. Flatten nested JSON structures
        2. Clean HTML from string values
        3. Handle list/list-of-dicts columns (may create multiple rows)
        4. Sanitize column names for Keboola compatibility
        5. Add required output columns (id)

        Args:
            records: List of raw API records

        Yields:
            Transformed records ready for CSV output
        """
        input_count = len(records)
        output_count = 0

        for record in records:
            # Step 1: Flatten nested JSON (up to 2 levels deep)
            flattened = self._flatten_json(record)

            # Step 2: Clean HTML tags from string values
            cleaned = self._clean_html(flattened)

            # Step 3: Handle list columns and list-of-dicts columns
            # Note: This may explode one record into multiple rows
            rows = self._handle_lists(cleaned)

            for row in rows:
                # Step 4: Sanitize column names for Keboola Storage compatibility
                sanitized = self._sanitize_columns(row)

                # Step 5: Add required output columns (id)
                final_row = self._add_output_columns(sanitized)

                output_count += 1
                yield final_row

        logging.info(
            f"Transformed {input_count} records into {output_count} rows for table {self.table_name}"
        )

    def _flatten_json(
        self, data: dict[str, Any], parent_key: str = "", level: int = 0
    ) -> dict[str, Any]:
        """
        Flatten nested JSON dictionaries up to 2 levels.

        Converts dot notation to underscores (e.g., user.name -> user_name).

        Args:
            data: Dictionary to flatten
            parent_key: Parent key for nested items
            level: Current nesting level

        Returns:
            Flattened dictionary
        """
        items = {}

        for key, value in data.items():
            new_key = f"{parent_key}_{key}" if parent_key else key

            if isinstance(value, dict) and level < 2:
                # Recurse into nested dict (up to 2 levels)
                items.update(self._flatten_json(value, new_key, level + 1))
            else:
                items[new_key] = value

        return items

    def _clean_html(self, data: dict[str, Any]) -> dict[str, Any]:
        """
        Remove HTML tags from string values.

        Args:
            data: Dictionary with potentially HTML-containing strings

        Returns:
            Dictionary with cleaned strings
        """
        cleaned = {}
        html_pattern = re.compile(r"<.*?>")

        for key, value in data.items():
            if isinstance(value, str):
                # Remove HTML tags
                cleaned_value = html_pattern.sub("", value)
                # Convert empty strings and whitespace to None
                if not cleaned_value or cleaned_value.isspace():
                    cleaned[key] = None
                else:
                    cleaned[key] = cleaned_value
            else:
                cleaned[key] = value

        return cleaned

    def _handle_lists(self, data: dict[str, Any]) -> list[dict[str, Any]]:
        """
        Handle list columns and list of dicts columns.

        - list_columns: Explode lists into multiple rows
        - list_of_dicts_columns: Split into multiple rows and flatten dict keys

        Args:
            data: Dictionary with potential list values

        Returns:
            List of dictionaries (may be multiple rows from a single input)
        """
        rows = [data]

        # Handle list_columns first
        for list_col in self.list_columns:
            if list_col not in data:
                continue

            value = data[list_col]
            if not isinstance(value, list):
                continue

            # Explode list into multiple rows
            new_rows = []
            for row in rows:
                if not value:  # Empty list
                    new_rows.append(row)
                else:
                    for item in value:
                        new_row = row.copy()
                        new_row[list_col] = item
                        new_rows.append(new_row)
            rows = new_rows

        # Handle list_of_dicts_columns
        for list_dict_col in self.list_of_dicts_columns:
            if list_dict_col not in data:
                continue

            value = data[list_dict_col]
            if not isinstance(value, list):
                continue

            # Explode list and flatten dicts
            new_rows = []
            for row in rows:
                if not value:  # Empty list
                    # Remove the list column
                    new_row = {k: v for k, v in row.items() if k != list_dict_col}
                    new_rows.append(new_row)
                else:
                    for item in value:
                        new_row = {k: v for k, v in row.items() if k != list_dict_col}
                        if isinstance(item, dict):
                            # Flatten dict keys as new columns
                            for dict_key, dict_value in item.items():
                                new_row[f"{list_dict_col}_{dict_key}"] = dict_value
                        new_rows.append(new_row)
            rows = new_rows

        return rows

    def _sanitize_columns(self, data: dict[str, Any]) -> dict[str, Any]:
        """
        Sanitize column names using keboola.utils.header_normalizer.

        Args:
            data: Dictionary with unsanitized column names

        Returns:
            Dictionary with sanitized column names
        """
        sanitized = {}
        for key, value in data.items():
            # Use header_normalizer to clean column names
            clean_key = self.header_normalizer._normalize_column_name(key)
            sanitized[clean_key] = value

        return sanitized

    def _add_output_columns(self, data: dict[str, Any]) -> dict[str, Any]:
        """
        Add required output columns: id.

        Args:
            data: Dictionary with data columns

        Returns:
            Dictionary with id column added
        """
        output = {}
        key_columns = self.primary_keys + self.secondary_keys

        id_parts = []
        for key in key_columns:
            value = data.get(key)
            if value is not None:
                id_parts.append(str(value))

        output["id"] = "_".join(id_parts) if id_parts else ""

        # Add all other data columns
        for key, value in data.items():
            output[key] = value

        return output



================================================
FILE: tests/__init__.py
================================================
import sys
from pathlib import Path

sys.path.append(str((Path(__file__).resolve().parent.parent / "src")))



================================================
FILE: tests/test_component.py
================================================
import sys
import unittest
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from keboola.component.exceptions import UserException  # noqa: E402
from configuration import Configuration, RowConfiguration  # noqa: E402


class TestConfiguration(unittest.TestCase):
    """Test global configuration validation."""

    def test_valid_configuration(self):
        """Test valid global configuration parameters."""
        config = Configuration(
            connection={
                "url": "https://demo.daktela.com",
                "username": "test_user",
                "#password": "test_password"
            }
        )

        self.assertEqual(config.connection.username, "test_user")
        self.assertEqual(config.connection.url, "https://demo.daktela.com")
        self.assertEqual(config.connection.verify_ssl, True)

    def test_url_storage(self):
        """Test URL is properly stored."""
        config = Configuration(
            connection={
                "url": "https://mycompany.daktela.com",
                "username": "test",
                "#password": "test"
            }
        )

        self.assertEqual(config.connection.url, "https://mycompany.daktela.com")

    def test_missing_url(self):
        """Test validation fails when URL is missing."""
        with self.assertRaises(UserException):
            Configuration.from_dict({
                "connection": {
                    "username": "test",
                    "#password": "test"
                    # Missing required 'url' field
                }
            })

    def test_default_values(self):
        """Test default values for optional fields."""
        config = Configuration(
            connection={
                "url": "https://demo.daktela.com",
                "username": "test",
                "#password": "test"
            }
        )

        self.assertEqual(config.advanced.batch_size, 1000)
        self.assertEqual(config.advanced.max_concurrent_requests, 10)
        self.assertEqual(config.advanced.max_concurrent_endpoints, 3)
        self.assertEqual(config.debug, False)


class TestRowConfiguration(unittest.TestCase):
    """Test row configuration validation."""

    def test_valid_row_configuration(self):
        """Test valid row configuration."""
        row_config = RowConfiguration(
            endpoint="contacts",
            date_from="7 days ago",
            date_to="today"
        )

        self.assertEqual(row_config.endpoint, "contacts")
        self.assertEqual(row_config.date_from, "7 days ago")
        self.assertEqual(row_config.date_to, "today")
        self.assertIsNone(row_config.fields)
        self.assertEqual(row_config.destination.incremental, False)
        self.assertIsNone(row_config.destination.primary_key)

    def test_row_configuration_with_fields(self):
        """Test row configuration with fields specified."""
        row_config = RowConfiguration(
            endpoint="contacts",
            date_from="7 days ago",
            date_to="today",
            fields=["name", "email", "phone"]
        )

        self.assertEqual(row_config.endpoint, "contacts")
        self.assertIsNotNone(row_config.fields)
        self.assertEqual(len(row_config.fields), 3)
        self.assertIn("name", row_config.fields)

    def test_missing_required_field(self):
        """Test validation fails when required field is missing."""
        with self.assertRaises(UserException):
            RowConfiguration.from_dict({
                "date_from": "7 days ago",
                "date_to": "today"
                # Missing required 'endpoint' field
            })

    def test_multiple_row_configs(self):
        """Test creating multiple row configurations."""
        row1 = RowConfiguration(
            endpoint="contacts",
            date_from="7 days ago",
            date_to="today"
        )
        row2 = RowConfiguration(
            endpoint="activities",
            date_from="3 days ago",
            date_to="today",
            fields=["time", "user", "title"]
        )
        row3 = RowConfiguration(
            endpoint="tickets",
            date_from="7 days ago",
            date_to="today"
        )

        rows = [row1, row2, row3]
        self.assertEqual(len(rows), 3)
        self.assertEqual(rows[0].endpoint, "contacts")
        self.assertEqual(rows[1].endpoint, "activities")
        self.assertIsNotNone(rows[1].fields)
        self.assertEqual(rows[2].endpoint, "tickets")


if __name__ == "__main__":
    unittest.main()



================================================
FILE: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on:
  push:  # skip the workflow on the main branch without tags
    branches-ignore:
      - main
    tags:
      - "*"

concurrency: ci-${{ github.ref }}  # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: keboola.ex-daktela
  KBC_DEVELOPERPORTAL_VENDOR: keboola
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: ${{ vars.KBC_DEVELOPERPORTAL_USERNAME }}

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}  # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (optional) test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: ""  # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }}  # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo ${raw/*origin\//} | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}-${{ github.run_id }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh


