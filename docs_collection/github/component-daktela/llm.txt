Directory structure:
└── keboola-component-daktela/
    ├── README.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── pyproject.toml
    ├── uv.lock
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   ├── sourceCodeUrl.md
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_test.sh
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── configuration.py
    │   ├── daktela_client.py
    │   ├── extractor.py
    │   ├── table_definitions.json
    │   └── transformer.py
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── .github/
        └── workflows/
            └── push.yml

================================================
FILE: README.md
================================================
Daktela Extractor
=================

Keboola component for extracting data from Daktela CRM/Contact Center API v6.

**Table of Contents:**

[TOC]

Description
===========

The Daktela Extractor component integrates with the Daktela CRM/Contact Center API v6 to extract specified data and produce CSV output files compatible with Keboola storage. The component supports:

- **Token-based authentication** with configurable SSL verification
- **State management** for true incremental processing
- **Date range filtering** with multiple format support
- **Parallel extraction** for independent tables with concurrency limiting
- **Sequential extraction** for dependent tables
- **Incremental loading** with automatic state tracking
- **Batched processing** for memory-efficient extraction of large datasets
- **Automatic retry logic** with linear backoff
- **Data transformation** and flattening
- **Support for list columns** and nested data structures
- **Automatic manifest generation** from table definitions
- **Component-level error handling** with proper cleanup

Prerequisites
=============

You need:
- Valid Daktela account credentials (username and password)
- Access to a Daktela instance (e.g., mycompany.daktela.com)
- Configured table definitions in `image_parameters`

Features
========

| **Feature**             | **Description**                               |
|-------------------------|-----------------------------------------------|
| Token Authentication    | Secure token-based authentication             |
| State Management        | Automatic state tracking for incremental runs |
| Date Range Filter       | Specify date range for data retrieval         |
| Incremental Loading     | Append new data with state-based filtering    |
| Parallel Extraction     | Extract independent tables concurrently       |
| Concurrency Limiting    | Configurable limit on concurrent API requests |
| Batched Processing      | Memory-efficient processing for large datasets|
| Dependent Tables        | Support for parent-child table relationships  |
| Data Transformation     | Flatten nested JSON and normalize columns     |
| Retry Logic             | Automatic retry with linear backoff           |
| List Handling           | Support for list columns and list of dicts    |
| Automatic Manifests     | Generate manifests from table definitions     |
| SSL Configuration       | Configurable SSL certificate verification     |
| Error Handling          | Component-level error handling with cleanup   |

Configuration
=============

Authentication
--------------

- **Username** (required): Daktela account username
- **Password** (required): Daktela account password (encrypted)
- **Server** (required): Server identifier (e.g., `democz`)
  - The component will construct the URL as `https://{server}.daktela.com`

Date Range
----------

- **From** (required): Start date for extraction
  - Formats: `today` or `0` (current datetime - 30 min), negative integer (today - N days), or `YYYY-MM-DD`
  - Example: `-7` (7 days ago)

- **To** (required): End date for extraction
  - Same formats as `from`
  - Must be at least one day after `from`
  - Example: `today`

Tables
------

- **Tables** (required): Comma-separated list of tables to extract
  - Example: `contacts,activities,tickets`

- **Incremental** (optional): Enable incremental loading (default: `false`)
  - `true`: Append new data to existing data and save state for next run
  - `false`: Replace existing data
  - When enabled, the component saves the current timestamp after successful extraction
  - On subsequent runs, this timestamp is used as the `from_date` to fetch only new data

Performance & Security
----------------------

- **Verify SSL** (optional): Enable SSL certificate verification (default: `true`)
  - `true`: Verify SSL certificates (recommended for production)
  - `false`: Disable SSL verification (for testing with self-signed certificates only)
  - **WARNING**: Disabling SSL verification is insecure and should only be used for testing

- **Max Concurrent Requests** (optional): Maximum concurrent API requests (default: `10`)
  - Range: 1-50
  - Lower this value if experiencing API rate limiting
  - Higher values may improve performance but can overwhelm the API

- **Batch Size** (optional): Records to process in each batch (default: `1000`)
  - Range: 100-100,000
  - Higher values use more memory but may be faster
  - Lower values use less memory and are better for very large tables
  - Recommended: 1000-10000 for most use cases

Debug
-----

- **Debug** (optional): Enable verbose debug logging (default: `false`)

Image Parameters
================

The component uses `image_parameters` in `config.json` to define table configurations. Each table configuration includes:

**Data Extraction:**
- **fields**: List of field names to extract
- **primary_keys**: Primary key fields for building compound IDs
- **secondary_keys**: Secondary key fields for compound IDs
- **keys**: Key fields to prefix with server name
- **no_prefix_columns**: Columns that should not be prefixed with server name
- **list_columns**: Columns containing lists to explode into rows
- **list_of_dicts_columns**: Columns containing lists of dicts to flatten
- **filters**: Additional API filters
- **requirements**: Tables that must be extracted first
- **parent_table**: For dependent tables, the parent table name
- **parent_id_field**: For dependent tables, the ID field in parent table

**Manifest Generation (Optional):**
- **manifest_destination**: Override destination table name (defaults to `server_tablename`)
- **manifest_primary_key**: Override primary key columns (defaults to `["id"]`)
- **manifest_metadata**: Add metadata to manifest file (e.g., KBC descriptions)

Project-Specific Configuration
-------------------------------

The component supports project-specific overrides. Configuration structure:

```json
{
  "image_parameters": {
    "default": {
      "table_name": { /* table config */ }
    },
    "PROJECT_ID": {
      "table_name": { /* project-specific overrides */ }
    }
  }
}
```

See `component_config/image_parameters_example.json` for a complete example.

Output
======

The component produces CSV files with the following structure:

- **Filename**: `{server}_{table_name}.csv`
- **Location**: `data/out/tables/`
- **Format**: UTF-8 encoded, comma-separated, no header
- **Manifest**: JSON manifest file with columns, primary_key, and incremental flag

Output Columns
--------------

Every output row includes:

1. **server**: Server name (e.g., `mycompany`)
2. **id**: Compound primary key (concatenation of primary and secondary key values)
3. **data columns**: All configured fields

Key columns (primary_keys, secondary_keys, keys) are automatically prefixed with the server name unless listed in `no_prefix_columns`.

Development
-----------

To customize the local data folder path, replace the `CUSTOM_FOLDER` placeholder with your desired path in the `docker-compose.yml` file:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Clone this repository, initialize the workspace, and run the component using the following
commands:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
git clone https://github.com/keboola/component-daktela component-daktela
cd component-daktela
docker-compose build
docker-compose run --rm dev
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Run the test suite and perform lint checks using this command:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
docker-compose run --rm test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Documentation
=============

For detailed documentation on specific features:

- **[Manifest Configuration](docs/MANIFEST_CONFIGURATION.md)**: Learn how to customize manifest generation from table definitions

Integration
===========

For details about deployment and integration with Keboola, refer to the
[deployment section of the developer
documentation](https://developers.keboola.com/extend/component/deployment/).



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh



================================================
FILE: Dockerfile
================================================
FROM python:3.13-slim
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

# uncomment the following line should you have any troubles installing certain packages which require C/C++ extensions
# to be compiled during installation, eg. numpy, psycopg2, …
# RUN apt-get update && apt-get install -y build-essential

WORKDIR /code/

COPY pyproject.toml .
COPY uv.lock .

ENV UV_PROJECT_ENVIRONMENT="/usr/local/"
RUN uv sync --all-groups --frozen

COPY src/ src
COPY tests/ tests
COPY scripts/ scripts
COPY flake8.cfg .
COPY deploy.sh .

CMD ["python", "-u", "src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    __pycache__,
    .git,
    .venv,
    venv
ignore = E203,W503
max-line-length = 120



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: pyproject.toml
================================================
[project]
name = "daktela"
dynamic = ["version"]
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "flake8>=7.2.0",
    "freezegun>=1.5.1",
    "keboola-component>=1.6.10",
    "keboola-http-client>=1.2.0",
    "keboola-utils>=1.1.0",
    "mock>=5.2.0",
    "pydantic>=2.11.3",
    "requests>=2.31.0",
    "ruff>=0.11.5",
]



================================================
FILE: uv.lock
================================================
version = 1
revision = 1
requires-python = ">=3.13"

[[package]]
name = "aiolimiter"
version = "1.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/23/b52debf471f7a1e42e362d959a3982bdcb4fe13a5d46e63d28868807a79c/aiolimiter-1.2.1.tar.gz", hash = "sha256:e02a37ea1a855d9e832252a105420ad4d15011505512a1a1d814647451b5cca9", size = 7185 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f3/ba/df6e8e1045aebc4778d19b8a3a9bc1808adb1619ba94ca354d9ba17d86c3/aiolimiter-1.2.1-py3-none-any.whl", hash = "sha256:d3f249e9059a20badcb56b61601a83556133655c11d1eb3dd3e04ff069e5f3c7", size = 6711 },
]

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anyio"
version = "4.12.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/16/ce/8a777047513153587e5434fd752e89334ac33e379aa3497db860eeb60377/anyio-4.12.0.tar.gz", hash = "sha256:73c693b567b0c55130c104d0b43a9baf3aa6a31fc6110116509f27bf75e21ec0", size = 228266 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7f/9c/36c5c37947ebfb8c7f22e0eb6e4d188ee2d53aa3880f3f2744fb894f0cb1/anyio-4.12.0-py3-none-any.whl", hash = "sha256:dad2376a628f98eeca4881fc56cd06affd18f659b17a747d3ff0307ced94b1bb", size = 113362 },
]

[[package]]
name = "certifi"
version = "2025.1.31"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1c/ab/c9f1e32b7b1bf505bf26f0ef697775960db7932abeb7b516de930ba2705f/certifi-2025.1.31.tar.gz", hash = "sha256:3d5da6925056f6f18f119200434a4780a94263f10d1c21d032a6f6b2baa20651", size = 167577 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/38/fc/bce832fd4fd99766c04d1ee0eead6b0ec6486fb100ae5e74c1d91292b982/certifi-2025.1.31-py3-none-any.whl", hash = "sha256:ca78db4565a652026a4db2bcdf68f2fb589ea80d0be70e03929ed730746b84fe", size = 166393 },
]

[[package]]
name = "charset-normalizer"
version = "3.4.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/16/b0/572805e227f01586461c80e0fd25d65a2115599cc9dad142fee4b747c357/charset_normalizer-3.4.1.tar.gz", hash = "sha256:44251f18cd68a75b56585dd00dae26183e102cd5e0f9f1466e6df5da2ed64ea3", size = 123188 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/38/94/ce8e6f63d18049672c76d07d119304e1e2d7c6098f0841b51c666e9f44a0/charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:aabfa34badd18f1da5ec1bc2715cadc8dca465868a4e73a0173466b688f29dda", size = 195698 },
    { url = "https://files.pythonhosted.org/packages/24/2e/dfdd9770664aae179a96561cc6952ff08f9a8cd09a908f259a9dfa063568/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:22e14b5d70560b8dd51ec22863f370d1e595ac3d024cb8ad7d308b4cd95f8313", size = 140162 },
    { url = "https://files.pythonhosted.org/packages/24/4e/f646b9093cff8fc86f2d60af2de4dc17c759de9d554f130b140ea4738ca6/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:8436c508b408b82d87dc5f62496973a1805cd46727c34440b0d29d8a2f50a6c9", size = 150263 },
    { url = "https://files.pythonhosted.org/packages/5e/67/2937f8d548c3ef6e2f9aab0f6e21001056f692d43282b165e7c56023e6dd/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2d074908e1aecee37a7635990b2c6d504cd4766c7bc9fc86d63f9c09af3fa11b", size = 142966 },
    { url = "https://files.pythonhosted.org/packages/52/ed/b7f4f07de100bdb95c1756d3a4d17b90c1a3c53715c1a476f8738058e0fa/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:955f8851919303c92343d2f66165294848d57e9bba6cf6e3625485a70a038d11", size = 144992 },
    { url = "https://files.pythonhosted.org/packages/96/2c/d49710a6dbcd3776265f4c923bb73ebe83933dfbaa841c5da850fe0fd20b/charset_normalizer-3.4.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:44ecbf16649486d4aebafeaa7ec4c9fed8b88101f4dd612dcaf65d5e815f837f", size = 147162 },
    { url = "https://files.pythonhosted.org/packages/b4/41/35ff1f9a6bd380303dea55e44c4933b4cc3c4850988927d4082ada230273/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:0924e81d3d5e70f8126529951dac65c1010cdf117bb75eb02dd12339b57749dd", size = 140972 },
    { url = "https://files.pythonhosted.org/packages/fb/43/c6a0b685fe6910d08ba971f62cd9c3e862a85770395ba5d9cad4fede33ab/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:2967f74ad52c3b98de4c3b32e1a44e32975e008a9cd2a8cc8966d6a5218c5cb2", size = 149095 },
    { url = "https://files.pythonhosted.org/packages/4c/ff/a9a504662452e2d2878512115638966e75633519ec11f25fca3d2049a94a/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:c75cb2a3e389853835e84a2d8fb2b81a10645b503eca9bcb98df6b5a43eb8886", size = 152668 },
    { url = "https://files.pythonhosted.org/packages/6c/71/189996b6d9a4b932564701628af5cee6716733e9165af1d5e1b285c530ed/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:09b26ae6b1abf0d27570633b2b078a2a20419c99d66fb2823173d73f188ce601", size = 150073 },
    { url = "https://files.pythonhosted.org/packages/e4/93/946a86ce20790e11312c87c75ba68d5f6ad2208cfb52b2d6a2c32840d922/charset_normalizer-3.4.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:fa88b843d6e211393a37219e6a1c1df99d35e8fd90446f1118f4216e307e48cd", size = 145732 },
    { url = "https://files.pythonhosted.org/packages/cd/e5/131d2fb1b0dddafc37be4f3a2fa79aa4c037368be9423061dccadfd90091/charset_normalizer-3.4.1-cp313-cp313-win32.whl", hash = "sha256:eb8178fe3dba6450a3e024e95ac49ed3400e506fd4e9e5c32d30adda88cbd407", size = 95391 },
    { url = "https://files.pythonhosted.org/packages/27/f2/4f9a69cc7712b9b5ad8fdb87039fd89abba997ad5cbe690d1835d40405b0/charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl", hash = "sha256:b1ac5992a838106edb89654e0aebfc24f5848ae2547d22c2c3f66454daa11971", size = 102702 },
    { url = "https://files.pythonhosted.org/packages/0e/f6/65ecc6878a89bb1c23a086ea335ad4bf21a588990c3f535a227b9eea9108/charset_normalizer-3.4.1-py3-none-any.whl", hash = "sha256:d98b1668f06378c6dbefec3b92299716b931cd4e6061f3c875a71ced1780ab85", size = 49767 },
]

[[package]]
name = "daktela"
source = { virtual = "." }
dependencies = [
    { name = "flake8" },
    { name = "freezegun" },
    { name = "keboola-component" },
    { name = "keboola-http-client" },
    { name = "keboola-utils" },
    { name = "mock" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "ruff" },
]

[package.metadata]
requires-dist = [
    { name = "flake8", specifier = ">=7.2.0" },
    { name = "freezegun", specifier = ">=1.5.1" },
    { name = "keboola-component", specifier = ">=1.6.10" },
    { name = "keboola-http-client", specifier = ">=1.2.0" },
    { name = "keboola-utils", specifier = ">=1.1.0" },
    { name = "mock", specifier = ">=5.2.0" },
    { name = "pydantic", specifier = ">=2.11.3" },
    { name = "requests", specifier = ">=2.31.0" },
    { name = "ruff", specifier = ">=0.11.5" },
]

[[package]]
name = "dateparser"
version = "1.2.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "python-dateutil" },
    { name = "pytz" },
    { name = "regex" },
    { name = "tzlocal" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bd/3f/d3207a05f5b6a78c66d86631e60bfba5af163738a599a5b9aa2c2737a09e/dateparser-1.2.1.tar.gz", hash = "sha256:7e4919aeb48481dbfc01ac9683c8e20bfe95bb715a38c1e9f6af889f4f30ccc3", size = 309924 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cf/0a/981c438c4cd84147c781e4e96c1d72df03775deb1bc76c5a6ee8afa89c62/dateparser-1.2.1-py3-none-any.whl", hash = "sha256:bdcac262a467e6260030040748ad7c10d6bacd4f3b9cdb4cfd2251939174508c", size = 295658 },
]

[[package]]
name = "deprecated"
version = "1.2.18"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wrapt" },
]
sdist = { url = "https://files.pythonhosted.org/packages/98/97/06afe62762c9a8a86af0cfb7bfdab22a43ad17138b07af5b1a58442690a2/deprecated-1.2.18.tar.gz", hash = "sha256:422b6f6d859da6f2ef57857761bfb392480502a64c3028ca9bbe86085d72115d", size = 2928744 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6e/c6/ac0b6c1e2d138f1002bcf799d330bd6d85084fece321e662a14223794041/Deprecated-1.2.18-py2.py3-none-any.whl", hash = "sha256:bd5011788200372a32418f888e326a09ff80d0214bd961147cfed01b5c018eec", size = 9998 },
]

[[package]]
name = "flake8"
version = "7.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mccabe" },
    { name = "pycodestyle" },
    { name = "pyflakes" },
]
sdist = { url = "https://files.pythonhosted.org/packages/e7/c4/5842fc9fc94584c455543540af62fd9900faade32511fab650e9891ec225/flake8-7.2.0.tar.gz", hash = "sha256:fa558ae3f6f7dbf2b4f22663e5343b6b6023620461f8d4ff2019ef4b5ee70426", size = 48177 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/83/5c/0627be4c9976d56b1217cb5187b7504e7fd7d3503f8bfd312a04077bd4f7/flake8-7.2.0-py2.py3-none-any.whl", hash = "sha256:93b92ba5bdb60754a6da14fa3b93a9361fd00a59632ada61fd7b130436c40343", size = 57786 },
]

[[package]]
name = "freezegun"
version = "1.5.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "python-dateutil" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2c/ef/722b8d71ddf4d48f25f6d78aa2533d505bf3eec000a7cacb8ccc8de61f2f/freezegun-1.5.1.tar.gz", hash = "sha256:b29dedfcda6d5e8e083ce71b2b542753ad48cfec44037b3fc79702e2980a89e9", size = 33697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/51/0b/0d7fee5919bccc1fdc1c2a7528b98f65c6f69b223a3fd8f809918c142c36/freezegun-1.5.1-py3-none-any.whl", hash = "sha256:bf111d7138a8abe55ab48a71755673dbaa4ab87f4cff5634a4442dfec34c15f1", size = 17569 },
]

[[package]]
name = "h11"
version = "0.16.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/01/ee/02a2c011bdab74c6fb3c75474d40b3052059d95df7e73351460c8588d963/h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1", size = 101250 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86", size = 37515 },
]

[[package]]
name = "httpcore"
version = "1.0.9"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/94/82699a10bca87a5556c9c59b5963f2d039dbd239f25bc2a63907a05a14cb/httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8", size = 85484 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55", size = 78784 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "keboola-component"
version = "1.6.10"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "deprecated" },
    { name = "pygelf" },
    { name = "pytz" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ef/dd/391f1e6eaae5e925f56f30788c40e38c8bce3a80ee898512e79ced2eabab/keboola.component-1.6.10.tar.gz", hash = "sha256:5f4c347e8e96bb4dff1fe1254217e88754a4edea8a1b147815552335dcfba941", size = 47336 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/4c/34/301082c106bff256e2871a5c5db54d691fdad77a9d8a89a52eef30cd18ed/keboola.component-1.6.10-py3-none-any.whl", hash = "sha256:9a13b73beb71373d9a2b456eb44f902cfcfc07747c084bfbfad761b5eaaa4d93", size = 42243 },
]

[[package]]
name = "keboola-http-client"
version = "1.2.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "aiolimiter" },
    { name = "httpx" },
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/0f/b9/8e43e2b7c1f2667a9bc40b96a0098dcdd5d77b8d937fa1312ebd99ad9561/keboola_http_client-1.2.0.tar.gz", hash = "sha256:b3a3bcdc096ab84cff19ffa65d2ee303032c73d2d8d8b8aa93a82fb5ba6da484", size = 18369 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/09/7d/1d2b64896f9fff44de82783194a0def1e683dbfe7a8491db6d88d9a403c9/keboola_http_client-1.2.0-py3-none-any.whl", hash = "sha256:de80f5866d4d0aafc3a67492dc3e2d31d6c7e53f79406d7130620c952a4da493", size = 12632 },
]

[[package]]
name = "keboola-utils"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "dateparser" },
    { name = "pytz" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a7/b8/ccfddc2eb510f7a6ab878ab8a6249a23494194780a436676da6c2f5d23c7/keboola.utils-1.1.0.tar.gz", hash = "sha256:e943dbda932d945bcd5edd51283eea8f7035249c9dac769d3e96d2f507b52f60", size = 9830 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/f4/6697a0c2ff512baa7b84413972e51d5449a0a145f68dc750f05a8b1da39d/keboola.utils-1.1.0-py3-none-any.whl", hash = "sha256:8c73faa4a81f371a2eecd8465b08a51b3f7608969dd91d38d5b3bcfad7ef0da5", size = 10131 },
]

[[package]]
name = "mccabe"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e7/ff/0ffefdcac38932a54d2b5eed4e0ba8a408f215002cd178ad1df0f2806ff8/mccabe-0.7.0.tar.gz", hash = "sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325", size = 9658 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/27/1a/1f68f9ba0c207934b35b86a8ca3aad8395a3d6dd7921c0686e23853ff5a9/mccabe-0.7.0-py2.py3-none-any.whl", hash = "sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e", size = 7350 },
]

[[package]]
name = "mock"
version = "5.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/8c/14c2ae915e5f9dca5a22edd68b35be94400719ccfa068a03e0fb63d0f6f6/mock-5.2.0.tar.gz", hash = "sha256:4e460e818629b4b173f32d08bf30d3af8123afbb8e04bb5707a1fd4799e503f0", size = 92796 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/d9/617e6af809bf3a1d468e0d58c3997b1dc219a9a9202e650d30c2fc85d481/mock-5.2.0-py3-none-any.whl", hash = "sha256:7ba87f72ca0e915175596069dbbcc7c75af7b5e9b9bc107ad6349ede0819982f", size = 31617 },
]

[[package]]
name = "pycodestyle"
version = "2.13.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/04/6e/1f4a62078e4d95d82367f24e685aef3a672abfd27d1a868068fed4ed2254/pycodestyle-2.13.0.tar.gz", hash = "sha256:c8415bf09abe81d9c7f872502a6eee881fbe85d8763dd5b9924bb0a01d67efae", size = 39312 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/07/be/b00116df1bfb3e0bb5b45e29d604799f7b91dd861637e4d448b4e09e6a3e/pycodestyle-2.13.0-py2.py3-none-any.whl", hash = "sha256:35863c5974a271c7a726ed228a14a4f6daf49df369d8c50cd9a6f58a5e143ba9", size = 31424 },
]

[[package]]
name = "pydantic"
version = "2.11.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/10/2e/ca897f093ee6c5f3b0bee123ee4465c50e75431c3d5b6a3b44a47134e891/pydantic-2.11.3.tar.gz", hash = "sha256:7471657138c16adad9322fe3070c0116dd6c3ad8d649300e3cbdfe91f4db4ec3", size = 785513 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b0/1d/407b29780a289868ed696d1616f4aad49d6388e5a77f567dcd2629dcd7b8/pydantic-2.11.3-py3-none-any.whl", hash = "sha256:a082753436a07f9ba1289c6ffa01cd93db3548776088aa917cc43b63f68fa60f", size = 443591 },
]

[[package]]
name = "pydantic-core"
version = "2.33.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/17/19/ed6a078a5287aea7922de6841ef4c06157931622c89c2a47940837b5eecd/pydantic_core-2.33.1.tar.gz", hash = "sha256:bcc9c6fdb0ced789245b02b7d6603e17d1563064ddcfc36f046b61c0c05dd9df", size = 434395 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7a/24/eed3466a4308d79155f1cdd5c7432c80ddcc4530ba8623b79d5ced021641/pydantic_core-2.33.1-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:70af6a21237b53d1fe7b9325b20e65cbf2f0a848cf77bed492b029139701e66a", size = 2033551 },
    { url = "https://files.pythonhosted.org/packages/ab/14/df54b1a0bc9b6ded9b758b73139d2c11b4e8eb43e8ab9c5847c0a2913ada/pydantic_core-2.33.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:282b3fe1bbbe5ae35224a0dbd05aed9ccabccd241e8e6b60370484234b456266", size = 1852785 },
    { url = "https://files.pythonhosted.org/packages/fa/96/e275f15ff3d34bb04b0125d9bc8848bf69f25d784d92a63676112451bfb9/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4b315e596282bbb5822d0c7ee9d255595bd7506d1cb20c2911a4da0b970187d3", size = 1897758 },
    { url = "https://files.pythonhosted.org/packages/b7/d8/96bc536e975b69e3a924b507d2a19aedbf50b24e08c80fb00e35f9baaed8/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:1dfae24cf9921875ca0ca6a8ecb4bb2f13c855794ed0d468d6abbec6e6dcd44a", size = 1986109 },
    { url = "https://files.pythonhosted.org/packages/90/72/ab58e43ce7e900b88cb571ed057b2fcd0e95b708a2e0bed475b10130393e/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:6dd8ecfde08d8bfadaea669e83c63939af76f4cf5538a72597016edfa3fad516", size = 2129159 },
    { url = "https://files.pythonhosted.org/packages/dc/3f/52d85781406886c6870ac995ec0ba7ccc028b530b0798c9080531b409fdb/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:2f593494876eae852dc98c43c6f260f45abdbfeec9e4324e31a481d948214764", size = 2680222 },
    { url = "https://files.pythonhosted.org/packages/f4/56/6e2ef42f363a0eec0fd92f74a91e0ac48cd2e49b695aac1509ad81eee86a/pydantic_core-2.33.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:948b73114f47fd7016088e5186d13faf5e1b2fe83f5e320e371f035557fd264d", size = 2006980 },
    { url = "https://files.pythonhosted.org/packages/4c/c0/604536c4379cc78359f9ee0aa319f4aedf6b652ec2854953f5a14fc38c5a/pydantic_core-2.33.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:e11f3864eb516af21b01e25fac915a82e9ddad3bb0fb9e95a246067398b435a4", size = 2120840 },
    { url = "https://files.pythonhosted.org/packages/1f/46/9eb764814f508f0edfb291a0f75d10854d78113fa13900ce13729aaec3ae/pydantic_core-2.33.1-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:549150be302428b56fdad0c23c2741dcdb5572413776826c965619a25d9c6bde", size = 2072518 },
    { url = "https://files.pythonhosted.org/packages/42/e3/fb6b2a732b82d1666fa6bf53e3627867ea3131c5f39f98ce92141e3e3dc1/pydantic_core-2.33.1-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:495bc156026efafd9ef2d82372bd38afce78ddd82bf28ef5276c469e57c0c83e", size = 2248025 },
    { url = "https://files.pythonhosted.org/packages/5c/9d/fbe8fe9d1aa4dac88723f10a921bc7418bd3378a567cb5e21193a3c48b43/pydantic_core-2.33.1-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:ec79de2a8680b1a67a07490bddf9636d5c2fab609ba8c57597e855fa5fa4dacd", size = 2254991 },
    { url = "https://files.pythonhosted.org/packages/aa/99/07e2237b8a66438d9b26482332cda99a9acccb58d284af7bc7c946a42fd3/pydantic_core-2.33.1-cp313-cp313-win32.whl", hash = "sha256:ee12a7be1742f81b8a65b36c6921022301d466b82d80315d215c4c691724986f", size = 1915262 },
    { url = "https://files.pythonhosted.org/packages/8a/f4/e457a7849beeed1e5defbcf5051c6f7b3c91a0624dd31543a64fc9adcf52/pydantic_core-2.33.1-cp313-cp313-win_amd64.whl", hash = "sha256:ede9b407e39949d2afc46385ce6bd6e11588660c26f80576c11c958e6647bc40", size = 1956626 },
    { url = "https://files.pythonhosted.org/packages/20/d0/e8d567a7cff7b04e017ae164d98011f1e1894269fe8e90ea187a3cbfb562/pydantic_core-2.33.1-cp313-cp313-win_arm64.whl", hash = "sha256:aa687a23d4b7871a00e03ca96a09cad0f28f443690d300500603bd0adba4b523", size = 1909590 },
    { url = "https://files.pythonhosted.org/packages/ef/fd/24ea4302d7a527d672c5be06e17df16aabfb4e9fdc6e0b345c21580f3d2a/pydantic_core-2.33.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:401d7b76e1000d0dd5538e6381d28febdcacb097c8d340dde7d7fc6e13e9f95d", size = 1812963 },
    { url = "https://files.pythonhosted.org/packages/5f/95/4fbc2ecdeb5c1c53f1175a32d870250194eb2fdf6291b795ab08c8646d5d/pydantic_core-2.33.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:7aeb055a42d734c0255c9e489ac67e75397d59c6fbe60d155851e9782f276a9c", size = 1986896 },
    { url = "https://files.pythonhosted.org/packages/71/ae/fe31e7f4a62431222d8f65a3bd02e3fa7e6026d154a00818e6d30520ea77/pydantic_core-2.33.1-cp313-cp313t-win_amd64.whl", hash = "sha256:338ea9b73e6e109f15ab439e62cb3b78aa752c7fd9536794112e14bee02c8d18", size = 1931810 },
]

[[package]]
name = "pyflakes"
version = "3.3.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/af/cc/1df338bd7ed1fa7c317081dcf29bf2f01266603b301e6858856d346a12b3/pyflakes-3.3.2.tar.gz", hash = "sha256:6dfd61d87b97fba5dcfaaf781171ac16be16453be6d816147989e7f6e6a9576b", size = 64175 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/15/40/b293a4fa769f3b02ab9e387c707c4cbdc34f073f945de0386107d4e669e6/pyflakes-3.3.2-py2.py3-none-any.whl", hash = "sha256:5039c8339cbb1944045f4ee5466908906180f13cc99cc9949348d10f82a5c32a", size = 63164 },
]

[[package]]
name = "pygelf"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/fe/d3/73d1fe74a156f9a0e519bedc87815ed309e64af19c73b94352e4c0959ddb/pygelf-0.4.2.tar.gz", hash = "sha256:d0bb8f45ff648a9a187713f4a05c09f685fcb8add7b04bb7471f20071bd11aad", size = 11991 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/03/cd/4afdddbc73f54ddf31d16137ef81c3d47192d75754b3115d925926081fd6/pygelf-0.4.2-py3-none-any.whl", hash = "sha256:ab57d1b26bffa014e29ae645ee51d2aa2f0c0cb419c522f2d24a237090b894a1", size = 8714 },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892 },
]

[[package]]
name = "pytz"
version = "2025.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f8/bf/abbd3cdfb8fbc7fb3d4d38d320f2441b1e7cbe29be4f23797b4a2b5d8aac/pytz-2025.2.tar.gz", hash = "sha256:360b9e3dbb49a209c21ad61809c7fb453643e048b38924c765813546746e81c3", size = 320884 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl", hash = "sha256:5ddf76296dd8c44c26eb8f4b6f35488f3ccbf6fbbd7adee0b7262d43f0ec2f00", size = 509225 },
]

[[package]]
name = "regex"
version = "2024.11.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8e/5f/bd69653fbfb76cf8604468d3b4ec4c403197144c7bfe0e6a5fc9e02a07cb/regex-2024.11.6.tar.gz", hash = "sha256:7ab159b063c52a0333c884e4679f8d7a85112ee3078fe3d9004b2dd875585519", size = 399494 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/90/73/bcb0e36614601016552fa9344544a3a2ae1809dc1401b100eab02e772e1f/regex-2024.11.6-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a6ba92c0bcdf96cbf43a12c717eae4bc98325ca3730f6b130ffa2e3c3c723d84", size = 483525 },
    { url = "https://files.pythonhosted.org/packages/0f/3f/f1a082a46b31e25291d830b369b6b0c5576a6f7fb89d3053a354c24b8a83/regex-2024.11.6-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:525eab0b789891ac3be914d36893bdf972d483fe66551f79d3e27146191a37d4", size = 288324 },
    { url = "https://files.pythonhosted.org/packages/09/c9/4e68181a4a652fb3ef5099e077faf4fd2a694ea6e0f806a7737aff9e758a/regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:086a27a0b4ca227941700e0b31425e7a28ef1ae8e5e05a33826e17e47fbfdba0", size = 284617 },
    { url = "https://files.pythonhosted.org/packages/fc/fd/37868b75eaf63843165f1d2122ca6cb94bfc0271e4428cf58c0616786dce/regex-2024.11.6-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bde01f35767c4a7899b7eb6e823b125a64de314a8ee9791367c9a34d56af18d0", size = 795023 },
    { url = "https://files.pythonhosted.org/packages/c4/7c/d4cd9c528502a3dedb5c13c146e7a7a539a3853dc20209c8e75d9ba9d1b2/regex-2024.11.6-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b583904576650166b3d920d2bcce13971f6f9e9a396c673187f49811b2769dc7", size = 833072 },
    { url = "https://files.pythonhosted.org/packages/4f/db/46f563a08f969159c5a0f0e722260568425363bea43bb7ae370becb66a67/regex-2024.11.6-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:1c4de13f06a0d54fa0d5ab1b7138bfa0d883220965a29616e3ea61b35d5f5fc7", size = 823130 },
    { url = "https://files.pythonhosted.org/packages/db/60/1eeca2074f5b87df394fccaa432ae3fc06c9c9bfa97c5051aed70e6e00c2/regex-2024.11.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3cde6e9f2580eb1665965ce9bf17ff4952f34f5b126beb509fee8f4e994f143c", size = 796857 },
    { url = "https://files.pythonhosted.org/packages/10/db/ac718a08fcee981554d2f7bb8402f1faa7e868c1345c16ab1ebec54b0d7b/regex-2024.11.6-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:0d7f453dca13f40a02b79636a339c5b62b670141e63efd511d3f8f73fba162b3", size = 784006 },
    { url = "https://files.pythonhosted.org/packages/c2/41/7da3fe70216cea93144bf12da2b87367590bcf07db97604edeea55dac9ad/regex-2024.11.6-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:59dfe1ed21aea057a65c6b586afd2a945de04fc7db3de0a6e3ed5397ad491b07", size = 781650 },
    { url = "https://files.pythonhosted.org/packages/a7/d5/880921ee4eec393a4752e6ab9f0fe28009435417c3102fc413f3fe81c4e5/regex-2024.11.6-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b97c1e0bd37c5cd7902e65f410779d39eeda155800b65fc4d04cc432efa9bc6e", size = 789545 },
    { url = "https://files.pythonhosted.org/packages/dc/96/53770115e507081122beca8899ab7f5ae28ae790bfcc82b5e38976df6a77/regex-2024.11.6-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:f9d1e379028e0fc2ae3654bac3cbbef81bf3fd571272a42d56c24007979bafb6", size = 853045 },
    { url = "https://files.pythonhosted.org/packages/31/d3/1372add5251cc2d44b451bd94f43b2ec78e15a6e82bff6a290ef9fd8f00a/regex-2024.11.6-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:13291b39131e2d002a7940fb176e120bec5145f3aeb7621be6534e46251912c4", size = 860182 },
    { url = "https://files.pythonhosted.org/packages/ed/e3/c446a64984ea9f69982ba1a69d4658d5014bc7a0ea468a07e1a1265db6e2/regex-2024.11.6-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:4f51f88c126370dcec4908576c5a627220da6c09d0bff31cfa89f2523843316d", size = 787733 },
    { url = "https://files.pythonhosted.org/packages/2b/f1/e40c8373e3480e4f29f2692bd21b3e05f296d3afebc7e5dcf21b9756ca1c/regex-2024.11.6-cp313-cp313-win32.whl", hash = "sha256:63b13cfd72e9601125027202cad74995ab26921d8cd935c25f09c630436348ff", size = 262122 },
    { url = "https://files.pythonhosted.org/packages/45/94/bc295babb3062a731f52621cdc992d123111282e291abaf23faa413443ea/regex-2024.11.6-cp313-cp313-win_amd64.whl", hash = "sha256:2b3361af3198667e99927da8b84c1b010752fa4b1115ee30beaa332cabc3ef1a", size = 273545 },
]

[[package]]
name = "requests"
version = "2.32.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/c9/74/b3ff8e6c8446842c3f5c837e9c3dfcfe2018ea6ecef224c710c85ef728f4/requests-2.32.5.tar.gz", hash = "sha256:dbba0bac56e100853db0ea71b82b4dfd5fe2bf6d3754a8893c3af500cec7d7cf", size = 134517 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/db/4254e3eabe8020b458f1a747140d32277ec7a271daf1d235b70dc0b4e6e3/requests-2.32.5-py3-none-any.whl", hash = "sha256:2462f94637a34fd532264295e186976db0f5d453d1cdd31473c85a6a161affb6", size = 64738 },
]

[[package]]
name = "ruff"
version = "0.11.5"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/45/71/5759b2a6b2279bb77fe15b1435b89473631c2cd6374d45ccdb6b785810be/ruff-0.11.5.tar.gz", hash = "sha256:cae2e2439cb88853e421901ec040a758960b576126dab520fa08e9de431d1bef", size = 3976488 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/23/db/6efda6381778eec7f35875b5cbefd194904832a1153d68d36d6b269d81a8/ruff-0.11.5-py3-none-linux_armv6l.whl", hash = "sha256:2561294e108eb648e50f210671cc56aee590fb6167b594144401532138c66c7b", size = 10103150 },
    { url = "https://files.pythonhosted.org/packages/44/f2/06cd9006077a8db61956768bc200a8e52515bf33a8f9b671ee527bb10d77/ruff-0.11.5-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:ac12884b9e005c12d0bd121f56ccf8033e1614f736f766c118ad60780882a077", size = 10898637 },
    { url = "https://files.pythonhosted.org/packages/18/f5/af390a013c56022fe6f72b95c86eb7b2585c89cc25d63882d3bfe411ecf1/ruff-0.11.5-py3-none-macosx_11_0_arm64.whl", hash = "sha256:4bfd80a6ec559a5eeb96c33f832418bf0fb96752de0539905cf7b0cc1d31d779", size = 10236012 },
    { url = "https://files.pythonhosted.org/packages/b8/ca/b9bf954cfed165e1a0c24b86305d5c8ea75def256707f2448439ac5e0d8b/ruff-0.11.5-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0947c0a1afa75dcb5db4b34b070ec2bccee869d40e6cc8ab25aca11a7d527794", size = 10415338 },
    { url = "https://files.pythonhosted.org/packages/d9/4d/2522dde4e790f1b59885283f8786ab0046958dfd39959c81acc75d347467/ruff-0.11.5-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:ad871ff74b5ec9caa66cb725b85d4ef89b53f8170f47c3406e32ef040400b038", size = 9965277 },
    { url = "https://files.pythonhosted.org/packages/e5/7a/749f56f150eef71ce2f626a2f6988446c620af2f9ba2a7804295ca450397/ruff-0.11.5-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:e6cf918390cfe46d240732d4d72fa6e18e528ca1f60e318a10835cf2fa3dc19f", size = 11541614 },
    { url = "https://files.pythonhosted.org/packages/89/b2/7d9b8435222485b6aac627d9c29793ba89be40b5de11584ca604b829e960/ruff-0.11.5-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:56145ee1478582f61c08f21076dc59153310d606ad663acc00ea3ab5b2125f82", size = 12198873 },
    { url = "https://files.pythonhosted.org/packages/00/e0/a1a69ef5ffb5c5f9c31554b27e030a9c468fc6f57055886d27d316dfbabd/ruff-0.11.5-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e5f66f8f1e8c9fc594cbd66fbc5f246a8d91f916cb9667e80208663ec3728304", size = 11670190 },
    { url = "https://files.pythonhosted.org/packages/05/61/c1c16df6e92975072c07f8b20dad35cd858e8462b8865bc856fe5d6ccb63/ruff-0.11.5-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:80b4df4d335a80315ab9afc81ed1cff62be112bd165e162b5eed8ac55bfc8470", size = 13902301 },
    { url = "https://files.pythonhosted.org/packages/79/89/0af10c8af4363304fd8cb833bd407a2850c760b71edf742c18d5a87bb3ad/ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3068befab73620b8a0cc2431bd46b3cd619bc17d6f7695a3e1bb166b652c382a", size = 11350132 },
    { url = "https://files.pythonhosted.org/packages/b9/e1/ecb4c687cbf15164dd00e38cf62cbab238cad05dd8b6b0fc68b0c2785e15/ruff-0.11.5-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:f5da2e710a9641828e09aa98b92c9ebbc60518fdf3921241326ca3e8f8e55b8b", size = 10312937 },
    { url = "https://files.pythonhosted.org/packages/cf/4f/0e53fe5e500b65934500949361e3cd290c5ba60f0324ed59d15f46479c06/ruff-0.11.5-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:ef39f19cb8ec98cbc762344921e216f3857a06c47412030374fffd413fb8fd3a", size = 9936683 },
    { url = "https://files.pythonhosted.org/packages/04/a8/8183c4da6d35794ae7f76f96261ef5960853cd3f899c2671961f97a27d8e/ruff-0.11.5-py3-none-musllinux_1_2_i686.whl", hash = "sha256:b2a7cedf47244f431fd11aa5a7e2806dda2e0c365873bda7834e8f7d785ae159", size = 10950217 },
    { url = "https://files.pythonhosted.org/packages/26/88/9b85a5a8af21e46a0639b107fcf9bfc31da4f1d263f2fc7fbe7199b47f0a/ruff-0.11.5-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:81be52e7519f3d1a0beadcf8e974715b2dfc808ae8ec729ecfc79bddf8dbb783", size = 11404521 },
    { url = "https://files.pythonhosted.org/packages/fc/52/047f35d3b20fd1ae9ccfe28791ef0f3ca0ef0b3e6c1a58badd97d450131b/ruff-0.11.5-py3-none-win32.whl", hash = "sha256:e268da7b40f56e3eca571508a7e567e794f9bfcc0f412c4b607931d3af9c4afe", size = 10320697 },
    { url = "https://files.pythonhosted.org/packages/b9/fe/00c78010e3332a6e92762424cf4c1919065707e962232797d0b57fd8267e/ruff-0.11.5-py3-none-win_amd64.whl", hash = "sha256:6c6dc38af3cfe2863213ea25b6dc616d679205732dc0fb673356c2d69608f800", size = 11378665 },
    { url = "https://files.pythonhosted.org/packages/43/7c/c83fe5cbb70ff017612ff36654edfebec4b1ef79b558b8e5fd933bab836b/ruff-0.11.5-py3-none-win_arm64.whl", hash = "sha256:67e241b4314f4eacf14a601d586026a962f4002a475aa702c69980a38087aa4e", size = 10460287 },
]

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050 },
]

[[package]]
name = "typing-extensions"
version = "4.13.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f6/37/23083fcd6e35492953e8d2aaaa68b860eb422b34627b13f2ce3eb6106061/typing_extensions-4.13.2.tar.gz", hash = "sha256:e6c81219bd689f51865d9e372991c540bda33a0379d5573cddb9a3a23f7caaef", size = 106967 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl", hash = "sha256:a439e7c04b49fec3e5d3e2beaa21755cadbbdc391694e28ccdd36ca4a1408f8c", size = 45806 },
]

[[package]]
name = "typing-inspection"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/82/5c/e6082df02e215b846b4b8c0b887a64d7d08ffaba30605502639d44c06b82/typing_inspection-0.4.0.tar.gz", hash = "sha256:9765c87de36671694a67904bf2c96e395be9c6439bb6c87b5142569dcdd65122", size = 76222 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/31/08/aa4fdfb71f7de5176385bd9e90852eaf6b5d622735020ad600f2bab54385/typing_inspection-0.4.0-py3-none-any.whl", hash = "sha256:50e72559fcd2a6367a19f7a7e610e6afcb9fac940c650290eed893d61386832f", size = 14125 },
]

[[package]]
name = "tzdata"
version = "2025.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/95/32/1a225d6164441be760d75c2c42e2780dc0873fe382da3e98a2e1e48361e5/tzdata-2025.2.tar.gz", hash = "sha256:b60a638fcc0daffadf82fe0f57e53d06bdec2f36c4df66280ae79bce6bd6f2b9", size = 196380 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl", hash = "sha256:1a403fada01ff9221ca8044d701868fa132215d84beb92242d9acd2147f667a8", size = 347839 },
]

[[package]]
name = "tzlocal"
version = "5.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "tzdata", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/8b/2e/c14812d3d4d9cd1773c6be938f89e5735a1f11a9f184ac3639b93cef35d5/tzlocal-5.3.1.tar.gz", hash = "sha256:cceffc7edecefea1f595541dbd6e990cb1ea3d19bf01b2809f362a03dd7921fd", size = 30761 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c2/14/e2a54fabd4f08cd7af1c07030603c3356b74da07f7cc056e600436edfa17/tzlocal-5.3.1-py3-none-any.whl", hash = "sha256:eb1a66c3ef5847adf7a834f1be0800581b683b5608e74f86ecbcef8ab91bb85d", size = 18026 },
]

[[package]]
name = "urllib3"
version = "2.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/8a/78/16493d9c386d8e60e442a35feac5e00f0913c0f4b7c217c11e8ec2ff53e0/urllib3-2.4.0.tar.gz", hash = "sha256:414bc6535b787febd7567804cc015fee39daab8ad86268f1310a9250697de466", size = 390672 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6b/11/cc635220681e93a0183390e26485430ca2c7b5f9d33b15c74c2861cb8091/urllib3-2.4.0-py3-none-any.whl", hash = "sha256:4e16665048960a0900c702d4a66415956a584919c03361cac9f1df5c5dd7e813", size = 128680 },
]

[[package]]
name = "wrapt"
version = "1.17.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/c3/fc/e91cc220803d7bc4db93fb02facd8461c37364151b8494762cc88b0fbcef/wrapt-1.17.2.tar.gz", hash = "sha256:41388e9d4d1522446fe79d3213196bd9e3b301a336965b9e27ca2788ebd122f3", size = 55531 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/b9/0ffd557a92f3b11d4c5d5e0c5e4ad057bd9eb8586615cdaf901409920b14/wrapt-1.17.2-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:6ed6ffac43aecfe6d86ec5b74b06a5be33d5bb9243d055141e8cabb12aa08125", size = 53800 },
    { url = "https://files.pythonhosted.org/packages/c0/ef/8be90a0b7e73c32e550c73cfb2fa09db62234227ece47b0e80a05073b375/wrapt-1.17.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:35621ae4c00e056adb0009f8e86e28eb4a41a4bfa8f9bfa9fca7d343fe94f998", size = 38824 },
    { url = "https://files.pythonhosted.org/packages/36/89/0aae34c10fe524cce30fe5fc433210376bce94cf74d05b0d68344c8ba46e/wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:a604bf7a053f8362d27eb9fefd2097f82600b856d5abe996d623babd067b1ab5", size = 38920 },
    { url = "https://files.pythonhosted.org/packages/3b/24/11c4510de906d77e0cfb5197f1b1445d4fec42c9a39ea853d482698ac681/wrapt-1.17.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5cbabee4f083b6b4cd282f5b817a867cf0b1028c54d445b7ec7cfe6505057cf8", size = 88690 },
    { url = "https://files.pythonhosted.org/packages/71/d7/cfcf842291267bf455b3e266c0c29dcb675b5540ee8b50ba1699abf3af45/wrapt-1.17.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:49703ce2ddc220df165bd2962f8e03b84c89fee2d65e1c24a7defff6f988f4d6", size = 80861 },
    { url = "https://files.pythonhosted.org/packages/d5/66/5d973e9f3e7370fd686fb47a9af3319418ed925c27d72ce16b791231576d/wrapt-1.17.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8112e52c5822fc4253f3901b676c55ddf288614dc7011634e2719718eaa187dc", size = 89174 },
    { url = "https://files.pythonhosted.org/packages/a7/d3/8e17bb70f6ae25dabc1aaf990f86824e4fd98ee9cadf197054e068500d27/wrapt-1.17.2-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9fee687dce376205d9a494e9c121e27183b2a3df18037f89d69bd7b35bcf59e2", size = 86721 },
    { url = "https://files.pythonhosted.org/packages/6f/54/f170dfb278fe1c30d0ff864513cff526d624ab8de3254b20abb9cffedc24/wrapt-1.17.2-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:18983c537e04d11cf027fbb60a1e8dfd5190e2b60cc27bc0808e653e7b218d1b", size = 79763 },
    { url = "https://files.pythonhosted.org/packages/4a/98/de07243751f1c4a9b15c76019250210dd3486ce098c3d80d5f729cba029c/wrapt-1.17.2-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:703919b1633412ab54bcf920ab388735832fdcb9f9a00ae49387f0fe67dad504", size = 87585 },
    { url = "https://files.pythonhosted.org/packages/f9/f0/13925f4bd6548013038cdeb11ee2cbd4e37c30f8bfd5db9e5a2a370d6e20/wrapt-1.17.2-cp313-cp313-win32.whl", hash = "sha256:abbb9e76177c35d4e8568e58650aa6926040d6a9f6f03435b7a522bf1c487f9a", size = 36676 },
    { url = "https://files.pythonhosted.org/packages/bf/ae/743f16ef8c2e3628df3ddfd652b7d4c555d12c84b53f3d8218498f4ade9b/wrapt-1.17.2-cp313-cp313-win_amd64.whl", hash = "sha256:69606d7bb691b50a4240ce6b22ebb319c1cfb164e5f6569835058196e0f3a845", size = 38871 },
    { url = "https://files.pythonhosted.org/packages/3d/bc/30f903f891a82d402ffb5fda27ec1d621cc97cb74c16fea0b6141f1d4e87/wrapt-1.17.2-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:4a721d3c943dae44f8e243b380cb645a709ba5bd35d3ad27bc2ed947e9c68192", size = 56312 },
    { url = "https://files.pythonhosted.org/packages/8a/04/c97273eb491b5f1c918857cd26f314b74fc9b29224521f5b83f872253725/wrapt-1.17.2-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:766d8bbefcb9e00c3ac3b000d9acc51f1b399513f44d77dfe0eb026ad7c9a19b", size = 40062 },
    { url = "https://files.pythonhosted.org/packages/4e/ca/3b7afa1eae3a9e7fefe499db9b96813f41828b9fdb016ee836c4c379dadb/wrapt-1.17.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:e496a8ce2c256da1eb98bd15803a79bee00fc351f5dfb9ea82594a3f058309e0", size = 40155 },
    { url = "https://files.pythonhosted.org/packages/89/be/7c1baed43290775cb9030c774bc53c860db140397047cc49aedaf0a15477/wrapt-1.17.2-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:40d615e4fe22f4ad3528448c193b218e077656ca9ccb22ce2cb20db730f8d306", size = 113471 },
    { url = "https://files.pythonhosted.org/packages/32/98/4ed894cf012b6d6aae5f5cc974006bdeb92f0241775addad3f8cd6ab71c8/wrapt-1.17.2-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a5aaeff38654462bc4b09023918b7f21790efb807f54c000a39d41d69cf552cb", size = 101208 },
    { url = "https://files.pythonhosted.org/packages/ea/fd/0c30f2301ca94e655e5e057012e83284ce8c545df7661a78d8bfca2fac7a/wrapt-1.17.2-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9a7d15bbd2bc99e92e39f49a04653062ee6085c0e18b3b7512a4f2fe91f2d681", size = 109339 },
    { url = "https://files.pythonhosted.org/packages/75/56/05d000de894c4cfcb84bcd6b1df6214297b8089a7bd324c21a4765e49b14/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:e3890b508a23299083e065f435a492b5435eba6e304a7114d2f919d400888cc6", size = 110232 },
    { url = "https://files.pythonhosted.org/packages/53/f8/c3f6b2cf9b9277fb0813418e1503e68414cd036b3b099c823379c9575e6d/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:8c8b293cd65ad716d13d8dd3624e42e5a19cc2a2f1acc74b30c2c13f15cb61a6", size = 100476 },
    { url = "https://files.pythonhosted.org/packages/a7/b1/0bb11e29aa5139d90b770ebbfa167267b1fc548d2302c30c8f7572851738/wrapt-1.17.2-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:4c82b8785d98cdd9fed4cac84d765d234ed3251bd6afe34cb7ac523cb93e8b4f", size = 106377 },
    { url = "https://files.pythonhosted.org/packages/6a/e1/0122853035b40b3f333bbb25f1939fc1045e21dd518f7f0922b60c156f7c/wrapt-1.17.2-cp313-cp313t-win32.whl", hash = "sha256:13e6afb7fe71fe7485a4550a8844cc9ffbe263c0f1a1eea569bc7091d4898555", size = 37986 },
    { url = "https://files.pythonhosted.org/packages/09/5e/1655cf481e079c1f22d0cabdd4e51733679932718dc23bf2db175f329b76/wrapt-1.17.2-cp313-cp313t-win_amd64.whl", hash = "sha256:eaf675418ed6b3b31c7a989fd007fa7c3be66ce14e5c3b27336383604c9da85c", size = 40750 },
    { url = "https://files.pythonhosted.org/packages/2d/82/f56956041adef78f849db6b289b282e72b55ab8045a75abad81898c28d19/wrapt-1.17.2-py3-none-any.whl", hash = "sha256:b18f2d1533a71f069c7f82d524a52599053d4c7166e9dd374ae2136b7f40f7c8", size = 23594 },
]



================================================
FILE: component_config/component_long_description.md
================================================
Daktela long description


================================================
FILE: component_config/component_short_description.md
================================================
Daktela short description


================================================
FILE: component_config/configRowSchema.json
================================================
{}


================================================
FILE: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Daktela Extractor Configuration",
  "required": [
    "username",
    "#password",
    "server",
    "date_from",
    "date_to",
    "tables"
  ],
  "properties": {
    "username": {
      "type": "string",
      "title": "Username",
      "description": "Daktela account username",
      "propertyOrder": 1
    },
    "#password": {
      "type": "string",
      "title": "Password",
      "description": "Daktela account password (encrypted)",
      "format": "password",
      "propertyOrder": 2
    },
    "server": {
      "type": "string",
      "title": "Server",
      "description": "Daktela server identifier (e.g., 'mycompany' for mycompany.daktela.com)",
      "propertyOrder": 3
    },
    "date_from": {
      "type": "string",
      "title": "Date From",
      "description": "Start date for extraction. Supported formats: 'today', 'yesterday', '5 hours ago', '3 days ago', '4 months ago', '2 years ago'",
      "default": "7 days ago",
      "propertyOrder": 4
    },
    "date_to": {
      "type": "string",
      "title": "Date To",
      "description": "End date for extraction. Supported formats: 'today', 'yesterday', '5 hours ago', '3 days ago', '4 months ago', '2 years ago'",
      "default": "today",
      "propertyOrder": 5
    },
    "tables": {
      "type": "array",
      "title": "Tables",
      "description": "List of tables to extract (e.g., ['contacts', 'activities', 'tickets'])",
      "items": {
        "type": "string"
      },
      "minItems": 1,
      "propertyOrder": 6
    },
    "incremental": {
      "type": "boolean",
      "title": "Incremental Load",
      "description": "If enabled, new data will be appended to existing data and state will be saved for the next run. If disabled, existing data will be replaced.",
      "default": false,
      "propertyOrder": 7
    },
    "verify_ssl": {
      "type": "boolean",
      "title": "Verify SSL Certificates",
      "description": "Enable SSL certificate verification. Disable only for testing with self-signed certificates. WARNING: Disabling SSL verification is insecure.",
      "default": true,
      "propertyOrder": 8
    },
    "max_concurrent_requests": {
      "type": "integer",
      "title": "Max Concurrent Requests",
      "description": "Maximum number of concurrent API requests. Lower this value if experiencing rate limiting.",
      "default": 10,
      "minimum": 1,
      "maximum": 50,
      "propertyOrder": 9
    },
    "batch_size": {
      "type": "integer",
      "title": "Batch Size",
      "description": "Number of records to process in each batch. Higher values use more memory but may be faster.",
      "default": 10000,
      "minimum": 100,
      "maximum": 100000,
      "propertyOrder": 10
    },
    "debug": {
      "type": "boolean",
      "title": "Debug Mode",
      "description": "Enable verbose debug logging",
      "default": false,
      "propertyOrder": 11
    }
  }
}



================================================
FILE: component_config/configuration_description.md
================================================
Configuration description.


================================================
FILE: component_config/documentationUrl.md
================================================
https://github.com/keboola/component-daktela/blob/master/README.md


================================================
FILE: component_config/licenseUrl.md
================================================
https://github.com/keboola/component-daktela/blob/master/LICENSE.md


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sourceCodeUrl.md
================================================
https://github.com/keboola/component-daktela


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": []
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "username": "test_user",
    "#password": "test_password",
    "server": "demo",
    "date_from": "-7",
    "date_to": "today",
    "tables": ["contacts", "activities", "tickets"],
    "incremental": false,
    "verify_ssl": true,
    "max_concurrent_requests": 10,
    "batch_size": 10000,
    "debug": true
  }
}



================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover



================================================
FILE: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi


================================================
FILE: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"


================================================
FILE: src/component.py
================================================
"""
Daktela Extractor Component main class.
"""

import asyncio
import csv
import json
import logging
import sys
import traceback
import keboola.utils
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

from keboola.component.base import ComponentBase
from keboola.component.exceptions import UserException

from configuration import Configuration
from daktela_client import DaktelaApiClient
from extractor import DaktelaExtractor


class Component(ComponentBase):
    """
    Daktela Extractor Component.

    Extracts data from Daktela CRM/Contact Center API v6 and produces CSV outputs
    compatible with Keboola storage.
    """

    def __init__(self) -> None:
        super().__init__()
        self.params: Optional[Configuration] = None
        self._table_definitions: Dict[str, Any] = {}

    def run(self) -> None:
        """Main execution - orchestrates the component workflow."""
        try:
            self.params = self._validate_and_get_configuration()

            # Apply state for incremental processing
            self._apply_state()

            # Run async extraction
            asyncio.run(self._run_async_extraction())

            # Save state after successful extraction
            self._save_state()

            logging.info("Daktela extraction completed successfully")

        except UserException as err:
            logging.error(f"Configuration/API error: {err}")
            print(err, file=sys.stderr)
            sys.exit(1)

        except Exception:
            logging.exception("Unhandled error in component execution")
            traceback.print_exc(file=sys.stderr)
            sys.exit(2)

    async def _run_async_extraction(self) -> None:
        """Run the async extraction process."""
        # Use async context manager for API client (auth happens in __init__)
        async with self._initialize_api_client() as api_client:
            table_configs = self._load_table_configurations()
            extractor = self._create_extractor(api_client, table_configs)
            await extractor.extract_all()

    def _validate_and_get_configuration(self) -> Configuration:
        """Load and validate configuration parameters."""
        params = Configuration(**self.configuration.parameters)

        logging.info(f"Starting Daktela extraction from {params.server}")
        logging.info(f"Date range: {params.date_from} to {params.date_to}")
        logging.info(f"Tables to extract: {params.tables}")
        logging.info(f"Incremental mode: {params.incremental}")

        return params

    def _initialize_api_client(self) -> DaktelaApiClient:
        """Initialize and return configured API client (authenticates during init)."""
        params = self._require_params()
        return DaktelaApiClient(
            url=params.url,
            username=params.username,
            password=params.password,
            max_concurrent=params.max_concurrent_requests,
            verify_ssl=params.verify_ssl,
        )

    def _load_table_configurations(self) -> Dict[str, Any]:
        """Load table configurations from JSON file."""
        config_file = Path(__file__).parent / "table_definitions.json"
        try:
            with open(config_file, "r") as f:
                table_configs = json.load(f)
            logging.info(f"Loading table definitions from {config_file}")
            return table_configs
        except FileNotFoundError:
            raise UserException(f"Table definitions file not found: {config_file}")
        except json.JSONDecodeError as e:
            raise UserException(f"Invalid JSON in table definitions file: {e}")

    def _create_extractor(
        self,
        api_client: DaktelaApiClient,
        table_configs: Dict[str, Any],
    ) -> DaktelaExtractor:
        """Create and configure the extractor."""
        params = self._require_params()

        # Parse dates using keboola utils
        from_datetime = keboola.utils.get_past_date(params.date_from).strftime("%Y-%m-%d %H:%M:%S")
        to_datetime = keboola.utils.get_past_date(params.date_to).strftime("%Y-%m-%d %H:%M:%S")

        return DaktelaExtractor(
            api_client=api_client,
            table_configs=table_configs,
            component=self,
            server=params.server,
            incremental=params.incremental,
            from_datetime=from_datetime,
            to_datetime=to_datetime,
            requested_tables=params.tables,
            batch_size=params.batch_size,
        )

    def _apply_state(self) -> None:
        """Apply state for incremental processing."""
        params = self._require_params()
        if params.incremental:
            state = self.get_state_file()
            last_timestamp = state.get("last_timestamp")

            if last_timestamp:
                logging.info(
                    f"Incremental load: using last_timestamp={last_timestamp} as date_from"
                )
                # Override date_from with last successful run timestamp
                params.date_from = last_timestamp
            else:
                logging.info(
                    "Incremental load: no previous state found, performing full extraction"
                )

    def _save_state(self) -> None:
        """Save state after successful extraction."""
        params = self._require_params()
        if params.incremental:
            current_timestamp = datetime.now(timezone.utc).isoformat()
            state = {
                "last_timestamp": current_timestamp,
                "tables_extracted": params.tables,
                "server": params.server,
            }

            self.write_state_file(state)
            logging.info(f"Saved state: last_timestamp={current_timestamp}")

    def write_table_data(
        self,
        table_name: str,
        records: List[Dict[str, Any]],
        table_config: Dict[str, Any],
        incremental: bool,
        columns: List[str],
    ) -> None:
        """
        Write table data using create_out_table_definition and write_manifest pattern.

        Args:
            table_name: Name of the output table (e.g., "server_tablename.csv")
            records: List of records to write
            table_config: Table configuration dict
            incremental: Whether to use incremental mode
            columns: List of column names
        """
        table_definitions = self._get_table_definitions()

        # Create table definition on first write
        if table_name not in table_definitions:
            out_table = self.create_out_table_definition(
                table_name,
                columns=columns,
                primary_key=table_config.get("manifest_primary_key", ["id"]),
                incremental=incremental,
            )

            table_definitions[table_name] = out_table

            # Write header
            with open(out_table.full_path, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=columns)
                writer.writeheader()

            logging.info(f"Created table definition for {table_name} with {len(columns)} columns")

        # Get table definition
        out_table = table_definitions.get(table_name)

        if not out_table:
            raise UserException(f"Table definition not found for {table_name}. This should not happen.")

        # Append records
        if records:
            with open(out_table.full_path, "a", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=columns, extrasaction="ignore")
                for record in records:
                    # Ensure all columns are present
                    row = {col: record.get(col) for col in columns}
                    writer.writerow(row)

            logging.info(f"Wrote {len(records)} records to {table_name}")

    def finalize_table(self, table_name: str) -> None:
        """
        Finalize table by writing manifest.

        Args:
            table_name: Name of the output table
        """
        out_table = self._get_table_definitions().get(table_name)

        if out_table:
            self.write_manifest(out_table)
            logging.info(f"Wrote manifest for {table_name}")
        else:
            logging.warning(f"No table definition found for {table_name}, skipping manifest")

    def _get_table_definitions(self) -> Dict[str, Any]:
        """Return initialized table definitions container."""
        if not hasattr(self, "_table_definitions"):
            self._table_definitions = {}
        return self._table_definitions

    def _require_params(self) -> Configuration:
        """Return initialized configuration or raise if missing."""
        if not self.params:
            raise UserException("Component parameters are not initialized.")
        return self.params


"""
Main entrypoint
"""
if __name__ == "__main__":
    comp = Component()
    # this triggers the run method by default and is controlled by the configuration.action parameter
    # Error handling is done in the run() method
    comp.execute_action()



================================================
FILE: src/configuration.py
================================================
import logging

from keboola.component.exceptions import UserException
from pydantic import BaseModel, Field, ValidationError

DEFAULT_MAX_CONCURRENT_REQUESTS = 10  # Default maximum number of concurrent API requests

DEFAULT_BATCH_SIZE = 10000  # Default batch size for processing records before writing to CSV


class Configuration(BaseModel):
    username: str
    password: str = Field(alias="#password")
    server: str
    date_from: str
    date_to: str
    tables: list[str]
    incremental: bool = False
    verify_ssl: bool = True
    max_concurrent_requests: int = DEFAULT_MAX_CONCURRENT_REQUESTS
    batch_size: int = DEFAULT_BATCH_SIZE
    debug: bool = False

    def __init__(self, **data):
        try:
            super().__init__(**data)
        except ValidationError as e:
            error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
            raise UserException(f"Validation Error: {', '.join(error_messages)}")

        if self.debug:
            logging.debug("Component will run in Debug mode")

    @property
    def url(self) -> str:
        """Build URL from server name."""
        return f"https://{self.server}.daktela.com"



================================================
FILE: src/daktela_client.py
================================================
"""
Async HTTP client for Daktela API with authentication, retry logic and pagination.
"""

import asyncio
import logging
import warnings
import requests
from typing import Dict, List, Any, Optional

from keboola.http_client import AsyncHttpClient
from keboola.component.exceptions import UserException

from configuration import DEFAULT_MAX_CONCURRENT_REQUESTS, DEFAULT_BATCH_SIZE

# API Client constants
DEFAULT_MAX_RETRIES = 8
"""Maximum number of retry attempts for failed API requests."""

DEFAULT_PAGE_LIMIT = 1000
"""Default number of records to fetch per API request."""

LINEAR_BACKOFF_SECONDS = 1
"""Base multiplier for linear backoff retry delay (delay = attempt * multiplier)."""

AUTH_TIMEOUT_SECONDS = 30
"""Timeout for authentication requests."""


class DaktelaApiClient:
    """Async HTTP client for Daktela API with built-in authentication, retry and pagination."""

    def __init__(
        self,
        url: str,
        username: str,
        password: str,
        max_retries: int = DEFAULT_MAX_RETRIES,
        max_concurrent: int = DEFAULT_MAX_CONCURRENT_REQUESTS,
        verify_ssl: bool = True,
    ):
        """
        Initialize API client and authenticate.

        Args:
            url: Base URL for Daktela API
            username: Daktela account username
            password: Daktela account password
            max_retries: Maximum number of retry attempts
            max_concurrent: Maximum concurrent requests
            verify_ssl: Whether to verify SSL certificates (default: True)
        """
        self.url = url
        self.username = username
        self.password = password
        self.max_retries = max_retries
        self.max_concurrent = max_concurrent
        self.verify_ssl = verify_ssl
        self.client = None  # Will be initialized in __aenter__
        self.semaphore = asyncio.Semaphore(max_concurrent)

        # Authenticate synchronously during initialization
        self.access_token = self._authenticate()

    def _authenticate(self) -> str:
        """
        Authenticate with Daktela API and retrieve access token.

        Returns:
            str: Access token for subsequent API requests

        Raises:
            UserException: If authentication fails or connection error occurs
        """
        login_url = f"{self.url}/api/v6/login.json"
        params = {"username": self.username, "password": self.password, "only_token": 1}

        try:
            logging.info(f"Attempting to authenticate with Daktela API at {self.url}")
            if not self.verify_ssl:
                warnings.filterwarnings("ignore", message="Unverified HTTPS request")
                logging.warning(
                    "SSL verification is disabled for authentication. This is insecure."
                )

            response = requests.post(
                login_url,
                params=params,
                verify=self.verify_ssl,
                timeout=AUTH_TIMEOUT_SECONDS,
            )

            # Check for successful response
            if response.status_code != 200:
                raise UserException(
                    f"Invalid response from Daktela API. Status code: {response.status_code}. "
                    f"Response: {response.text[:200]}"
                )

            # Parse response
            try:
                result = response.json()
            except Exception as e:
                raise UserException(
                    f"Failed to parse authentication response: {str(e)}"
                )

            # Extract access token
            if "result" not in result or not result["result"]:
                raise UserException(
                    f"Invalid token in authentication response. Response: {response.text[:200]}"
                )

            access_token = result["result"]
            logging.info("Successfully authenticated with Daktela API")

            return access_token

        except requests.exceptions.ConnectionError as e:
            raise UserException(
                f"Server not responding. Failed to connect to {self.url}: {str(e)}"
            )
        except requests.exceptions.Timeout as e:
            raise UserException(
                f"Connection timeout when connecting to {self.url}: {str(e)}"
            )
        except requests.exceptions.RequestException as e:
            raise UserException(f"Request failed: {str(e)}")

    async def __aenter__(self):
        """Async context manager entry."""
        self.client = AsyncHttpClient(self.url, verify_ssl=self.verify_ssl)
        await self.client.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.client:
            await self.client.__aexit__(exc_type, exc_val, exc_tb)

    async def fetch_table_data_batched(
        self,
        table_name: str,
        fields: List[str],
        filters: Dict[str, Any],
        limit: int = DEFAULT_PAGE_LIMIT,
        batch_size: int = DEFAULT_BATCH_SIZE,
    ):
        """
        Fetch data for a table in batches (generator for memory efficiency).

        Args:
            table_name: Name of the table to fetch
            fields: List of fields to include
            filters: Dictionary of filters to apply
            limit: Number of records per page
            batch_size: Number of records to accumulate before yielding

        Yields:
            Batches of records from the API
        """
        # Build endpoint (relative to base URL)
        endpoint = f"api/v6/{table_name}.json"

        # Build query parameters
        params = {"accessToken": self.access_token}

        # Add fields if specified
        if fields:
            params["fields"] = ",".join(fields)

        # Add filters
        params.update(filters)

        # First, get total count
        params_count = params.copy()
        params_count["skip"] = 0
        params_count["take"] = 1

        logging.info(f"Fetching total count for table: {table_name}")
        first_response = await self._request_with_retry(endpoint, params_count)

        if not first_response or "result" not in first_response:
            logging.warning(f"No data found for table: {table_name}")
            return

        total = first_response["result"].get("total", 0)
        logging.info(
            f"Table {table_name}: Total entries: {total}, Batches: {(total + limit - 1) // limit}"
        )

        if total == 0:
            return

        # Fetch pages in batches to manage memory
        batch = []
        for offset in range(0, total, limit):
            params_page = params.copy()
            params_page["skip"] = offset
            params_page["take"] = limit

            records = await self._fetch_page_direct(
                endpoint, params_page, table_name, offset
            )
            batch.extend(records)

            # Yield batch when it reaches batch_size
            if len(batch) >= batch_size:
                logging.debug(f"Yielding batch of {len(batch)} records")
                yield batch
                batch = []

        # Yield remaining records
        if batch:
            logging.debug(f"Yielding final batch of {len(batch)} records")
            yield batch

    async def fetch_table_data(
        self,
        table_name: str,
        fields: List[str],
        filters: Dict[str, Any],
        limit: int = DEFAULT_PAGE_LIMIT,
    ) -> List[Dict[str, Any]]:
        """
        Fetch all data for a table with pagination (loads all into memory).

        Note: For large datasets, consider using fetch_table_data_batched() instead.

        Args:
            table_name: Name of the table to fetch
            fields: List of fields to include
            filters: Dictionary of filters to apply
            limit: Number of records per page

        Returns:
            List of records from the API
        """
        # Build endpoint (relative to base URL)
        endpoint = f"api/v6/{table_name}.json"

        # Build query parameters
        params = {"accessToken": self.access_token}

        # Add fields if specified
        if fields:
            params["fields"] = ",".join(fields)

        # Add filters
        params.update(filters)

        # First, get total count
        params_count = params.copy()
        params_count["skip"] = 0
        params_count["take"] = 1

        logging.info(f"Fetching total count for table: {table_name}")
        first_response = await self._request_with_retry(endpoint, params_count)

        if not first_response or "result" not in first_response:
            logging.warning(f"No data found for table: {table_name}")
            return []

        total = first_response["result"].get("total", 0)
        logging.info(
            f"Table {table_name}: Total entries: {total}, Batches: {(total + limit - 1) // limit}"
        )

        if total == 0:
            return []

        # Fetch all pages
        all_records = []
        tasks = []

        for offset in range(0, total, limit):
            params_page = params.copy()
            params_page["skip"] = offset
            params_page["take"] = limit
            tasks.append(self._fetch_page(endpoint, params_page, table_name, offset))

        # Execute all requests concurrently
        results = await asyncio.gather(*tasks)

        # Combine all results
        for records in results:
            if records:
                all_records.extend(records)

        logging.info(f"Table {table_name}: Fetched {len(all_records)} records")
        return all_records

    async def fetch_dependent_table_data(
        self,
        parent_table: str,
        parent_id: str,
        child_table: str,
        fields: List[str],
        filters: Dict[str, Any],
    ) -> List[Dict[str, Any]]:
        """
        Fetch data for a dependent table (child of parent).

        Args:
            parent_table: Name of parent table
            parent_id: ID of parent record
            child_table: Name of child table
            fields: List of fields to include
            filters: Dictionary of filters to apply

        Returns:
            List of records from the API
        """
        # Build endpoint for dependent table
        endpoint = f"api/v6/{parent_table}/{parent_id}/{child_table}.json"

        # Build query parameters
        params = {"accessToken": self.access_token}

        # Add fields if specified
        if fields:
            params["fields"] = ",".join(fields)

        # Add filters
        params.update(filters)

        logging.debug(
            f"Fetching dependent table: {child_table} for parent {parent_table}:{parent_id}"
        )
        response = await self._request_with_retry(endpoint, params)

        if not response or "result" not in response:
            return []

        data = response["result"].get("data", [])
        return data if isinstance(data, list) else []

    async def _fetch_page(
        self, endpoint: str, params: Dict[str, Any], table_name: str, offset: int
    ) -> List[Dict[str, Any]]:
        """
        Fetch a single page of data with concurrency limiting.

        Args:
            endpoint: API endpoint (relative to base URL)
            params: Query parameters
            table_name: Name of table (for logging)
            offset: Offset for this page

        Returns:
            List of records from this page
        """
        async with self.semaphore:
            return await self._fetch_page_direct(endpoint, params, table_name, offset)

    async def _fetch_page_direct(
        self, endpoint: str, params: Dict[str, Any], table_name: str, offset: int
    ) -> List[Dict[str, Any]]:
        """
        Fetch a single page of data without concurrency limiting.

        Args:
            endpoint: API endpoint (relative to base URL)
            params: Query parameters
            table_name: Name of table (for logging)
            offset: Offset for this page

        Returns:
            List of records from this page
        """
        logging.debug(f"Fetching {table_name} page at offset {offset}")
        response = await self._request_with_retry(endpoint, params)

        if not response or "result" not in response:
            return []

        data = response["result"].get("data", [])
        return data if isinstance(data, list) else []

    async def _request_with_retry(
        self, endpoint: str, params: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """
        Make HTTP request with linear backoff retry logic.

        Args:
            endpoint: API endpoint (relative to base URL)
            params: Query parameters

        Returns:
            Response JSON as dictionary

        Raises:
            UserException: After all retries exhausted
        """
        last_exception = None

        for attempt in range(self.max_retries):
            try:
                # AsyncHttpClient.get() returns parsed JSON directly
                # It raises exceptions for non-200 status codes
                data = await self.client.get(endpoint, params=params)
                return data

            except asyncio.TimeoutError as e:
                logging.warning(
                    f"Request timeout. Attempt {attempt + 1}/{self.max_retries}"
                )
                last_exception = e

            except Exception as e:
                logging.warning(
                    f"Request failed: {str(e)}. Attempt {attempt + 1}/{self.max_retries}"
                )
                last_exception = e

            # Linear backoff: 1s, 2s, 3s, ..., max_retries seconds
            if attempt < self.max_retries - 1:
                wait_time = (attempt + 1) * LINEAR_BACKOFF_SECONDS
                logging.debug(f"Waiting {wait_time}s before retry...")
                await asyncio.sleep(wait_time)

        # All retries exhausted
        error_msg = f"Failed to fetch data after {self.max_retries} attempts"
        if last_exception:
            error_msg += f": {str(last_exception)}"
        raise UserException(error_msg)



================================================
FILE: src/extractor.py
================================================
"""
Main extractor module for Daktela data extraction.

Handles extraction orchestration for both parallel and dependent tables.
"""

import asyncio
import csv
import logging
from pathlib import Path
from typing import Any, Dict, List, Set, TYPE_CHECKING

from daktela_client import DaktelaApiClient
from transformer import DataTransformer
from keboola.component.exceptions import UserException

if TYPE_CHECKING:
    from component import Component


class DaktelaExtractor:
    """Main extractor class that orchestrates data extraction."""

    def __init__(
        self,
        api_client: DaktelaApiClient,
        table_configs: Dict[str, Any],
        component: "Component",
        server: str,
        incremental: bool,
        from_datetime: str,
        to_datetime: str,
        requested_tables: List[str],
        batch_size: int = 10000,
    ):
        """
        Initialize extractor.

        Args:
            api_client: Configured API client
            table_configs: Dictionary of table configurations
            component: Component instance for writing tables
            server: Server name
            incremental: Whether to use incremental mode
            from_datetime: Start datetime for extraction
            to_datetime: End datetime for extraction
            requested_tables: List of table names to extract
            batch_size: Number of records to process in each batch (default: 10000)
        """
        self.api_client = api_client
        self.table_configs = table_configs
        self.component = component
        self.server = server
        self.incremental = incremental
        self.from_datetime = from_datetime
        self.to_datetime = to_datetime
        self.requested_tables = requested_tables
        self.batch_size = batch_size
        self.invalid_activity_ids: Set[str] = set()
        self._table_columns: Dict[str, List[str]] = {}

    async def extract_all(self):
        """Extract all requested tables."""
        logging.info(f"Starting extraction for {len(self.requested_tables)} tables")

        # Filter to only configured tables
        tables_to_extract = []
        for table_name in self.requested_tables:
            if table_name not in self.table_configs:
                logging.warning(
                    f"Table '{table_name}' not found in configuration. Skipping."
                )
                continue
            tables_to_extract.append(table_name)

        if not tables_to_extract:
            raise UserException("No valid tables to extract")

        # Separate into parallel and sequential tables
        parallel_tables = [
            t for t in tables_to_extract if self._should_extract_in_parallel(t)
        ]
        sequential_tables = [
            t for t in tables_to_extract if not self._should_extract_in_parallel(t)
        ]

        logging.info(
            f"Extracting {len(parallel_tables)} tables in parallel and "
            f"{len(sequential_tables)} tables sequentially"
        )

        # Extract parallel tables first
        if parallel_tables:
            await self._extract_parallel_tables(parallel_tables)

        # Extract sequential tables (activities, activities_statuses, dependent tables)
        if sequential_tables:
            await self._extract_sequential_tables(sequential_tables)

        logging.info("Extraction completed successfully")

    def _should_extract_in_parallel(self, table_name: str) -> bool:
        """Check if this table can be extracted in parallel with others."""
        config = self.table_configs[table_name]
        has_requirements = len(config.get("requirements", [])) > 0
        is_activities = table_name in ["activities", "activities_statuses"]
        return not has_requirements and not is_activities

    def _is_dependent_table(self, table_name: str) -> bool:
        """Check if this table depends on a parent table."""
        config = self.table_configs[table_name]
        return bool(config.get("parent_table"))

    async def _extract_parallel_tables(self, tables: List[str]):
        """
        Extract multiple tables in parallel.

        Args:
            tables: List of table names to extract
        """
        logging.info(f"Extracting {len(tables)} tables in parallel")

        tasks = []
        for table_name in tables:
            tasks.append(self._extract_table(table_name))

        await asyncio.gather(*tasks)

    async def _extract_sequential_tables(self, tables: List[str]):
        """
        Extract tables sequentially (for activities and dependent tables).

        Args:
            tables: List of table names to extract
        """
        logging.info(f"Extracting {len(tables)} tables sequentially")

        for table_name in tables:
            await self._extract_table(table_name)

    async def _extract_table(self, table_name: str):
        """
        Extract a single table using batched processing for memory efficiency.

        Args:
            table_name: Name of table to extract
        """
        logging.info(f"Extracting table: {table_name}")

        table_config = self.table_configs[table_name]

        # Check if this is a dependent table
        if self._is_dependent_table(table_name):
            await self._extract_dependent_table(table_name)
            return

        # Build filters
        filters = self._build_filters(table_config)

        # Initialize transformer
        transformer = DataTransformer(self.server, table_name, table_config)

        # Table output name
        output_table_name = f"{self.server}_{table_name}.csv"

        # Fetch and process data in batches
        total_records = 0
        async for batch in self.api_client.fetch_table_data_batched(
            table_name=table_name,
            fields=table_config.get("fields", []),
            filters=filters,
            batch_size=self.batch_size,
        ):
            if not batch:
                continue

            # Transform batch
            transformed_records, invalid_ids = transformer.transform_records(batch)

            # Track invalid activity IDs
            if table_name == "activities" and invalid_ids:
                self.invalid_activity_ids.update(invalid_ids)

            if not transformed_records:
                continue

            total_records += self._write_records(
                output_table_name, table_config, transformed_records
            )

        # Finalize table (write manifest)
        if total_records > 0:
            self.component.finalize_table(output_table_name)
            logging.info(
                f"Completed extraction for table: {table_name} ({total_records} records)"
            )
        else:
            logging.warning(f"No data found for table: {table_name}")

    async def _extract_dependent_table(self, table_name: str):
        """
        Extract a dependent table (child of parent table).

        Args:
            table_name: Name of dependent table to extract
        """
        table_config = self.table_configs[table_name]
        parent_table = table_config.get("parent_table")
        parent_id_field = table_config.get("parent_id_field", "id")

        logging.info(
            f"Extracting dependent table: {table_name} "
            f"(parent: {parent_table})"
        )

        # Read parent IDs from CSV
        parent_file = Path(self.component.tables_out_path) / f"{self.server}_{parent_table}.csv"
        parent_ids = self._read_column_values(parent_file, parent_id_field)

        if not parent_ids:
            logging.warning(
                f"No parent IDs found for dependent table: {table_name}"
            )
            return

        # Filter out invalid activity IDs if parent is activities
        if parent_table == "activities":
            parent_ids = [
                pid for pid in parent_ids if pid not in self.invalid_activity_ids
            ]
            logging.info(
                f"Filtered out {len(self.invalid_activity_ids)} invalid activity IDs"
            )

        # Build filters
        filters = self._build_filters(table_config)

        # Initialize transformer
        transformer = DataTransformer(self.server, table_name, table_config)

        # Table output name
        output_table_name = f"{self.server}_{table_name}.csv"

        # Fetch and process data for each parent ID in batches
        batch = []
        total_records = 0

        for parent_id in parent_ids:
            records = await self.api_client.fetch_dependent_table_data(
                parent_table=parent_table,
                parent_id=parent_id,
                child_table=table_name,
                fields=table_config.get("fields", []),
                filters=filters,
            )

            if records:
                # Transform records
                transformed, _ = transformer.transform_records(records)
                batch.extend(transformed)
                total_records += len(transformed)

            # Write batch when it reaches threshold
            if len(batch) >= self.batch_size:
                self._write_records(output_table_name, table_config, batch)
                logging.debug(
                    f"Wrote batch of {len(batch)} records for table {table_name}"
                )
                batch = []

        # Write remaining records
        if batch:
            self._write_records(output_table_name, table_config, batch)
            logging.debug(
                f"Wrote final batch of {len(batch)} records for table {table_name}"
            )

        # Finalize table (write manifest)
        if total_records > 0:
            self.component.finalize_table(output_table_name)
            logging.info(
                f"Completed extraction for dependent table {table_name}: {total_records} records"
            )
        else:
            logging.warning(f"No data found for dependent table: {table_name}")

    def _build_filters(self, table_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Build filters for API request.

        Args:
            table_config: Table configuration dict

        Returns:
            Dictionary of filters
        """
        filters = {}

        # Add date range filters
        filters["from"] = self.from_datetime
        filters["to"] = self.to_datetime

        # Add table-specific filters from config
        config_filters = table_config.get("filters", {})
        if config_filters:
            filters.update(config_filters)

        return filters

    def _get_columns(self, sample_record: Dict[str, Any]) -> List[str]:
        """
        Get ordered list of columns for output.

        Args:
            sample_record: Sample record to extract columns from

        Returns:
            Ordered list of column names
        """
        # Start with server and id
        columns = ["server", "id"]

        # Add all other columns from sample record
        for key in sample_record.keys():
            if key not in columns:
                columns.append(key)

        return columns

    def _read_column_values(self, file_path: Path, column_name: str) -> List[str]:
        """Read non-empty values from a specific CSV column."""
        if not file_path.exists():
            logging.warning(f"CSV file not found: {file_path}")
            return []

        values: List[str] = []
        with open(file_path, "r", encoding="utf-8", newline="") as f:
            reader = csv.DictReader(f)
            for row in reader:
                value = row.get(column_name)
                if value:
                    values.append(value)

        logging.info(f"Read {len(values)} values from column {column_name} in {file_path}")
        return values

    def _write_records(
        self,
        output_table_name: str,
        table_config: Dict[str, Any],
        records: List[Dict[str, Any]],
    ) -> int:
        """Write a batch of records via the component and return written count."""
        if not records:
            return 0

        if output_table_name not in self._table_columns:
            self._table_columns[output_table_name] = self._get_columns(records[0])

        self.component.write_table_data(
            table_name=output_table_name,
            records=records,
            table_config=table_config,
            incremental=self.incremental,
            columns=self._table_columns[output_table_name],
        )

        return len(records)



================================================
FILE: src/table_definitions.json
================================================
{
  "contacts": {
    "fields": ["name", "title", "email", "phone", "description", "created", "edited"],
    "primary_keys": ["name"]
  },
  "activities": {
    "fields": ["name", "title", "description", "created", "edited", "user_id"],
    "primary_keys": ["name"],
    "keys": ["user_id"]
  },
  "activities_statuses": {
    "fields": ["name", "title", "description", "created", "edited"],
    "primary_keys": ["name"]
  },
  "activities_email": {
    "fields": ["name", "subject", "from_email", "to_email", "created", "edited"],
    "primary_keys": ["name"],
    "parent_table": "activities",
    "parent_id_field": "id"
  },
  "tickets": {
    "fields": ["name", "title", "subject", "description", "created", "edited", "user_id"],
    "primary_keys": ["name"],
    "keys": ["user_id"],
    "list_columns": ["tags"]
  },
  "users": {
    "fields": ["name", "title", "email", "created", "edited"],
    "primary_keys": ["name"]
  }
}



================================================
FILE: src/transformer.py
================================================
"""
Data transformation module for Daktela extractor.

This module transforms raw API responses into CSV-ready format through a series of steps:
1. Flatten nested JSON structures (up to 2 levels)
2. Clean HTML tags from string values
3. Handle list columns and list-of-dicts columns
4. Filter columns to keep only configured fields
5. Sanitize column names for Keboola compatibility
6. Add required output columns (server, id)

The transformation pipeline ensures data consistency and compatibility with
Keboola Storage tables.
"""

import re
import logging
from typing import Dict, List, Any
from keboola.utils.header_normalizer import DefaultHeaderNormalizer


class DataTransformer:
    """Transforms raw API data into structured CSV-ready format."""

    def __init__(self, server: str, table_name: str, table_config: Dict[str, Any]):
        """
        Initialize data transformer.

        Args:
            server: Server name for prefixing
            table_name: Name of the table
            table_config: Configuration dict for the table being transformed
        """
        self.server = server
        self.table_name = table_name
        self.fields = table_config.get("fields", [])
        self.primary_keys = table_config.get("primary_keys", [])
        self.secondary_keys = table_config.get("secondary_keys", [])
        self.keys = table_config.get("keys", [])
        self.no_prefix_columns = table_config.get("no_prefix_columns", [])
        self.list_columns = table_config.get("list_columns", [])
        self.list_of_dicts_columns = table_config.get("list_of_dicts_columns", [])
        self.header_normalizer = DefaultHeaderNormalizer()

    def transform_records(
        self, records: List[Dict[str, Any]]
    ) -> tuple[List[Dict[str, Any]], List[str]]:
        """
        Transform list of API records into CSV-ready format.

        This method applies a transformation pipeline with the following steps:
        1. Flatten nested JSON structures
        2. Clean HTML from string values
        3. Handle list/list-of-dicts columns (may create multiple rows)
        4. Filter to keep only configured fields
        5. Sanitize column names for Keboola compatibility
        6. Add required output columns (server, id)
        7. Handle special cases (activities table validation)

        Args:
            records: List of raw API records

        Returns:
            Tuple of (transformed_records, invalid_activity_ids)
            - transformed_records: List of transformed records ready for CSV output
            - invalid_activity_ids: List of activity IDs that are invalid (missing primary key)
        """
        transformed = []
        invalid_activity_ids = []

        for record in records:
            # Step 1: Flatten nested JSON (up to 2 levels deep)
            flattened = self._flatten_json(record)

            # Step 2: Clean HTML tags from string values
            cleaned = self._clean_html(flattened)

            # Step 3: Handle list columns and list-of-dicts columns
            # Note: This may explode one record into multiple rows
            rows = self._handle_lists(cleaned)

            for row in rows:
                # Step 4: Filter columns to only keep configured fields
                filtered = self._filter_columns(row)

                # Step 5: Sanitize column names for Keboola Storage compatibility
                sanitized = self._sanitize_columns(filtered)

                # Step 6: Add required output columns (server, id)
                final_row = self._add_output_columns(sanitized)

                # Step 7: Handle special cases for activities table
                # Validate that activities have a proper primary key
                if self.table_name == "activities":
                    if not final_row.get("id") or final_row["id"] == self.server + "_":
                        # Track invalid activity ID for filtering dependent tables
                        if "name" in record:
                            invalid_activity_ids.append(record["name"])
                        continue  # Skip this record

                # Rename conflicting 'name' column for activities table
                # (to avoid conflict with primary key field)
                if self.table_name == "activities" and "name" in final_row:
                    final_row["activities_name"] = final_row.pop("name")

                transformed.append(final_row)

        logging.info(
            f"Transformed {len(records)} records into {len(transformed)} rows for table {self.table_name}"
        )

        if invalid_activity_ids:
            logging.warning(
                f"Found {len(invalid_activity_ids)} invalid activities (missing primary key): "
                f"{', '.join(invalid_activity_ids[:10])}"
            )

        return transformed, invalid_activity_ids

    def _flatten_json(
        self, data: Dict[str, Any], parent_key: str = "", level: int = 0
    ) -> Dict[str, Any]:
        """
        Flatten nested JSON dictionaries up to 2 levels.

        Converts dot notation to underscores (e.g., user.name -> user_name).

        Args:
            data: Dictionary to flatten
            parent_key: Parent key for nested items
            level: Current nesting level

        Returns:
            Flattened dictionary
        """
        items = {}

        for key, value in data.items():
            new_key = f"{parent_key}_{key}" if parent_key else key

            if isinstance(value, dict) and level < 2:
                # Recurse into nested dict (up to 2 levels)
                items.update(self._flatten_json(value, new_key, level + 1))
            else:
                items[new_key] = value

        return items

    def _clean_html(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Remove HTML tags from string values.

        Args:
            data: Dictionary with potentially HTML-containing strings

        Returns:
            Dictionary with cleaned strings
        """
        cleaned = {}
        html_pattern = re.compile(r"<.*?>")

        for key, value in data.items():
            if isinstance(value, str):
                # Remove HTML tags
                cleaned_value = html_pattern.sub("", value)
                # Convert empty strings and whitespace to None
                if not cleaned_value or cleaned_value.isspace():
                    cleaned[key] = None
                else:
                    cleaned[key] = cleaned_value
            else:
                cleaned[key] = value

        return cleaned

    def _handle_lists(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Handle list columns and list of dicts columns.

        - list_columns: Explode lists into multiple rows
        - list_of_dicts_columns: Split into multiple rows and flatten dict keys

        Args:
            data: Dictionary with potential list values

        Returns:
            List of dictionaries (may be multiple rows from a single input)
        """
        rows = [data]

        # Handle list_columns first
        for list_col in self.list_columns:
            if list_col not in data:
                continue

            value = data[list_col]
            if not isinstance(value, list):
                continue

            # Explode list into multiple rows
            new_rows = []
            for row in rows:
                if not value:  # Empty list
                    new_rows.append(row)
                else:
                    for item in value:
                        new_row = row.copy()
                        new_row[list_col] = item
                        new_rows.append(new_row)
            rows = new_rows

        # Handle list_of_dicts_columns
        for list_dict_col in self.list_of_dicts_columns:
            if list_dict_col not in data:
                continue

            value = data[list_dict_col]
            if not isinstance(value, list):
                continue

            # Explode list and flatten dicts
            new_rows = []
            for row in rows:
                if not value:  # Empty list
                    # Remove the list column
                    new_row = {k: v for k, v in row.items() if k != list_dict_col}
                    new_rows.append(new_row)
                else:
                    for item in value:
                        new_row = {k: v for k, v in row.items() if k != list_dict_col}
                        if isinstance(item, dict):
                            # Flatten dict keys as new columns
                            for dict_key, dict_value in item.items():
                                new_row[f"{list_dict_col}_{dict_key}"] = dict_value
                        new_rows.append(new_row)
            rows = new_rows

        return rows

    def _filter_columns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Filter to keep only configured fields.

        Args:
            data: Dictionary with all columns

        Returns:
            Dictionary with only configured columns
        """
        if not self.fields:
            return data

        # Keep only configured fields and key fields
        keep_fields = set(self.fields)
        keep_fields.update(self.primary_keys)
        keep_fields.update(self.secondary_keys)
        keep_fields.update(self.keys)

        filtered = {}
        for key, value in data.items():
            # Check if key matches any configured field
            if key in keep_fields:
                filtered[key] = value
            # Also check for dynamically created columns from list_of_dicts
            elif any(
                key.startswith(f"{col}_")
                for col in self.list_of_dicts_columns
            ):
                filtered[key] = value

        return filtered

    def _sanitize_columns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Sanitize column names using keboola.utils.header_normalizer.

        Args:
            data: Dictionary with unsanitized column names

        Returns:
            Dictionary with sanitized column names
        """
        sanitized = {}
        for key, value in data.items():
            # Use header_normalizer to clean column names
            clean_key = self.header_normalizer._normalize_column_name(key)
            sanitized[clean_key] = value

        return sanitized

    def _add_output_columns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Add required output columns: server and id.

        Args:
            data: Dictionary with data columns

        Returns:
            Dictionary with server and id columns added
        """
        output = {"server": self.server}
        key_columns = self.primary_keys + self.secondary_keys

        id_parts = []
        for key in key_columns:
            value = data.get(key)
            if value is None:
                continue
            prefixed_value = self._prefixed_value(key, value)
            if prefixed_value is not None:
                id_parts.append(str(prefixed_value))

        output["id"] = "_".join(id_parts) if id_parts else ""

        keyed_fields = set(key_columns + self.keys)
        for key, value in data.items():
            if key in keyed_fields:
                output[key] = self._prefixed_value(key, value)
            else:
                output[key] = value

        return output

    def _prefixed_value(self, key: str, value: Any) -> Any:
        """Add server prefix to values unless exempted."""
        if key in self.no_prefix_columns or value is None:
            return value
        return f"{self.server}_{value}"



================================================
FILE: tests/__init__.py
================================================
import sys
from pathlib import Path

sys.path.append(str((Path(__file__).resolve().parent.parent / "src")))



================================================
FILE: tests/test_component.py
================================================
import os
import sys
import unittest
from pathlib import Path

import mock
from freezegun import freeze_time

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from configuration import Configuration
from keboola.component.exceptions import UserException


class TestConfiguration(unittest.TestCase):
    """Test configuration validation."""

    def test_valid_configuration(self):
        """Test valid configuration parameters."""
        config = Configuration(
            username="test_user",
            **{"#password": "test_password"},
            server="demo",
            **{"from": "-7"},
            **{"to": "today"},
            tables="contacts,activities"
        )

        self.assertEqual(config.username, "test_user")
        self.assertEqual(config.server, "demo")
        self.assertEqual(config.url, "https://demo.daktela.com")
        self.assertEqual(len(config.get_tables_list()), 2)

    def test_url_construction_from_server(self):
        """Test URL is constructed from server parameter."""
        config = Configuration(
            username="test",
            **{"#password": "test"},
            server="mycompany",
            **{"from": "-7"},
            **{"to": "today"},
            tables="contacts"
        )

        self.assertEqual(config.url, "https://mycompany.daktela.com")
        self.assertEqual(config.server, "mycompany")

    def test_url_validation(self):
        """Test URL validation fails for invalid URLs."""
        with self.assertRaises(UserException):
            Configuration(
                username="test",
                **{"#password": "test"},
                url="https://invalid.com",
                **{"from": "-7"},
                **{"to": "today"},
                tables="contacts"
            )

    def test_missing_url_and_server(self):
        """Test validation fails when both url and server are missing."""
        with self.assertRaises(UserException):
            Configuration(
                username="test",
                **{"#password": "test"},
                **{"from": "-7"},
                **{"to": "today"},
                tables="contacts"
            )

    def test_date_validation(self):
        """Test date validation."""
        # Valid dates
        config = Configuration(
            username="test",
            **{"#password": "test"},
            server="demo",
            **{"from": "2024-01-01"},
            **{"to": "2024-01-10"},
            tables="contacts"
        )
        self.assertIsNotNone(config.get_from_datetime())
        self.assertIsNotNone(config.get_to_datetime())

    def test_date_formats(self):
        """Test various date formats."""
        # Today
        config = Configuration(
            username="test",
            **{"#password": "test"},
            server="demo",
            **{"from": "-7"},
            **{"to": "today"},
            tables="contacts"
        )
        self.assertIsNotNone(config.get_from_datetime())
        self.assertIsNotNone(config.get_to_datetime())

    def test_tables_list_parsing(self):
        """Test table list parsing."""
        config = Configuration(
            username="test",
            **{"#password": "test"},
            server="demo",
            **{"from": "-7"},
            **{"to": "today"},
            tables="contacts, activities, tickets"
        )

        tables = config.get_tables_list()
        self.assertEqual(len(tables), 3)
        self.assertIn("contacts", tables)
        self.assertIn("activities", tables)
        self.assertIn("tickets", tables)


if __name__ == "__main__":
    unittest.main()



================================================
FILE: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on:
  push:  # skip the workflow on the main branch without tags
    branches-ignore:
      - main
    tags:
      - "*"

concurrency: ci-${{ github.ref }}  # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: keboola.ex-daktela
  KBC_DEVELOPERPORTAL_VENDOR: keboola
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: ${{ vars.KBC_DEVELOPERPORTAL_USERNAME }}

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}  # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (optional) test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: ""  # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }}  # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo ${raw/*origin\//} | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh


