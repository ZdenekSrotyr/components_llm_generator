Directory structure:
└── keboola-component-shopify-v2/
    ├── README.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── LICENSE.md
    ├── pyproject.toml
    ├── uv.lock
    ├── .flake8
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   └── sourceCodeUrl.md
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run_kbc_tests.ps1
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── configuration.py
    │   └── shopify_cli/
    │       ├── __init__.py
    │       ├── client.py
    │       ├── query_loader.py
    │       └── queries/
    │           ├── BulkCustomers.graphql
    │           ├── BulkEvents.graphql
    │           ├── BulkInventory.graphql
    │           ├── BulkLocations.graphql
    │           ├── BulkOperationStatus.graphql
    │           ├── BulkOrders.graphql
    │           ├── BulkProducts.graphql
    │           ├── GetCustomers.graphql
    │           ├── GetInventoryItems.graphql
    │           ├── GetInventoryLevels.graphql
    │           ├── GetLocations.graphql
    │           ├── GetOrders.graphql
    │           ├── GetProductDrafts.graphql
    │           ├── GetProductMetafields.graphql
    │           ├── GetProducts.graphql
    │           ├── GetVariantMetafields.graphql
    │           └── fragments/
    │               ├── OrderTransactions.graphql
    │               ├── ProductMetafields.graphql
    │               └── VariantMetafields.graphql
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── .github/
        └── workflows/
            └── push.yml

================================================
FILE: README.md
================================================
# Shopify GraphQL Extractor v2

A Keboola component for extracting data from Shopify using GraphQL API. This is the second version of the Shopify extractor, built specifically to use GraphQL instead of the deprecated REST API.

## Description

This component extracts data from Shopify stores using the modern GraphQL Admin API with bulk operations for efficient data extraction. It features a DuckDB-powered data processing engine that automatically normalizes complex nested JSON data into relational tables with proper data types. The component supports comprehensive data extraction across multiple Shopify endpoints with advanced date filtering and custom query capabilities.

## Features

| **Feature**             | **Description**                               |
|-------------------------|-----------------------------------------------|
| GraphQL API             | Uses modern Shopify GraphQL Admin API v2025-10 |
| Bulk Operations         | Efficient bulk data extraction for large datasets |
| DuckDB Processing       | Advanced data processing with automatic type detection |
| Data Normalization      | Converts nested JSON into normalized relational tables |
| Multiple Endpoints      | 10+ supported endpoints including orders, products, customers, inventory |
| Date Range Filtering    | Filter data by date ranges across all bulk operations |
| Flexible Date Formats   | Supports ISO dates (YYYY-MM-DD) and relative formats ("1 week ago", "now") |
| Custom Bulk Queries     | Execute custom GraphQL bulk operations |
| Type Detection          | Automatic data type detection and conversion  |
| Relational Output       | Normalized tables with proper relationships   |
| Error Handling          | Comprehensive error handling and logging      |

## Prerequisites

- Shopify store with Admin API access
- Admin API access token with appropriate permissions
- Python 3.13
- DuckDB (automatically installed)

## Supported Endpoints

The component supports the following Shopify GraphQL endpoints using **bulk operations** for efficient data extraction:

### Core Endpoints (Bulk Operations)

- **products** - Extract active products with variants and metafields
- **products_drafts** - Extract draft products
- **products_archived** - Extract archived products
- **products_unlisted** - Extract unlisted products
- **orders** - Extract order data with line items, customer info, and addresses
- **customers** - Extract customer data with addresses and marketing preferences
- **inventory** - Extract inventory levels across locations
- **locations** - Extract store location information
- **events** - Extract system events and activity logs

### Endpoint Options

- **product_metafields** - Include product-level metafields in products extraction
- **variant_metafields** - Include product variant metafields in products extraction
- **order_transactions** - Include transactions in orders extraction

### Custom Queries

The component also supports custom GraphQL bulk operations (mutations), allowing you to execute any custom bulk query against the Shopify API.

## Configuration

### Required Parameters

- **#api_token** - Your Shopify Admin API access token
- **store_name** - Your Shopify store name (without .myshopify.com)

### Optional Parameters

- **api_version** - Shopify API version (default: "2025-10")
- **endpoints** - Object with boolean flags for each endpoint to enable:
  - **products** - Extract active products (default: false)
  - **products_drafts** - Extract draft products (default: false)
  - **products_archived** - Extract archived products (default: false)
  - **products_unlisted** - Extract unlisted products (default: false)
  - **product_metafields** - Include product metafields (default: false)
  - **variant_metafields** - Include variant metafields (default: false)
  - **orders** - Extract orders (default: false)
  - **order_transactions** - Include order transactions (default: false)
  - **customers** - Extract customers (default: false)
  - **inventory** - Extract inventory (default: false)
  - **locations** - Extract locations (default: false)
- **loading_options** - Date filtering and loading behavior:
  - **date_since** - Start date for extraction (ISO format YYYY-MM-DD or relative like "1 week ago", "2 months ago")
  - **date_to** - End date for extraction (ISO format YYYY-MM-DD or relative like "now", "yesterday")
  - **fetch_parameter** - Field to filter by: "updated_at" or "created_at" (default: "updated_at")
  - **incremental_output** - Load type: 0=Full Load, 1=Incremental Update (default: 1)
- **events** - Array of event configurations for events endpoint (default: [])
- **custom_queries** - Array of custom bulk query configurations:
  - **name** - Query name (used for output table name)
  - **query** - GraphQL bulk operation mutation string
- **debug** - Enable debug logging and save raw JSONL files (default: false)

### Example Configuration

```json
{
  "parameters": {
    "#api_token": "your_shopify_admin_api_token_here",
    "store_name": "your-shop-name",
    "api_version": "2025-10",
    "endpoints": {
      "orders": true,
      "order_transactions": true,
      "products": true,
      "products_drafts": true,
      "product_metafields": true,
      "variant_metafields": true,
      "customers": true,
      "inventory": true,
      "locations": true
    },
    "loading_options": {
      "date_since": "1 month ago",
      "date_to": "now",
      "fetch_parameter": "updated_at",
      "incremental_output": 1
    },
    "events": [],
    "custom_queries": [
      {
        "name": "my_custom_query",
        "query": "mutation { bulkOperationRunQuery(query: \"\"\"{ products(query: \\\"status:active\\\") { edges { node { id title } } } }\"\"\") { bulkOperation { id status } userErrors { field message } } }"
      }
    ],
    "debug": false
  }
}
```

## Output

The component uses DuckDB to automatically process bulk operation results into CSV tables with proper data types. Each endpoint generates CSV files with all data preserved, including nested JSON structures:

### Output Tables

#### Bulk Operations (Primary Method)

- **orders.csv** - Orders with all nested data (line items, customer info, transactions as JSON)
- **products.csv** - Products with all nested data (variants, metafields, images as JSON)
- **customers.csv** - Customer data with addresses and preferences (nested as JSON)
- **inventory.csv** - Inventory levels across locations
- **locations.csv** - Store location information
- **events.csv** - System event logs
- **{custom_query_name}.csv** - Custom query results


### Data Types and Manifests

All CSV files include Keboola manifest files (`.csv.manifest`) with:
- Proper column data types (detected by DuckDB)
- Primary key definitions for relational integrity
- Component metadata for data lineage

Data types are automatically detected and mapped:
- Strings: `VARCHAR` → `STRING`
- Numbers: `BIGINT`, `DOUBLE` → `INTEGER`, `FLOAT`
- Dates: `TIMESTAMP` → `TIMESTAMP`
- Booleans: `BOOLEAN` → `BOOLEAN`

## Architecture

The component leverages several key technologies:

- **Shopify GraphQL API**: Uses the modern Admin API for efficient data retrieval
- **DuckDB**: In-memory analytical database for data processing and type detection
- **Pydantic**: Configuration validation and type safety
- **Keboola Component Framework**: Integration with Keboola platform

### Data Processing Pipeline

1. **Configuration Validation**: Pydantic models validate input parameters
2. **Date Parsing**: Convert ISO or relative date formats to API-compatible format
3. **Bulk Operation Initiation**: Shopify client initiates GraphQL bulk operations with filters
4. **Operation Polling**: Monitor bulk operation status until completion
5. **JSONL Download**: Download bulk operation results to temporary files
6. **DuckDB Processing**: Load JSONL data into DuckDB for processing
7. **Type Detection**: DuckDB automatically detects and assigns proper data types
8. **CSV Export**: Tables are exported as CSV with typed manifest files
9. **Cleanup**: Temporary files are removed from system temp directory

## Dependencies

The component requires the following Python packages:

- `keboola-component>=1.6.13` - Keboola platform integration
- `pydantic>=2.11.9` - Configuration validation
- `duckdb>=1.4.0` - Data processing engine
- `requests>=2.31.0` - HTTP client
- `dateparser>=1.2.0` - Flexible date parsing (ISO and relative formats)

Development dependencies:
- `flake8>=7.3.0` - Code linting

Development
-----------

To customize the local data folder path, replace the `CUSTOM_FOLDER` placeholder with your desired path in the `docker-compose.yml` file:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Clone this repository, initialize the workspace, and run the component using the following
commands:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
git clone https://github.com/keboola/component-shopify-v2 ex_shopify_v2
cd ex_shopify_v2
docker-compose build
docker-compose run --rm dev
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Run the test suite and perform lint checks using this command:

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
docker-compose run --rm test
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Integration
===========

For details about deployment and integration with Keboola, refer to the
[deployment section of the developer
documentation](https://developers.keboola.com/extend/component/deployment/).



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.13-slim
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/

WORKDIR /code/

COPY pyproject.toml .
COPY uv.lock .
ENV UV_PROJECT_ENVIRONMENT="/usr/local/"
RUN uv sync --all-groups --frozen

COPY src/ src
COPY tests/ tests
COPY scripts/ scripts
COPY .flake8 .
COPY deploy.sh .

CMD ["python", "-u", "src/component.py"]



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: pyproject.toml
================================================
[project]
name = "ex-shopify-v2"
dynamic = ["version"]
readme = "README.md"
requires-python = "~=3.13.0"
dependencies = [
    "freezegun>=1.5.5",
    "keboola-component>=1.6.13",
    "keboola-http-client>=1.0.1",
    "keboola-utils>=1.1.0",
    "mock>=5.2.0",
    "pydantic>=2.11.9",
    "shopifyapi>=12.7.0",
    "requests>=2.31.0",
    "gql>=4.0.0",
    "duckdb>=1.4.0",
    "dateparser>=1.2.2",
]

[dependency-groups]
dev = [
    "deptry>=0.23.1",
    "flake8>=7.3.0",
    "ruff>=0.13.3",
]

[tool.ruff]
line-length = 120



================================================
FILE: uv.lock
================================================
version = 1
revision = 3
requires-python = "==3.13.*"

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081, upload-time = "2024-05-20T21:33:25.928Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643, upload-time = "2024-05-20T21:33:24.1Z" },
]

[[package]]
name = "anyio"
version = "4.11.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/c6/78/7d432127c41b50bccba979505f272c16cbcadcc33645d5fa3a738110ae75/anyio-4.11.0.tar.gz", hash = "sha256:82a8d0b81e318cc5ce71a5f1f8b5c4e63619620b63141ef8c995fa0db95a57c4", size = 219094, upload-time = "2025-09-23T09:19:12.58Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/15/b3/9b1a8074496371342ec1e796a96f99c82c945a339cd81a8e73de28b4cf9e/anyio-4.11.0-py3-none-any.whl", hash = "sha256:0287e96f4d26d4149305414d4e3bc32f0dcd0862365a4bddea19d7a1ec38c4fc", size = 109097, upload-time = "2025-09-23T09:19:10.601Z" },
]

[[package]]
name = "backoff"
version = "2.2.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/47/d7/5bbeb12c44d7c4f2fb5b56abce497eb5ed9f34d85701de869acedd602619/backoff-2.2.1.tar.gz", hash = "sha256:03f829f5bb1923180821643f8753b0502c3b682293992485b0eef2807afa5cba", size = 17001, upload-time = "2022-10-05T19:19:32.061Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/df/73/b6e24bd22e6720ca8ee9a85a0c4a2971af8497d8f3193fa05390cbd46e09/backoff-2.2.1-py3-none-any.whl", hash = "sha256:63579f9a0628e06278f7e47b7d7d5b6ce20dc65c5e96a6f3ca99a6adca0396e8", size = 15148, upload-time = "2022-10-05T19:19:30.546Z" },
]

[[package]]
name = "certifi"
version = "2025.10.5"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4c/5b/b6ce21586237c77ce67d01dc5507039d444b630dd76611bbca2d8e5dcd91/certifi-2025.10.5.tar.gz", hash = "sha256:47c09d31ccf2acf0be3f701ea53595ee7e0b8fa08801c6624be771df09ae7b43", size = 164519, upload-time = "2025-10-05T04:12:15.808Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e4/37/af0d2ef3967ac0d6113837b44a4f0bfe1328c2b9763bd5b1744520e5cfed/certifi-2025.10.5-py3-none-any.whl", hash = "sha256:0f212c2744a9bb6de0c56639a6f68afe01ecd92d91f14ae897c4fe7bbeeef0de", size = 163286, upload-time = "2025-10-05T04:12:14.03Z" },
]

[[package]]
name = "charset-normalizer"
version = "3.4.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/13/69/33ddede1939fdd074bce5434295f38fae7136463422fe4fd3e0e89b98062/charset_normalizer-3.4.4.tar.gz", hash = "sha256:94537985111c35f28720e43603b8e7b43a6ecfb2ce1d3058bbe955b73404e21a", size = 129418, upload-time = "2025-10-14T04:42:32.879Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/97/45/4b3a1239bbacd321068ea6e7ac28875b03ab8bc0aa0966452db17cd36714/charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:e1f185f86a6f3403aa2420e815904c67b2f9ebc443f045edd0de921108345794", size = 208091, upload-time = "2025-10-14T04:41:13.346Z" },
    { url = "https://files.pythonhosted.org/packages/7d/62/73a6d7450829655a35bb88a88fca7d736f9882a27eacdca2c6d505b57e2e/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:6b39f987ae8ccdf0d2642338faf2abb1862340facc796048b604ef14919e55ed", size = 147936, upload-time = "2025-10-14T04:41:14.461Z" },
    { url = "https://files.pythonhosted.org/packages/89/c5/adb8c8b3d6625bef6d88b251bbb0d95f8205831b987631ab0c8bb5d937c2/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:3162d5d8ce1bb98dd51af660f2121c55d0fa541b46dff7bb9b9f86ea1d87de72", size = 144180, upload-time = "2025-10-14T04:41:15.588Z" },
    { url = "https://files.pythonhosted.org/packages/91/ed/9706e4070682d1cc219050b6048bfd293ccf67b3d4f5a4f39207453d4b99/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:81d5eb2a312700f4ecaa977a8235b634ce853200e828fbadf3a9c50bab278328", size = 161346, upload-time = "2025-10-14T04:41:16.738Z" },
    { url = "https://files.pythonhosted.org/packages/d5/0d/031f0d95e4972901a2f6f09ef055751805ff541511dc1252ba3ca1f80cf5/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:5bd2293095d766545ec1a8f612559f6b40abc0eb18bb2f5d1171872d34036ede", size = 158874, upload-time = "2025-10-14T04:41:17.923Z" },
    { url = "https://files.pythonhosted.org/packages/f5/83/6ab5883f57c9c801ce5e5677242328aa45592be8a00644310a008d04f922/charset_normalizer-3.4.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:a8a8b89589086a25749f471e6a900d3f662d1d3b6e2e59dcecf787b1cc3a1894", size = 153076, upload-time = "2025-10-14T04:41:19.106Z" },
    { url = "https://files.pythonhosted.org/packages/75/1e/5ff781ddf5260e387d6419959ee89ef13878229732732ee73cdae01800f2/charset_normalizer-3.4.4-cp313-cp313-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:bc7637e2f80d8530ee4a78e878bce464f70087ce73cf7c1caf142416923b98f1", size = 150601, upload-time = "2025-10-14T04:41:20.245Z" },
    { url = "https://files.pythonhosted.org/packages/d7/57/71be810965493d3510a6ca79b90c19e48696fb1ff964da319334b12677f0/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:f8bf04158c6b607d747e93949aa60618b61312fe647a6369f88ce2ff16043490", size = 150376, upload-time = "2025-10-14T04:41:21.398Z" },
    { url = "https://files.pythonhosted.org/packages/e5/d5/c3d057a78c181d007014feb7e9f2e65905a6c4ef182c0ddf0de2924edd65/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:554af85e960429cf30784dd47447d5125aaa3b99a6f0683589dbd27e2f45da44", size = 144825, upload-time = "2025-10-14T04:41:22.583Z" },
    { url = "https://files.pythonhosted.org/packages/e6/8c/d0406294828d4976f275ffbe66f00266c4b3136b7506941d87c00cab5272/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:74018750915ee7ad843a774364e13a3db91682f26142baddf775342c3f5b1133", size = 162583, upload-time = "2025-10-14T04:41:23.754Z" },
    { url = "https://files.pythonhosted.org/packages/d7/24/e2aa1f18c8f15c4c0e932d9287b8609dd30ad56dbe41d926bd846e22fb8d/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_riscv64.whl", hash = "sha256:c0463276121fdee9c49b98908b3a89c39be45d86d1dbaa22957e38f6321d4ce3", size = 150366, upload-time = "2025-10-14T04:41:25.27Z" },
    { url = "https://files.pythonhosted.org/packages/e4/5b/1e6160c7739aad1e2df054300cc618b06bf784a7a164b0f238360721ab86/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:362d61fd13843997c1c446760ef36f240cf81d3ebf74ac62652aebaf7838561e", size = 160300, upload-time = "2025-10-14T04:41:26.725Z" },
    { url = "https://files.pythonhosted.org/packages/7a/10/f882167cd207fbdd743e55534d5d9620e095089d176d55cb22d5322f2afd/charset_normalizer-3.4.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9a26f18905b8dd5d685d6d07b0cdf98a79f3c7a918906af7cc143ea2e164c8bc", size = 154465, upload-time = "2025-10-14T04:41:28.322Z" },
    { url = "https://files.pythonhosted.org/packages/89/66/c7a9e1b7429be72123441bfdbaf2bc13faab3f90b933f664db506dea5915/charset_normalizer-3.4.4-cp313-cp313-win32.whl", hash = "sha256:9b35f4c90079ff2e2edc5b26c0c77925e5d2d255c42c74fdb70fb49b172726ac", size = 99404, upload-time = "2025-10-14T04:41:29.95Z" },
    { url = "https://files.pythonhosted.org/packages/c4/26/b9924fa27db384bdcd97ab83b4f0a8058d96ad9626ead570674d5e737d90/charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl", hash = "sha256:b435cba5f4f750aa6c0a0d92c541fb79f69a387c91e61f1795227e4ed9cece14", size = 107092, upload-time = "2025-10-14T04:41:31.188Z" },
    { url = "https://files.pythonhosted.org/packages/af/8f/3ed4bfa0c0c72a7ca17f0380cd9e4dd842b09f664e780c13cff1dcf2ef1b/charset_normalizer-3.4.4-cp313-cp313-win_arm64.whl", hash = "sha256:542d2cee80be6f80247095cc36c418f7bddd14f4a6de45af91dfad36d817bba2", size = 100408, upload-time = "2025-10-14T04:41:32.624Z" },
    { url = "https://files.pythonhosted.org/packages/0a/4c/925909008ed5a988ccbb72dcc897407e5d6d3bd72410d69e051fc0c14647/charset_normalizer-3.4.4-py3-none-any.whl", hash = "sha256:7a32c560861a02ff789ad905a2fe94e3f840803362c84fecf1851cb4cf3dc37f", size = 53402, upload-time = "2025-10-14T04:42:31.76Z" },
]

[[package]]
name = "click"
version = "8.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/46/61/de6cd827efad202d7057d93e0fed9294b96952e188f7384832791c7b2254/click-8.3.0.tar.gz", hash = "sha256:e7b8232224eba16f4ebe410c25ced9f7875cb5f3263ffc93cc3e8da705e229c4", size = 276943, upload-time = "2025-09-18T17:32:23.696Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/db/d3/9dcc0f5797f070ec8edf30fbadfb200e71d9db6b84d211e3b2085a7589a0/click-8.3.0-py3-none-any.whl", hash = "sha256:9b9f285302c6e3064f4330c05f05b81945b2a39544279343e6e7c5f27a9baddc", size = 107295, upload-time = "2025-09-18T17:32:22.42Z" },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697, upload-time = "2022-10-25T02:36:22.414Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335, upload-time = "2022-10-25T02:36:20.889Z" },
]

[[package]]
name = "dateparser"
version = "1.2.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "python-dateutil" },
    { name = "pytz" },
    { name = "regex" },
    { name = "tzlocal" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a9/30/064144f0df1749e7bb5faaa7f52b007d7c2d08ec08fed8411aba87207f68/dateparser-1.2.2.tar.gz", hash = "sha256:986316f17cb8cdc23ea8ce563027c5ef12fc725b6fb1d137c14ca08777c5ecf7", size = 329840, upload-time = "2025-06-26T09:29:23.211Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/22/f020c047ae1346613db9322638186468238bcfa8849b4668a22b97faad65/dateparser-1.2.2-py3-none-any.whl", hash = "sha256:5a5d7211a09013499867547023a2a0c91d5a27d15dd4dbcea676ea9fe66f2482", size = 315453, upload-time = "2025-06-26T09:29:21.412Z" },
]

[[package]]
name = "deprecated"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "wrapt" },
]
sdist = { url = "https://files.pythonhosted.org/packages/49/85/12f0a49a7c4ffb70572b6c2ef13c90c88fd190debda93b23f026b25f9634/deprecated-1.3.1.tar.gz", hash = "sha256:b1b50e0ff0c1fddaa5708a2c6b0a6588bb09b892825ab2b214ac9ea9d92a5223", size = 2932523, upload-time = "2025-10-30T08:19:02.757Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/84/d0/205d54408c08b13550c733c4b85429e7ead111c7f0014309637425520a9a/deprecated-1.3.1-py2.py3-none-any.whl", hash = "sha256:597bfef186b6f60181535a29fbe44865ce137a5079f295b479886c82729d5f3f", size = 11298, upload-time = "2025-10-30T08:19:00.758Z" },
]

[[package]]
name = "deptry"
version = "0.23.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "packaging" },
    { name = "requirements-parser" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a3/31/3e2f4a9b43bd807b28a49d673b9b5f8dcc7265d43950b24e875ba90e6205/deptry-0.23.1.tar.gz", hash = "sha256:5d23e0ef25f3c56405c05383a476edda55944563c5c47a3e9249ed3ec860d382", size = 460016, upload-time = "2025-07-31T05:54:49.681Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/cb/d0/9785c0e7fdab12f5324467d70ba65ad03b9d4071a13fc182b6d98bab6208/deptry-0.23.1-cp39-abi3-macosx_10_12_x86_64.whl", hash = "sha256:f0b231d098fb5b48d8973c9f192c353ffdd395770063424969fa7f15ddfea7d8", size = 1768731, upload-time = "2025-07-31T05:54:47.348Z" },
    { url = "https://files.pythonhosted.org/packages/c5/4b/46aded35e0de153936b2214e49e5935179eed9f23cbd3a9a0cd9a5ab0abd/deptry-0.23.1-cp39-abi3-macosx_11_0_arm64.whl", hash = "sha256:bf057f514bb2fa18a2b192a7f7372bd14577ff46b11486933e8383dfef461983", size = 1667240, upload-time = "2025-07-31T05:54:43.956Z" },
    { url = "https://files.pythonhosted.org/packages/ef/f7/206330f68280a1af7edb8bea87f383dbaa4e3b02b37199d40f86e4c43048/deptry-0.23.1-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1ee3f5663bb1c048e2aaf25a4d9e6d09cc1f3b3396ee248980878c6a6c9c0e21", size = 1772019, upload-time = "2025-07-31T05:54:31.165Z" },
    { url = "https://files.pythonhosted.org/packages/c5/80/51a9e94349b47013e2fd78fd221b12202a7866cd2e0882cfd87d63055e88/deptry-0.23.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ae0366dc5f50a5fb29cf90de1110c5e368513de6c1b2dac439f2817f3f752616", size = 1855973, upload-time = "2025-07-31T05:54:37.733Z" },
    { url = "https://files.pythonhosted.org/packages/d5/7a/bff10ddd26ce39c56a9a35bdc98fcf44c2befe5954c8da4bb895e3f750bb/deptry-0.23.1-cp39-abi3-musllinux_1_1_aarch64.whl", hash = "sha256:ab156a90a9eda5819aeb1c1da585dd4d5ec509029399a38771a49e78f40db90f", size = 1946957, upload-time = "2025-07-31T05:54:34.567Z" },
    { url = "https://files.pythonhosted.org/packages/7e/b6/c80b190cbd817d1f75f8d02d4b6f4d430b2f3014a09d3895684e291e473b/deptry-0.23.1-cp39-abi3-musllinux_1_1_x86_64.whl", hash = "sha256:651c7eb168233755152fcc468713c024d64a03069645187edb4a17ba61ce6133", size = 2025282, upload-time = "2025-07-31T05:54:40.906Z" },
    { url = "https://files.pythonhosted.org/packages/3c/58/1dfb7a6c4ec2daf123264d2c30f53f45791fee46cd0244be5bf97597d2aa/deptry-0.23.1-cp39-abi3-win_amd64.whl", hash = "sha256:8da1e8f70e7086ebc228f3a4a3cfb5aa127b09b5eef60d694503d6bb79809025", size = 1631377, upload-time = "2025-07-31T05:54:51.951Z" },
    { url = "https://files.pythonhosted.org/packages/18/d3/667b974cf42fc50245a8028beb9966643ee214ca567cc6df6e876feca5ed/deptry-0.23.1-cp39-abi3-win_arm64.whl", hash = "sha256:f589497a5809717db4dcf2aa840f2847c0a4c489331608e538850b6a9ab1c30b", size = 1551113, upload-time = "2025-07-31T05:54:50.679Z" },
]

[[package]]
name = "duckdb"
version = "1.4.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ea/e7/21cf50a3d52ffceee1f0bcc3997fa96a5062e6bab705baee4f6c4e33cce5/duckdb-1.4.1.tar.gz", hash = "sha256:f903882f045d057ebccad12ac69975952832edfe133697694854bb784b8d6c76", size = 18461687, upload-time = "2025-10-07T10:37:28.605Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d7/08/705988c33e38665c969f7876b3ca4328be578554aa7e3dc0f34158da3e64/duckdb-1.4.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:46496a2518752ae0c6c5d75d4cdecf56ea23dd098746391176dd8e42cf157791", size = 29077070, upload-time = "2025-10-07T10:36:59.83Z" },
    { url = "https://files.pythonhosted.org/packages/99/c5/7c9165f1e6b9069441bcda4da1e19382d4a2357783d37ff9ae238c5c41ac/duckdb-1.4.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:1c65ae7e9b541cea07d8075343bcfebdecc29a3c0481aa6078ee63d51951cfcd", size = 16167506, upload-time = "2025-10-07T10:37:02.24Z" },
    { url = "https://files.pythonhosted.org/packages/38/46/267f4a570a0ee3ae6871ddc03435f9942884284e22a7ba9b7cb252ee69b6/duckdb-1.4.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:598d1a314e34b65d9399ddd066ccce1eeab6a60a2ef5885a84ce5ed62dbaf729", size = 13762330, upload-time = "2025-10-07T10:37:04.581Z" },
    { url = "https://files.pythonhosted.org/packages/15/7b/c4f272a40c36d82df20937d93a1780eb39ab0107fe42b62cba889151eab9/duckdb-1.4.1-cp313-cp313-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:e2f16b8def782d484a9f035fc422bb6f06941ed0054b4511ddcdc514a7fb6a75", size = 18504687, upload-time = "2025-10-07T10:37:06.991Z" },
    { url = "https://files.pythonhosted.org/packages/17/fc/9b958751f0116d7b0406406b07fa6f5a10c22d699be27826d0b896f9bf51/duckdb-1.4.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:a5a7d0aed068a5c33622a8848857947cab5cfb3f2a315b1251849bac2c74c492", size = 20513823, upload-time = "2025-10-07T10:37:09.349Z" },
    { url = "https://files.pythonhosted.org/packages/30/79/4f544d73fcc0513b71296cb3ebb28a227d22e80dec27204977039b9fa875/duckdb-1.4.1-cp313-cp313-win_amd64.whl", hash = "sha256:280fd663dacdd12bb3c3bf41f3e5b2e5b95e00b88120afabb8b8befa5f335c6f", size = 12336460, upload-time = "2025-10-07T10:37:12.154Z" },
]

[[package]]
name = "ex-shopify-v2"
source = { virtual = "." }
dependencies = [
    { name = "dateparser" },
    { name = "duckdb" },
    { name = "freezegun" },
    { name = "gql" },
    { name = "keboola-component" },
    { name = "keboola-http-client" },
    { name = "keboola-utils" },
    { name = "mock" },
    { name = "pydantic" },
    { name = "requests" },
    { name = "shopifyapi" },
]

[package.dev-dependencies]
dev = [
    { name = "deptry" },
    { name = "flake8" },
    { name = "ruff" },
]

[package.metadata]
requires-dist = [
    { name = "dateparser", specifier = ">=1.2.2" },
    { name = "duckdb", specifier = ">=1.4.0" },
    { name = "freezegun", specifier = ">=1.5.5" },
    { name = "gql", specifier = ">=4.0.0" },
    { name = "keboola-component", specifier = ">=1.6.13" },
    { name = "keboola-http-client", specifier = ">=1.0.1" },
    { name = "keboola-utils", specifier = ">=1.1.0" },
    { name = "mock", specifier = ">=5.2.0" },
    { name = "pydantic", specifier = ">=2.11.9" },
    { name = "requests", specifier = ">=2.31.0" },
    { name = "shopifyapi", specifier = ">=12.7.0" },
]

[package.metadata.requires-dev]
dev = [
    { name = "deptry", specifier = ">=0.23.1" },
    { name = "flake8", specifier = ">=7.3.0" },
    { name = "ruff", specifier = ">=0.13.3" },
]

[[package]]
name = "flake8"
version = "7.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mccabe" },
    { name = "pycodestyle" },
    { name = "pyflakes" },
]
sdist = { url = "https://files.pythonhosted.org/packages/9b/af/fbfe3c4b5a657d79e5c47a2827a362f9e1b763336a52f926126aa6dc7123/flake8-7.3.0.tar.gz", hash = "sha256:fe044858146b9fc69b551a4b490d69cf960fcb78ad1edcb84e7fbb1b4a8e3872", size = 48326, upload-time = "2025-06-20T19:31:35.838Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/9f/56/13ab06b4f93ca7cac71078fbe37fcea175d3216f31f85c3168a6bbd0bb9a/flake8-7.3.0-py2.py3-none-any.whl", hash = "sha256:b9696257b9ce8beb888cdbe31cf885c90d31928fe202be0889a7cdafad32f01e", size = 57922, upload-time = "2025-06-20T19:31:34.425Z" },
]

[[package]]
name = "freezegun"
version = "1.5.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "python-dateutil" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/dd/23e2f4e357f8fd3bdff613c1fe4466d21bfb00a6177f238079b17f7b1c84/freezegun-1.5.5.tar.gz", hash = "sha256:ac7742a6cc6c25a2c35e9292dfd554b897b517d2dec26891a2e8debf205cb94a", size = 35914, upload-time = "2025-08-09T10:39:08.338Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5e/2e/b41d8a1a917d6581fc27a35d05561037b048e47df50f27f8ac9c7e27a710/freezegun-1.5.5-py3-none-any.whl", hash = "sha256:cd557f4a75cf074e84bc374249b9dd491eaeacd61376b9eb3c423282211619d2", size = 19266, upload-time = "2025-08-09T10:39:06.636Z" },
]

[[package]]
name = "gql"
version = "4.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "backoff" },
    { name = "graphql-core" },
    { name = "yarl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/06/9f/cf224a88ed71eb223b7aa0b9ff0aa10d7ecc9a4acdca2279eb046c26d5dc/gql-4.0.0.tar.gz", hash = "sha256:f22980844eb6a7c0266ffc70f111b9c7e7c7c13da38c3b439afc7eab3d7c9c8e", size = 215644, upload-time = "2025-08-17T14:32:35.397Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ac/94/30bbd09e8d45339fa77a48f5778d74d47e9242c11b3cd1093b3d994770a5/gql-4.0.0-py3-none-any.whl", hash = "sha256:f3beed7c531218eb24d97cb7df031b4a84fdb462f4a2beb86e2633d395937479", size = 89900, upload-time = "2025-08-17T14:32:34.029Z" },
]

[[package]]
name = "graphql-core"
version = "3.2.7"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ac/9b/037a640a2983b09aed4a823f9cf1729e6d780b0671f854efa4727a7affbe/graphql_core-3.2.7.tar.gz", hash = "sha256:27b6904bdd3b43f2a0556dad5d579bdfdeab1f38e8e8788e555bdcb586a6f62c", size = 513484, upload-time = "2025-11-01T22:30:40.436Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0a/14/933037032608787fb92e365883ad6a741c235e0ff992865ec5d904a38f1e/graphql_core-3.2.7-py3-none-any.whl", hash = "sha256:17fc8f3ca4a42913d8e24d9ac9f08deddf0a0b2483076575757f6c412ead2ec0", size = 207262, upload-time = "2025-11-01T22:30:38.912Z" },
]

[[package]]
name = "idna"
version = "3.11"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/6f/6d/0703ccc57f3a7233505399edb88de3cbd678da106337b9fcde432b65ed60/idna-3.11.tar.gz", hash = "sha256:795dafcc9c04ed0c1fb032c2aa73654d8e8c5023a7df64a53f39190ada629902", size = 194582, upload-time = "2025-10-12T14:55:20.501Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0e/61/66938bbb5fc52dbdf84594873d5b51fb1f7c7794e9c0f5bd885f30bc507b/idna-3.11-py3-none-any.whl", hash = "sha256:771a87f49d9defaf64091e6e6fe9c18d4833f140bd19464795bc32d966ca37ea", size = 71008, upload-time = "2025-10-12T14:55:18.883Z" },
]

[[package]]
name = "keboola-component"
version = "1.6.13"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "deprecated" },
    { name = "pygelf" },
    { name = "pytz" },
]
sdist = { url = "https://files.pythonhosted.org/packages/91/e9/80a83c04bccaad7f5d7b6e8b9c0305085db2bd22838f7323f57aaaefd2f4/keboola.component-1.6.13.tar.gz", hash = "sha256:11b072da1cab39233ff798217a876cdacf17f446decdb89735f295ca20662874", size = 59550, upload-time = "2025-09-15T14:00:47.874Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a4/35/f4dab3d75a155260b90c72bb37d03a9bfd24dc18f52ccea2d61e587dbef6/keboola.component-1.6.13-py3-none-any.whl", hash = "sha256:eceda4c2d083b3857d6eb98e30d7bfe48a48ed0f8567e6898218cee4d9391318", size = 44151, upload-time = "2025-09-15T14:00:46.291Z" },
]

[[package]]
name = "keboola-http-client"
version = "1.0.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "requests" },
]
sdist = { url = "https://files.pythonhosted.org/packages/bd/46/81805cea9f7eff8af310a43e39b312794c9143b89fdcfdc5f502ed1818c6/keboola.http_client-1.0.1.tar.gz", hash = "sha256:58f828c61a709ac484e85e2acb5e78eae2db402e7846769b739a6c336384fa81", size = 10513, upload-time = "2024-12-05T13:53:56.975Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/27/18/293557d9245a1ec0e0826202ca341bb769033121d1bd1991226db908a5df/keboola.http_client-1.0.1-py3-none-any.whl", hash = "sha256:5d570789433c65325937f6d466cc6b8edc0558f984f03846f0200ae5c5b4b140", size = 8958, upload-time = "2024-12-05T13:53:55.413Z" },
]

[[package]]
name = "keboola-utils"
version = "1.1.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "dateparser" },
    { name = "pytz" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a7/b8/ccfddc2eb510f7a6ab878ab8a6249a23494194780a436676da6c2f5d23c7/keboola.utils-1.1.0.tar.gz", hash = "sha256:e943dbda932d945bcd5edd51283eea8f7035249c9dac769d3e96d2f507b52f60", size = 9830, upload-time = "2021-04-09T11:11:49.828Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f9/f4/6697a0c2ff512baa7b84413972e51d5449a0a145f68dc750f05a8b1da39d/keboola.utils-1.1.0-py3-none-any.whl", hash = "sha256:8c73faa4a81f371a2eecd8465b08a51b3f7608969dd91d38d5b3bcfad7ef0da5", size = 10131, upload-time = "2021-04-09T11:11:48.826Z" },
]

[[package]]
name = "mccabe"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e7/ff/0ffefdcac38932a54d2b5eed4e0ba8a408f215002cd178ad1df0f2806ff8/mccabe-0.7.0.tar.gz", hash = "sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325", size = 9658, upload-time = "2022-01-24T01:14:51.113Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/27/1a/1f68f9ba0c207934b35b86a8ca3aad8395a3d6dd7921c0686e23853ff5a9/mccabe-0.7.0-py2.py3-none-any.whl", hash = "sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e", size = 7350, upload-time = "2022-01-24T01:14:49.62Z" },
]

[[package]]
name = "mock"
version = "5.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/07/8c/14c2ae915e5f9dca5a22edd68b35be94400719ccfa068a03e0fb63d0f6f6/mock-5.2.0.tar.gz", hash = "sha256:4e460e818629b4b173f32d08bf30d3af8123afbb8e04bb5707a1fd4799e503f0", size = 92796, upload-time = "2025-03-03T12:31:42.911Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/d9/617e6af809bf3a1d468e0d58c3997b1dc219a9a9202e650d30c2fc85d481/mock-5.2.0-py3-none-any.whl", hash = "sha256:7ba87f72ca0e915175596069dbbcc7c75af7b5e9b9bc107ad6349ede0819982f", size = 31617, upload-time = "2025-03-03T12:31:41.518Z" },
]

[[package]]
name = "multidict"
version = "6.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/80/1e/5492c365f222f907de1039b91f922b93fa4f764c713ee858d235495d8f50/multidict-6.7.0.tar.gz", hash = "sha256:c6e99d9a65ca282e578dfea819cfa9c0a62b2499d8677392e09feaf305e9e6f5", size = 101834, upload-time = "2025-10-06T14:52:30.657Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d2/86/33272a544eeb36d66e4d9a920602d1a2f57d4ebea4ef3cdfe5a912574c95/multidict-6.7.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:bee7c0588aa0076ce77c0ea5d19a68d76ad81fcd9fe8501003b9a24f9d4000f6", size = 76135, upload-time = "2025-10-06T14:49:54.26Z" },
    { url = "https://files.pythonhosted.org/packages/91/1c/eb97db117a1ebe46d457a3d235a7b9d2e6dcab174f42d1b67663dd9e5371/multidict-6.7.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:7ef6b61cad77091056ce0e7ce69814ef72afacb150b7ac6a3e9470def2198159", size = 45117, upload-time = "2025-10-06T14:49:55.82Z" },
    { url = "https://files.pythonhosted.org/packages/f1/d8/6c3442322e41fb1dd4de8bd67bfd11cd72352ac131f6368315617de752f1/multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:9c0359b1ec12b1d6849c59f9d319610b7f20ef990a6d454ab151aa0e3b9f78ca", size = 43472, upload-time = "2025-10-06T14:49:57.048Z" },
    { url = "https://files.pythonhosted.org/packages/75/3f/e2639e80325af0b6c6febdf8e57cc07043ff15f57fa1ef808f4ccb5ac4cd/multidict-6.7.0-cp313-cp313-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:cd240939f71c64bd658f186330603aac1a9a81bf6273f523fca63673cb7378a8", size = 249342, upload-time = "2025-10-06T14:49:58.368Z" },
    { url = "https://files.pythonhosted.org/packages/5d/cc/84e0585f805cbeaa9cbdaa95f9a3d6aed745b9d25700623ac89a6ecff400/multidict-6.7.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:a60a4d75718a5efa473ebd5ab685786ba0c67b8381f781d1be14da49f1a2dc60", size = 257082, upload-time = "2025-10-06T14:49:59.89Z" },
    { url = "https://files.pythonhosted.org/packages/b0/9c/ac851c107c92289acbbf5cfb485694084690c1b17e555f44952c26ddc5bd/multidict-6.7.0-cp313-cp313-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:53a42d364f323275126aff81fb67c5ca1b7a04fda0546245730a55c8c5f24bc4", size = 240704, upload-time = "2025-10-06T14:50:01.485Z" },
    { url = "https://files.pythonhosted.org/packages/50/cc/5f93e99427248c09da95b62d64b25748a5f5c98c7c2ab09825a1d6af0e15/multidict-6.7.0-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:3b29b980d0ddbecb736735ee5bef69bb2ddca56eff603c86f3f29a1128299b4f", size = 266355, upload-time = "2025-10-06T14:50:02.955Z" },
    { url = "https://files.pythonhosted.org/packages/ec/0c/2ec1d883ceb79c6f7f6d7ad90c919c898f5d1c6ea96d322751420211e072/multidict-6.7.0-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:f8a93b1c0ed2d04b97a5e9336fd2d33371b9a6e29ab7dd6503d63407c20ffbaf", size = 267259, upload-time = "2025-10-06T14:50:04.446Z" },
    { url = "https://files.pythonhosted.org/packages/c6/2d/f0b184fa88d6630aa267680bdb8623fb69cb0d024b8c6f0d23f9a0f406d3/multidict-6.7.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:9ff96e8815eecacc6645da76c413eb3b3d34cfca256c70b16b286a687d013c32", size = 254903, upload-time = "2025-10-06T14:50:05.98Z" },
    { url = "https://files.pythonhosted.org/packages/06/c9/11ea263ad0df7dfabcad404feb3c0dd40b131bc7f232d5537f2fb1356951/multidict-6.7.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:7516c579652f6a6be0e266aec0acd0db80829ca305c3d771ed898538804c2036", size = 252365, upload-time = "2025-10-06T14:50:07.511Z" },
    { url = "https://files.pythonhosted.org/packages/41/88/d714b86ee2c17d6e09850c70c9d310abac3d808ab49dfa16b43aba9d53fd/multidict-6.7.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:040f393368e63fb0f3330e70c26bfd336656bed925e5cbe17c9da839a6ab13ec", size = 250062, upload-time = "2025-10-06T14:50:09.074Z" },
    { url = "https://files.pythonhosted.org/packages/15/fe/ad407bb9e818c2b31383f6131ca19ea7e35ce93cf1310fce69f12e89de75/multidict-6.7.0-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:b3bc26a951007b1057a1c543af845f1c7e3e71cc240ed1ace7bf4484aa99196e", size = 249683, upload-time = "2025-10-06T14:50:10.714Z" },
    { url = "https://files.pythonhosted.org/packages/8c/a4/a89abdb0229e533fb925e7c6e5c40201c2873efebc9abaf14046a4536ee6/multidict-6.7.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:7b022717c748dd1992a83e219587aabe45980d88969f01b316e78683e6285f64", size = 261254, upload-time = "2025-10-06T14:50:12.28Z" },
    { url = "https://files.pythonhosted.org/packages/8d/aa/0e2b27bd88b40a4fb8dc53dd74eecac70edaa4c1dd0707eb2164da3675b3/multidict-6.7.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:9600082733859f00d79dee64effc7aef1beb26adb297416a4ad2116fd61374bd", size = 257967, upload-time = "2025-10-06T14:50:14.16Z" },
    { url = "https://files.pythonhosted.org/packages/d0/8e/0c67b7120d5d5f6d874ed85a085f9dc770a7f9d8813e80f44a9fec820bb7/multidict-6.7.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:94218fcec4d72bc61df51c198d098ce2b378e0ccbac41ddbed5ef44092913288", size = 250085, upload-time = "2025-10-06T14:50:15.639Z" },
    { url = "https://files.pythonhosted.org/packages/ba/55/b73e1d624ea4b8fd4dd07a3bb70f6e4c7c6c5d9d640a41c6ffe5cdbd2a55/multidict-6.7.0-cp313-cp313-win32.whl", hash = "sha256:a37bd74c3fa9d00be2d7b8eca074dc56bd8077ddd2917a839bd989612671ed17", size = 41713, upload-time = "2025-10-06T14:50:17.066Z" },
    { url = "https://files.pythonhosted.org/packages/32/31/75c59e7d3b4205075b4c183fa4ca398a2daf2303ddf616b04ae6ef55cffe/multidict-6.7.0-cp313-cp313-win_amd64.whl", hash = "sha256:30d193c6cc6d559db42b6bcec8a5d395d34d60c9877a0b71ecd7c204fcf15390", size = 45915, upload-time = "2025-10-06T14:50:18.264Z" },
    { url = "https://files.pythonhosted.org/packages/31/2a/8987831e811f1184c22bc2e45844934385363ee61c0a2dcfa8f71b87e608/multidict-6.7.0-cp313-cp313-win_arm64.whl", hash = "sha256:ea3334cabe4d41b7ccd01e4d349828678794edbc2d3ae97fc162a3312095092e", size = 43077, upload-time = "2025-10-06T14:50:19.853Z" },
    { url = "https://files.pythonhosted.org/packages/e8/68/7b3a5170a382a340147337b300b9eb25a9ddb573bcdfff19c0fa3f31ffba/multidict-6.7.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:ad9ce259f50abd98a1ca0aa6e490b58c316a0fce0617f609723e40804add2c00", size = 83114, upload-time = "2025-10-06T14:50:21.223Z" },
    { url = "https://files.pythonhosted.org/packages/55/5c/3fa2d07c84df4e302060f555bbf539310980362236ad49f50eeb0a1c1eb9/multidict-6.7.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:07f5594ac6d084cbb5de2df218d78baf55ef150b91f0ff8a21cc7a2e3a5a58eb", size = 48442, upload-time = "2025-10-06T14:50:22.871Z" },
    { url = "https://files.pythonhosted.org/packages/fc/56/67212d33239797f9bd91962bb899d72bb0f4c35a8652dcdb8ed049bef878/multidict-6.7.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:0591b48acf279821a579282444814a2d8d0af624ae0bc600aa4d1b920b6e924b", size = 46885, upload-time = "2025-10-06T14:50:24.258Z" },
    { url = "https://files.pythonhosted.org/packages/46/d1/908f896224290350721597a61a69cd19b89ad8ee0ae1f38b3f5cd12ea2ac/multidict-6.7.0-cp313-cp313t-manylinux1_i686.manylinux_2_28_i686.manylinux_2_5_i686.whl", hash = "sha256:749a72584761531d2b9467cfbdfd29487ee21124c304c4b6cb760d8777b27f9c", size = 242588, upload-time = "2025-10-06T14:50:25.716Z" },
    { url = "https://files.pythonhosted.org/packages/ab/67/8604288bbd68680eee0ab568fdcb56171d8b23a01bcd5cb0c8fedf6e5d99/multidict-6.7.0-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:6b4c3d199f953acd5b446bf7c0de1fe25d94e09e79086f8dc2f48a11a129cdf1", size = 249966, upload-time = "2025-10-06T14:50:28.192Z" },
    { url = "https://files.pythonhosted.org/packages/20/33/9228d76339f1ba51e3efef7da3ebd91964d3006217aae13211653193c3ff/multidict-6.7.0-cp313-cp313t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:9fb0211dfc3b51efea2f349ec92c114d7754dd62c01f81c3e32b765b70c45c9b", size = 228618, upload-time = "2025-10-06T14:50:29.82Z" },
    { url = "https://files.pythonhosted.org/packages/f8/2d/25d9b566d10cab1c42b3b9e5b11ef79c9111eaf4463b8c257a3bd89e0ead/multidict-6.7.0-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:a027ec240fe73a8d6281872690b988eed307cd7d91b23998ff35ff577ca688b5", size = 257539, upload-time = "2025-10-06T14:50:31.731Z" },
    { url = "https://files.pythonhosted.org/packages/b6/b1/8d1a965e6637fc33de3c0d8f414485c2b7e4af00f42cab3d84e7b955c222/multidict-6.7.0-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:d1d964afecdf3a8288789df2f5751dc0a8261138c3768d9af117ed384e538fad", size = 256345, upload-time = "2025-10-06T14:50:33.26Z" },
    { url = "https://files.pythonhosted.org/packages/ba/0c/06b5a8adbdeedada6f4fb8d8f193d44a347223b11939b42953eeb6530b6b/multidict-6.7.0-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:caf53b15b1b7df9fbd0709aa01409000a2b4dd03a5f6f5cc548183c7c8f8b63c", size = 247934, upload-time = "2025-10-06T14:50:34.808Z" },
    { url = "https://files.pythonhosted.org/packages/8f/31/b2491b5fe167ca044c6eb4b8f2c9f3b8a00b24c432c365358eadac5d7625/multidict-6.7.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:654030da3197d927f05a536a66186070e98765aa5142794c9904555d3a9d8fb5", size = 245243, upload-time = "2025-10-06T14:50:36.436Z" },
    { url = "https://files.pythonhosted.org/packages/61/1a/982913957cb90406c8c94f53001abd9eafc271cb3e70ff6371590bec478e/multidict-6.7.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:2090d3718829d1e484706a2f525e50c892237b2bf9b17a79b059cb98cddc2f10", size = 235878, upload-time = "2025-10-06T14:50:37.953Z" },
    { url = "https://files.pythonhosted.org/packages/be/c0/21435d804c1a1cf7a2608593f4d19bca5bcbd7a81a70b253fdd1c12af9c0/multidict-6.7.0-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:2d2cfeec3f6f45651b3d408c4acec0ebf3daa9bc8a112a084206f5db5d05b754", size = 243452, upload-time = "2025-10-06T14:50:39.574Z" },
    { url = "https://files.pythonhosted.org/packages/54/0a/4349d540d4a883863191be6eb9a928846d4ec0ea007d3dcd36323bb058ac/multidict-6.7.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:4ef089f985b8c194d341eb2c24ae6e7408c9a0e2e5658699c92f497437d88c3c", size = 252312, upload-time = "2025-10-06T14:50:41.612Z" },
    { url = "https://files.pythonhosted.org/packages/26/64/d5416038dbda1488daf16b676e4dbfd9674dde10a0cc8f4fc2b502d8125d/multidict-6.7.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e93a0617cd16998784bf4414c7e40f17a35d2350e5c6f0bd900d3a8e02bd3762", size = 246935, upload-time = "2025-10-06T14:50:43.972Z" },
    { url = "https://files.pythonhosted.org/packages/9f/8c/8290c50d14e49f35e0bd4abc25e1bc7711149ca9588ab7d04f886cdf03d9/multidict-6.7.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:f0feece2ef8ebc42ed9e2e8c78fc4aa3cf455733b507c09ef7406364c94376c6", size = 243385, upload-time = "2025-10-06T14:50:45.648Z" },
    { url = "https://files.pythonhosted.org/packages/ef/a0/f83ae75e42d694b3fbad3e047670e511c138be747bc713cf1b10d5096416/multidict-6.7.0-cp313-cp313t-win32.whl", hash = "sha256:19a1d55338ec1be74ef62440ca9e04a2f001a04d0cc49a4983dc320ff0f3212d", size = 47777, upload-time = "2025-10-06T14:50:47.154Z" },
    { url = "https://files.pythonhosted.org/packages/dc/80/9b174a92814a3830b7357307a792300f42c9e94664b01dee8e457551fa66/multidict-6.7.0-cp313-cp313t-win_amd64.whl", hash = "sha256:3da4fb467498df97e986af166b12d01f05d2e04f978a9c1c680ea1988e0bc4b6", size = 53104, upload-time = "2025-10-06T14:50:48.851Z" },
    { url = "https://files.pythonhosted.org/packages/cc/28/04baeaf0428d95bb7a7bea0e691ba2f31394338ba424fb0679a9ed0f4c09/multidict-6.7.0-cp313-cp313t-win_arm64.whl", hash = "sha256:b4121773c49a0776461f4a904cdf6264c88e42218aaa8407e803ca8025872792", size = 45503, upload-time = "2025-10-06T14:50:50.16Z" },
    { url = "https://files.pythonhosted.org/packages/b7/da/7d22601b625e241d4f23ef1ebff8acfc60da633c9e7e7922e24d10f592b3/multidict-6.7.0-py3-none-any.whl", hash = "sha256:394fc5c42a333c9ffc3e421a4c85e08580d990e08b99f6bf35b4132114c5dcb3", size = 12317, upload-time = "2025-10-06T14:52:29.272Z" },
]

[[package]]
name = "packaging"
version = "25.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a1/d4/1fc4078c65507b51b96ca8f8c3ba19e6a61c8253c72794544580a7b6c24d/packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f", size = 165727, upload-time = "2025-04-19T11:48:59.673Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484", size = 66469, upload-time = "2025-04-19T11:48:57.875Z" },
]

[[package]]
name = "propcache"
version = "0.4.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/9e/da/e9fc233cf63743258bff22b3dfa7ea5baef7b5bc324af47a0ad89b8ffc6f/propcache-0.4.1.tar.gz", hash = "sha256:f48107a8c637e80362555f37ecf49abe20370e557cc4ab374f04ec4423c97c3d", size = 46442, upload-time = "2025-10-08T19:49:02.291Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bf/df/6d9c1b6ac12b003837dde8a10231a7344512186e87b36e855bef32241942/propcache-0.4.1-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:43eedf29202c08550aac1d14e0ee619b0430aaef78f85864c1a892294fbc28cf", size = 77750, upload-time = "2025-10-08T19:47:07.648Z" },
    { url = "https://files.pythonhosted.org/packages/8b/e8/677a0025e8a2acf07d3418a2e7ba529c9c33caf09d3c1f25513023c1db56/propcache-0.4.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:d62cdfcfd89ccb8de04e0eda998535c406bf5e060ffd56be6c586cbcc05b3311", size = 44780, upload-time = "2025-10-08T19:47:08.851Z" },
    { url = "https://files.pythonhosted.org/packages/89/a4/92380f7ca60f99ebae761936bc48a72a639e8a47b29050615eef757cb2a7/propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:cae65ad55793da34db5f54e4029b89d3b9b9490d8abe1b4c7ab5d4b8ec7ebf74", size = 46308, upload-time = "2025-10-08T19:47:09.982Z" },
    { url = "https://files.pythonhosted.org/packages/2d/48/c5ac64dee5262044348d1d78a5f85dd1a57464a60d30daee946699963eb3/propcache-0.4.1-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:333ddb9031d2704a301ee3e506dc46b1fe5f294ec198ed6435ad5b6a085facfe", size = 208182, upload-time = "2025-10-08T19:47:11.319Z" },
    { url = "https://files.pythonhosted.org/packages/c6/0c/cd762dd011a9287389a6a3eb43aa30207bde253610cca06824aeabfe9653/propcache-0.4.1-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:fd0858c20f078a32cf55f7e81473d96dcf3b93fd2ccdb3d40fdf54b8573df3af", size = 211215, upload-time = "2025-10-08T19:47:13.146Z" },
    { url = "https://files.pythonhosted.org/packages/30/3e/49861e90233ba36890ae0ca4c660e95df565b2cd15d4a68556ab5865974e/propcache-0.4.1-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:678ae89ebc632c5c204c794f8dab2837c5f159aeb59e6ed0539500400577298c", size = 218112, upload-time = "2025-10-08T19:47:14.913Z" },
    { url = "https://files.pythonhosted.org/packages/f1/8b/544bc867e24e1bd48f3118cecd3b05c694e160a168478fa28770f22fd094/propcache-0.4.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:d472aeb4fbf9865e0c6d622d7f4d54a4e101a89715d8904282bb5f9a2f476c3f", size = 204442, upload-time = "2025-10-08T19:47:16.277Z" },
    { url = "https://files.pythonhosted.org/packages/50/a6/4282772fd016a76d3e5c0df58380a5ea64900afd836cec2c2f662d1b9bb3/propcache-0.4.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:4d3df5fa7e36b3225954fba85589da77a0fe6a53e3976de39caf04a0db4c36f1", size = 199398, upload-time = "2025-10-08T19:47:17.962Z" },
    { url = "https://files.pythonhosted.org/packages/3e/ec/d8a7cd406ee1ddb705db2139f8a10a8a427100347bd698e7014351c7af09/propcache-0.4.1-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:ee17f18d2498f2673e432faaa71698032b0127ebf23ae5974eeaf806c279df24", size = 196920, upload-time = "2025-10-08T19:47:19.355Z" },
    { url = "https://files.pythonhosted.org/packages/f6/6c/f38ab64af3764f431e359f8baf9e0a21013e24329e8b85d2da32e8ed07ca/propcache-0.4.1-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:580e97762b950f993ae618e167e7be9256b8353c2dcd8b99ec100eb50f5286aa", size = 203748, upload-time = "2025-10-08T19:47:21.338Z" },
    { url = "https://files.pythonhosted.org/packages/d6/e3/fa846bd70f6534d647886621388f0a265254d30e3ce47e5c8e6e27dbf153/propcache-0.4.1-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:501d20b891688eb8e7aa903021f0b72d5a55db40ffaab27edefd1027caaafa61", size = 205877, upload-time = "2025-10-08T19:47:23.059Z" },
    { url = "https://files.pythonhosted.org/packages/e2/39/8163fc6f3133fea7b5f2827e8eba2029a0277ab2c5beee6c1db7b10fc23d/propcache-0.4.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9a0bd56e5b100aef69bd8562b74b46254e7c8812918d3baa700c8a8009b0af66", size = 199437, upload-time = "2025-10-08T19:47:24.445Z" },
    { url = "https://files.pythonhosted.org/packages/93/89/caa9089970ca49c7c01662bd0eeedfe85494e863e8043565aeb6472ce8fe/propcache-0.4.1-cp313-cp313-win32.whl", hash = "sha256:bcc9aaa5d80322bc2fb24bb7accb4a30f81e90ab8d6ba187aec0744bc302ad81", size = 37586, upload-time = "2025-10-08T19:47:25.736Z" },
    { url = "https://files.pythonhosted.org/packages/f5/ab/f76ec3c3627c883215b5c8080debb4394ef5a7a29be811f786415fc1e6fd/propcache-0.4.1-cp313-cp313-win_amd64.whl", hash = "sha256:381914df18634f5494334d201e98245c0596067504b9372d8cf93f4bb23e025e", size = 40790, upload-time = "2025-10-08T19:47:26.847Z" },
    { url = "https://files.pythonhosted.org/packages/59/1b/e71ae98235f8e2ba5004d8cb19765a74877abf189bc53fc0c80d799e56c3/propcache-0.4.1-cp313-cp313-win_arm64.whl", hash = "sha256:8873eb4460fd55333ea49b7d189749ecf6e55bf85080f11b1c4530ed3034cba1", size = 37158, upload-time = "2025-10-08T19:47:27.961Z" },
    { url = "https://files.pythonhosted.org/packages/83/ce/a31bbdfc24ee0dcbba458c8175ed26089cf109a55bbe7b7640ed2470cfe9/propcache-0.4.1-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:92d1935ee1f8d7442da9c0c4fa7ac20d07e94064184811b685f5c4fada64553b", size = 81451, upload-time = "2025-10-08T19:47:29.445Z" },
    { url = "https://files.pythonhosted.org/packages/25/9c/442a45a470a68456e710d96cacd3573ef26a1d0a60067e6a7d5e655621ed/propcache-0.4.1-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:473c61b39e1460d386479b9b2f337da492042447c9b685f28be4f74d3529e566", size = 46374, upload-time = "2025-10-08T19:47:30.579Z" },
    { url = "https://files.pythonhosted.org/packages/f4/bf/b1d5e21dbc3b2e889ea4327044fb16312a736d97640fb8b6aa3f9c7b3b65/propcache-0.4.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:c0ef0aaafc66fbd87842a3fe3902fd889825646bc21149eafe47be6072725835", size = 48396, upload-time = "2025-10-08T19:47:31.79Z" },
    { url = "https://files.pythonhosted.org/packages/f4/04/5b4c54a103d480e978d3c8a76073502b18db0c4bc17ab91b3cb5092ad949/propcache-0.4.1-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:f95393b4d66bfae908c3ca8d169d5f79cd65636ae15b5e7a4f6e67af675adb0e", size = 275950, upload-time = "2025-10-08T19:47:33.481Z" },
    { url = "https://files.pythonhosted.org/packages/b4/c1/86f846827fb969c4b78b0af79bba1d1ea2156492e1b83dea8b8a6ae27395/propcache-0.4.1-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:c07fda85708bc48578467e85099645167a955ba093be0a2dcba962195676e859", size = 273856, upload-time = "2025-10-08T19:47:34.906Z" },
    { url = "https://files.pythonhosted.org/packages/36/1d/fc272a63c8d3bbad6878c336c7a7dea15e8f2d23a544bda43205dfa83ada/propcache-0.4.1-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:af223b406d6d000830c6f65f1e6431783fc3f713ba3e6cc8c024d5ee96170a4b", size = 280420, upload-time = "2025-10-08T19:47:36.338Z" },
    { url = "https://files.pythonhosted.org/packages/07/0c/01f2219d39f7e53d52e5173bcb09c976609ba30209912a0680adfb8c593a/propcache-0.4.1-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:a78372c932c90ee474559c5ddfffd718238e8673c340dc21fe45c5b8b54559a0", size = 263254, upload-time = "2025-10-08T19:47:37.692Z" },
    { url = "https://files.pythonhosted.org/packages/2d/18/cd28081658ce597898f0c4d174d4d0f3c5b6d4dc27ffafeef835c95eb359/propcache-0.4.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:564d9f0d4d9509e1a870c920a89b2fec951b44bf5ba7d537a9e7c1ccec2c18af", size = 261205, upload-time = "2025-10-08T19:47:39.659Z" },
    { url = "https://files.pythonhosted.org/packages/7a/71/1f9e22eb8b8316701c2a19fa1f388c8a3185082607da8e406a803c9b954e/propcache-0.4.1-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:17612831fda0138059cc5546f4d12a2aacfb9e47068c06af35c400ba58ba7393", size = 247873, upload-time = "2025-10-08T19:47:41.084Z" },
    { url = "https://files.pythonhosted.org/packages/4a/65/3d4b61f36af2b4eddba9def857959f1016a51066b4f1ce348e0cf7881f58/propcache-0.4.1-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:41a89040cb10bd345b3c1a873b2bf36413d48da1def52f268a055f7398514874", size = 262739, upload-time = "2025-10-08T19:47:42.51Z" },
    { url = "https://files.pythonhosted.org/packages/2a/42/26746ab087faa77c1c68079b228810436ccd9a5ce9ac85e2b7307195fd06/propcache-0.4.1-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:e35b88984e7fa64aacecea39236cee32dd9bd8c55f57ba8a75cf2399553f9bd7", size = 263514, upload-time = "2025-10-08T19:47:43.927Z" },
    { url = "https://files.pythonhosted.org/packages/94/13/630690fe201f5502d2403dd3cfd451ed8858fe3c738ee88d095ad2ff407b/propcache-0.4.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:6f8b465489f927b0df505cbe26ffbeed4d6d8a2bbc61ce90eb074ff129ef0ab1", size = 257781, upload-time = "2025-10-08T19:47:45.448Z" },
    { url = "https://files.pythonhosted.org/packages/92/f7/1d4ec5841505f423469efbfc381d64b7b467438cd5a4bbcbb063f3b73d27/propcache-0.4.1-cp313-cp313t-win32.whl", hash = "sha256:2ad890caa1d928c7c2965b48f3a3815c853180831d0e5503d35cf00c472f4717", size = 41396, upload-time = "2025-10-08T19:47:47.202Z" },
    { url = "https://files.pythonhosted.org/packages/48/f0/615c30622316496d2cbbc29f5985f7777d3ada70f23370608c1d3e081c1f/propcache-0.4.1-cp313-cp313t-win_amd64.whl", hash = "sha256:f7ee0e597f495cf415bcbd3da3caa3bd7e816b74d0d52b8145954c5e6fd3ff37", size = 44897, upload-time = "2025-10-08T19:47:48.336Z" },
    { url = "https://files.pythonhosted.org/packages/fd/ca/6002e46eccbe0e33dcd4069ef32f7f1c9e243736e07adca37ae8c4830ec3/propcache-0.4.1-cp313-cp313t-win_arm64.whl", hash = "sha256:929d7cbe1f01bb7baffb33dc14eb5691c95831450a26354cd210a8155170c93a", size = 39789, upload-time = "2025-10-08T19:47:49.876Z" },
    { url = "https://files.pythonhosted.org/packages/5b/5a/bc7b4a4ef808fa59a816c17b20c4bef6884daebbdf627ff2a161da67da19/propcache-0.4.1-py3-none-any.whl", hash = "sha256:af2a6052aeb6cf17d3e46ee169099044fd8224cbaf75c76a2ef596e8163e2237", size = 13305, upload-time = "2025-10-08T19:49:00.792Z" },
]

[[package]]
name = "pyactiveresource"
version = "2.2.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/2e/07/69bc0e9753e700d1afdcbbf4617d8f22560e73ecd89264721e8d34510cec/pyactiveresource-2.2.2.tar.gz", hash = "sha256:2f03844652dc206d9a086b0b15564d78dcec55786fa5fe0055dd2119e0dffdd8", size = 19659, upload-time = "2021-02-04T21:19:12.73Z" }

[[package]]
name = "pycodestyle"
version = "2.14.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/11/e0/abfd2a0d2efe47670df87f3e3a0e2edda42f055053c85361f19c0e2c1ca8/pycodestyle-2.14.0.tar.gz", hash = "sha256:c4b5b517d278089ff9d0abdec919cd97262a3367449ea1c8b49b91529167b783", size = 39472, upload-time = "2025-06-20T18:49:48.75Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d7/27/a58ddaf8c588a3ef080db9d0b7e0b97215cee3a45df74f3a94dbbf5c893a/pycodestyle-2.14.0-py2.py3-none-any.whl", hash = "sha256:dd6bf7cb4ee77f8e016f9c8e74a35ddd9f67e1d5fd4184d86c3b98e07099f42d", size = 31594, upload-time = "2025-06-20T18:49:47.491Z" },
]

[[package]]
name = "pydantic"
version = "2.12.3"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
    { name = "typing-inspection" },
]
sdist = { url = "https://files.pythonhosted.org/packages/f3/1e/4f0a3233767010308f2fd6bd0814597e3f63f1dc98304a9112b8759df4ff/pydantic-2.12.3.tar.gz", hash = "sha256:1da1c82b0fc140bb0103bc1441ffe062154c8d38491189751ee00fd8ca65ce74", size = 819383, upload-time = "2025-10-17T15:04:21.222Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a1/6b/83661fa77dcefa195ad5f8cd9af3d1a7450fd57cc883ad04d65446ac2029/pydantic-2.12.3-py3-none-any.whl", hash = "sha256:6986454a854bc3bc6e5443e1369e06a3a456af9d339eda45510f517d9ea5c6bf", size = 462431, upload-time = "2025-10-17T15:04:19.346Z" },
]

[[package]]
name = "pydantic-core"
version = "2.41.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/df/18/d0944e8eaaa3efd0a91b0f1fc537d3be55ad35091b6a87638211ba691964/pydantic_core-2.41.4.tar.gz", hash = "sha256:70e47929a9d4a1905a67e4b687d5946026390568a8e952b92824118063cee4d5", size = 457557, upload-time = "2025-10-14T10:23:47.909Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/13/d0/c20adabd181a029a970738dfe23710b52a31f1258f591874fcdec7359845/pydantic_core-2.41.4-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:85e050ad9e5f6fe1004eec65c914332e52f429bc0ae12d6fa2092407a462c746", size = 2105688, upload-time = "2025-10-14T10:20:54.448Z" },
    { url = "https://files.pythonhosted.org/packages/00/b6/0ce5c03cec5ae94cca220dfecddc453c077d71363b98a4bbdb3c0b22c783/pydantic_core-2.41.4-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:e7393f1d64792763a48924ba31d1e44c2cfbc05e3b1c2c9abb4ceeadd912cced", size = 1910807, upload-time = "2025-10-14T10:20:56.115Z" },
    { url = "https://files.pythonhosted.org/packages/68/3e/800d3d02c8beb0b5c069c870cbb83799d085debf43499c897bb4b4aaff0d/pydantic_core-2.41.4-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:94dab0940b0d1fb28bcab847adf887c66a27a40291eedf0b473be58761c9799a", size = 1956669, upload-time = "2025-10-14T10:20:57.874Z" },
    { url = "https://files.pythonhosted.org/packages/60/a4/24271cc71a17f64589be49ab8bd0751f6a0a03046c690df60989f2f95c2c/pydantic_core-2.41.4-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:de7c42f897e689ee6f9e93c4bec72b99ae3b32a2ade1c7e4798e690ff5246e02", size = 2051629, upload-time = "2025-10-14T10:21:00.006Z" },
    { url = "https://files.pythonhosted.org/packages/68/de/45af3ca2f175d91b96bfb62e1f2d2f1f9f3b14a734afe0bfeff079f78181/pydantic_core-2.41.4-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:664b3199193262277b8b3cd1e754fb07f2c6023289c815a1e1e8fb415cb247b1", size = 2224049, upload-time = "2025-10-14T10:21:01.801Z" },
    { url = "https://files.pythonhosted.org/packages/af/8f/ae4e1ff84672bf869d0a77af24fd78387850e9497753c432875066b5d622/pydantic_core-2.41.4-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d95b253b88f7d308b1c0b417c4624f44553ba4762816f94e6986819b9c273fb2", size = 2342409, upload-time = "2025-10-14T10:21:03.556Z" },
    { url = "https://files.pythonhosted.org/packages/18/62/273dd70b0026a085c7b74b000394e1ef95719ea579c76ea2f0cc8893736d/pydantic_core-2.41.4-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:a1351f5bbdbbabc689727cb91649a00cb9ee7203e0a6e54e9f5ba9e22e384b84", size = 2069635, upload-time = "2025-10-14T10:21:05.385Z" },
    { url = "https://files.pythonhosted.org/packages/30/03/cf485fff699b4cdaea469bc481719d3e49f023241b4abb656f8d422189fc/pydantic_core-2.41.4-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:1affa4798520b148d7182da0615d648e752de4ab1a9566b7471bc803d88a062d", size = 2194284, upload-time = "2025-10-14T10:21:07.122Z" },
    { url = "https://files.pythonhosted.org/packages/f9/7e/c8e713db32405dfd97211f2fc0a15d6bf8adb7640f3d18544c1f39526619/pydantic_core-2.41.4-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:7b74e18052fea4aa8dea2fb7dbc23d15439695da6cbe6cfc1b694af1115df09d", size = 2137566, upload-time = "2025-10-14T10:21:08.981Z" },
    { url = "https://files.pythonhosted.org/packages/04/f7/db71fd4cdccc8b75990f79ccafbbd66757e19f6d5ee724a6252414483fb4/pydantic_core-2.41.4-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:285b643d75c0e30abda9dc1077395624f314a37e3c09ca402d4015ef5979f1a2", size = 2316809, upload-time = "2025-10-14T10:21:10.805Z" },
    { url = "https://files.pythonhosted.org/packages/76/63/a54973ddb945f1bca56742b48b144d85c9fc22f819ddeb9f861c249d5464/pydantic_core-2.41.4-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:f52679ff4218d713b3b33f88c89ccbf3a5c2c12ba665fb80ccc4192b4608dbab", size = 2311119, upload-time = "2025-10-14T10:21:12.583Z" },
    { url = "https://files.pythonhosted.org/packages/f8/03/5d12891e93c19218af74843a27e32b94922195ded2386f7b55382f904d2f/pydantic_core-2.41.4-cp313-cp313-win32.whl", hash = "sha256:ecde6dedd6fff127c273c76821bb754d793be1024bc33314a120f83a3c69460c", size = 1981398, upload-time = "2025-10-14T10:21:14.584Z" },
    { url = "https://files.pythonhosted.org/packages/be/d8/fd0de71f39db91135b7a26996160de71c073d8635edfce8b3c3681be0d6d/pydantic_core-2.41.4-cp313-cp313-win_amd64.whl", hash = "sha256:d081a1f3800f05409ed868ebb2d74ac39dd0c1ff6c035b5162356d76030736d4", size = 2030735, upload-time = "2025-10-14T10:21:16.432Z" },
    { url = "https://files.pythonhosted.org/packages/72/86/c99921c1cf6650023c08bfab6fe2d7057a5142628ef7ccfa9921f2dda1d5/pydantic_core-2.41.4-cp313-cp313-win_arm64.whl", hash = "sha256:f8e49c9c364a7edcbe2a310f12733aad95b022495ef2a8d653f645e5d20c1564", size = 1973209, upload-time = "2025-10-14T10:21:18.213Z" },
    { url = "https://files.pythonhosted.org/packages/36/0d/b5706cacb70a8414396efdda3d72ae0542e050b591119e458e2490baf035/pydantic_core-2.41.4-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:ed97fd56a561f5eb5706cebe94f1ad7c13b84d98312a05546f2ad036bafe87f4", size = 1877324, upload-time = "2025-10-14T10:21:20.363Z" },
    { url = "https://files.pythonhosted.org/packages/de/2d/cba1fa02cfdea72dfb3a9babb067c83b9dff0bbcb198368e000a6b756ea7/pydantic_core-2.41.4-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a870c307bf1ee91fc58a9a61338ff780d01bfae45922624816878dce784095d2", size = 1884515, upload-time = "2025-10-14T10:21:22.339Z" },
    { url = "https://files.pythonhosted.org/packages/07/ea/3df927c4384ed9b503c9cc2d076cf983b4f2adb0c754578dfb1245c51e46/pydantic_core-2.41.4-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:d25e97bc1f5f8f7985bdc2335ef9e73843bb561eb1fa6831fdfc295c1c2061cf", size = 2042819, upload-time = "2025-10-14T10:21:26.683Z" },
    { url = "https://files.pythonhosted.org/packages/6a/ee/df8e871f07074250270a3b1b82aad4cd0026b588acd5d7d3eb2fcb1471a3/pydantic_core-2.41.4-cp313-cp313t-win_amd64.whl", hash = "sha256:d405d14bea042f166512add3091c1af40437c2e7f86988f3915fabd27b1e9cd2", size = 1995866, upload-time = "2025-10-14T10:21:28.951Z" },
    { url = "https://files.pythonhosted.org/packages/fc/de/b20f4ab954d6d399499c33ec4fafc46d9551e11dc1858fb7f5dca0748ceb/pydantic_core-2.41.4-cp313-cp313t-win_arm64.whl", hash = "sha256:19f3684868309db5263a11bace3c45d93f6f24afa2ffe75a647583df22a2ff89", size = 1970034, upload-time = "2025-10-14T10:21:30.869Z" },
]

[[package]]
name = "pyflakes"
version = "3.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/45/dc/fd034dc20b4b264b3d015808458391acbf9df40b1e54750ef175d39180b1/pyflakes-3.4.0.tar.gz", hash = "sha256:b24f96fafb7d2ab0ec5075b7350b3d2d2218eab42003821c06344973d3ea2f58", size = 64669, upload-time = "2025-06-20T18:45:27.834Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c2/2f/81d580a0fb83baeb066698975cb14a618bdbed7720678566f1b046a95fe8/pyflakes-3.4.0-py2.py3-none-any.whl", hash = "sha256:f742a7dbd0d9cb9ea41e9a24a918996e8170c799fa528688d40dd582c8265f4f", size = 63551, upload-time = "2025-06-20T18:45:26.937Z" },
]

[[package]]
name = "pygelf"
version = "0.4.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/49/91/ac1605bb40092ae41fbb833ee55447f72e19ce5459efa6bd3beecc67e971/pygelf-0.4.3.tar.gz", hash = "sha256:8ed972563be3c8f168483f01dbf522b6bc697959c97a3f4881324b3f79638911", size = 11017, upload-time = "2025-06-14T19:21:19.832Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d4/ee/ebac3de919431912e0be380fafd01059a091a489f6b5d7896c2a04548895/pygelf-0.4.3-py3-none-any.whl", hash = "sha256:0876c99a77f9f021834982c9808205b3239fabf5886788d701f31b495b65c8ae", size = 8750, upload-time = "2025-06-14T19:21:16.953Z" },
]

[[package]]
name = "pyjwt"
version = "2.10.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e7/46/bd74733ff231675599650d3e47f361794b22ef3e3770998dda30d3b63726/pyjwt-2.10.1.tar.gz", hash = "sha256:3cc5772eb20009233caf06e9d8a0577824723b44e6648ee0a2aedb6cf9381953", size = 87785, upload-time = "2024-11-28T03:43:29.933Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/61/ad/689f02752eeec26aed679477e80e632ef1b682313be70793d798c1d5fc8f/PyJWT-2.10.1-py3-none-any.whl", hash = "sha256:dcdd193e30abefd5debf142f9adfcdd2b58004e644f25406ffaebd50bd98dacb", size = 22997, upload-time = "2024-11-28T03:43:27.893Z" },
]

[[package]]
name = "python-dateutil"
version = "2.9.0.post0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/66/c0/0c8b6ad9f17a802ee498c46e004a0eb49bc148f2fd230864601a86dcf6db/python-dateutil-2.9.0.post0.tar.gz", hash = "sha256:37dd54208da7e1cd875388217d5e00ebd4179249f90fb72437e91a35459a0ad3", size = 342432, upload-time = "2024-03-01T18:36:20.211Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl", hash = "sha256:a8b2bc7bffae282281c8140a97d3aa9c14da0b136dfe83f850eea9a5f7470427", size = 229892, upload-time = "2024-03-01T18:36:18.57Z" },
]

[[package]]
name = "pytz"
version = "2025.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f8/bf/abbd3cdfb8fbc7fb3d4d38d320f2441b1e7cbe29be4f23797b4a2b5d8aac/pytz-2025.2.tar.gz", hash = "sha256:360b9e3dbb49a209c21ad61809c7fb453643e048b38924c765813546746e81c3", size = 320884, upload-time = "2025-03-25T02:25:00.538Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl", hash = "sha256:5ddf76296dd8c44c26eb8f4b6f35488f3ccbf6fbbd7adee0b7262d43f0ec2f00", size = 509225, upload-time = "2025-03-25T02:24:58.468Z" },
]

[[package]]
name = "pyyaml"
version = "6.0.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/05/8e/961c0007c59b8dd7729d542c61a4d537767a59645b82a0b521206e1e25c2/pyyaml-6.0.3.tar.gz", hash = "sha256:d76623373421df22fb4cf8817020cbb7ef15c725b9d5e45f17e189bfc384190f", size = 130960, upload-time = "2025-09-25T21:33:16.546Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/11/0fd08f8192109f7169db964b5707a2f1e8b745d4e239b784a5a1dd80d1db/pyyaml-6.0.3-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:8da9669d359f02c0b91ccc01cac4a67f16afec0dac22c2ad09f46bee0697eba8", size = 181669, upload-time = "2025-09-25T21:32:23.673Z" },
    { url = "https://files.pythonhosted.org/packages/b1/16/95309993f1d3748cd644e02e38b75d50cbc0d9561d21f390a76242ce073f/pyyaml-6.0.3-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:2283a07e2c21a2aa78d9c4442724ec1eb15f5e42a723b99cb3d822d48f5f7ad1", size = 173252, upload-time = "2025-09-25T21:32:25.149Z" },
    { url = "https://files.pythonhosted.org/packages/50/31/b20f376d3f810b9b2371e72ef5adb33879b25edb7a6d072cb7ca0c486398/pyyaml-6.0.3-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:ee2922902c45ae8ccada2c5b501ab86c36525b883eff4255313a253a3160861c", size = 767081, upload-time = "2025-09-25T21:32:26.575Z" },
    { url = "https://files.pythonhosted.org/packages/49/1e/a55ca81e949270d5d4432fbbd19dfea5321eda7c41a849d443dc92fd1ff7/pyyaml-6.0.3-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:a33284e20b78bd4a18c8c2282d549d10bc8408a2a7ff57653c0cf0b9be0afce5", size = 841159, upload-time = "2025-09-25T21:32:27.727Z" },
    { url = "https://files.pythonhosted.org/packages/74/27/e5b8f34d02d9995b80abcef563ea1f8b56d20134d8f4e5e81733b1feceb2/pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:0f29edc409a6392443abf94b9cf89ce99889a1dd5376d94316ae5145dfedd5d6", size = 801626, upload-time = "2025-09-25T21:32:28.878Z" },
    { url = "https://files.pythonhosted.org/packages/f9/11/ba845c23988798f40e52ba45f34849aa8a1f2d4af4b798588010792ebad6/pyyaml-6.0.3-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:f7057c9a337546edc7973c0d3ba84ddcdf0daa14533c2065749c9075001090e6", size = 753613, upload-time = "2025-09-25T21:32:30.178Z" },
    { url = "https://files.pythonhosted.org/packages/3d/e0/7966e1a7bfc0a45bf0a7fb6b98ea03fc9b8d84fa7f2229e9659680b69ee3/pyyaml-6.0.3-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:eda16858a3cab07b80edaf74336ece1f986ba330fdb8ee0d6c0d68fe82bc96be", size = 794115, upload-time = "2025-09-25T21:32:31.353Z" },
    { url = "https://files.pythonhosted.org/packages/de/94/980b50a6531b3019e45ddeada0626d45fa85cbe22300844a7983285bed3b/pyyaml-6.0.3-cp313-cp313-win32.whl", hash = "sha256:d0eae10f8159e8fdad514efdc92d74fd8d682c933a6dd088030f3834bc8e6b26", size = 137427, upload-time = "2025-09-25T21:32:32.58Z" },
    { url = "https://files.pythonhosted.org/packages/97/c9/39d5b874e8b28845e4ec2202b5da735d0199dbe5b8fb85f91398814a9a46/pyyaml-6.0.3-cp313-cp313-win_amd64.whl", hash = "sha256:79005a0d97d5ddabfeeea4cf676af11e647e41d81c9a7722a193022accdb6b7c", size = 154090, upload-time = "2025-09-25T21:32:33.659Z" },
    { url = "https://files.pythonhosted.org/packages/73/e8/2bdf3ca2090f68bb3d75b44da7bbc71843b19c9f2b9cb9b0f4ab7a5a4329/pyyaml-6.0.3-cp313-cp313-win_arm64.whl", hash = "sha256:5498cd1645aa724a7c71c8f378eb29ebe23da2fc0d7a08071d89469bf1d2defb", size = 140246, upload-time = "2025-09-25T21:32:34.663Z" },
]

[[package]]
name = "regex"
version = "2025.11.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/cc/a9/546676f25e573a4cf00fe8e119b78a37b6a8fe2dc95cda877b30889c9c45/regex-2025.11.3.tar.gz", hash = "sha256:1fedc720f9bb2494ce31a58a1631f9c82df6a09b49c19517ea5cc280b4541e01", size = 414669, upload-time = "2025-11-03T21:34:22.089Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/a7/dda24ebd49da46a197436ad96378f17df30ceb40e52e859fc42cac45b850/regex-2025.11.3-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:c1e448051717a334891f2b9a620fe36776ebf3dd8ec46a0b877c8ae69575feb4", size = 489081, upload-time = "2025-11-03T21:31:55.9Z" },
    { url = "https://files.pythonhosted.org/packages/19/22/af2dc751aacf88089836aa088a1a11c4f21a04707eb1b0478e8e8fb32847/regex-2025.11.3-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:9b5aca4d5dfd7fbfbfbdaf44850fcc7709a01146a797536a8f84952e940cca76", size = 291123, upload-time = "2025-11-03T21:31:57.758Z" },
    { url = "https://files.pythonhosted.org/packages/a3/88/1a3ea5672f4b0a84802ee9891b86743438e7c04eb0b8f8c4e16a42375327/regex-2025.11.3-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:04d2765516395cf7dda331a244a3282c0f5ae96075f728629287dfa6f76ba70a", size = 288814, upload-time = "2025-11-03T21:32:01.12Z" },
    { url = "https://files.pythonhosted.org/packages/fb/8c/f5987895bf42b8ddeea1b315c9fedcfe07cadee28b9c98cf50d00adcb14d/regex-2025.11.3-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:5d9903ca42bfeec4cebedba8022a7c97ad2aab22e09573ce9976ba01b65e4361", size = 798592, upload-time = "2025-11-03T21:32:03.006Z" },
    { url = "https://files.pythonhosted.org/packages/99/2a/6591ebeede78203fa77ee46a1c36649e02df9eaa77a033d1ccdf2fcd5d4e/regex-2025.11.3-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:639431bdc89d6429f6721625e8129413980ccd62e9d3f496be618a41d205f160", size = 864122, upload-time = "2025-11-03T21:32:04.553Z" },
    { url = "https://files.pythonhosted.org/packages/94/d6/be32a87cf28cf8ed064ff281cfbd49aefd90242a83e4b08b5a86b38e8eb4/regex-2025.11.3-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:f117efad42068f9715677c8523ed2be1518116d1c49b1dd17987716695181efe", size = 912272, upload-time = "2025-11-03T21:32:06.148Z" },
    { url = "https://files.pythonhosted.org/packages/62/11/9bcef2d1445665b180ac7f230406ad80671f0fc2a6ffb93493b5dd8cd64c/regex-2025.11.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:4aecb6f461316adf9f1f0f6a4a1a3d79e045f9b71ec76055a791affa3b285850", size = 803497, upload-time = "2025-11-03T21:32:08.162Z" },
    { url = "https://files.pythonhosted.org/packages/e5/a7/da0dc273d57f560399aa16d8a68ae7f9b57679476fc7ace46501d455fe84/regex-2025.11.3-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:3b3a5f320136873cc5561098dfab677eea139521cb9a9e8db98b7e64aef44cbc", size = 787892, upload-time = "2025-11-03T21:32:09.769Z" },
    { url = "https://files.pythonhosted.org/packages/da/4b/732a0c5a9736a0b8d6d720d4945a2f1e6f38f87f48f3173559f53e8d5d82/regex-2025.11.3-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:75fa6f0056e7efb1f42a1c34e58be24072cb9e61a601340cc1196ae92326a4f9", size = 858462, upload-time = "2025-11-03T21:32:11.769Z" },
    { url = "https://files.pythonhosted.org/packages/0c/f5/a2a03df27dc4c2d0c769220f5110ba8c4084b0bfa9ab0f9b4fcfa3d2b0fc/regex-2025.11.3-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:dbe6095001465294f13f1adcd3311e50dd84e5a71525f20a10bd16689c61ce0b", size = 850528, upload-time = "2025-11-03T21:32:13.906Z" },
    { url = "https://files.pythonhosted.org/packages/d6/09/e1cd5bee3841c7f6eb37d95ca91cdee7100b8f88b81e41c2ef426910891a/regex-2025.11.3-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:454d9b4ae7881afbc25015b8627c16d88a597479b9dea82b8c6e7e2e07240dc7", size = 789866, upload-time = "2025-11-03T21:32:15.748Z" },
    { url = "https://files.pythonhosted.org/packages/eb/51/702f5ea74e2a9c13d855a6a85b7f80c30f9e72a95493260193c07f3f8d74/regex-2025.11.3-cp313-cp313-win32.whl", hash = "sha256:28ba4d69171fc6e9896337d4fc63a43660002b7da53fc15ac992abcf3410917c", size = 266189, upload-time = "2025-11-03T21:32:17.493Z" },
    { url = "https://files.pythonhosted.org/packages/8b/00/6e29bb314e271a743170e53649db0fdb8e8ff0b64b4f425f5602f4eb9014/regex-2025.11.3-cp313-cp313-win_amd64.whl", hash = "sha256:bac4200befe50c670c405dc33af26dad5a3b6b255dd6c000d92fe4629f9ed6a5", size = 277054, upload-time = "2025-11-03T21:32:19.042Z" },
    { url = "https://files.pythonhosted.org/packages/25/f1/b156ff9f2ec9ac441710764dda95e4edaf5f36aca48246d1eea3f1fd96ec/regex-2025.11.3-cp313-cp313-win_arm64.whl", hash = "sha256:2292cd5a90dab247f9abe892ac584cb24f0f54680c73fcb4a7493c66c2bf2467", size = 270325, upload-time = "2025-11-03T21:32:21.338Z" },
    { url = "https://files.pythonhosted.org/packages/20/28/fd0c63357caefe5680b8ea052131acbd7f456893b69cc2a90cc3e0dc90d4/regex-2025.11.3-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:1eb1ebf6822b756c723e09f5186473d93236c06c579d2cc0671a722d2ab14281", size = 491984, upload-time = "2025-11-03T21:32:23.466Z" },
    { url = "https://files.pythonhosted.org/packages/df/ec/7014c15626ab46b902b3bcc4b28a7bae46d8f281fc7ea9c95e22fcaaa917/regex-2025.11.3-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:1e00ec2970aab10dc5db34af535f21fcf32b4a31d99e34963419636e2f85ae39", size = 292673, upload-time = "2025-11-03T21:32:25.034Z" },
    { url = "https://files.pythonhosted.org/packages/23/ab/3b952ff7239f20d05f1f99e9e20188513905f218c81d52fb5e78d2bf7634/regex-2025.11.3-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:a4cb042b615245d5ff9b3794f56be4138b5adc35a4166014d31d1814744148c7", size = 291029, upload-time = "2025-11-03T21:32:26.528Z" },
    { url = "https://files.pythonhosted.org/packages/21/7e/3dc2749fc684f455f162dcafb8a187b559e2614f3826877d3844a131f37b/regex-2025.11.3-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:44f264d4bf02f3176467d90b294d59bf1db9fe53c141ff772f27a8b456b2a9ed", size = 807437, upload-time = "2025-11-03T21:32:28.363Z" },
    { url = "https://files.pythonhosted.org/packages/1b/0b/d529a85ab349c6a25d1ca783235b6e3eedf187247eab536797021f7126c6/regex-2025.11.3-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:7be0277469bf3bd7a34a9c57c1b6a724532a0d235cd0dc4e7f4316f982c28b19", size = 873368, upload-time = "2025-11-03T21:32:30.4Z" },
    { url = "https://files.pythonhosted.org/packages/7d/18/2d868155f8c9e3e9d8f9e10c64e9a9f496bb8f7e037a88a8bed26b435af6/regex-2025.11.3-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:0d31e08426ff4b5b650f68839f5af51a92a5b51abd8554a60c2fbc7c71f25d0b", size = 914921, upload-time = "2025-11-03T21:32:32.123Z" },
    { url = "https://files.pythonhosted.org/packages/2d/71/9d72ff0f354fa783fe2ba913c8734c3b433b86406117a8db4ea2bf1c7a2f/regex-2025.11.3-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:e43586ce5bd28f9f285a6e729466841368c4a0353f6fd08d4ce4630843d3648a", size = 812708, upload-time = "2025-11-03T21:32:34.305Z" },
    { url = "https://files.pythonhosted.org/packages/e7/19/ce4bf7f5575c97f82b6e804ffb5c4e940c62609ab2a0d9538d47a7fdf7d4/regex-2025.11.3-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:0f9397d561a4c16829d4e6ff75202c1c08b68a3bdbfe29dbfcdb31c9830907c6", size = 795472, upload-time = "2025-11-03T21:32:36.364Z" },
    { url = "https://files.pythonhosted.org/packages/03/86/fd1063a176ffb7b2315f9a1b08d17b18118b28d9df163132615b835a26ee/regex-2025.11.3-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:dd16e78eb18ffdb25ee33a0682d17912e8cc8a770e885aeee95020046128f1ce", size = 868341, upload-time = "2025-11-03T21:32:38.042Z" },
    { url = "https://files.pythonhosted.org/packages/12/43/103fb2e9811205e7386366501bc866a164a0430c79dd59eac886a2822950/regex-2025.11.3-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:ffcca5b9efe948ba0661e9df0fa50d2bc4b097c70b9810212d6b62f05d83b2dd", size = 854666, upload-time = "2025-11-03T21:32:40.079Z" },
    { url = "https://files.pythonhosted.org/packages/7d/22/e392e53f3869b75804762c7c848bd2dd2abf2b70fb0e526f58724638bd35/regex-2025.11.3-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:c56b4d162ca2b43318ac671c65bd4d563e841a694ac70e1a976ac38fcf4ca1d2", size = 799473, upload-time = "2025-11-03T21:32:42.148Z" },
    { url = "https://files.pythonhosted.org/packages/4f/f9/8bd6b656592f925b6845fcbb4d57603a3ac2fb2373344ffa1ed70aa6820a/regex-2025.11.3-cp313-cp313t-win32.whl", hash = "sha256:9ddc42e68114e161e51e272f667d640f97e84a2b9ef14b7477c53aac20c2d59a", size = 268792, upload-time = "2025-11-03T21:32:44.13Z" },
    { url = "https://files.pythonhosted.org/packages/e5/87/0e7d603467775ff65cd2aeabf1b5b50cc1c3708556a8b849a2fa4dd1542b/regex-2025.11.3-cp313-cp313t-win_amd64.whl", hash = "sha256:7a7c7fdf755032ffdd72c77e3d8096bdcb0eb92e89e17571a196f03d88b11b3c", size = 280214, upload-time = "2025-11-03T21:32:45.853Z" },
    { url = "https://files.pythonhosted.org/packages/8d/d0/2afc6f8e94e2b64bfb738a7c2b6387ac1699f09f032d363ed9447fd2bb57/regex-2025.11.3-cp313-cp313t-win_arm64.whl", hash = "sha256:df9eb838c44f570283712e7cff14c16329a9f0fb19ca492d21d4b7528ee6821e", size = 271469, upload-time = "2025-11-03T21:32:48.026Z" },
]

[[package]]
name = "requests"
version = "2.32.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "charset-normalizer" },
    { name = "idna" },
    { name = "urllib3" },
]
sdist = { url = "https://files.pythonhosted.org/packages/c9/74/b3ff8e6c8446842c3f5c837e9c3dfcfe2018ea6ecef224c710c85ef728f4/requests-2.32.5.tar.gz", hash = "sha256:dbba0bac56e100853db0ea71b82b4dfd5fe2bf6d3754a8893c3af500cec7d7cf", size = 134517, upload-time = "2025-08-18T20:46:02.573Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/1e/db/4254e3eabe8020b458f1a747140d32277ec7a271daf1d235b70dc0b4e6e3/requests-2.32.5-py3-none-any.whl", hash = "sha256:2462f94637a34fd532264295e186976db0f5d453d1cdd31473c85a6a161affb6", size = 64738, upload-time = "2025-08-18T20:46:00.542Z" },
]

[[package]]
name = "requirements-parser"
version = "0.13.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "packaging" },
]
sdist = { url = "https://files.pythonhosted.org/packages/95/96/fb6dbfebb524d5601d359a47c78fe7ba1eef90fc4096404aa60c9a906fbb/requirements_parser-0.13.0.tar.gz", hash = "sha256:0843119ca2cb2331de4eb31b10d70462e39ace698fd660a915c247d2301a4418", size = 22630, upload-time = "2025-05-21T13:42:05.464Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/bd/60/50fbb6ffb35f733654466f1a90d162bcbea358adc3b0871339254fbc37b2/requirements_parser-0.13.0-py3-none-any.whl", hash = "sha256:2b3173faecf19ec5501971b7222d38f04cb45bb9d87d0ad629ca71e2e62ded14", size = 14782, upload-time = "2025-05-21T13:42:04.007Z" },
]

[[package]]
name = "ruff"
version = "0.14.3"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/75/62/50b7727004dfe361104dfbf898c45a9a2fdfad8c72c04ae62900224d6ecf/ruff-0.14.3.tar.gz", hash = "sha256:4ff876d2ab2b161b6de0aa1f5bd714e8e9b4033dc122ee006925fbacc4f62153", size = 5558687, upload-time = "2025-10-31T00:26:26.878Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ce/8e/0c10ff1ea5d4360ab8bfca4cb2c9d979101a391f3e79d2616c9bf348cd26/ruff-0.14.3-py3-none-linux_armv6l.whl", hash = "sha256:876b21e6c824f519446715c1342b8e60f97f93264012de9d8d10314f8a79c371", size = 12535613, upload-time = "2025-10-31T00:25:44.302Z" },
    { url = "https://files.pythonhosted.org/packages/d3/c8/6724f4634c1daf52409fbf13fefda64aa9c8f81e44727a378b7b73dc590b/ruff-0.14.3-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:b6fd8c79b457bedd2abf2702b9b472147cd860ed7855c73a5247fa55c9117654", size = 12855812, upload-time = "2025-10-31T00:25:47.793Z" },
    { url = "https://files.pythonhosted.org/packages/de/03/db1bce591d55fd5f8a08bb02517fa0b5097b2ccabd4ea1ee29aa72b67d96/ruff-0.14.3-py3-none-macosx_11_0_arm64.whl", hash = "sha256:71ff6edca490c308f083156938c0c1a66907151263c4abdcb588602c6e696a14", size = 11944026, upload-time = "2025-10-31T00:25:49.657Z" },
    { url = "https://files.pythonhosted.org/packages/0b/75/4f8dbd48e03272715d12c87dc4fcaaf21b913f0affa5f12a4e9c6f8a0582/ruff-0.14.3-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:786ee3ce6139772ff9272aaf43296d975c0217ee1b97538a98171bf0d21f87ed", size = 12356818, upload-time = "2025-10-31T00:25:51.949Z" },
    { url = "https://files.pythonhosted.org/packages/ec/9b/506ec5b140c11d44a9a4f284ea7c14ebf6f8b01e6e8917734a3325bff787/ruff-0.14.3-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:cd6291d0061811c52b8e392f946889916757610d45d004e41140d81fb6cd5ddc", size = 12336745, upload-time = "2025-10-31T00:25:54.248Z" },
    { url = "https://files.pythonhosted.org/packages/c7/e1/c560d254048c147f35e7f8131d30bc1f63a008ac61595cf3078a3e93533d/ruff-0.14.3-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:a497ec0c3d2c88561b6d90f9c29f5ae68221ac00d471f306fa21fa4264ce5fcd", size = 13101684, upload-time = "2025-10-31T00:25:56.253Z" },
    { url = "https://files.pythonhosted.org/packages/a5/32/e310133f8af5cd11f8cc30f52522a3ebccc5ea5bff4b492f94faceaca7a8/ruff-0.14.3-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:e231e1be58fc568950a04fbe6887c8e4b85310e7889727e2b81db205c45059eb", size = 14535000, upload-time = "2025-10-31T00:25:58.397Z" },
    { url = "https://files.pythonhosted.org/packages/a2/a1/7b0470a22158c6d8501eabc5e9b6043c99bede40fa1994cadf6b5c2a61c7/ruff-0.14.3-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:469e35872a09c0e45fecf48dd960bfbce056b5db2d5e6b50eca329b4f853ae20", size = 14156450, upload-time = "2025-10-31T00:26:00.889Z" },
    { url = "https://files.pythonhosted.org/packages/0a/96/24bfd9d1a7f532b560dcee1a87096332e461354d3882124219bcaff65c09/ruff-0.14.3-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3d6bc90307c469cb9d28b7cfad90aaa600b10d67c6e22026869f585e1e8a2db0", size = 13568414, upload-time = "2025-10-31T00:26:03.291Z" },
    { url = "https://files.pythonhosted.org/packages/a7/e7/138b883f0dfe4ad5b76b58bf4ae675f4d2176ac2b24bdd81b4d966b28c61/ruff-0.14.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0e2f8a0bbcffcfd895df39c9a4ecd59bb80dca03dc43f7fb63e647ed176b741e", size = 13315293, upload-time = "2025-10-31T00:26:05.708Z" },
    { url = "https://files.pythonhosted.org/packages/33/f4/c09bb898be97b2eb18476b7c950df8815ef14cf956074177e9fbd40b7719/ruff-0.14.3-py3-none-manylinux_2_31_riscv64.whl", hash = "sha256:678fdd7c7d2d94851597c23ee6336d25f9930b460b55f8598e011b57c74fd8c5", size = 13539444, upload-time = "2025-10-31T00:26:08.09Z" },
    { url = "https://files.pythonhosted.org/packages/9c/aa/b30a1db25fc6128b1dd6ff0741fa4abf969ded161599d07ca7edd0739cc0/ruff-0.14.3-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:1ec1ac071e7e37e0221d2f2dbaf90897a988c531a8592a6a5959f0603a1ecf5e", size = 12252581, upload-time = "2025-10-31T00:26:10.297Z" },
    { url = "https://files.pythonhosted.org/packages/da/13/21096308f384d796ffe3f2960b17054110a9c3828d223ca540c2b7cc670b/ruff-0.14.3-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:afcdc4b5335ef440d19e7df9e8ae2ad9f749352190e96d481dc501b753f0733e", size = 12307503, upload-time = "2025-10-31T00:26:12.646Z" },
    { url = "https://files.pythonhosted.org/packages/cb/cc/a350bac23f03b7dbcde3c81b154706e80c6f16b06ff1ce28ed07dc7b07b0/ruff-0.14.3-py3-none-musllinux_1_2_i686.whl", hash = "sha256:7bfc42f81862749a7136267a343990f865e71fe2f99cf8d2958f684d23ce3dfa", size = 12675457, upload-time = "2025-10-31T00:26:15.044Z" },
    { url = "https://files.pythonhosted.org/packages/cb/76/46346029fa2f2078826bc88ef7167e8c198e58fe3126636e52f77488cbba/ruff-0.14.3-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:a65e448cfd7e9c59fae8cf37f9221585d3354febaad9a07f29158af1528e165f", size = 13403980, upload-time = "2025-10-31T00:26:17.81Z" },
    { url = "https://files.pythonhosted.org/packages/9f/a4/35f1ef68c4e7b236d4a5204e3669efdeefaef21f0ff6a456792b3d8be438/ruff-0.14.3-py3-none-win32.whl", hash = "sha256:f3d91857d023ba93e14ed2d462ab62c3428f9bbf2b4fbac50a03ca66d31991f7", size = 12500045, upload-time = "2025-10-31T00:26:20.503Z" },
    { url = "https://files.pythonhosted.org/packages/03/15/51960ae340823c9859fb60c63301d977308735403e2134e17d1d2858c7fb/ruff-0.14.3-py3-none-win_amd64.whl", hash = "sha256:d7b7006ac0756306db212fd37116cce2bd307e1e109375e1c6c106002df0ae5f", size = 13594005, upload-time = "2025-10-31T00:26:22.533Z" },
    { url = "https://files.pythonhosted.org/packages/b7/73/4de6579bac8e979fca0a77e54dec1f1e011a0d268165eb8a9bc0982a6564/ruff-0.14.3-py3-none-win_arm64.whl", hash = "sha256:26eb477ede6d399d898791d01961e16b86f02bc2486d0d1a7a9bb2379d055dc1", size = 12590017, upload-time = "2025-10-31T00:26:24.52Z" },
]

[[package]]
name = "shopifyapi"
version = "12.7.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pyactiveresource" },
    { name = "pyjwt" },
    { name = "pyyaml" },
    { name = "six" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b7/83/1308e0034c8903f07eb70c8514b59c789622255a81f18ae0e0ae8d5b8541/shopifyapi-12.7.0.tar.gz", hash = "sha256:d7a903eb8df659aba4b8dce6ad3d8dec0395b1a5f353bf59299eda6aa4fac289", size = 33676, upload-time = "2024-11-04T18:29:53.002Z" }

[[package]]
name = "six"
version = "1.17.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/94/e7/b2c673351809dca68a0e064b6af791aa332cf192da575fd474ed7d6f16a2/six-1.17.0.tar.gz", hash = "sha256:ff70335d468e7eb6ec65b95b99d3a2836546063f63acc5171de367e834932a81", size = 34031, upload-time = "2024-12-04T17:35:28.174Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl", hash = "sha256:4721f391ed90541fddacab5acf947aa0d3dc7d27b2e1e8eda2be8970586c3274", size = 11050, upload-time = "2024-12-04T17:35:26.475Z" },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372, upload-time = "2024-02-25T23:20:04.057Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235, upload-time = "2024-02-25T23:20:01.196Z" },
]

[[package]]
name = "typing-extensions"
version = "4.15.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/72/94/1a15dd82efb362ac84269196e94cf00f187f7ed21c242792a923cdb1c61f/typing_extensions-4.15.0.tar.gz", hash = "sha256:0cea48d173cc12fa28ecabc3b837ea3cf6f38c6d1136f85cbaaf598984861466", size = 109391, upload-time = "2025-08-25T13:49:26.313Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/18/67/36e9267722cc04a6b9f15c7f3441c2363321a3ea07da7ae0c0707beb2a9c/typing_extensions-4.15.0-py3-none-any.whl", hash = "sha256:f0fa19c6845758ab08074a0cfa8b7aecb71c999ca73d62883bc25cc018c4e548", size = 44614, upload-time = "2025-08-25T13:49:24.86Z" },
]

[[package]]
name = "typing-inspection"
version = "0.4.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/55/e3/70399cb7dd41c10ac53367ae42139cf4b1ca5f36bb3dc6c9d33acdb43655/typing_inspection-0.4.2.tar.gz", hash = "sha256:ba561c48a67c5958007083d386c3295464928b01faa735ab8547c5692e87f464", size = 75949, upload-time = "2025-10-01T02:14:41.687Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/dc/9b/47798a6c91d8bdb567fe2698fe81e0c6b7cb7ef4d13da4114b41d239f65d/typing_inspection-0.4.2-py3-none-any.whl", hash = "sha256:4ed1cacbdc298c220f1bd249ed5287caa16f34d44ef4e9c3d0cbad5b521545e7", size = 14611, upload-time = "2025-10-01T02:14:40.154Z" },
]

[[package]]
name = "tzdata"
version = "2025.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/95/32/1a225d6164441be760d75c2c42e2780dc0873fe382da3e98a2e1e48361e5/tzdata-2025.2.tar.gz", hash = "sha256:b60a638fcc0daffadf82fe0f57e53d06bdec2f36c4df66280ae79bce6bd6f2b9", size = 196380, upload-time = "2025-03-23T13:54:43.652Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl", hash = "sha256:1a403fada01ff9221ca8044d701868fa132215d84beb92242d9acd2147f667a8", size = 347839, upload-time = "2025-03-23T13:54:41.845Z" },
]

[[package]]
name = "tzlocal"
version = "5.3.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "tzdata", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/8b/2e/c14812d3d4d9cd1773c6be938f89e5735a1f11a9f184ac3639b93cef35d5/tzlocal-5.3.1.tar.gz", hash = "sha256:cceffc7edecefea1f595541dbd6e990cb1ea3d19bf01b2809f362a03dd7921fd", size = 30761, upload-time = "2025-03-05T21:17:41.549Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/c2/14/e2a54fabd4f08cd7af1c07030603c3356b74da07f7cc056e600436edfa17/tzlocal-5.3.1-py3-none-any.whl", hash = "sha256:eb1a66c3ef5847adf7a834f1be0800581b683b5608e74f86ecbcef8ab91bb85d", size = 18026, upload-time = "2025-03-05T21:17:39.857Z" },
]

[[package]]
name = "urllib3"
version = "2.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/15/22/9ee70a2574a4f4599c47dd506532914ce044817c7752a79b6a51286319bc/urllib3-2.5.0.tar.gz", hash = "sha256:3fc47733c7e419d4bc3f6b3dc2b4f890bb743906a30d56ba4a5bfa4bbff92760", size = 393185, upload-time = "2025-06-18T14:07:41.644Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/a7/c2/fe1e52489ae3122415c51f387e221dd0773709bad6c6cdaa599e8a2c5185/urllib3-2.5.0-py3-none-any.whl", hash = "sha256:e6b01673c0fa6a13e374b50871808eb3bf7046c4b125b216f6bf1cc604cff0dc", size = 129795, upload-time = "2025-06-18T14:07:40.39Z" },
]

[[package]]
name = "wrapt"
version = "2.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/49/19/5e5bcd855d808892fe02d49219f97a50f64cd6d8313d75df3494ee97b1a3/wrapt-2.0.0.tar.gz", hash = "sha256:35a542cc7a962331d0279735c30995b024e852cf40481e384fd63caaa391cbb9", size = 81722, upload-time = "2025-10-19T23:47:54.07Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/18/0a/dd88abfe756b1aa79f0777e5ee4ce9e4b5dc4999bd805e9b04b52efc7b18/wrapt-2.0.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:e2ea096db28d5eb64d381af0e93464621ace38a7003a364b6b5ffb7dd713aabe", size = 78083, upload-time = "2025-10-19T23:46:16.937Z" },
    { url = "https://files.pythonhosted.org/packages/7f/b9/8afebc1655a863bb2178b23c2d699b8743f3a7dab466904adc6155f3c858/wrapt-2.0.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:c92b5a82d28491e3f14f037e1aae99a27a5e6e0bb161e65f52c0445a3fa7c940", size = 61156, upload-time = "2025-10-19T23:46:17.927Z" },
    { url = "https://files.pythonhosted.org/packages/bb/8b/f710a6528ccc52e21943f42c8cf64814cde90f9adbd3bcd58c7c274b4f75/wrapt-2.0.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:81d234718aabe632d179fac52c7f69f0f99fbaac4d4bcd670e62462bbcbfcad7", size = 61641, upload-time = "2025-10-19T23:46:19.229Z" },
    { url = "https://files.pythonhosted.org/packages/e4/5f/e4eabd0cc6684c5b208c2abc5c3459449c4d15be1694a9bbcf51e0e135fd/wrapt-2.0.0-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:db2eea83c43f84e4e41dbbb4c1de371a53166e55f900a6b130c3ef51c6345c1a", size = 121454, upload-time = "2025-10-19T23:46:21.808Z" },
    { url = "https://files.pythonhosted.org/packages/6f/c4/ec31ee17cc7866960d323609ba7402be786d211a6d713a59f776c4270bb3/wrapt-2.0.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:65f50e356c425c061e1e17fe687ff30e294fed9bf3441dc1f13ef73859c2a817", size = 123063, upload-time = "2025-10-19T23:46:23.545Z" },
    { url = "https://files.pythonhosted.org/packages/b0/2b/a4b10c3c0022e40aeae9bec009bafb049f440493f0575ebb27ecf61c32f8/wrapt-2.0.0-cp313-cp313-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:887f2a667e3cbfb19e204032d42ad7dedaa43972e4861dc7a3d51ae951d9b578", size = 117401, upload-time = "2025-10-19T23:46:20.433Z" },
    { url = "https://files.pythonhosted.org/packages/2a/4a/ade23a76967e1f148e461076a4d0e24a7950a5f18b394c9107fe60224ae2/wrapt-2.0.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:9054829da4be461e3ad3192e4b6bbf1fc18af64c9975ce613aec191924e004dc", size = 121485, upload-time = "2025-10-19T23:46:24.85Z" },
    { url = "https://files.pythonhosted.org/packages/cb/ba/33b5f3e2edede4e1cfd259f0d9c203cf370f259bb9b215dd58fc6cbb94e9/wrapt-2.0.0-cp313-cp313-musllinux_1_2_riscv64.whl", hash = "sha256:b952ffd77133a5a2798ee3feb18e51b0a299d2f440961e5bb7737dbb02e57289", size = 116276, upload-time = "2025-10-19T23:46:27.006Z" },
    { url = "https://files.pythonhosted.org/packages/eb/bf/b7f95bb4529a35ca11eb95d48f9d1a563b495471f7cf404c644566fb4293/wrapt-2.0.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:e25fde03c480061b8234d8ee4863eb5f40a9be4fb258ce105b364de38fc6bcf9", size = 120578, upload-time = "2025-10-19T23:46:28.679Z" },
    { url = "https://files.pythonhosted.org/packages/f8/71/984849df6f052592474a44aafd6b847e1cffad39b0debc5390a04aa46331/wrapt-2.0.0-cp313-cp313-win32.whl", hash = "sha256:49e982b7860d325094978292a49e0418833fc7fc42c0dc7cd0b7524d7d06ee74", size = 58178, upload-time = "2025-10-19T23:46:32.372Z" },
    { url = "https://files.pythonhosted.org/packages/f9/3b/4e1fc0f2e1355fbc55ab248311bf4c958dbbd96bd9183b9e96882cc16213/wrapt-2.0.0-cp313-cp313-win_amd64.whl", hash = "sha256:6e5c86389d9964050ce50babe247d172a5e3911d59a64023b90db2b4fa00ae7c", size = 60423, upload-time = "2025-10-19T23:46:30.041Z" },
    { url = "https://files.pythonhosted.org/packages/20/0a/9384e0551f56fe361f41bb8f209a13bb9ef689c3a18264225b249849b12c/wrapt-2.0.0-cp313-cp313-win_arm64.whl", hash = "sha256:b96fdaa4611e05c7231937930567d3c16782be9dbcf03eb9f60d83e57dd2f129", size = 58918, upload-time = "2025-10-19T23:46:31.056Z" },
    { url = "https://files.pythonhosted.org/packages/68/70/37b90d3ee5bf0d0dc4859306383da08b685c9a51abff6fd6b0a7c052e117/wrapt-2.0.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:f2c7b7fead096dbf1dcc455b7f59facb05de3f5bfb04f60a69f98cdfe6049e5f", size = 81980, upload-time = "2025-10-19T23:46:33.368Z" },
    { url = "https://files.pythonhosted.org/packages/95/23/0ce69cc90806b90b3ee4cfd9ad8d2ee9becc3a1aab7df3c3bfc7d0904cb6/wrapt-2.0.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:04c7c8393f25b11c0faa5d907dd9eb462e87e4e7ba55e308a046d7ed37f4bbe2", size = 62900, upload-time = "2025-10-19T23:46:34.415Z" },
    { url = "https://files.pythonhosted.org/packages/54/76/03ec08170c02f38f3be3646977920976b968e0b704a0693a98f95d02f4d2/wrapt-2.0.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:a93e0f8b376c0735b2f4daf58018b4823614d2b896cb72b6641c4d3dbdca1d75", size = 63636, upload-time = "2025-10-19T23:46:35.643Z" },
    { url = "https://files.pythonhosted.org/packages/75/c1/04ce0511e504cdcd84cdb6980bc7d4efa38ac358e8103d6dd0cd278bfc6d/wrapt-2.0.0-cp313-cp313t-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:b42d13603da4416c43c430dbc6313c8d7ff745c40942f146ed4f6dd02c7d2547", size = 152650, upload-time = "2025-10-19T23:46:38.717Z" },
    { url = "https://files.pythonhosted.org/packages/17/06/cd2e32b5f744701189c954f9ab5eee449c86695b13f414bb8ea7a83f6d48/wrapt-2.0.0-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:c8bbd2472abf8c33480ad2314b1f8fac45d592aba6cc093e8839a7b2045660e6", size = 158811, upload-time = "2025-10-19T23:46:40.875Z" },
    { url = "https://files.pythonhosted.org/packages/7d/a2/a6d920695cca62563c1b969064e5cd2051344a6e330c184b6f80383d87e4/wrapt-2.0.0-cp313-cp313t-manylinux_2_31_riscv64.manylinux_2_39_riscv64.whl", hash = "sha256:e64a3a1fd9a308ab9b815a2ad7a65b679730629dbf85f8fc3f7f970d634ee5df", size = 146033, upload-time = "2025-10-19T23:46:37.351Z" },
    { url = "https://files.pythonhosted.org/packages/c6/90/7fd2abe4ec646bc43cb6b0d05086be6fcf15e64f06f51fc4198804396d68/wrapt-2.0.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:d61214525eaf88e0d0edf3d1ad5b5889863c6f88e588c6cdc6aa4ee5d1f10a4a", size = 155673, upload-time = "2025-10-19T23:46:42.582Z" },
    { url = "https://files.pythonhosted.org/packages/5f/8d/6cce7f8c41633e677ac8aa34e84b53a22a645ec2a680deb991785ca2798d/wrapt-2.0.0-cp313-cp313t-musllinux_1_2_riscv64.whl", hash = "sha256:04f7a5f92c5f7324a1735043cc467b1295a1c5b4e0c1395472b7c44706e3dc61", size = 144364, upload-time = "2025-10-19T23:46:44.381Z" },
    { url = "https://files.pythonhosted.org/packages/72/42/9570349e03afa9d83daf7f33ffb17e8cdc62d7e84c0d09005d0f51912efa/wrapt-2.0.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:2356f76cb99b3de5b4e5b8210367fbbb81c7309fe39b622f5d199dd88eb7f765", size = 150275, upload-time = "2025-10-19T23:46:45.662Z" },
    { url = "https://files.pythonhosted.org/packages/f2/d8/448728e6fe030e5c4f1022c82cd3af1de1c672fa53d2d5b36b32a55ce7bf/wrapt-2.0.0-cp313-cp313t-win32.whl", hash = "sha256:0a921b657a224e40e4bc161b5d33934583b34f0c9c5bdda4e6ac66f9d2fcb849", size = 59867, upload-time = "2025-10-19T23:46:49.593Z" },
    { url = "https://files.pythonhosted.org/packages/8f/b1/ad812b1fe1cd85f6498dc3a3c9809a1e880d6108283b1735119bec217041/wrapt-2.0.0-cp313-cp313t-win_amd64.whl", hash = "sha256:c16f6d4eea98080f6659a8a7fc559d4a0a337ee66960659265cad2c8a40f7c0f", size = 63170, upload-time = "2025-10-19T23:46:46.87Z" },
    { url = "https://files.pythonhosted.org/packages/7f/29/c105b1e76650c82823c491952a7a8eafe09b78944f7a43f22d37ed860229/wrapt-2.0.0-cp313-cp313t-win_arm64.whl", hash = "sha256:52878edc13dc151c58a9966621d67163a80654bc6cff4b2e1c79fa62d0352b26", size = 60339, upload-time = "2025-10-19T23:46:47.862Z" },
    { url = "https://files.pythonhosted.org/packages/00/5c/c34575f96a0a038579683c7f10fca943c15c7946037d1d254ab9db1536ec/wrapt-2.0.0-py3-none-any.whl", hash = "sha256:02482fb0df89857e35427dfb844319417e14fae05878f295ee43fa3bf3b15502", size = 43998, upload-time = "2025-10-19T23:47:52.858Z" },
]

[[package]]
name = "yarl"
version = "1.22.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "multidict" },
    { name = "propcache" },
]
sdist = { url = "https://files.pythonhosted.org/packages/57/63/0c6ebca57330cd313f6102b16dd57ffaf3ec4c83403dcb45dbd15c6f3ea1/yarl-1.22.0.tar.gz", hash = "sha256:bebf8557577d4401ba8bd9ff33906f1376c877aa78d1fe216ad01b4d6745af71", size = 187169, upload-time = "2025-10-06T14:12:55.963Z" }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ea/f3/d67de7260456ee105dc1d162d43a019ecad6b91e2f51809d6cddaa56690e/yarl-1.22.0-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:8dee9c25c74997f6a750cd317b8ca63545169c098faee42c84aa5e506c819b53", size = 139980, upload-time = "2025-10-06T14:10:14.601Z" },
    { url = "https://files.pythonhosted.org/packages/01/88/04d98af0b47e0ef42597b9b28863b9060bb515524da0a65d5f4db160b2d5/yarl-1.22.0-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:01e73b85a5434f89fc4fe27dcda2aff08ddf35e4d47bbbea3bdcd25321af538a", size = 93424, upload-time = "2025-10-06T14:10:16.115Z" },
    { url = "https://files.pythonhosted.org/packages/18/91/3274b215fd8442a03975ce6bee5fe6aa57a8326b29b9d3d56234a1dca244/yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:22965c2af250d20c873cdbee8ff958fb809940aeb2e74ba5f20aaf6b7ac8c70c", size = 93821, upload-time = "2025-10-06T14:10:17.993Z" },
    { url = "https://files.pythonhosted.org/packages/61/3a/caf4e25036db0f2da4ca22a353dfeb3c9d3c95d2761ebe9b14df8fc16eb0/yarl-1.22.0-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:b4f15793aa49793ec8d1c708ab7f9eded1aa72edc5174cae703651555ed1b601", size = 373243, upload-time = "2025-10-06T14:10:19.44Z" },
    { url = "https://files.pythonhosted.org/packages/6e/9e/51a77ac7516e8e7803b06e01f74e78649c24ee1021eca3d6a739cb6ea49c/yarl-1.22.0-cp313-cp313-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:e5542339dcf2747135c5c85f68680353d5cb9ffd741c0f2e8d832d054d41f35a", size = 342361, upload-time = "2025-10-06T14:10:21.124Z" },
    { url = "https://files.pythonhosted.org/packages/d4/f8/33b92454789dde8407f156c00303e9a891f1f51a0330b0fad7c909f87692/yarl-1.22.0-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:5c401e05ad47a75869c3ab3e35137f8468b846770587e70d71e11de797d113df", size = 387036, upload-time = "2025-10-06T14:10:22.902Z" },
    { url = "https://files.pythonhosted.org/packages/d9/9a/c5db84ea024f76838220280f732970aa4ee154015d7f5c1bfb60a267af6f/yarl-1.22.0-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:243dda95d901c733f5b59214d28b0120893d91777cb8aa043e6ef059d3cddfe2", size = 397671, upload-time = "2025-10-06T14:10:24.523Z" },
    { url = "https://files.pythonhosted.org/packages/11/c9/cd8538dc2e7727095e0c1d867bad1e40c98f37763e6d995c1939f5fdc7b1/yarl-1.22.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:bec03d0d388060058f5d291a813f21c011041938a441c593374da6077fe21b1b", size = 377059, upload-time = "2025-10-06T14:10:26.406Z" },
    { url = "https://files.pythonhosted.org/packages/a1/b9/ab437b261702ced75122ed78a876a6dec0a1b0f5e17a4ac7a9a2482d8abe/yarl-1.22.0-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:b0748275abb8c1e1e09301ee3cf90c8a99678a4e92e4373705f2a2570d581273", size = 365356, upload-time = "2025-10-06T14:10:28.461Z" },
    { url = "https://files.pythonhosted.org/packages/b2/9d/8e1ae6d1d008a9567877b08f0ce4077a29974c04c062dabdb923ed98e6fe/yarl-1.22.0-cp313-cp313-musllinux_1_2_armv7l.whl", hash = "sha256:47fdb18187e2a4e18fda2c25c05d8251a9e4a521edaed757fef033e7d8498d9a", size = 361331, upload-time = "2025-10-06T14:10:30.541Z" },
    { url = "https://files.pythonhosted.org/packages/ca/5a/09b7be3905962f145b73beb468cdd53db8aa171cf18c80400a54c5b82846/yarl-1.22.0-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:c7044802eec4524fde550afc28edda0dd5784c4c45f0be151a2d3ba017daca7d", size = 382590, upload-time = "2025-10-06T14:10:33.352Z" },
    { url = "https://files.pythonhosted.org/packages/aa/7f/59ec509abf90eda5048b0bc3e2d7b5099dffdb3e6b127019895ab9d5ef44/yarl-1.22.0-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:139718f35149ff544caba20fce6e8a2f71f1e39b92c700d8438a0b1d2a631a02", size = 385316, upload-time = "2025-10-06T14:10:35.034Z" },
    { url = "https://files.pythonhosted.org/packages/e5/84/891158426bc8036bfdfd862fabd0e0fa25df4176ec793e447f4b85cf1be4/yarl-1.22.0-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:e1b51bebd221006d3d2f95fbe124b22b247136647ae5dcc8c7acafba66e5ee67", size = 374431, upload-time = "2025-10-06T14:10:37.76Z" },
    { url = "https://files.pythonhosted.org/packages/bb/49/03da1580665baa8bef5e8ed34c6df2c2aca0a2f28bf397ed238cc1bbc6f2/yarl-1.22.0-cp313-cp313-win32.whl", hash = "sha256:d3e32536234a95f513bd374e93d717cf6b2231a791758de6c509e3653f234c95", size = 81555, upload-time = "2025-10-06T14:10:39.649Z" },
    { url = "https://files.pythonhosted.org/packages/9a/ee/450914ae11b419eadd067c6183ae08381cfdfcb9798b90b2b713bbebddda/yarl-1.22.0-cp313-cp313-win_amd64.whl", hash = "sha256:47743b82b76d89a1d20b83e60d5c20314cbd5ba2befc9cda8f28300c4a08ed4d", size = 86965, upload-time = "2025-10-06T14:10:41.313Z" },
    { url = "https://files.pythonhosted.org/packages/98/4d/264a01eae03b6cf629ad69bae94e3b0e5344741e929073678e84bf7a3e3b/yarl-1.22.0-cp313-cp313-win_arm64.whl", hash = "sha256:5d0fcda9608875f7d052eff120c7a5da474a6796fe4d83e152e0e4d42f6d1a9b", size = 81205, upload-time = "2025-10-06T14:10:43.167Z" },
    { url = "https://files.pythonhosted.org/packages/88/fc/6908f062a2f77b5f9f6d69cecb1747260831ff206adcbc5b510aff88df91/yarl-1.22.0-cp313-cp313t-macosx_10_13_universal2.whl", hash = "sha256:719ae08b6972befcba4310e49edb1161a88cdd331e3a694b84466bd938a6ab10", size = 146209, upload-time = "2025-10-06T14:10:44.643Z" },
    { url = "https://files.pythonhosted.org/packages/65/47/76594ae8eab26210b4867be6f49129861ad33da1f1ebdf7051e98492bf62/yarl-1.22.0-cp313-cp313t-macosx_10_13_x86_64.whl", hash = "sha256:47d8a5c446df1c4db9d21b49619ffdba90e77c89ec6e283f453856c74b50b9e3", size = 95966, upload-time = "2025-10-06T14:10:46.554Z" },
    { url = "https://files.pythonhosted.org/packages/ab/ce/05e9828a49271ba6b5b038b15b3934e996980dd78abdfeb52a04cfb9467e/yarl-1.22.0-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:cfebc0ac8333520d2d0423cbbe43ae43c8838862ddb898f5ca68565e395516e9", size = 97312, upload-time = "2025-10-06T14:10:48.007Z" },
    { url = "https://files.pythonhosted.org/packages/d1/c5/7dffad5e4f2265b29c9d7ec869c369e4223166e4f9206fc2243ee9eea727/yarl-1.22.0-cp313-cp313t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:4398557cbf484207df000309235979c79c4356518fd5c99158c7d38203c4da4f", size = 361967, upload-time = "2025-10-06T14:10:49.997Z" },
    { url = "https://files.pythonhosted.org/packages/50/b2/375b933c93a54bff7fc041e1a6ad2c0f6f733ffb0c6e642ce56ee3b39970/yarl-1.22.0-cp313-cp313t-manylinux2014_armv7l.manylinux_2_17_armv7l.manylinux_2_31_armv7l.whl", hash = "sha256:2ca6fd72a8cd803be290d42f2dec5cdcd5299eeb93c2d929bf060ad9efaf5de0", size = 323949, upload-time = "2025-10-06T14:10:52.004Z" },
    { url = "https://files.pythonhosted.org/packages/66/50/bfc2a29a1d78644c5a7220ce2f304f38248dc94124a326794e677634b6cf/yarl-1.22.0-cp313-cp313t-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:ca1f59c4e1ab6e72f0a23c13fca5430f889634166be85dbf1013683e49e3278e", size = 361818, upload-time = "2025-10-06T14:10:54.078Z" },
    { url = "https://files.pythonhosted.org/packages/46/96/f3941a46af7d5d0f0498f86d71275696800ddcdd20426298e572b19b91ff/yarl-1.22.0-cp313-cp313t-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:6c5010a52015e7c70f86eb967db0f37f3c8bd503a695a49f8d45700144667708", size = 372626, upload-time = "2025-10-06T14:10:55.767Z" },
    { url = "https://files.pythonhosted.org/packages/c1/42/8b27c83bb875cd89448e42cd627e0fb971fa1675c9ec546393d18826cb50/yarl-1.22.0-cp313-cp313t-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:9d7672ecf7557476642c88497c2f8d8542f8e36596e928e9bcba0e42e1e7d71f", size = 341129, upload-time = "2025-10-06T14:10:57.985Z" },
    { url = "https://files.pythonhosted.org/packages/49/36/99ca3122201b382a3cf7cc937b95235b0ac944f7e9f2d5331d50821ed352/yarl-1.22.0-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:3b7c88eeef021579d600e50363e0b6ee4f7f6f728cd3486b9d0f3ee7b946398d", size = 346776, upload-time = "2025-10-06T14:10:59.633Z" },
    { url = "https://files.pythonhosted.org/packages/85/b4/47328bf996acd01a4c16ef9dcd2f59c969f495073616586f78cd5f2efb99/yarl-1.22.0-cp313-cp313t-musllinux_1_2_armv7l.whl", hash = "sha256:f4afb5c34f2c6fecdcc182dfcfc6af6cccf1aa923eed4d6a12e9d96904e1a0d8", size = 334879, upload-time = "2025-10-06T14:11:01.454Z" },
    { url = "https://files.pythonhosted.org/packages/c2/ad/b77d7b3f14a4283bffb8e92c6026496f6de49751c2f97d4352242bba3990/yarl-1.22.0-cp313-cp313t-musllinux_1_2_ppc64le.whl", hash = "sha256:59c189e3e99a59cf8d83cbb31d4db02d66cda5a1a4374e8a012b51255341abf5", size = 350996, upload-time = "2025-10-06T14:11:03.452Z" },
    { url = "https://files.pythonhosted.org/packages/81/c8/06e1d69295792ba54d556f06686cbd6a7ce39c22307100e3fb4a2c0b0a1d/yarl-1.22.0-cp313-cp313t-musllinux_1_2_s390x.whl", hash = "sha256:5a3bf7f62a289fa90f1990422dc8dff5a458469ea71d1624585ec3a4c8d6960f", size = 356047, upload-time = "2025-10-06T14:11:05.115Z" },
    { url = "https://files.pythonhosted.org/packages/4b/b8/4c0e9e9f597074b208d18cef227d83aac36184bfbc6eab204ea55783dbc5/yarl-1.22.0-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:de6b9a04c606978fdfe72666fa216ffcf2d1a9f6a381058d4378f8d7b1e5de62", size = 342947, upload-time = "2025-10-06T14:11:08.137Z" },
    { url = "https://files.pythonhosted.org/packages/e0/e5/11f140a58bf4c6ad7aca69a892bff0ee638c31bea4206748fc0df4ebcb3a/yarl-1.22.0-cp313-cp313t-win32.whl", hash = "sha256:1834bb90991cc2999f10f97f5f01317f99b143284766d197e43cd5b45eb18d03", size = 86943, upload-time = "2025-10-06T14:11:10.284Z" },
    { url = "https://files.pythonhosted.org/packages/31/74/8b74bae38ed7fe6793d0c15a0c8207bbb819cf287788459e5ed230996cdd/yarl-1.22.0-cp313-cp313t-win_amd64.whl", hash = "sha256:ff86011bd159a9d2dfc89c34cfd8aff12875980e3bd6a39ff097887520e60249", size = 93715, upload-time = "2025-10-06T14:11:11.739Z" },
    { url = "https://files.pythonhosted.org/packages/69/66/991858aa4b5892d57aef7ee1ba6b4d01ec3b7eb3060795d34090a3ca3278/yarl-1.22.0-cp313-cp313t-win_arm64.whl", hash = "sha256:7861058d0582b847bc4e3a4a4c46828a410bca738673f35a29ba3ca5db0b473b", size = 83857, upload-time = "2025-10-06T14:11:13.586Z" },
    { url = "https://files.pythonhosted.org/packages/73/ae/b48f95715333080afb75a4504487cbe142cae1268afc482d06692d605ae6/yarl-1.22.0-py3-none-any.whl", hash = "sha256:1380560bdba02b6b6c90de54133c81c9f2a453dee9912fe58c1dcced1edb7cff", size = 46814, upload-time = "2025-10-06T14:12:53.872Z" },
]



================================================
FILE: .flake8
================================================
[flake8]
exclude =
    __pycache__,
    .git,
    .venv,
    venv
ignore = E203,W503
max-line-length = 120



================================================
FILE: component_config/component_long_description.md
================================================
A Keboola component for extracting data from Shopify using GraphQL API. This is the second version of the Shopify extractor, built specifically to use GraphQL instead of the deprecated REST API.



================================================
FILE: component_config/component_short_description.md
================================================
Shopify is a commerce platform that allows anyone to set up an online store and sell their products.



================================================
FILE: component_config/configRowSchema.json
================================================
{}


================================================
FILE: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "extractor configuration",
  "required": [
    "#api_token",
    "store_name",
    "loading_options",
    "endpoints"
  ],
  "properties": {
    "#api_token": {
      "type": "string",
      "title": "Admin API access token",
      "format": "password",
      "description": "Admin API access token of your Shopify custom app.",
      "propertyOrder": 0
    },
    "store_name": {
      "type": "string",
      "title": "Store name",
      "description": "Shopify store name found in URL, e.g. [store_name].myshopify.com",
      "propertyOrder": 10
    },
    "api_version": {
      "type": "string",
      "title": "API version",
      "default": "2025-10",
      "readOnly": true,
      "description": "The API version, gets updated regularly based on the <a href=\"https://shopify.dev/api/usage/versioning#release-schedule\">Shopify release cycle</a>",
      "propertyOrder": 20
    },
    "endpoints": {
      "type": "object",
      "title": "Endpoints",
      "propertyOrder": 30,
      "properties": {
        "products": {
          "type": "boolean",
          "title": "Products",
          "format": "checkbox",
          "default": false,
          "description": "Get Products data using bulk operations",
          "options": {
            "tooltip": "See [the GraphQL query](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/BulkProducts.graphql)"
          },
          "propertyOrder": 40
        },
        "products_drafts": {
          "type": "boolean",
          "title": "Products – Drafts",
          "format": "checkbox",
          "default": false,
          "options": {
            "dependencies": {
              "products": true
            }
          },
          "description": "Include draft products",
          "propertyOrder": 50
        },
        "products_archived": {
          "type": "boolean",
          "title": "Products – Archived",
          "format": "checkbox",
          "default": false,
          "options": {
            "dependencies": {
              "products": true
            }
          },
          "description": "Include archived products",
          "propertyOrder": 60
        },
        "products_unlisted": {
          "type": "boolean",
          "title": "Products – Unlisted",
          "format": "checkbox",
          "default": false,
          "options": {
            "dependencies": {
              "products": true
            }
          },
          "description": "Include unlisted products",
          "propertyOrder": 65
        },
        "product_metafields": {
          "type": "boolean",
          "title": "Products – Metafields",
          "format": "checkbox",
          "default": false,
          "options": {
            "dependencies": {
              "products": true
            },
            "tooltip": "See [the GraphQL query](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/BulkProducts.graphql) and [the replacement fragment](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/fragments/ProductMetafields.graphql)"
          },
          "description": "Include product metafields",
          "propertyOrder": 70
        },
        "variant_metafields": {
          "type": "boolean",
          "title": "Products – Variant Metafields",
          "format": "checkbox",
          "default": false,
          "options": {
            "dependencies": {
              "products": true
            },
            "tooltip": "See [the GraphQL query](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/BulkProducts.graphql) and [the replacement fragment](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/fragments/VariantMetafields.graphql)"
          },
          "description": "Include variant metafields",
          "propertyOrder": 80
        },
        "customers": {
          "type": "boolean",
          "title": "Customers",
          "format": "checkbox",
          "default": false,
          "description": "Get Customers data using bulk operations",
          "options": {
            "tooltip": "See [the GraphQL query](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/BulkCustomers.graphql)"
          },
          "propertyOrder": 90
        },
        "orders": {
          "type": "boolean",
          "title": "Orders",
          "format": "checkbox",
          "default": false,
          "description": "Get Orders data using bulk operations",
          "options": {
            "tooltip": "See [the GraphQL query](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/BulkOrders.graphql)"
          },
          "propertyOrder": 100

        },
        "order_transactions": {
          "type": "boolean",
          "title": "Orders – Transactions",
          "format": "checkbox",
          "default": false,
          "options": {
            "dependencies": {
              "orders": true
            },
            "tooltip": "See [the GraphQL query](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/BulkOrders.graphql) and [the replacement fragment](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/fragments/OrderTransactions.graphql)"
          },
          "description": "Include order transactions",
          "propertyOrder": 110
        },
        "inventory": {
          "type": "boolean",
          "title": "Inventory",
          "format": "checkbox",
          "default": false,
          "description": "Get inventory levels",
          "options": {
            "tooltip": "See [the GraphQL query](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/BulkInventory.graphql)"
          },
          "propertyOrder": 120
        },
        "locations": {
          "type": "boolean",
          "title": "Locations",
          "format": "checkbox",
          "default": false,
          "description": "Get store locations",
          "options": {
            "tooltip": "See [the GraphQL query](https://github.com/keboola/component-shopify-v2/blob/main/src/shopify_cli/queries/BulkCustomers.graphql)"
          },
          "propertyOrder": 130
        }
      }
    },
    "events": {
      "title": "Events",
      "maxItems": 1,
      "description": "Download Events",
      "propertyOrder": 140,
      "type": "array",
      "items": {
        "type": "object",
        "title": "Setup",
        "required": [
          "filters",
          "types"
        ],
        "properties": {
          "types": {
            "type": "string",
            "title": "Event Types",
            "format": "textarea",
            "options": {
              "input_height": "100px"
            },
            "description": "Comma separated list of required event types. e.g. 'confirmed', 'create', 'destroy'. If omitted all possible types are downloaded.  Different resources generate different types of event. See the docs for a list of possible verbs.",
            "uniqueItems": true,
            "propertyOrder": 150
          },
          "filters": {
            "title": "Types",
            "format": "select",
            "description": "Resource Types",
            "uniqueItems": true,
            "propertyOrder": 160,
            "type": "array",
            "items": {
              "enum": [
                "Article",
                "Blog",
                "Collection",
                "Comment",
                "Order",
                "Page",
                "PriceRule",
                "Product",
                "ApiPermission"
              ],
              "type": "string"
            }
          }
        }
      }
    },
    "custom_queries": {
      "title": "Custom Bulk Queries",
      "description": "Execute custom GraphQL bulk operations to download data. Note: Only bulk operation mutations are supported, not regular queries.",
      "propertyOrder": 170,
      "type": "array",
      "items": {
        "type": "object",
        "title": "Bulk Query Setup",
        "required": [
          "name",
          "query"
        ],
        "properties": {
          "name": {
            "type": "string",
            "title": "Query Name",
            "description": "Name of the custom query. Used for naming the output table (e.g., 'my_products' creates 'my_products.csv').",
            "propertyOrder": 180
          },
          "query": {
            "type": "string",
            "title": "GraphQL Bulk Operation Mutation",
            "format": "textarea",
            "options": {
              "input_height": "300px"
            },
            "description": "The GraphQL bulk operation mutation. Must be in format: mutation { bulkOperationRunQuery(query: \"\"\"...\"\"\") { bulkOperation { id status } userErrors { field message } } }. See documentation for examples.",
            "propertyOrder": 190
          }
        }
      }
    },
    "loading_options": {
      "type": "object",
      "title": "Loading Options",
      "description": "Data is fetched incrementally based on the last updated datetime. NOTE: Column names longer than 64 characters will be automatically shortened, more in <a href=\"https://github.com/keboola/component-shopify/blob/main/README.md\">documenation.</a>",
      "propertyOrder": 200,
      "format": "grid",
      "required": [
        "incremental_output",
        "date_since",
        "date_to"
      ],
      "properties": {
        "date_since": {
          "type": "string",
          "title": "Period start date [including]",
          "default": "1 week ago",
          "description": "Date in YYYY-MM-DD format or <a href='https://dateparser.readthedocs.io/' target='_new'>dateparser</a> string i.e. 5 days ago, 1 month ago, yesterday, etc. If left empty, all records are downloaded.",
          "propertyOrder": 210
        },
        "date_to": {
          "type": "string",
          "title": "Period end date [excluding]",
          "default": "now",
          "description": "Date in YYYY-MM-DD format or <a href='https://dateparser.readthedocs.io/' target='_new'>dateparser</a> string i.e. 5 days ago, 1 month ago, yesterday, etc. If left empty, it is set to the current datetime.",
          "propertyOrder": 220
        },
        "fetch_parameter": {
          "enum": [
            "updated_at",
            "created_at"
          ],
          "type": "string",
          "title": "Fetch parameter",
          "default": "updated_at",
          "description": "Field/parameter to be used for filtering data from API.",
          "propertyOrder": 230
        },
        "incremental_output": {
          "enum": [
            0,
            1
          ],
          "type": "number",
          "title": "Load type",
          "default": 1,
          "options": {
            "enum_titles": [
              "Full Load",
              "Incremental Update"
            ]
          },
          "description": "If set to Incremental update, the result tables will be updated based on primary key. Full load overwrites the destination table each time. NOTE: If you wish to remove deleted records, this needs to be set to Full load and the Period from attribute empty.",
          "propertyOrder": 240
        }
      }
    }
  }
}



================================================
FILE: component_config/configuration_description.md
================================================
Configuration description.


================================================
FILE: component_config/documentationUrl.md
================================================
https://github.com/keboola/component-shopify-v2/blob/master/README.md


================================================
FILE: component_config/licenseUrl.md
================================================
https://github.com/keboola/component-shopify-v2/blob/master/LICENSE.md


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sourceCodeUrl.md
================================================
https://github.com/keboola/component-shopify-v2



================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8
python -m unittest discover


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi


================================================
FILE: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"


================================================
FILE: src/component.py
================================================
# src/component.py
import json
import logging
import re
import shutil
import tempfile
import time
from collections import OrderedDict, defaultdict
from pathlib import Path
from typing import Any

import dateparser
import duckdb
from keboola.component.base import ComponentBase
from keboola.component.dao import BaseType, ColumnDefinition, SupportedDataTypes
from keboola.component.exceptions import UserException

from configuration import PRODUCTS_ENDPOINTS, Configuration
from shopify_cli.client import BulkOperationResult, ShopifyGraphQLClient


class Component(ComponentBase):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.logger = logging.getLogger(__name__)
        db_path = "debug.duckdb" if self.configuration.parameters.get("debug") else ":memory:"
        self.conn = duckdb.connect(db_path)
        self.conn.execute("SET temp_directory='/tmp/duckdb_temp'")
        self.conn.execute("SET memory_limit='256MB'")
        self.conn.execute("SET preserve_insertion_order=false")
        self.params = Configuration(**self.configuration.parameters)

        if self.params.debug:
            self.logger.debug(f"DuckDB database saved to: {db_path}")

    def _camel_to_snake(self, name: str) -> str:
        """Convert camelCase to snake_case"""
        name = re.sub("(.)([A-Z][a-z]+)", r"\1_\2", name)
        return re.sub("([a-z0-9])([A-Z])", r"\1_\2", name).lower()

    def _scan_jsonl_keys(self, jsonl_path: str) -> dict[str, set[str]]:
        """Scan JSONL file to detect which keys exist for each entity type"""
        entity_keys = defaultdict(set)

        with open(jsonl_path) as f:
            for line in f:
                if obj := json.loads(line):
                    if entity_id := obj.get("id"):
                        if match := re.search(r"gid://shopify/([^/]+)/", entity_id):
                            entity_type = match.group(1)
                            entity_keys[entity_type].update(obj.keys())

        return entity_keys

    def _normalize_table(self, table_name: str) -> str:
        """
        Convert all STRUCT and LIST columns to JSON strings with proper double quotes.
        Rename all columns from camelCase to snake_case.
        """
        columns_info = self.conn.execute(f'DESCRIBE "{table_name}"').fetchall()
        select_parts = []
        needs_conversion = False

        for col_name, col_type, *_ in columns_info:
            col_type_clean = col_type.strip()
            snake_name = self._camel_to_snake(col_name)

            if (
                col_type_clean.startswith("STRUCT(")
                or col_type_clean.endswith("[]")
                or "LIST" in col_type_clean.upper()
            ):
                select_parts.append(f'json("{col_name}") AS "{snake_name}"')
                needs_conversion = True
            else:
                select_parts.append(f'"{col_name}" AS "{snake_name}"')
                if col_name != snake_name:
                    needs_conversion = True

        if not needs_conversion:
            return table_name

        normalized_table = f"{table_name}_json"

        self.conn.execute(f'DROP TABLE IF EXISTS "{normalized_table}"')
        self.conn.execute(f'CREATE TABLE "{normalized_table}" AS SELECT {", ".join(select_parts)} FROM "{table_name}"')

        return normalized_table

    def _decompose_json_columns(self, table_name: str, normalized_table: str):
        """
        Decompose JSON columns into separate child tables with proper relationships.
        Arrays become separate rows, objects become separate tables with 1:1 relationship.
        """
        columns_info = self.conn.execute(f'DESCRIBE "{table_name}"').fetchall()
        primary_key_col = "id"

        for col_name, col_type, *_ in columns_info:
            col_type_str = str(col_type).upper()

            if not ("STRUCT" in col_type_str or "LIST" in col_type_str or col_type_str.endswith("[]")):
                continue

            snake_col_name = self._camel_to_snake(col_name)
            self.logger.info(f"Decomposing column: {col_name} ({col_type}) in {table_name}")

            sample = self.conn.execute(
                f'SELECT "{col_name}" FROM "{table_name}" WHERE "{col_name}" IS NOT NULL LIMIT 1'
            ).fetchone()

            if not sample or not sample[0]:
                self.logger.debug(f"No data found for column {col_name}")
                continue

            sample_value = sample[0]

            if isinstance(sample_value, list):
                self._create_array_child_table(table_name, col_name, snake_col_name, primary_key_col)
            elif isinstance(sample_value, dict):
                self._create_object_child_table(table_name, col_name, snake_col_name, primary_key_col)

    def _create_array_child_table(self, parent_table: str, column_name: str, snake_col_name: str, parent_pk: str):
        """Create a child table for array/list JSON columns"""
        child_table_name = f"{parent_table}_{snake_col_name}"

        try:
            self.conn.execute(f'DROP TABLE IF EXISTS "{child_table_name}"')

            self.conn.execute(f"""
                CREATE TABLE "{child_table_name}" AS
                SELECT
                    "{parent_pk}" as parent_id,
                    ROW_NUMBER() OVER (PARTITION BY "{parent_pk}" ORDER BY (SELECT NULL)) as row_number,
                    UNNEST("{column_name}") as item
                FROM "{parent_table}"
                WHERE "{column_name}" IS NOT NULL AND len("{column_name}") > 0
            """)

            item_columns = self.conn.execute(f'DESCRIBE "{child_table_name}"').fetchall()

            if any("STRUCT" in str(col[1]) for col in item_columns):
                flattened_table = f"{child_table_name}_flat"
                self.conn.execute(f'DROP TABLE IF EXISTS "{flattened_table}"')

                struct_cols = []
                for col in item_columns:
                    if col[0] == "item" and "STRUCT" in str(col[1]):
                        struct_fields = self.conn.execute(
                            f'SELECT * FROM (SELECT item FROM "{child_table_name}" LIMIT 1)'
                        ).fetchone()

                        if struct_fields and struct_fields[0]:
                            for key in struct_fields[0].keys():
                                snake_key = self._camel_to_snake(key)
                                struct_cols.append(f"item['{key}'] as {snake_key}")

                if struct_cols:
                    select_clause = f"parent_id, row_number, {', '.join(struct_cols)}"
                    self.conn.execute(f"""
                        CREATE TABLE "{flattened_table}" AS
                        SELECT {select_clause}
                        FROM "{child_table_name}"
                    """)

                    self.conn.execute(f'DROP TABLE "{child_table_name}"')
                    self.conn.execute(f'ALTER TABLE "{flattened_table}" RENAME TO "{child_table_name}"')

            normalized_child = self._normalize_table(child_table_name)
            self._export_table_with_manifest(child_table_name, normalized_child)

            if not self.params.debug and normalized_child != child_table_name:
                self.conn.execute(f'DROP TABLE IF EXISTS "{child_table_name}"')

            self.logger.info(f"Created child table: {child_table_name}")

        except Exception as e:
            self.logger.warning(f"Failed to decompose array column {column_name}: {str(e)}")

    def _create_object_child_table(self, parent_table: str, column_name: str, snake_col_name: str, parent_pk: str):
        """Create a child table for object JSON columns"""
        child_table_name = f"{parent_table}_{snake_col_name}"

        try:
            sample = self.conn.execute(
                f'SELECT "{column_name}" FROM "{parent_table}" WHERE "{column_name}" IS NOT NULL LIMIT 1'
            ).fetchone()

            if not sample or not sample[0]:
                return

            sample_obj = sample[0]
            if not isinstance(sample_obj, dict):
                return

            field_selects = [f'"{parent_pk}" as parent_id']
            for key in sample_obj.keys():
                snake_key = self._camel_to_snake(key)
                field_selects.append(f"\"{column_name}\"['{key}'] as {snake_key}")

            self.conn.execute(f'DROP TABLE IF EXISTS "{child_table_name}"')
            self.conn.execute(f"""
                CREATE TABLE "{child_table_name}" AS
                SELECT {", ".join(field_selects)}
                FROM "{parent_table}"
                WHERE "{column_name}" IS NOT NULL
            """)

            normalized_child = self._normalize_table(child_table_name)
            self._export_table_with_manifest(child_table_name, normalized_child)

            if not self.params.debug and normalized_child != child_table_name:
                self.conn.execute(f'DROP TABLE IF EXISTS "{child_table_name}"')

            self.logger.info(f"Created child table: {child_table_name}")

        except Exception as e:
            self.logger.warning(f"Failed to decompose object column {column_name}: {str(e)}")

    def _parse_date_to_iso(self, date_str: str | None) -> str | None:
        """
        Parse date string (ISO format or relative like '1 week ago') to ISO format for Shopify API

        Args:
            date_str: Date string in ISO format (YYYY-MM-DD) or relative format ('1 week ago', 'now', etc.)

        Returns:
            ISO formatted date string (YYYY-MM-DD) or None if input is None
        """
        if not date_str:
            return None

        date_str = date_str.strip()

        try:
            parsed_date = dateparser.parse(date_str)

            if parsed_date is None:
                raise UserException(
                    f"Could not parse date '{date_str}'. Please use ISO format (YYYY-MM-DD) or relative format "
                    "like '1 week ago', 'now', etc."
                )

            return parsed_date.strftime(r"%Y-%m-%d")

        except Exception as e:
            raise UserException(f"Invalid date format '{date_str}': {str(e)}")

    def run(self):
        """
        Main execution code
        """
        params = Configuration(**self.configuration.parameters)

        client = ShopifyGraphQLClient(
            store_name=params.store_name,
            api_token=params.api_token,
            api_version=params.api_version,
            debug=params.debug,
        )

        enabled_endpoints = params.enabled_endpoints
        self.logger.info(f"Starting data extraction for endpoints: {enabled_endpoints}")

        products_endpoints_processed = False

        for endpoint in enabled_endpoints:
            if endpoint in PRODUCTS_ENDPOINTS and products_endpoints_processed:
                self.logger.info(f"Skipping already processed products endpoint: {endpoint}")
                continue
            self.logger.info(f"Processing endpoint: {endpoint}")
            self._process_endpoint(client, endpoint, params)
            if endpoint in PRODUCTS_ENDPOINTS:
                products_endpoints_processed = True

        if params.events:
            self.logger.info("Processing events endpoint")
            self._process_endpoint(client, "events", params)

        if params.custom_queries:
            self.logger.info(f"Processing {len(params.custom_queries)} custom bulk queries")
            for custom_query in params.custom_queries:
                self.logger.info(f"Processing custom bulk query: {custom_query.name}")
                self._process_custom_query(client, custom_query)

        self.logger.info("Data extraction completed successfully")

    def _process_endpoint(self, client: ShopifyGraphQLClient, endpoint: str, params: Configuration):
        """
        Process a specific endpoint using DuckDB
        """
        endpoint_methods = {
            "products": self._extract_products_bulk,
            "products_drafts": self._extract_products_bulk,
            "products_archived": self._extract_products_bulk,
            "products_unlisted": self._extract_products_bulk,
            "products_legacy": self._extract_products_legacy,
            "orders": self._extract_orders_bulk,
            "orders_legacy": self._extract_orders_legacy,
            "customers": self._extract_customers_bulk,
            "customers_legacy": self._extract_customers_legacy,
            "inventory": self._extract_inventory_bulk,
            "inventory_legacy": self._extract_inventory_levels,
            "locations": self._extract_locations_bulk,
            "events": self._extract_events,
        }

        try:
            extraction_method = endpoint_methods.get(endpoint)
            if extraction_method:
                extraction_method(client, params)
            else:
                self.logger.warning(f"Unknown endpoint: {endpoint}")
        except Exception as e:
            self.logger.error(f"Error processing endpoint {endpoint}: {str(e)}")
            raise UserException(f"Failed to process endpoint {endpoint}: {str(e)}")

    def _extract_orders_legacy(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract orders data using DuckDB (legacy one-by-one method)"""
        self.logger.info("Extracting orders data (legacy method)")

        all_orders = []
        for batch in client.get_orders(
            date_since=self._parse_date_to_iso(params.loading_options.date_since),
            date_to=self._parse_date_to_iso(params.loading_options.date_to),
            batch_size=params.batch_size,
        ):
            all_orders.extend(batch)

        if all_orders:
            self._process_with_duckdb("orders_legacy", all_orders, params)
            self.logger.info(f"Successfully extracted {len(all_orders)} orders")
        else:
            self.logger.info("No orders found")

    def _extract_orders_bulk(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract orders using Shopify bulk operations"""
        self.logger.info("Extracting orders using bulk operations")

        with tempfile.NamedTemporaryFile(mode="w+", suffix=".jsonl", delete=False) as tmp:
            temp_jsonl = tmp.name

        result = client.get_orders_bulk(
            temp_jsonl,
            include_transactions=params.endpoints.order_transactions,
            date_since=self._parse_date_to_iso(params.loading_options.date_since),
            date_to=self._parse_date_to_iso(params.loading_options.date_to),
            fetch_parameter=params.loading_options.fetch_parameter,
        )

        if result.item_count > 0:
            self._process_bulk_orders(result)
        else:
            self.logger.info("No orders found")
            Path(result.file_path).unlink(missing_ok=True)

    def _extract_products_legacy(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract products data using DuckDB (legacy one-by-one method)"""
        self.logger.info("Extracting products data (legacy method)")

        all_products = []
        for batch in client.get_products(batch_size=params.batch_size):
            all_products.extend(batch)

        if all_products:
            self._process_with_duckdb("products_legacy", all_products, params)
            self.logger.info(f"Successfully extracted {len(all_products)} products")
        else:
            self.logger.info("No products found")

    def _extract_products_bulk(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract products using Shopify bulk operations"""
        self.logger.info("Extracting products using bulk operations")

        statuses = []
        if params.endpoints.products:
            statuses.append("active")
        if params.endpoints.products_drafts:
            statuses.append("draft")
        if params.endpoints.products_archived:
            statuses.append("archived")
        if params.endpoints.products_unlisted:
            statuses.append("unlisted")

        if not statuses:
            self.logger.warning("No product status selected, skipping products extraction")
            return

        status_filter = ",".join(statuses)
        self.logger.info(f"Fetching products with statuses: {status_filter}")

        with tempfile.NamedTemporaryFile(mode="w+", suffix=".jsonl", delete=False) as tmp:
            temp_jsonl = tmp.name

        result = client.get_products_bulk(
            temp_jsonl,
            status=status_filter,
            include_product_metafields=params.endpoints.product_metafields,
            include_variant_metafields=params.endpoints.variant_metafields,
            date_since=self._parse_date_to_iso(params.loading_options.date_since),
            date_to=self._parse_date_to_iso(params.loading_options.date_to),
            fetch_parameter=params.loading_options.fetch_parameter,
        )

        if result.item_count > 0:
            self._process_bulk_products(result)
        else:
            self.logger.info("No products found")
            Path(result.file_path).unlink(missing_ok=True)

    def _process_bulk_result(self, bulk_result: BulkOperationResult, table_name: str, entity_name: str | None = None):
        """Generic method to process bulk operation results"""
        if entity_name is None:
            entity_name = table_name
        process_start = time.time()

        self.logger.info(f"Processing {bulk_result.item_count} {entity_name} from {bulk_result.file_path}")

        try:
            entity_keys = self._scan_jsonl_keys(bulk_result.file_path)

            self.conn.execute(f'DROP TABLE IF EXISTS "{table_name}"')
            self.conn.execute(
                f"CREATE TABLE \"{table_name}\" AS SELECT * FROM read_json_auto('{bulk_result.file_path}')"
            )

            normalized_table = self._normalize_table(table_name)
            self._export_table_with_manifest(table_name, normalized_table, entity_keys)
            self._decompose_json_columns(table_name, normalized_table)

            if not self.params.debug:
                self.conn.execute(f'DROP TABLE IF EXISTS "{table_name}"')

            result_count = self.conn.execute(f"SELECT COUNT(*) FROM {normalized_table}").fetchone()
            row_count = result_count[0] if result_count else 0

            process_time = time.time() - process_start
            self.logger.info(
                f"{entity_name.capitalize()} processing complete: {row_count} items in {process_time:.2f}s "
                f"(API wait: {bulk_result.api_wait_time:.2f}s, download: {bulk_result.download_time:.2f}s, "
                f"process: {process_time:.2f}s)"
            )
        finally:
            if self.params.debug:
                debug_file = f"bulk_{table_name}_download.jsonl"
                shutil.copy2(bulk_result.file_path, debug_file)
            Path(bulk_result.file_path).unlink(missing_ok=True)

    def _process_bulk_products(self, bulk_result: BulkOperationResult):
        self._process_bulk_result(bulk_result, "product")

    def _process_bulk_orders(self, bulk_result: BulkOperationResult):
        self._process_bulk_result(bulk_result, "order")

    def _extract_customers_legacy(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract customers data using DuckDB (legacy one-by-one method)"""
        self.logger.info("Extracting customers data (legacy method)")

        all_customers = []
        for batch in client.get_customers(batch_size=params.batch_size):
            all_customers.extend(batch)

        if all_customers:
            self._process_with_duckdb("customers_legacy", all_customers, params)
            self.logger.info(f"Successfully extracted {len(all_customers)} customers")
        else:
            self.logger.info("No customers found")

    def _extract_customers_bulk(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract customers using Shopify bulk operations"""
        self.logger.info("Extracting customers using bulk operations")

        with tempfile.NamedTemporaryFile(mode="w+", suffix=".jsonl", delete=False) as tmp:
            temp_jsonl = tmp.name

        result = client.get_customers_bulk(
            temp_jsonl,
            date_since=self._parse_date_to_iso(params.loading_options.date_since),
            date_to=self._parse_date_to_iso(params.loading_options.date_to),
            fetch_parameter=params.loading_options.fetch_parameter,
        )

        if result.item_count > 0:
            self._process_bulk_customers(result)
        else:
            self.logger.info("No customers found")
            Path(result.file_path).unlink(missing_ok=True)

    def _process_bulk_customers(self, bulk_result: BulkOperationResult):
        self._process_bulk_result(bulk_result, "customer")

    def _extract_inventory_bulk(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract inventory using Shopify bulk operations"""
        self.logger.info("Extracting inventory using bulk operations")

        with tempfile.NamedTemporaryFile(mode="w+", suffix=".jsonl", delete=False) as tmp:
            temp_jsonl = tmp.name

        result = client.get_inventory_bulk(
            temp_jsonl,
            date_since=self._parse_date_to_iso(params.loading_options.date_since),
            date_to=self._parse_date_to_iso(params.loading_options.date_to),
            fetch_parameter=params.loading_options.fetch_parameter,
        )

        if result.item_count > 0:
            self._process_bulk_inventory(result)
        else:
            self.logger.info("No inventory found")
            Path(result.file_path).unlink(missing_ok=True)

    def _process_bulk_inventory(self, bulk_result: BulkOperationResult):
        self._process_bulk_result(bulk_result, "inventory", "inventory items")

    def _extract_locations_bulk(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract locations using Shopify bulk operations"""
        self.logger.info("Extracting locations using bulk operations")

        with tempfile.NamedTemporaryFile(mode="w+", suffix=".jsonl", delete=False) as tmp:
            temp_jsonl = tmp.name

        result = client.get_locations_bulk(temp_jsonl)

        if result.item_count > 0:
            self._process_bulk_locations(result)
        else:
            self.logger.info("No locations found")
            Path(result.file_path).unlink(missing_ok=True)

    def _process_bulk_locations(self, bulk_result: BulkOperationResult):
        self._process_bulk_result(bulk_result, "location")

    def _extract_inventory_levels(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract inventory levels data using DuckDB"""
        self.logger.info("Extracting inventory levels data")

        all_inventory_levels = []
        for batch in client.get_inventory_levels(batch_size=params.batch_size):
            all_inventory_levels.extend(batch)

        if all_inventory_levels:
            self._process_with_duckdb("inventory_levels", all_inventory_levels, params)
            self.logger.info(f"Successfully extracted {len(all_inventory_levels)} inventory levels")
        else:
            self.logger.info("No inventory levels found")

    def _extract_events(self, client: ShopifyGraphQLClient, params: Configuration):
        """Extract events using Shopify bulk operations"""
        self.logger.info("Extracting events using bulk operations")

        with tempfile.NamedTemporaryFile(mode="w+", suffix=".jsonl", delete=False) as tmp:
            temp_jsonl = tmp.name

        result = client.get_events_bulk(
            temp_jsonl,
            date_since=self._parse_date_to_iso(params.loading_options.date_since),
            date_to=self._parse_date_to_iso(params.loading_options.date_to),
        )

        if result.item_count > 0:
            self._process_bulk_events(result)
        else:
            self.logger.info("No events found")
            Path(result.file_path).unlink(missing_ok=True)

    def _process_bulk_events(self, bulk_result: BulkOperationResult):
        self._process_bulk_result(bulk_result, "event")

    def _process_with_duckdb(self, table_name: str, data: list[dict[str, Any]], params: Configuration):
        """
        Process data using DuckDB for type detection and normalization
        """
        if not data:
            return

        file_def = self.create_out_file_definition(f"{table_name}_temp.json")
        temp_json = Path(file_def.full_path)
        with open(temp_json, "w") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        try:
            self.conn.execute(f"""
                CREATE OR REPLACE TABLE {table_name}_raw AS
                SELECT * FROM read_json_auto('{temp_json}')
            """)

            if table_name == "orders":
                self._create_orders_tables(table_name)
            elif table_name == "products":
                self._create_products_tables(table_name)
            elif table_name == "inventory_items":
                self._create_inventory_tables(table_name)
            else:
                self._export_table_with_manifest(f"{table_name}_raw")

        finally:
            if not self.configuration.parameters.debug:
                temp_json.unlink()

    def _create_orders_tables(self, table_name: str):
        self.conn.execute(f"""
            CREATE OR REPLACE TABLE orders AS
            SELECT
                id,
                name,
                email,
                phone,
                createdAt,
                updatedAt,
                processedAt,
                cancelledAt,
                cancelReason,
                totalPriceSet.shopMoney.amount as totalPrice,
                totalPriceSet.shopMoney.currencyCode as totalPriceCurrency,
                subtotalPriceSet.shopMoney.amount as subtotalPrice,
                subtotalPriceSet.shopMoney.currencyCode as subtotalPriceCurrency,
                totalTaxSet.shopMoney.amount as totalTax,
                totalTaxSet.shopMoney.currencyCode as totalTaxCurrency,
                totalShippingPriceSet.shopMoney.amount as totalShippingPrice,
                totalShippingPriceSet.shopMoney.currencyCode as totalShippingPriceCurrency,
                customer.id as customerId,
                customer.firstName as customerFirstName,
                customer.lastName as customerLastName,
                customer.email as customerEmail,
                customer.phone as customerPhone
            FROM {table_name}_raw
        """)

        self.conn.execute(f"""
            CREATE OR REPLACE TABLE order_line_items AS
            SELECT
                o.id as orderId,
                item->>'$.node.id' as lineItemId,
                item->>'$.node.title' as title,
                CAST(item->>'$.node.quantity' AS INTEGER) as quantity,
                item->>'$.node.sku' as sku,
                item->>'$.node.variant.id' as variantId,
                item->>'$.node.variant.title' as variantTitle,
                item->>'$.node.variant.sku' as variantSku,
                item->>'$.node.variant.price' as variantPrice
            FROM {table_name}_raw o,
            UNNEST(o.lineItems.edges) as t(item)
        """)

        self._export_table_with_manifest("orders")
        self._export_table_with_manifest("order_line_items")

    def _create_products_tables(self, table_name: str):
        self.conn.execute(f"""
            CREATE OR REPLACE TABLE products AS
            SELECT
                id,
                title,
                handle,
                description,
                productType,
                vendor,
                createdAt,
                updatedAt,
                publishedAt,
                status,
                tags
            FROM {table_name}_raw
        """)

        self.conn.execute(f"""
            CREATE OR REPLACE TABLE product_variants AS
            SELECT
                p.id as productId,
                variant->>'$.node.id' as variantId,
                variant->>'$.node.title' as title,
                variant->>'$.node.sku' as sku,
                variant->>'$.node.price' as price,
                variant->>'$.node.compareAtPrice' as compareAtPrice,
                CAST(variant->>'$.node.inventoryQuantity' AS INTEGER) as inventoryQuantity,
                CAST(variant->>'$.node.weight' AS DOUBLE) as weight,
                variant->>'$.node.weightUnit' as weightUnit
            FROM {table_name}_raw p,
            UNNEST(p.variants.edges) as t(variant)
        """)

        self._export_table_with_manifest("products")
        self._export_table_with_manifest("product_variants")

    def _create_inventory_tables(self, table_name: str):
        self.conn.execute(f"""
            CREATE OR REPLACE TABLE inventory_items AS
            SELECT
                id,
                sku,
                tracked,
                createdAt,
                updatedAt,
                countryCodeOfOrigin,
                harmonizedSystemCode,
                provinceCodeOfOrigin,
                requiresShipping,
                unitCost.amount as unitCostAmount,
                unitCost.currencyCode as unitCostCurrency,
                variant.id as variantId,
                variant.title as variantTitle,
                variant.sku as variantSku,
                variant.price as variantPrice,
                variant.product.id as productId,
                variant.product.title as productTitle,
                variant.product.handle as productHandle
            FROM {table_name}_raw
        """)

        self.conn.execute(f"""
            CREATE OR REPLACE TABLE inventory_levels AS
            SELECT
                i.id as inventoryItemId,
                level->>'$.node.id' as levelId,
                CAST(level->>'$.node.available' AS INTEGER) as available,
                level->>'$.node.location.id' as locationId,
                level->>'$.node.location.name' as locationName
            FROM {table_name}_raw i,
            UNNEST(i.inventoryLevels.edges) as t(level)
        """)

        self._export_table_with_manifest("inventory_items")
        self._export_table_with_manifest("inventory_levels")

    def _export_table_with_manifest(
        self, table_name: str, normalized_table: str | None = None, entity_keys: dict[str, set[str]] | None = None
    ):
        if normalized_table is None:
            normalized_table = table_name
        table_meta = self.conn.execute(f'DESCRIBE "{normalized_table}"').fetchall()

        has_id = any(col[0] == "id" for col in table_meta)
        entity_types = []

        if has_id:
            try:
                entity_types_result = self.conn.execute(f"""
                    SELECT DISTINCT regexp_extract(id, 'gid://shopify/([^/]+)/', 1) as entity_type
                    FROM "{normalized_table}"
                    WHERE id IS NOT NULL AND id LIKE 'gid://shopify/%'
                """).fetchall()
                entity_types = [et[0] for et in entity_types_result if et[0]]
            except Exception:
                pass

        if len(entity_types) > 1:
            self.logger.info(f"Splitting {table_name} by entity types: {', '.join(entity_types)}")
            for entity_type in entity_types:
                snake_entity = self._camel_to_snake(entity_type)
                self._export_entity_type(normalized_table, snake_entity, entity_type, table_meta, entity_keys)
        else:
            self._export_single_table(table_name, normalized_table, table_meta)

    def _export_entity_type(
        self,
        normalized_table: str,
        entity_name: str,
        entity_type: str,
        table_meta: list,
        entity_keys: dict[str, set[str]] | None,
    ):
        if entity_keys and entity_type in entity_keys:
            jsonl_keys_snake = {self._camel_to_snake(k) for k in entity_keys[entity_type]}
            valid_columns = [c[0] for c in table_meta if c[0] in jsonl_keys_snake]
        else:
            valid_columns = [c[0] for c in table_meta]

        schema = OrderedDict(
            {
                ("parent_id" if c[0] == "__parent_id" else c[0]): ColumnDefinition(
                    data_types=BaseType(dtype=self.convert_base_types(c[1])),
                    primary_key=False,
                )
                for c in table_meta
            }
        )

        out_table = self.create_out_table_definition(
            f"{entity_name}.csv",
            schema=schema,
            primary_key=self._get_primary_key(entity_name),
            incremental=bool(self.params.loading_options.incremental_output),
            has_header=True,
        )

        try:
            renamed_columns = []
            for col in valid_columns:
                if col == "__parent_id":
                    renamed_columns.append('"__parent_id" AS "parent_id"')
                else:
                    renamed_columns.append(f'"{col}"')
            column_list = ", ".join(renamed_columns)

            q = f"""
                COPY (
                    SELECT {column_list}
                    FROM "{normalized_table}"
                    WHERE id LIKE 'gid://shopify/{entity_type}/%'
                ) TO '{out_table.full_path}' (HEADER, DELIMITER ',', FORCE_QUOTE *)
            """
            logging.debug(f"Running query: {q}; ")
            self.conn.execute(q)
            self.write_manifest(out_table)
            self.logger.info(f"Exported entity type: {entity_name} ({entity_type})")
        except duckdb.ConversionException as e:
            raise UserException(f"Error during query execution: {e}")

    def _export_single_table(self, table_name: str, normalized_table: str, table_meta: list):
        schema = OrderedDict(
            {
                c[0]: ColumnDefinition(
                    data_types=BaseType(dtype=self.convert_base_types(c[1])),
                    primary_key=False,
                )
                for c in table_meta
            }
        )

        out_table = self.create_out_table_definition(
            f"{table_name}.csv",
            schema=schema,
            primary_key=self._get_primary_key(table_name),
            incremental=bool(self.params.loading_options.incremental_output),
            has_header=True,
        )

        try:
            q = f"COPY \"{normalized_table}\" TO '{out_table.full_path}' (HEADER, DELIMITER ',', FORCE_QUOTE *)"
            logging.debug(f"Running query: {q}; ")
            self.conn.execute(q)
            self.write_manifest(out_table)
        except duckdb.ConversionException as e:
            raise UserException(f"Error during query execution: {e}")

    def _get_primary_key(self, table_name: str) -> list[str]:
        """Define primary keys for different tables"""
        primary_keys = {
            "order": ["id"],
            "order_legacy": ["id"],
            "product": ["id"],
            "product_legacy": ["id"],
            "customer": ["id"],
            "customer_legacy": ["id"],
            "inventory": ["id"],
            "inventory_item": ["id"],
            "inventory_level": ["parent_id", "id"],
            "location": ["id"],
            "event": ["id"],
        }

        if table_name in primary_keys:
            return primary_keys[table_name]

        if "_" in table_name:
            try:
                columns = [col[0] for col in self.conn.execute(f'DESCRIBE "{table_name}"').fetchall()]
                if "parent_id" in columns and "row_number" in columns:
                    return ["parent_id", "row_number"]
                elif "parent_id" in columns:
                    return ["parent_id"]
                elif "id" in columns:
                    return ["id"]
            except Exception:
                pass

        return []

    @staticmethod
    def convert_base_types(dtype: str) -> SupportedDataTypes:
        if dtype in [
            "TINYINT",
            "SMALLINT",
            "INTEGER",
            "BIGINT",
            "HUGEINT",
            "UTINYINT",
            "USMALLINT",
            "UINTEGER",
            "UBIGINT",
            "UHUGEINT",
        ]:
            return SupportedDataTypes.INTEGER
        elif dtype in ["REAL", "DECIMAL"]:
            return SupportedDataTypes.NUMERIC
        elif dtype == "DOUBLE":
            return SupportedDataTypes.FLOAT
        elif dtype == "BOOLEAN":
            return SupportedDataTypes.BOOLEAN
        elif dtype in ["TIMESTAMP", "TIMESTAMP WITH TIME ZONE"]:
            return SupportedDataTypes.TIMESTAMP
        elif dtype == "DATE":
            return SupportedDataTypes.DATE
        else:
            return SupportedDataTypes.STRING

    def _process_custom_query(self, client: ShopifyGraphQLClient, custom_query):
        """Process a custom GraphQL bulk query"""
        self.logger.info(f"Executing custom bulk query: {custom_query.name}")

        with tempfile.NamedTemporaryFile(mode="w+", suffix=".jsonl", delete=False) as tmp:
            temp_jsonl = tmp.name

        result = client.execute_custom_bulk_query(custom_query.query, temp_jsonl)

        if result.item_count > 0:
            self._process_bulk_custom(result, custom_query.name)
        else:
            self.logger.info(f"Custom bulk query '{custom_query.name}' returned no results")
            Path(result.file_path).unlink(missing_ok=True)

    def _process_bulk_custom(self, bulk_result: BulkOperationResult, table_name: str):
        self._process_bulk_result(bulk_result, table_name, f"custom query '{table_name}'")

    # ... ostatní extract metody zůstávají stejné, jen volají _process_with_duckdb


"""
    Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)



================================================
FILE: src/configuration.py
================================================
import logging

from keboola.component.exceptions import UserException
from pydantic import BaseModel, Field, ValidationError, field_validator

PRODUCTS_ENDPOINTS = {"products", "products_drafts", "products_archived", "products_unlisted"}
EXCLUDE_FROM_ENDPOINTS = {"product_metafields", "variant_metafields", "order_transactions"}


class CustomQuery(BaseModel):
    """Custom GraphQL bulk operation configuration"""

    name: str = Field(..., description="Query name (used for output table name)")
    query: str = Field(..., description="GraphQL bulk operation mutation string")

    @field_validator("name")
    def validate_name(cls, v):
        if not v or len(v.strip()) == 0:
            raise UserException("Custom query name cannot be empty")
        sanitized = v.strip().lower().replace(" ", "_").replace("-", "_")
        return sanitized

    @field_validator("query")
    def validate_query(cls, v):
        if not v or len(v.strip()) == 0:
            raise UserException("Custom bulk operation query cannot be empty")
        return v.strip()


class LoadingOptions(BaseModel):
    date_since: str | None = Field(default=None, description="Start date for data extraction (YYYY-MM-DD)")
    date_to: str | None = Field(default=None, description="End date for data extraction (YYYY-MM-DD)")
    fetch_parameter: str = Field(default="updated_at", description="Field to filter by (updated_at or created_at)")
    incremental_output: int = Field(default=1, description="Load type: 0=Full Load, 1=Incremental Update")


class Endpoints(BaseModel):
    """Endpoints configuration - boolean flags for each endpoint"""

    products: bool = False
    products_drafts: bool = False
    products_archived: bool = False
    products_unlisted: bool = False
    product_metafields: bool = False
    variant_metafields: bool = False
    orders: bool = False
    order_transactions: bool = False
    customers: bool = False
    inventory: bool = False

    # not sure whether we need these
    inventory_items: bool = False
    locations: bool = False

    # legacy endpoints (downloading items in batches instead of bulk download)
    products_legacy: bool = False
    orders_legacy: bool = False
    customers_legacy: bool = False

    def get_enabled_endpoints(self) -> list[str]:
        """Get list of enabled endpoint names (excludes metafield toggles as they're not standalone endpoints)"""
        enabled = []
        for field_name, field_value in self.model_dump().items():
            if field_value and field_name not in EXCLUDE_FROM_ENDPOINTS:
                enabled.append(field_name)
        return enabled


class Configuration(BaseModel):
    store_name: str = Field(..., description="Shopify store name (without .myshopify.com)")
    api_version: str = Field(default="2025-10", description="Shopify API version")
    api_token: str = Field(alias="#api_token", description="Shopify Admin API access token")
    endpoints: Endpoints = Field(default_factory=Endpoints, description="Endpoints configuration")
    events: list[dict] = Field(default_factory=list, description="Events configuration")
    custom_queries: list[CustomQuery] = Field(default_factory=list, description="Custom GraphQL bulk operations")
    loading_options: LoadingOptions = Field(default_factory=LoadingOptions)
    debug: bool = Field(default=False, description="Enable debug mode")

    # keeping as a hidden argument untiil we eventually remove the batch GraphQL endpoints support
    batch_size: int = Field(default=50, ge=1, le=250, description="Number of records per batch")

    def __init__(self, **data):
        try:
            super().__init__(**data)
        except ValidationError as e:
            error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
            raise UserException(f"Validation Error: {', '.join(error_messages)}")

        if self.debug:
            logging.debug("Component will run in Debug mode")

    @field_validator("api_token")
    def validate_api_token(cls, v):
        if not v or len(v.strip()) == 0:
            raise UserException("API token cannot be empty")
        return v.strip()

    @field_validator("store_name")
    def validate_store_name(cls, v):
        if not v or len(v.strip()) == 0:
            raise UserException("Store name cannot be empty")
        # Remove .myshopify.com if present
        store_name = v.strip().lower()
        if store_name.endswith(".myshopify.com"):
            store_name = store_name[:-14]
        return store_name

    @property
    def shop_url(self) -> str:
        """Get the full Shopify shop URL"""
        return f"https://{self.store_name}.myshopify.com"

    @property
    def enabled_endpoints(self) -> list[str]:
        """Get list of enabled endpoint names"""
        return self.endpoints.get_enabled_endpoints()



================================================
FILE: src/shopify_cli/__init__.py
================================================
[Empty file]


================================================
FILE: src/shopify_cli/client.py
================================================
import json
import logging
import time
from collections.abc import Iterator
from dataclasses import dataclass
from functools import wraps
from typing import Any
from urllib.request import urlopen

import shopify
from keboola.component.exceptions import UserException

from .query_loader import QueryLoader

TOTAL_ITEMS_LIMIT: int | None = None  # None for production, count for testing


@dataclass
class BulkOperationResult:
    """Result from a bulk operation"""

    file_path: str
    item_count: int
    api_wait_time: float  # Time spent waiting for Shopify to process
    download_time: float  # Time spent downloading the JSONL file


def log_bulk_performance(entity_name: str):
    """Decorator to log performance metrics for bulk operations"""

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            logger = logging.getLogger(__name__)

            result = func(*args, **kwargs)

            if result:
                total_time = result.api_wait_time + result.download_time
                items_per_second = result.item_count / total_time if total_time > 0 else 0
                logger.info(
                    f"{entity_name.capitalize()} bulk operation: "
                    f"API wait {result.api_wait_time:.2f}s, "
                    f"download {result.download_time:.2f}s, "
                    f"total {total_time:.2f}s "
                    f"({result.item_count} items, {items_per_second:.2f} items/s)"
                )

            return result

        return wrapper

    return decorator


class ShopifyGraphQLClient:
    """
    Shopify GraphQL API client for data extraction
    """

    def __init__(self, store_name: str, api_token: str, api_version: str, debug: bool):
        """
        Initialize Shopify GraphQL client

        Args:
            store_name: Shopify store name (without .myshopify.com)
            api_token: Shopify Admin API access token
            api_version: Shopify API version
            debug: Enable debug mode (saves JSONL files)
        """
        self.store_name = store_name
        self.api_token = api_token
        self.api_version = api_version
        self.debug = debug
        self.logger = logging.getLogger(__name__)

        # Initialize query loader
        self.query_loader = QueryLoader()

        # Setup Shopify session
        self._setup_session()

    def _setup_session(self):
        """Setup Shopify session for API calls"""
        try:
            shop_url = f"https://{self.store_name}.myshopify.com"
            session = shopify.Session(shop_url, self.api_version, self.api_token)
            shopify.ShopifyResource.activate_session(session)
            self.logger.info(f"Successfully connected to Shopify store: {self.store_name}")
        except Exception as e:
            raise UserException(f"Failed to connect to Shopify store: {str(e)}")

    def execute_query(
        self, query: str, variables: dict[str, Any] | None = None, max_retries: int = 5
    ) -> dict[str, Any]:
        """
        Execute GraphQL query with retry logic for throttling

        Args:
            query: GraphQL query string
            variables: Query variables
            max_retries: Maximum number of retries for throttled requests

        Returns:
            Query response data
        """
        retry_count = 0
        base_wait = 1  # Start with 1 second

        while retry_count <= max_retries:
            try:
                client = shopify.GraphQL()
                result_str = client.execute(query, variables=variables)

                result = json.loads(result_str)

                if "errors" in result:
                    error_messages = [error.get("message", "Unknown error") for error in result["errors"]]

                    # Check if it's a throttling error
                    if any("throttled" in msg.lower() for msg in error_messages):
                        if retry_count < max_retries:
                            wait_time = base_wait * (2**retry_count)  # Exponential backoff
                            self.logger.warning(
                                f"API throttled. Waiting {wait_time}s before retry {retry_count + 1}/{max_retries}"
                            )
                            time.sleep(wait_time)
                            retry_count += 1
                            continue

                    raise UserException(f"GraphQL query failed: {'; '.join(error_messages)}")

                return result.get("data", {})
            except Exception as e:
                if isinstance(e, UserException):
                    raise
                raise UserException(f"Failed to execute GraphQL query: {str(e)}")

        # If all retries exhausted
        raise UserException(f"GraphQL query failed after {max_retries} retries due to throttling")

    def _paginate(
        self,
        query: str,
        data_key: str,
        batch_size: int,
        max_items: int | None = TOTAL_ITEMS_LIMIT,
        extra_variables: dict | None = None,
    ) -> Iterator[list[dict[str, Any]]]:
        """
        Generic pagination helper for GraphQL queries

        Args:
            query: GraphQL query string
            data_key: Key in response data containing the edges
            batch_size: Number of items per batch
            max_items: Maximum total items to fetch (for testing)
            extra_variables: Additional variables to pass to the query

        Yields:
            List of item dictionaries
        """
        cursor = None
        total_fetched = 0

        while True:
            variables = {"first": batch_size}
            if cursor:
                variables["after"] = cursor
            if extra_variables:
                variables.update(extra_variables)

            data = self.execute_query(query, variables)
            collection_data = data.get(data_key, {})
            edges = collection_data.get("edges", [])

            if not edges:
                break

            # Extract nodes from edges
            items = [edge["node"] for edge in edges]

            # Apply max_items limit if specified
            if max_items is not None:
                remaining = max_items - total_fetched
                if len(items) > remaining:
                    items = items[:remaining]
                    self.logger.info(f"Sliced batch to {len(items)} items to respect limit")
                total_fetched += len(items)
                self.logger.info(f"Fetched {len(items)} items (total: {total_fetched}/{max_items})")

            yield items

            # Check if we've reached the limit
            if max_items is not None and total_fetched >= max_items:
                self.logger.info(f"Reached max_items limit of {max_items}, stopping pagination")
                break

            page_info = collection_data.get("pageInfo", {})
            if not page_info.get("hasNextPage", False):
                break

            cursor = page_info.get("endCursor")

    def get_orders(
        self,
        date_since: str | None = None,
        date_to: str | None = None,
        batch_size: int = 50,
    ) -> Iterator[list[dict[str, Any]]]:
        """
        Get orders with pagination

        Args:
            date_since: Start date for filtering (YYYY-MM-DD)
            date_to: End date for filtering (YYYY-MM-DD)
            batch_size: Number of orders per batch

        Yields:
            List of order dictionaries
        """
        query = self.query_loader.load_query("GetOrders")

        # Build date filter query
        date_filter = ""
        if date_since or date_to:
            date_conditions = []
            if date_since:
                date_conditions.append(f"created_at:>={date_since}")
            if date_to:
                date_conditions.append(f"created_at:<={date_to}")
            date_filter = f'query: "{" ".join(date_conditions)}"'

        # Modify query to include date filter if needed
        if date_filter:
            query = query.replace(
                "query GetOrders($first: Int!, $after: String, $query: String)",
                "query GetOrders($first: Int!, $after: String)",
            )
            query = query.replace(
                "orders(first: $first, after: $after, query: $query)",
                f"orders(first: $first, after: $after, {date_filter})",
            )
        else:
            query = query.replace(
                "query GetOrders($first: Int!, $after: String, $query: String)",
                "query GetOrders($first: Int!, $after: String)",
            )
            query = query.replace(
                "orders(first: $first, after: $after, query: $query)",
                "orders(first: $first, after: $after)",
            )

        yield from self._paginate(query, "orders", batch_size)

    def get_products(self, batch_size: int = 50) -> Iterator[list[dict[str, Any]]]:
        """
        Get products with pagination

        Args:
            batch_size: Number of products per batch

        Yields:
            List of product dictionaries
        """
        query = self.query_loader.load_query("GetProducts")
        yield from self._paginate(query, "products", batch_size)

    def get_customers(self, batch_size: int = 50) -> Iterator[list[dict[str, Any]]]:
        """
        Get customers with pagination

        Args:
            batch_size: Number of customers per batch

        Yields:
            List of customer dictionaries
        """
        query = self.query_loader.load_query("GetCustomers")
        yield from self._paginate(query, "customers", batch_size)

    def get_inventory_items(self, batch_size: int = 50) -> Iterator[list[dict[str, Any]]]:
        """
        Get inventory items with pagination

        Args:
            batch_size: Number of inventory items per batch

        Yields:
            List of inventory item dictionaries
        """
        query = self.query_loader.load_query("GetInventoryItems")
        yield from self._paginate(query, "inventoryItems", batch_size)

    def get_product_drafts(self, batch_size: int = 50) -> Iterator[list[dict[str, Any]]]:
        """
        Get product drafts with pagination

        Args:
            batch_size: Number of product drafts per batch

        Yields:
            List of product draft dictionaries
        """
        query = self.query_loader.load_query("GetProducts")
        yield from self._paginate(query, "productDrafts", batch_size)

    def get_product_metafields(self, batch_size: int = 50) -> Iterator[list[dict[str, Any]]]:
        """
        Get product metafields with pagination

        Args:
            batch_size: Number of metafields per batch

        Yields:
            List of metafield dictionaries
        """
        query = self.query_loader.load_query("GetProductMetafields")
        yield from self._paginate(query, "metafields", batch_size)

    def get_variant_metafields(self, batch_size: int = 50) -> Iterator[list[dict[str, Any]]]:
        """
        Get variant metafields with pagination

        Args:
            batch_size: Number of metafields per batch

        Yields:
            List of metafield dictionaries
        """
        query = self.query_loader.load_query("GetVariantMetafields")
        yield from self._paginate(query, "metafields", batch_size)

    def get_inventory_levels(self, batch_size: int = 50) -> Iterator[list[dict[str, Any]]]:
        """
        Get inventory levels with pagination

        Args:
            batch_size: Number of inventory levels per batch

        Yields:
            List of inventory level dictionaries
        """
        query = self.query_loader.load_query("GetInventoryLevels")
        yield from self._paginate(query, "inventoryLevels", batch_size)

    def get_products_archived(self, batch_size: int = 50) -> Iterator[list[dict[str, Any]]]:
        """
        Get archived products with pagination

        Args:
            batch_size: Number of products per batch

        Yields:
            List of archived product dictionaries
        """
        query = self.query_loader.load_query("GetProducts")
        # Modify query to filter for archived products
        query = query.replace(
            "products(first: $first, after: $after)", 'products(first: $first, after: $after, query: "status:archived")'
        )
        yield from self._paginate(query, "products", batch_size)

    @log_bulk_performance("products")
    def get_products_bulk(
        self,
        temp_file_path: str,
        status: str | None = None,
        include_product_metafields: bool = False,
        include_variant_metafields: bool = False,
        date_since: str | None = None,
        date_to: str | None = None,
        fetch_parameter: str = "updated_at",
    ) -> BulkOperationResult:
        """
        Get all products using Shopify's bulk operations

        Args:
            status: Product status filter - can be single value
                or comma-separated (e.g., "ACTIVE", "ACTIVE,DRAFT,ARCHIVED")
                If None, all products regardless of status will be fetched
            temp_file_path: Path where JSONL results will be saved
            include_product_metafields: Whether to include product metafields in the response
            include_variant_metafields: Whether to include variant metafields in the response
            date_since: Start date for filtering (YYYY-MM-DD format)
            date_to: End date for filtering (YYYY-MM-DD format)
            fetch_parameter: Field to filter by ('updated_at' or 'created_at')

        Returns:
            BulkOperationResult with file path and timing info (item_count will be 0 if no results)
        """
        api_wait_start = time.time()

        filters = []
        if status:
            filters.append(f"status:{status}")
        if date_since:
            filters.append(f"{fetch_parameter}:>='{date_since}'")
        if date_to:
            filters.append(f"{fetch_parameter}:<'{date_to}'")

        query_filter = " AND ".join(filters) if filters else ""
        log_status = f" with status={status}" if status else ""

        log_dates = ""
        if date_since and date_to:
            log_dates = f" from {date_since} to {date_to}"
        elif date_since:
            log_dates = f" from {date_since}"
        elif date_to:
            log_dates = f" until {date_to}"

        metafields_log = []
        if include_product_metafields:
            metafields_log.append("product metafields")
        if include_variant_metafields:
            metafields_log.append("variant metafields")
        log_metafields = f" (including {' & '.join(metafields_log)})" if metafields_log else ""

        self.logger.info(f"Starting bulk operation for products{log_status}{log_metafields}{log_dates}")

        # Start bulk operation - load mutation directly
        mutation_file = self.query_loader.queries_dir / "BulkProducts.graphql"
        with open(mutation_file) as f:
            mutation = f.read()

        # Inject product metafields if requested
        if include_product_metafields:
            metafields_fragment_file = self.query_loader.queries_dir / "fragments" / "ProductMetafields.graphql"
            with open(metafields_fragment_file) as f:
                product_metafields_fragment = f.read()
            mutation = mutation.replace("__METAFIELDS_PLACEHOLDER__", product_metafields_fragment)
        else:
            mutation = mutation.replace("__METAFIELDS_PLACEHOLDER__", "")

        # Inject variant metafields if requested
        if include_variant_metafields:
            var_metafields_frag_file = self.query_loader.queries_dir / "fragments" / "VariantMetafields.graphql"
            with open(var_metafields_frag_file) as f:
                var_metafields_fragment = f.read()
            mutation = mutation.replace("__VARIANT_METAFIELDS_PLACEHOLDER__", var_metafields_fragment)
        else:
            mutation = mutation.replace("__VARIANT_METAFIELDS_PLACEHOLDER__", "")

        if query_filter:
            mutation = mutation.replace("__QUERY_FILTER__", f'(query: "{query_filter}")')
        else:
            mutation = mutation.replace("__QUERY_FILTER__", "")

        result = self.execute_query(mutation)

        bulk_op = result.get("bulkOperationRunQuery", {}).get("bulkOperation", {})
        user_errors = result.get("bulkOperationRunQuery", {}).get("userErrors", [])

        if user_errors:
            raise UserException(f"Bulk operation failed: {user_errors}")

        operation_id = bulk_op.get("id")
        self.logger.info(f"Bulk operation started: {operation_id}")

        # Poll for completion
        status_file = self.query_loader.queries_dir / "BulkOperationStatus.graphql"
        with open(status_file) as f:
            status_query = f.read()

        poll_start = time.time()
        while True:
            elapsed = time.time() - poll_start
            sleep_interval = 5 if elapsed < 60 else 15
            time.sleep(sleep_interval)

            status_result = self.execute_query(status_query)
            current_op = status_result.get("currentBulkOperation", {})

            status = current_op.get("status")
            self.logger.debug(f"Bulk operation status: {status}")

            if status == "COMPLETED":
                url = current_op.get("url")
                object_count = current_op.get("objectCount", 0)
                api_wait_time = time.time() - api_wait_start

                if not url:
                    self.logger.info("Bulk operation completed with no results (empty dataset)")
                    with open(temp_file_path, "w") as f:
                        pass
                    return BulkOperationResult(
                        file_path=temp_file_path,
                        item_count=0,
                        api_wait_time=api_wait_time,
                        download_time=0.0,
                    )

                self.logger.info(f"Downloading results from: {url}")
                return self._download_bulk_results(url, int(object_count), "products", temp_file_path, api_wait_time)

            elif status in ["FAILED", "CANCELED"]:
                error = current_op.get("errorCode", "Unknown error")
                raise UserException(f"Bulk operation {status.lower()}: {error}")

    @log_bulk_performance("orders")
    def get_orders_bulk(
        self,
        temp_file_path: str,
        include_transactions: bool = False,
        date_since: str | None = None,
        date_to: str | None = None,
        fetch_parameter: str = "updated_at",
    ) -> BulkOperationResult:
        """
        Get all orders using Shopify's bulk operations

        Args:
            temp_file_path: Path where JSONL results will be saved
            include_transactions: Whether to include order transactions in the response
            date_since: Start date for filtering (YYYY-MM-DD format)
            date_to: End date for filtering (YYYY-MM-DD format)
            fetch_parameter: Field to filter by ('updated_at' or 'created_at')

        Returns:
            BulkOperationResult with file path and timing info (item_count will be 0 if no results)
        """
        api_wait_start = time.time()

        filters = []
        if date_since:
            filters.append(f"{fetch_parameter}:>='{date_since}'")
        if date_to:
            filters.append(f"{fetch_parameter}:<'{date_to}'")

        query_filter = " AND ".join(filters) if filters else ""
        log_transactions = " (including transactions)" if include_transactions else ""

        log_dates = ""
        if date_since and date_to:
            log_dates = f" from {date_since} to {date_to}"
        elif date_since:
            log_dates = f" from {date_since}"
        elif date_to:
            log_dates = f" until {date_to}"

        self.logger.info(f"Starting bulk operation for orders{log_transactions}{log_dates}")

        mutation_file = self.query_loader.queries_dir / "BulkOrders.graphql"
        with open(mutation_file) as f:
            mutation = f.read()

        if include_transactions:
            transactions_fragment_file = self.query_loader.queries_dir / "fragments" / "OrderTransactions.graphql"
            with open(transactions_fragment_file) as f:
                transactions_fragment = f.read()
            mutation = mutation.replace("__TRANSACTIONS_PLACEHOLDER__", transactions_fragment)
        else:
            mutation = mutation.replace("__TRANSACTIONS_PLACEHOLDER__", "")

        if query_filter:
            mutation = mutation.replace("__QUERY_FILTER__", f'(query: "{query_filter}")')
        else:
            mutation = mutation.replace("__QUERY_FILTER__", "")

        result = self.execute_query(mutation)

        bulk_op = result.get("bulkOperationRunQuery", {}).get("bulkOperation", {})
        user_errors = result.get("bulkOperationRunQuery", {}).get("userErrors", [])

        if user_errors:
            raise UserException(f"Bulk operation failed: {user_errors}")

        operation_id = bulk_op.get("id")
        self.logger.info(f"Bulk operation started: {operation_id}")

        # Poll for completion
        status_file = self.query_loader.queries_dir / "BulkOperationStatus.graphql"
        with open(status_file) as f:
            status_query = f.read()

        poll_start = time.time()
        while True:
            elapsed = time.time() - poll_start
            sleep_interval = 5 if elapsed < 60 else 15
            time.sleep(sleep_interval)

            status_result = self.execute_query(status_query)
            current_op = status_result.get("currentBulkOperation", {})

            status = current_op.get("status")
            self.logger.debug(f"Bulk operation status: {status}")

            if status == "COMPLETED":
                url = current_op.get("url")
                object_count = current_op.get("objectCount", 0)
                api_wait_time = time.time() - api_wait_start

                if not url:
                    self.logger.info("Bulk operation completed with no results (empty dataset)")
                    with open(temp_file_path, "w") as f:
                        pass
                    return BulkOperationResult(
                        file_path=temp_file_path,
                        item_count=0,
                        api_wait_time=api_wait_time,
                        download_time=0.0,
                    )

                self.logger.info(f"Downloading results from: {url}")
                return self._download_bulk_results(url, int(object_count), "orders", temp_file_path, api_wait_time)

            elif status in ["FAILED", "CANCELED"]:
                error = current_op.get("errorCode", "Unknown error")
                raise UserException(f"Bulk operation {status.lower()}: {error}")

    @log_bulk_performance("customers")
    def get_customers_bulk(
        self,
        temp_file_path: str,
        date_since: str | None = None,
        date_to: str | None = None,
        fetch_parameter: str = "updated_at",
    ) -> BulkOperationResult:
        """
        Get all customers using Shopify's bulk operations

        Args:
            temp_file_path: Path where JSONL results will be saved
            date_since: Start date for filtering (YYYY-MM-DD format)
            date_to: End date for filtering (YYYY-MM-DD format)
            fetch_parameter: Field to filter by ('updated_at' or 'created_at')

        Returns:
            BulkOperationResult with file path and timing info (item_count will be 0 if no results)
        """
        api_wait_start = time.time()

        filters = []
        if date_since:
            filters.append(f"{fetch_parameter}:>='{date_since}'")
        if date_to:
            filters.append(f"{fetch_parameter}:<'{date_to}'")

        query_filter = " AND ".join(filters) if filters else ""

        log_dates = ""
        if date_since and date_to:
            log_dates = f" from {date_since} to {date_to}"
        elif date_since:
            log_dates = f" from {date_since}"
        elif date_to:
            log_dates = f" until {date_to}"

        self.logger.info(f"Starting bulk operation for customers{log_dates}")

        mutation_file = self.query_loader.queries_dir / "BulkCustomers.graphql"
        with open(mutation_file) as f:
            mutation = f.read()

        if query_filter:
            mutation = mutation.replace("__QUERY_FILTER__", f'(query: "{query_filter}")')
        else:
            mutation = mutation.replace("__QUERY_FILTER__", "")

        result = self.execute_query(mutation)

        bulk_op = result.get("bulkOperationRunQuery", {}).get("bulkOperation", {})
        user_errors = result.get("bulkOperationRunQuery", {}).get("userErrors", [])

        if user_errors:
            raise UserException(f"Bulk operation failed: {user_errors}")

        operation_id = bulk_op.get("id")
        self.logger.info(f"Bulk operation started: {operation_id}")

        # Poll for completion
        status_file = self.query_loader.queries_dir / "BulkOperationStatus.graphql"
        with open(status_file) as f:
            status_query = f.read()

        poll_start = time.time()
        while True:
            elapsed = time.time() - poll_start
            sleep_interval = 5 if elapsed < 60 else 15
            time.sleep(sleep_interval)

            status_result = self.execute_query(status_query)
            current_op = status_result.get("currentBulkOperation", {})

            status = current_op.get("status")
            self.logger.debug(f"Bulk operation status: {status}")

            if status == "COMPLETED":
                url = current_op.get("url")
                object_count = current_op.get("objectCount", 0)
                api_wait_time = time.time() - api_wait_start

                if not url:
                    self.logger.info("Bulk operation completed with no results (empty dataset)")
                    with open(temp_file_path, "w") as f:
                        pass
                    return BulkOperationResult(
                        file_path=temp_file_path,
                        item_count=0,
                        api_wait_time=api_wait_time,
                        download_time=0.0,
                    )

                self.logger.info(f"Downloading results from: {url}")
                return self._download_bulk_results(url, int(object_count), "customers", temp_file_path, api_wait_time)

            elif status in ["FAILED", "CANCELED"]:
                error = current_op.get("errorCode", "Unknown error")
                raise UserException(f"Bulk operation {status.lower()}: {error}")

    @log_bulk_performance("inventory")
    def get_inventory_bulk(
        self,
        temp_file_path: str,
        date_since: str | None = None,
        date_to: str | None = None,
        fetch_parameter: str = "updated_at",
    ) -> BulkOperationResult:
        """
        Get all inventory items and levels using Shopify's bulk operations

        Args:
            temp_file_path: Path where JSONL results will be saved
            date_since: Start date for filtering (YYYY-MM-DD format)
            date_to: End date for filtering (YYYY-MM-DD format)
            fetch_parameter: Field to filter by ('updated_at' or 'created_at')

        Returns:
            BulkOperationResult with file path and timing info
        """
        api_wait_start = time.time()

        filters = []
        if date_since:
            filters.append(f"{fetch_parameter}:>='{date_since}'")
        if date_to:
            filters.append(f"{fetch_parameter}:<'{date_to}'")

        query_filter = " AND ".join(filters) if filters else ""

        log_dates = ""
        if date_since and date_to:
            log_dates = f" from {date_since} to {date_to}"
        elif date_since:
            log_dates = f" from {date_since}"
        elif date_to:
            log_dates = f" until {date_to}"

        self.logger.info(f"Starting bulk operation for inventory{log_dates}")

        mutation_file = self.query_loader.queries_dir / "BulkInventory.graphql"
        with open(mutation_file) as f:
            mutation = f.read()

        if query_filter:
            mutation = mutation.replace("__QUERY_FILTER__", f'(query: "{query_filter}")')
        else:
            mutation = mutation.replace("__QUERY_FILTER__", "")

        result = self.execute_query(mutation)

        bulk_op = result.get("bulkOperationRunQuery", {}).get("bulkOperation", {})
        user_errors = result.get("bulkOperationRunQuery", {}).get("userErrors", [])

        if user_errors:
            raise UserException(f"Bulk operation failed: {user_errors}")

        operation_id = bulk_op.get("id")
        self.logger.info(f"Bulk operation started: {operation_id}")

        status_file = self.query_loader.queries_dir / "BulkOperationStatus.graphql"
        with open(status_file) as f:
            status_query = f.read()

        poll_start = time.time()
        while True:
            elapsed = time.time() - poll_start
            sleep_interval = 5 if elapsed < 60 else 15
            time.sleep(sleep_interval)

            status_result = self.execute_query(status_query)
            current_op = status_result.get("currentBulkOperation", {})

            status = current_op.get("status")
            self.logger.debug(f"Bulk operation status: {status}")

            if status == "COMPLETED":
                url = current_op.get("url")
                object_count = current_op.get("objectCount", 0)
                api_wait_time = time.time() - api_wait_start

                if not url:
                    self.logger.info("Bulk operation completed with no results (empty dataset)")
                    with open(temp_file_path, "w") as f:
                        pass
                    return BulkOperationResult(
                        file_path=temp_file_path,
                        item_count=0,
                        api_wait_time=api_wait_time,
                        download_time=0.0,
                    )

                self.logger.info(f"Downloading results from: {url}")
                return self._download_bulk_results(url, int(object_count), "inventory", temp_file_path, api_wait_time)

            elif status in ["FAILED", "CANCELED"]:
                error = current_op.get("errorCode", "Unknown error")
                raise UserException(f"Bulk operation {status.lower()}: {error}")

    @log_bulk_performance("events")
    def get_events_bulk(
        self,
        temp_file_path: str,
        date_since: str | None = None,
        date_to: str | None = None,
    ) -> BulkOperationResult:
        """
        Get all events using Shopify's bulk operations

        Args:
            temp_file_path: Path where JSONL results will be saved
            date_since: Start date for filtering (YYYY-MM-DD format)
            date_to: End date for filtering (YYYY-MM-DD format)

        Returns:
            BulkOperationResult with file path and timing info
        """
        api_wait_start = time.time()

        filters = []
        if date_since:
            filters.append(f"created_at:>='{date_since}'")
        if date_to:
            filters.append(f"created_at:<'{date_to}'")

        query_filter = " AND ".join(filters) if filters else ""

        log_dates = ""
        if date_since and date_to:
            log_dates = f" from {date_since} to {date_to}"
        elif date_since:
            log_dates = f" from {date_since}"
        elif date_to:
            log_dates = f" until {date_to}"

        self.logger.info(f"Starting bulk operation for events{log_dates}")

        mutation_file = self.query_loader.queries_dir / "BulkEvents.graphql"
        with open(mutation_file) as f:
            mutation = f.read()

        if query_filter:
            mutation = mutation.replace("__QUERY_FILTER__", f'(query: "{query_filter}")')
        else:
            mutation = mutation.replace("__QUERY_FILTER__", "")

        result = self.execute_query(mutation)

        bulk_op = result.get("bulkOperationRunQuery", {}).get("bulkOperation", {})
        user_errors = result.get("bulkOperationRunQuery", {}).get("userErrors", [])

        if user_errors:
            raise UserException(f"Bulk operation failed: {user_errors}")

        operation_id = bulk_op.get("id")
        self.logger.info(f"Bulk operation started: {operation_id}")

        status_file = self.query_loader.queries_dir / "BulkOperationStatus.graphql"
        with open(status_file) as f:
            status_query = f.read()

        poll_start = time.time()
        while True:
            elapsed = time.time() - poll_start
            sleep_interval = 5 if elapsed < 60 else 15
            time.sleep(sleep_interval)

            status_result = self.execute_query(status_query)
            current_op = status_result.get("currentBulkOperation", {})

            status = current_op.get("status")
            self.logger.debug(f"Bulk operation status: {status}")

            if status == "COMPLETED":
                url = current_op.get("url")
                object_count = current_op.get("objectCount", 0)
                api_wait_time = time.time() - api_wait_start

                if not url:
                    self.logger.info("Bulk operation completed with no results (empty dataset)")
                    with open(temp_file_path, "w") as f:
                        pass
                    return BulkOperationResult(
                        file_path=temp_file_path,
                        item_count=0,
                        api_wait_time=api_wait_time,
                        download_time=0.0,
                    )

                self.logger.info(f"Downloading results from: {url}")
                return self._download_bulk_results(url, int(object_count), "events", temp_file_path, api_wait_time)

            elif status in ["FAILED", "CANCELED"]:
                error = current_op.get("errorCode", "Unknown error")
                raise UserException(f"Bulk operation {status.lower()}: {error}")

    @log_bulk_performance("locations")
    def get_locations_bulk(self, temp_file_path: str) -> BulkOperationResult:
        """
        Get all locations using Shopify's bulk operations

        Args:
            temp_file_path: Path where JSONL results will be saved

        Returns:
            BulkOperationResult with file path and timing info
        """
        api_wait_start = time.time()
        self.logger.info("Starting bulk operation for locations")

        mutation_file = self.query_loader.queries_dir / "BulkLocations.graphql"
        with open(mutation_file) as f:
            mutation = f.read()

        result = self.execute_query(mutation)

        bulk_op = result.get("bulkOperationRunQuery", {}).get("bulkOperation", {})
        user_errors = result.get("bulkOperationRunQuery", {}).get("userErrors", [])

        if user_errors:
            raise UserException(f"Bulk operation failed: {user_errors}")

        operation_id = bulk_op.get("id")
        self.logger.info(f"Bulk operation started: {operation_id}")

        status_file = self.query_loader.queries_dir / "BulkOperationStatus.graphql"
        with open(status_file) as f:
            status_query = f.read()

        poll_start = time.time()
        while True:
            elapsed = time.time() - poll_start
            sleep_interval = 5 if elapsed < 60 else 15
            time.sleep(sleep_interval)

            status_result = self.execute_query(status_query)
            current_op = status_result.get("currentBulkOperation", {})

            status = current_op.get("status")
            self.logger.debug(f"Bulk operation status: {status}")

            if status == "COMPLETED":
                url = current_op.get("url")
                object_count = current_op.get("objectCount", 0)
                api_wait_time = time.time() - api_wait_start

                if not url:
                    self.logger.info("Bulk operation completed with no results (empty dataset)")
                    with open(temp_file_path, "w") as f:
                        pass
                    return BulkOperationResult(
                        file_path=temp_file_path,
                        item_count=0,
                        api_wait_time=api_wait_time,
                        download_time=0.0,
                    )

                self.logger.info(f"Downloading results from: {url}")
                return self._download_bulk_results(url, int(object_count), "locations", temp_file_path, api_wait_time)

            elif status in ["FAILED", "CANCELED"]:
                error = current_op.get("errorCode", "Unknown error")
                raise UserException(f"Bulk operation {status.lower()}: {error}")

    def _download_bulk_results(
        self, url: str, item_count: int, entity_type: str, temp_file_path: str, api_wait_time: float
    ) -> BulkOperationResult:
        """
        Download JSONL results from bulk operation and save to file

        Args:
            url: URL to download JSONL from
            item_count: The number of items (for logging/debug)
            entity_type: Type of entity (for logging/debug)
            temp_file_path: Path where JSONL will be saved
            api_wait_time: Time spent waiting for bulk operation to complete

        Returns:
            BulkOperationResult with file path, item count, and timing info
        """
        download_start = time.time()

        with urlopen(url) as response:
            with open(temp_file_path, "wb") as f:
                chunk_size = 8192
                while True:
                    chunk = response.read(chunk_size)
                    if not chunk:
                        break
                    f.write(chunk)

        download_time = time.time() - download_start

        self.logger.info(f"Downloaded {item_count} items from bulk operation, saved to {temp_file_path}")

        return BulkOperationResult(
            file_path=temp_file_path,
            item_count=item_count,
            api_wait_time=api_wait_time,
            download_time=download_time,
        )

    @log_bulk_performance("custom")
    def execute_custom_bulk_query(self, query: str, temp_file_path: str) -> BulkOperationResult:
        """
        Execute a custom GraphQL query using bulk operations

        Args:
            query: GraphQL bulk operation mutation string
            temp_file_path: Path where JSONL results will be saved

        Returns:
            BulkOperationResult with file path and timing info
        """
        api_wait_start = time.time()
        self.logger.info("Starting custom bulk operation")

        result = self.execute_query(query)

        bulk_op = result.get("bulkOperationRunQuery", {}).get("bulkOperation", {})
        user_errors = result.get("bulkOperationRunQuery", {}).get("userErrors", [])

        if user_errors:
            raise UserException(f"Bulk operation failed: {user_errors}")

        operation_id = bulk_op.get("id")
        self.logger.info(f"Bulk operation started: {operation_id}")

        # Poll for completion
        status_file = self.query_loader.queries_dir / "BulkOperationStatus.graphql"
        with open(status_file) as f:
            status_query = f.read()

        poll_start = time.time()
        while True:
            elapsed = time.time() - poll_start
            sleep_interval = 5 if elapsed < 60 else 15
            time.sleep(sleep_interval)

            status_result = self.execute_query(status_query)
            current_op = status_result.get("currentBulkOperation", {})

            status = current_op.get("status")
            self.logger.debug(f"Bulk operation status: {status}")

            if status == "COMPLETED":
                url = current_op.get("url")
                object_count = current_op.get("objectCount", 0)
                api_wait_time = time.time() - api_wait_start

                if not url:
                    self.logger.info("Bulk operation completed with no results (empty dataset)")
                    with open(temp_file_path, "w") as f:
                        pass
                    return BulkOperationResult(
                        file_path=temp_file_path,
                        item_count=0,
                        api_wait_time=api_wait_time,
                        download_time=0.0,
                    )

                self.logger.info(f"Downloading results from: {url}")
                return self._download_bulk_results(url, int(object_count), "custom", temp_file_path, api_wait_time)

            elif status in ["FAILED", "CANCELED"]:
                error = current_op.get("errorCode", "Unknown error")
                raise UserException(f"Bulk operation {status.lower()}: {error}")



================================================
FILE: src/shopify_cli/query_loader.py
================================================
from pathlib import Path

MAP_QUERY_NAME_TO_FILE = {
    "orders": ("orders/orders.graphql", "GetOrders"),
    "customers": ("customers/customers.graphql", "GetCustomers"),
    "products": ("products/products.graphql", "GetProducts"),
    "inventory_items": ("inventory/inventory_items.graphql", "GetInventoryItems"),
    "locations": ("locations/locations.graphql", "GetLocations"),
    # .....
}


class QueryLoader:
    """
    Loads GraphQL queries from external .graphql files
    """

    def __init__(self, queries_dir: str | None = None):
        """
        Initialize query loader

        Args:
            queries_dir: Path to queries directory. If None, uses default location.
        """
        if queries_dir is None:
            current_dir = Path(__file__).parent
            self.queries_dir = current_dir / "queries"
        else:
            self.queries_dir = Path(queries_dir)
        self._queries_cache: dict[str, str] = {}

    def load_query(self, query_name: str) -> str:
        """
        Load a GraphQL query from file

        Args:
            query_name: Name of the query (without .graphql extension)

        Returns:
            GraphQL query string

        Raises:
            FileNotFoundError: If query file doesn't exist
            ValueError: If query not found in file
        """
        # Check cache first
        if query_name in self._queries_cache:
            return self._queries_cache[query_name]

        # Load from file
        query_file = self.queries_dir / f"{query_name}.graphql"

        if not query_file.exists():
            raise FileNotFoundError(f"Query file not found: {query_file}")

        with open(query_file) as f:
            content = f.read()

        # Extract the specific query from the file
        query = self._extract_query(content, query_name)

        # Cache the result
        self._queries_cache[query_name] = query

        return query

    def _extract_query(self, content: str, query_name: str) -> str:
        """
        Extract a specific query from GraphQL file content

        Args:
            content: Full file content
            query_name: Name of the query to extract

        Returns:
            Extracted query string

        Raises:
            ValueError: If query not found in content
        """
        lines = content.split("\n")
        query_lines = []
        in_query = False
        brace_count = 0

        for line in lines:
            if line.strip().startswith("#"):
                continue

            # Check if this line starts the query we want
            if f"query {query_name}" in line:
                in_query = True
                query_lines.append(line)
                # Count opening braces in this line
                brace_count += line.count("{")
                continue

            if in_query:
                query_lines.append(line)
                brace_count += line.count("{")
                brace_count -= line.count("}")

                # If we've closed all braces, we're done
                if brace_count == 0:
                    break

        if not query_lines:
            raise ValueError(f"Query '{query_name}' not found in file")

        return "\n".join(query_lines).strip()

    def get_available_queries(self) -> list:
        """
        Get list of available query files

        Returns:
            List of query names (without .graphql extension)
        """
        if not self.queries_dir.exists():
            return []

        query_files = list(self.queries_dir.glob("*.graphql"))
        return [f.stem for f in query_files]



================================================
FILE: src/shopify_cli/queries/BulkCustomers.graphql
================================================
mutation {
  bulkOperationRunQuery(
    query: """
    {
      customers__QUERY_FILTER__ {
        edges {
          node {
            id
            firstName
            lastName
            email
            phone
            createdAt
            updatedAt
            emailMarketingConsent {
              marketingState
              marketingOptInLevel
            }
            defaultAddress {
              id
              firstName
              lastName
              address1
              address2
              city
              province
              country
              zip
              phone
              name
              provinceCode
              countryCodeV2
            }
            addresses {
              id
              firstName
              lastName
              address1
              address2
              city
              province
              country
              zip
              phone
              name
              provinceCode
              countryCodeV2
            }
          }
        }
      }
    }
    """
  ) {
    bulkOperation {
      id
      status
    }
    userErrors {
      field
      message
    }
  }
}



================================================
FILE: src/shopify_cli/queries/BulkEvents.graphql
================================================
mutation {
  bulkOperationRunQuery(
    query: """
    {
      events__QUERY_FILTER__ {
        edges {
          node {
            id
            createdAt
            message
            action
            appTitle
            attributeToApp
            attributeToUser
            criticalAlert
          }
        }
      }
    }
    """
  ) {
    bulkOperation {
      id
      status
    }
    userErrors {
      field
      message
    }
  }
}



================================================
FILE: src/shopify_cli/queries/BulkInventory.graphql
================================================
mutation {
  bulkOperationRunQuery(
    query: """
    {
      inventoryItems__QUERY_FILTER__ {
        edges {
          node {
            id
            sku
            tracked
            createdAt
            updatedAt
            variant {
              id
              title
              sku
              product {
                id
                title
                handle
              }
            }
            inventoryLevels {
              edges {
                node {
                  id
                  quantities(names: ["available"]) {
                    name
                    quantity
                  }
                  location {
                    id
                    name
                    isActive
                  }
                  updatedAt
                }
              }
            }
          }
        }
      }
    }
    """
  ) {
    bulkOperation {
      id
      status
    }
    userErrors {
      field
      message
    }
  }
}



================================================
FILE: src/shopify_cli/queries/BulkLocations.graphql
================================================
mutation {
  bulkOperationRunQuery(
    query: """
    {
      locations {
        edges {
          node {
            id
            name
            address {
              address1
              address2
              city
              province
              country
              zip
              phone
            }
            isActive
            isPrimary
            isFulfillmentService
            fulfillmentService {
              id
              serviceName
            }
          }
        }
      }
    }
    """
  ) {
    bulkOperation {
      id
      status
    }
    userErrors {
      field
      message
    }
  }
}



================================================
FILE: src/shopify_cli/queries/BulkOperationStatus.graphql
================================================
query {
  currentBulkOperation {
    id
    status
    errorCode
    createdAt
    completedAt
    objectCount
    fileSize
    url
    partialDataUrl
  }
}



================================================
FILE: src/shopify_cli/queries/BulkOrders.graphql
================================================
mutation {
  bulkOperationRunQuery(
    query: """
    {
      orders__QUERY_FILTER__ {
        edges {
          node {
            id
            name
            email
            phone
            createdAt
            updatedAt
            processedAt
            cancelledAt
            cancelReason
            totalPriceSet {
              shopMoney {
                amount
                currencyCode
              }
            }
            subtotalPriceSet {
              shopMoney {
                amount
                currencyCode
              }
            }
            totalTaxSet {
              shopMoney {
                amount
                currencyCode
              }
            }
            totalShippingPriceSet {
              shopMoney {
                amount
                currencyCode
              }
            }
            customer {
              id
              firstName
              lastName
              email
              phone
            }
            lineItems {
              edges {
                node {
                  id
                  title
                  quantity
                  sku
                  variant {
                    id
                    title
                    sku
                    price
                  }
                }
              }
            }
            shippingAddress {
              firstName
              lastName
              address1
              address2
              city
              province
              country
              zip
              phone
            }
            billingAddress {
              firstName
              lastName
              address1
              address2
              city
              province
              country
              zip
              phone
            }
            __TRANSACTIONS_PLACEHOLDER__
          }
        }
      }
    }
    """
  ) {
    bulkOperation {
      id
      status
    }
    userErrors {
      field
      message
    }
  }
}



================================================
FILE: src/shopify_cli/queries/BulkProducts.graphql
================================================
mutation {
  bulkOperationRunQuery(
    query: """
    {
      products__QUERY_FILTER__ {
        edges {
          node {
            id
            title
            handle
            description
            vendor
            productType
            tags
            status
            createdAt
            updatedAt
            publishedAt
            totalInventory
            variants {
              edges {
                node {
                  id
                  title
                  sku
                  barcode
                  price
                  compareAtPrice
                  inventoryQuantity
                  __VARIANT_METAFIELDS_PLACEHOLDER__
                }
              }
            }
            images {
              edges {
                node {
                  id
                  url
                  altText
                  width
                  height
                }
              }
            }
            __METAFIELDS_PLACEHOLDER__
          }
        }
      }
    }
    """
  ) {
    bulkOperation {
      id
      status
    }
    userErrors {
      field
      message
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetCustomers.graphql
================================================
# Get all customers with pagination
query GetCustomers($first: Int!, $after: String) {
  customers(first: $first, after: $after) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        firstName
        lastName
        email
        phone
        createdAt
        updatedAt
        emailMarketingConsent {
          marketingState
          marketingOptInLevel
        }
        defaultAddress {
          id
          firstName
          lastName
          address1
          address2
          city
          province
          country
          zip
          phone
          name
          provinceCode
          countryCodeV2
        }
        addresses {
          id
          firstName
          lastName
          address1
          address2
          city
          province
          country
          zip
          phone
          name
          provinceCode
          countryCodeV2
        }
      }
    }
  }
}

# Get a specific customer by ID
query GetCustomer($id: ID!) {
  customer(id: $id) {
    id
    firstName
    lastName
    email
    phone
    createdAt
    updatedAt
    acceptsMarketing
    emailMarketingConsent {
      marketingState
      marketingOptInLevel
    }
    defaultAddress {
      id
      firstName
      lastName
      address1
      address2
      city
      province
      country
      zip
      phone
    }
    addresses(first: 10) {
      edges {
        node {
          id
          firstName
          lastName
          address1
          address2
          city
          province
          country
          zip
          phone
        }
      }
    }
    ordersCount
    totalSpent
  }
}

# Search customers
query SearchCustomers($query: String!, $first: Int!, $after: String) {
  customers(query: $query, first: $first, after: $after) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        firstName
        lastName
        email
        phone
        createdAt
        updatedAt
        acceptsMarketing
        ordersCount
        totalSpent
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetInventoryItems.graphql
================================================
# Get all inventory items with pagination
query GetInventoryItems($first: Int!, $after: String) {
  inventoryItems(first: $first, after: $after) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        sku
        tracked
        createdAt
        updatedAt
        variant {
          id
          title
          sku
          price
          product {
            id
            title
            handle
          }
        }
        inventoryLevels(first: 10) {
          edges {
            node {
              id
              location {
                id
                name
              }
            }
          }
        }
      }
    }
  }
}

# Get a specific inventory item by ID
query GetInventoryItem($id: ID!) {
  inventoryItem(id: $id) {
    id
    sku
    tracked
    createdAt
    updatedAt
    variant {
      id
      title
      sku
      price
      product {
        id
        title
        handle
      }
    }
    inventoryLevels(first: 10) {
      edges {
        node {
          id
          available
          location {
            id
            name
          }
        }
      }
    }
  }
}

# Get inventory levels for a specific location
query GetInventoryLevelsByLocation($locationId: ID!, $first: Int!, $after: String) {
  location(id: $locationId) {
    id
    name
    inventoryLevels(first: $first, after: $after) {
      pageInfo {
        hasNextPage
        hasPreviousPage
        startCursor
        endCursor
      }
      edges {
        node {
          id
          available
          item {
            id
            sku
            variant {
              id
              title
              sku
              product {
                id
                title
                handle
              }
            }
          }
        }
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetInventoryLevels.graphql
================================================
# Get inventory levels with pagination
query GetInventoryLevels($first: Int!, $after: String) {
  inventoryItems(first: $first, after: $after) {
    pageInfo {
      hasNextPage
      endCursor
    }
    edges {
      node {
        id
        sku
        tracked
        createdAt
        updatedAt
        variant {
          id
          title
          sku
          product {
            id
            title
            handle
          }
        }
        inventoryLevels(first: 250) {
          edges {
            node {
              id
              quantities(names: ["available"]) {
                name
                quantity
              }
              location {
                id
                name
                isActive
              }
              updatedAt
            }
          }
        }
      }
    }
  }
}

# Get inventory levels for a specific location
query GetInventoryLevelsByLocation($locationId: ID!, $first: Int!, $after: String) {
  location(id: $locationId) {
    id
    name
    inventoryLevels(first: $first, after: $after) {
      pageInfo {
        hasNextPage
        hasPreviousPage
        startCursor
        endCursor
      }
      edges {
        node {
          id
          available
          item {
            id
            sku
            tracked
            variant {
              id
              title
              sku
              product {
                id
                title
                handle
              }
            }
          }
          updatedAt
        }
      }
    }
  }
}

# Get inventory levels for a specific inventory item
query GetInventoryLevelsByItem($itemId: ID!, $first: Int!, $after: String) {
  inventoryItem(id: $itemId) {
    id
    sku
    tracked
    variant {
      id
      title
      sku
      product {
        id
        title
        handle
      }
    }
    inventoryLevels(first: $first, after: $after) {
      pageInfo {
        hasNextPage
        hasPreviousPage
        startCursor
        endCursor
      }
      edges {
        node {
          id
          available
          location {
            id
            name
            address {
              address1
              address2
              city
              province
              country
              zip
              phone
            }
            isActive
            isPrimary
          }
          updatedAt
        }
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetLocations.graphql
================================================
# Get all locations
query GetLocations {
  locations(first: 50) {
    edges {
      node {
        id
        name
        address {
          address1
          address2
          city
          province
          country
          zip
          phone
        }
        isActive
        isPrimary
        isFulfillmentService
        fulfillmentService {
          id
          serviceName
        }
      }
    }
  }
}

# Get a specific location by ID
query GetLocation($id: ID!) {
  location(id: $id) {
    id
    name
    address {
      address1
      address2
      city
      province
      country
      zip
      phone
    }
    isActive
    isPrimary
    isFulfillmentService
    fulfillmentService {
      id
      serviceName
    }
  }
}

# Get locations with inventory levels
query GetLocationsWithInventory($first: Int!) {
  locations(first: $first) {
    edges {
      node {
        id
        name
        address {
          address1
          address2
          city
          province
          country
          zip
          phone
        }
        isActive
        isPrimary
        isFulfillmentService
        fulfillmentService {
          id
          serviceName
        }
        inventoryLevels(first: 10) {
          edges {
            node {
              id
              available
              item {
                id
                sku
                variant {
                  id
                  title
                  sku
                }
              }
            }
          }
        }
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetOrders.graphql
================================================
# Get all orders with pagination
query GetOrders($first: Int!, $after: String, $query: String) {
  orders(first: $first, after: $after, query: $query) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        name
        email
        phone
        createdAt
        updatedAt
        processedAt
        cancelledAt
        cancelReason
        totalPriceSet {
          shopMoney {
            amount
            currencyCode
          }
        }
        subtotalPriceSet {
          shopMoney {
            amount
            currencyCode
          }
        }
        totalTaxSet {
          shopMoney {
            amount
            currencyCode
          }
        }
        totalShippingPriceSet {
          shopMoney {
            amount
            currencyCode
          }
        }
        customer {
          id
          firstName
          lastName
          email
          phone
        }
        lineItems(first: 250) {
          edges {
            node {
              id
              title
              quantity
              sku
              variant {
                id
                title
                sku
                price
              }
            }
          }
        }
        shippingAddress {
          firstName
          lastName
          address1
          address2
          city
          province
          country
          zip
          phone
        }
        billingAddress {
          firstName
          lastName
          address1
          address2
          city
          province
          country
          zip
          phone
        }
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetProductDrafts.graphql
================================================
# Get product drafts with pagination
query GetProductDrafts($first: Int!, $after: String) {
  productDrafts(first: $first, after: $after) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        title
        handle
        description
        vendor
        productType
        tags
        status
        createdAt
        updatedAt
        publishedAt
        totalInventory
        variants(first: 250) {
          edges {
            node {
              id
              title
              sku
              barcode
              price
              compareAtPrice
              weight
              weightUnit
              inventoryQuantity
            }
          }
        }
        images(first: 10) {
          edges {
            node {
              id
              url
              altText
              width
              height
            }
          }
        }
      }
    }
  }
}

# Get a specific product draft by ID
query GetProductDraft($id: ID!) {
  productDraft(id: $id) {
    id
    title
    handle
    description
    vendor
    productType
    tags
    status
    createdAt
    updatedAt
    publishedAt
    totalInventory
    variants(first: 250) {
      edges {
        node {
          id
          title
          sku
          barcode
          price
          compareAtPrice
          weight
          weightUnit
          inventoryQuantity
        }
      }
    }
    images(first: 10) {
      edges {
        node {
          id
          url
          altText
          width
          height
        }
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetProductMetafields.graphql
================================================
# Get product metafields with pagination
query GetProductMetafields($first: Int!, $after: String) {
  metafields(first: $first, after: $after, ownerType: PRODUCT) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        namespace
        key
        value
        type
        description
        owner {
          ... on Product {
            id
            title
            handle
          }
        }
        createdAt
        updatedAt
      }
    }
  }
}

# Get metafields for a specific product
query GetProductMetafieldsByProduct($productId: ID!, $first: Int!, $after: String) {
  product(id: $productId) {
    id
    title
    handle
    metafields(first: $first, after: $after) {
      pageInfo {
        hasNextPage
        hasPreviousPage
        startCursor
        endCursor
      }
      edges {
        node {
          id
          namespace
          key
          value
          type
          description
          createdAt
          updatedAt
        }
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetProducts.graphql
================================================
# Get all products with basic information
query GetProducts($first: Int!, $after: String, $query: String) {
  products(first: $first, after: $after, query: $query) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        title
        handle
        description
        vendor
        productType
        tags
        status
        createdAt
        updatedAt
        publishedAt
        totalInventory
        variants(first: 250) {
          edges {
            node {
              id
              title
              sku
              barcode
              price
              compareAtPrice
              inventoryQuantity
              inventoryItem {
                id
                inventoryLevels(first: 10) {
                  edges {
                    node {
                      location {
                        id
                        name
                      }
                    }
                  }
                }
              }
            }
          }
        }
        images(first: 10) {
          edges {
            node {
              id
              url
              altText
              width
              height
            }
          }
        }
      }
    }
  }
}

# Get a specific product by ID
query GetProduct($id: ID!) {
  product(id: $id) {
    id
    title
    handle
    description
    vendor
    productType
    tags
    status
    createdAt
    updatedAt
    publishedAt
    totalInventory
    variants(first: 250) {
      edges {
        node {
          id
          title
          sku
          barcode
          price
          compareAtPrice
          weight
          weightUnit
          inventoryQuantity
          inventoryItem {
            id
            inventoryLevels(first: 10) {
              edges {
                node {
                  available
                  location {
                    id
                    name
                  }
                }
              }
            }
          }
        }
      }
    }
    images(first: 10) {
      edges {
        node {
          id
          url
          altText
          width
          height
        }
      }
    }
  }
}

# Get products by handle
query GetProductByHandle($handle: String!) {
  productByHandle(handle: $handle) {
    id
    title
    handle
    description
    vendor
    productType
    tags
    status
    createdAt
    updatedAt
    publishedAt
    totalInventory
    variants(first: 250) {
      edges {
        node {
          id
          title
          sku
          barcode
          price
          compareAtPrice
          weight
          weightUnit
          inventoryQuantity
          inventoryItem {
            id
            inventoryLevels(first: 10) {
              edges {
                node {
                  available
                  location {
                    id
                    name
                  }
                }
              }
            }
          }
        }
      }
    }
    images(first: 10) {
      edges {
        node {
          id
          url
          altText
          width
          height
        }
      }
    }
  }
}

# Search products
query SearchProducts($query: String!, $first: Int!, $after: String) {
  products(query: $query, first: $first, after: $after) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        title
        handle
        description
        vendor
        productType
        tags
        status
        createdAt
        updatedAt
        publishedAt
        totalInventory
        variants(first: 10) {
          edges {
            node {
              id
              title
              sku
              price
              inventoryQuantity
            }
          }
        }
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/GetVariantMetafields.graphql
================================================
# Get variant metafields with pagination
query GetVariantMetafields($first: Int!, $after: String) {
  metafields(first: $first, after: $after, ownerType: PRODUCT_VARIANT) {
    pageInfo {
      hasNextPage
      hasPreviousPage
      startCursor
      endCursor
    }
    edges {
      node {
        id
        namespace
        key
        value
        type
        description
        owner {
          ... on ProductVariant {
            id
            title
            sku
            product {
              id
              title
              handle
            }
          }
        }
        createdAt
        updatedAt
      }
    }
  }
}

# Get metafields for a specific product variant
query GetVariantMetafieldsByVariant($variantId: ID!, $first: Int!, $after: String) {
  productVariant(id: $variantId) {
    id
    title
    sku
    product {
      id
      title
      handle
    }
    metafields(first: $first, after: $after) {
      pageInfo {
        hasNextPage
        hasPreviousPage
        startCursor
        endCursor
      }
      edges {
        node {
          id
          namespace
          key
          value
          type
          description
          createdAt
          updatedAt
        }
      }
    }
  }
}



================================================
FILE: src/shopify_cli/queries/fragments/OrderTransactions.graphql
================================================
transactions {
  id
  kind
  status
  test
  amountSet {
    shopMoney {
      amount
      currencyCode
    }
  }
  gateway
  createdAt
  processedAt
  errorCode
  authorizationCode
  authorizationExpiresAt
}



================================================
FILE: src/shopify_cli/queries/fragments/ProductMetafields.graphql
================================================
metafields {
  edges {
    node {
      id
      namespace
      key
      value
      type
      description
      createdAt
      updatedAt
    }
  }
}



================================================
FILE: src/shopify_cli/queries/fragments/VariantMetafields.graphql
================================================
metafields {
  edges {
    node {
      id
      namespace
      key
      value
      type
      description
      createdAt
      updatedAt
    }
  }
}



================================================
FILE: tests/__init__.py
================================================
import sys
from pathlib import Path

sys.path.append(str((Path(__file__).resolve().parent.parent / "src")))



================================================
FILE: tests/test_component.py
================================================
import os
import unittest

import mock
from freezegun import freeze_time

from component import Component
from configuration import Configuration


class TestComponent(unittest.TestCase):
    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {"KBC_DATADIR": "./non-existing-dir"})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()

    def test_configuration_validation(self):
        """Test configuration validation"""
        # Test valid configuration
        valid_config = {
            "store_name": "test-shop",
            "#api_token": "TEST_TOKEN",
            "endpoints": {"orders": True, "products": True},
            "batch_size": 50,
        }

        config = Configuration(**valid_config)
        self.assertEqual(config.store_name, "test-shop")
        self.assertEqual(config.api_token, "TEST_TOKEN")
        self.assertTrue(config.endpoints.orders)
        self.assertTrue(config.endpoints.products)
        self.assertEqual(config.batch_size, 50)

    def test_configuration_get_enabled_endpoints(self):
        """Test getting enabled endpoints"""
        config_data = {
            "store_name": "test-shop",
            "#api_token": "TEST_TOKEN",
            "endpoints": {"orders": True, "products": True, "customers": False},
        }

        config = Configuration(**config_data)
        enabled = config.enabled_endpoints
        self.assertIn("orders", enabled)
        self.assertIn("products", enabled)
        self.assertNotIn("customers", enabled)

    def test_configuration_store_name_cleanup(self):
        """Test store name cleanup removes .myshopify.com"""
        config_data = {"store_name": "test-shop.myshopify.com", "#api_token": "TEST_TOKEN"}

        config = Configuration(**config_data)
        self.assertEqual(config.store_name, "test-shop")


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()



================================================
FILE: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on:
  push:  # skip the workflow on the main branch without tags
    branches-ignore:
      - main
    tags:
      - "*"

concurrency: ci-${{ github.ref }}  # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: keboola.ex-shopify-v2
  KBC_DEVELOPERPORTAL_VENDOR: keboola
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: ${{ vars.KBC_DEVELOPERPORTAL_USERNAME }}

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}  # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (Optional) Test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: ""  # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }}  # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo $raw | sed "s/.*origin\///" | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 .
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh


