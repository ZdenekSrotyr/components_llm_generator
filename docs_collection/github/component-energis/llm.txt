Directory structure:
â””â”€â”€ keboola-component-energis/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ deploy.sh
    â”œâ”€â”€ docker-compose.yml
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ flake8.cfg
    â”œâ”€â”€ LICENSE.md
    â”œâ”€â”€ pytest.ini
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ component_config/
    â”‚   â”œâ”€â”€ component_long_description.md
    â”‚   â”œâ”€â”€ component_short_description.md
    â”‚   â”œâ”€â”€ configSchema.json
    â”‚   â”œâ”€â”€ configuration_description.md
    â”‚   â”œâ”€â”€ documentationUrl.md
    â”‚   â”œâ”€â”€ licenseUrl.md
    â”‚   â”œâ”€â”€ logger
    â”‚   â”œâ”€â”€ loggerConfiguration.json
    â”‚   â””â”€â”€ sourceCodeUrl.md
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ build_n_run.ps1
    â”‚   â”œâ”€â”€ build_n_test.sh
    â”‚   â”œâ”€â”€ run_kbc_tests.ps1
    â”‚   â””â”€â”€ developer_portal/
    â”‚       â”œâ”€â”€ fn_actions_md_update.sh
    â”‚       â””â”€â”€ update_properties.sh
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ api_client.py
    â”‚   â”œâ”€â”€ component.py
    â”‚   â”œâ”€â”€ configuration.py
    â”‚   â”œâ”€â”€ file_manager.py
    â”‚   â”œâ”€â”€ manifest_manager.py
    â”‚   â”œâ”€â”€ state_manager.py
    â”‚   â””â”€â”€ utils.py
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ test_api_client.py
    â”‚   â”œâ”€â”€ test_configuration.py
    â”‚   â”œâ”€â”€ test_file_manager.py
    â”‚   â”œâ”€â”€ test_manifest_manager.py
    â”‚   â”œâ”€â”€ test_state_manager.py
    â”‚   â””â”€â”€ test_utils.py
    â””â”€â”€ .github/
        â””â”€â”€ workflows/
            â””â”€â”€ push.yml

================================================
FILE: README.md
================================================
# **Energis Extractor**
=============

## **Description**

The **Energis Extractor** retrieves energy-related data from the **Energis API** and loads it into **Keboola Connection** for further analysis. It supports **incremental loading**, **various data granularities**, and **date range filtering** to allow precise data extraction.

---

## **Table of Contents**

- [Functionality Notes](#functionality-notes)
- [Prerequisites](#prerequisites)
- [Features](#features)
- [Supported Endpoints](#supported-endpoints)
- [Configuration](#configuration)
  - [Authentication Settings](#authentication-settings)
  - [Synchronization Options](#synchronization-options)
  - [Debug Mode](#debug-mode)
- [Output](#output)
- [Development](#development)
  - [Running Locally](#running-locally)
  - [Running Tests](#running-tests)
- [Integration](#integration)

---

## **Functionality Notes**

- Extracts **energy consumption and related metrics** from Energis.
- Supports **incremental data fetching** to avoid duplicate records.
- Allows **date-based filtering** to control extracted data ranges.
- Provides **structured output tables** ready for analysis.

---

## **Prerequisites**

Before using this component, ensure that:

1. You have **valid API credentials** for the Energis system.
2. Your user account has **appropriate permissions** to access required datasets.
3. You have registered the **Keboola Connection application** (if required).

---

## **Supported Endpoints**

This extractor currently supports the **`xexport` dataset**. If you require additional endpoints, submit your request at [ideas.keboola.com](https://ideas.keboola.com/).

---

## **Configuration**

For a full breakdown of configuration options, refer to the [Configuration Documentation](#).

### **Authentication Settings**

| **Property**        | **Required** | **Type**     | **Default** | **Description** |
|--------------------|------------|------------|------------|---------------|
| `authentication.username` | âœ… Yes | String | _(None)_ | Username for API authentication. |
| `authentication.#password` | âœ… Yes | String (password) | _(None)_ | Password for API authentication. |
| `authentication.environment` | âœ… Yes | Enum (`dev` / `prod`) | `prod` | Selects the API environment (development or production). |

> Note: Dev environment points to https://webenergis.eu/test/1.wsc/soap.r and prod one to: https://bilance.c-energy.cz/cgi-bin/1.wsc/soap.r

### **Synchronization Options**

| **Property**        | **Required** | **Type**     | **Default**  | **Description**                                                                               |
|--------------------|------------|------------|--------------|-----------------------------------------------------------------------------------------------|
| `sync_options.dataset` | âœ… Yes | Enum (`xexport`) | `xexport`    | Specifies the dataset for extraction.                                                         |
| `sync_options.nodes` | âœ… Yes | Array of Integers | `[]`         | List of node IDs for data retrieval.                                                          |
| `sync_options.date_from` | âœ… Yes | Date (`YYYY-MM-DD`) | `2024-12-01` | Start date for data extraction.                                                               |
| `sync_options.date_to` | âŒ No | Date (`YYYY-MM-DD`) | _(Today)_    | End date for data extraction. If not set, defaults to today.                                  |
| `sync_options.granularity` | âœ… Yes | Enum (`year`, `quarterYear`, `month`, `day`, `hour`, `quarterHour`, `minute`) | `day`        | Defines data granularity for extraction.                                                      |
| `sync_options.reload_full_data` | âŒ No | Boolean| _False_      | When enabled, retrieves the complete dataset from 'date_from', bypassing incremental loading. |

### **Debug Mode**
| **Property** | **Required** | **Type** | **Default** | **Description** |
|-------------|--------------|--------|------------|---------------|
| `debug` | âŒ No         | Boolean | `false` | Enables debug mode for additional logging. |

---

## **Output**

The extracted data is stored in **CSV tables** within **Keboola Storage**. Each dataset includes structured fields with **timestamps, node identifiers, and recorded values**.

For the exact output schema, refer to the [Output Schema Documentation](#).

---

## **Development**

### **Running Locally**
To customize the local data folder path, modify the `docker-compose.yml` file:

```yaml
volumes:
  - ./:/code
  - ./CUSTOM_FOLDER:/data



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.11-slim
ENV PYTHONIOENCODING utf-8

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential
RUN pip install flake8

COPY requirements.txt /code/requirements.txt
RUN pip install -r /code/requirements.txt

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

WORKDIR /code/

CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example
    venv
    .venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: pytest.ini
================================================
[pytest]
pythonpath = src


================================================
FILE: requirements.txt
================================================
annotated-types==0.7.0
attrs==25.1.0
certifi==2025.1.31
charset-normalizer==3.4.1
dateparser==1.2.1
Deprecated==1.2.18
freezegun==1.5.1
idna==3.10
isodate==0.7.2
keboola.component==1.6.10
keboola.http-client==1.0.1
keboola.utils==1.1.0
lxml==5.3.1
mock==5.1.0
platformdirs==4.3.6
pydantic==2.10.6
pydantic_core==2.27.2
pygelf==0.4.2
python-dateutil==2.9.0.post0
pytz==2025.1
regex==2024.11.6
requests==2.32.3
requests-file==2.1.0
requests-toolbelt==1.0.0
six==1.17.0
typing_extensions==4.12.2
tzlocal==5.3
urllib3==2.3.0
wrapt==1.17.2
zeep==4.3.1
httpx==0.28.1
pytest==8.3.5


================================================
FILE: component_config/component_long_description.md
================================================
The Energis API Extractor is a Keboola component that enables users to seamlessly retrieve and process energy consumption 
data from the Energis platform. The component is designed for efficient data extraction, supporting multiple 
levels of granularity, including:

- Yearly (`year`)
- Quarterly (`quarterYear`)
- Monthly (`month`)
- Daily (`day`)
- Hourly (`hour`)
- Quarter-hour (`quarterHour`)
- Minute (`minute`)

## ðŸ”§ Features & Functionality

- **Secure Authentication**: Uses username/password authentication for API access.
- **Flexible Data Selection**: Users can specify nodes, time range, and granularity for precise data extraction.
- **Incremental Fetching**: The component remembers the last processed date and fetches only new data.
- **Automatic Data Parsing**: Converts API responses into a structured format, handling various timestamp formats.
- **CSV Output & Manifest Generation**: Saves data as CSV and generates metadata for seamless integration into Keboola Storage.

## ðŸ“‚ Output Format

The extracted data is stored in Keboola Storage, with each dataset containing:

- **Node ID (uzel)** â€“ Unique identifier of the energy node.
- **Value (hodnota)** â€“ The recorded energy consumption or measurement.
- **Timestamp (cas)** â€“ The formatted date/time of the measurement.

## ðŸš€ Use Cases

- **Energy Monitoring & Reporting** â€“ Retrieve historical and real-time energy consumption data.
- **Data Integration for Analysis** â€“ Combine Energis data with other sources in Keboola for advanced analytics.
- **Automated ETL Pipelines** â€“ Automate data ingestion into data warehouses and BI tools.

## ðŸ”’ Security & Logging

Sensitive credentials are **masked in logs**, ensuring security. Detailed logging allows debugging and monitoring of API requests.

This component simplifies **data extraction from Energis**, enabling automated, scalable, and structured energy data processing within Keboola.


================================================
FILE: component_config/component_short_description.md
================================================
Energis API Extractor is a Keboola component designed to fetch energy data from the Energis API, 
process it according to user-defined parameters (nodes, granularity, and time range), 
and store it in Keboola Storage.


================================================
FILE: component_config/configSchema.json
================================================
{
    "type": "object",
    "title": "Energis Component Configuration",
    "required": ["authentication", "sync_options", "debug"],
    "properties": {
        "authentication": {
            "type": "object",
            "title": "Authentication Settings",
            "propertyOrder": 1,
            "required": ["username", "#password", "environment"],
            "properties": {
                "username": {
                    "type": "string",
                    "title": "Username",
                    "default": "",
                    "propertyOrder": 1
                },
                "#password": {
                    "type": "string",
                    "title": "Password",
                    "format": "password",
                    "propertyOrder": 2
                },
                "environment": {
                    "type": "string",
                    "title": "API Environment",
                    "enum": ["dev", "prod"],
                    "default": "prod",
                    "propertyOrder": 3,
                    "description": "Choose 'dev' for testing or 'prod' for production."
                }
            }
        },
        "sync_options": {
            "type": "object",
            "title": "Synchronization Options",
            "propertyOrder": 2,
            "required": ["dataset", "nodes", "date_from", "granularity"],
            "properties": {
                "dataset": {
                    "type": "string",
                    "title": "Dataset",
                    "enum": [
                        "xexport"
                    ],
                    "default": "xexport",
                    "propertyOrder": 1
                },
                "nodes": {
                    "type": "array",
                    "title": "List of Nodes",
                    "items": {
                        "type": "integer",
                        "title": "Node ID"
                    },
                    "default": [],
                    "propertyOrder": 2
                },
                "date_from": {
                    "type": "string",
                    "format": "date",
                    "title": "Date From",
                    "default": "2024-12-01",
                    "propertyOrder": 3
                },
                "date_to": {
                    "type": "string",
                    "format": "date",
                    "title": "Date To",
                    "description": "If not set, defaults to today's date dynamically",
                    "propertyOrder": 4
                },
                "granularity": {
                    "type": "string",
                    "title": "Data Granularity",
                    "enum": [
                        "year",
                        "quarterYear",
                        "month",
                        "day",
                        "hour",
                        "quarterHour",
                        "minute"
                    ],
                    "default": "day",
                    "propertyOrder": 5
                },
                "reload_full_data": {
                    "type": "boolean",
                    "title": "Reload Full Data",
                    "description": "When enabled, retrieves the complete dataset from 'date_from', bypassing incremental loading",
                    "default": false,
                    "propertyOrder": 6
                }
            }
        },
        "debug": {
            "type": "boolean",
            "title": "Enable Debug Mode",
            "default": false,
            "propertyOrder": 3
        }
    }
}


================================================
FILE: component_config/configuration_description.md
================================================
# ðŸ”§ Energis Component Configuration

This component allows you to retrieve energy data from the Energis API, process it based on defined criteria, and store 
it in Keboola Storage. Configure the authentication, data selection, and granularity settings as needed.

## Authentication Settings

> **Required:** âœ… Yes  
> **Description:** Provides API credentials for authentication.

| **Section**        | **Required** | **Description** |
|--------------------|-------------|---------------|
| **Authentication** | âœ… Yes       | Provides API credentials for authentication. |
| **Sync Options**   | âœ… Yes       | Defines data extraction settings, including dataset, nodes, and date range. |
| **Debug Mode**     | âŒ No         | Enables or disables debug logging for troubleshooting. |

## Synchronization Options

> **Required:** âœ… Yes  
> **Description:** Define the dataset, time range, and data granularity for extraction.

| **Property**                      | **Required** | **Type**     | **Default** | **Description** |
|-----------------------------------|--------------|------------|------------|---------------|
| **authentication.username**       | âœ… Yes        | String | _(None)_ | Username for API authentication. |
| **authentication.#password**      | âœ… Yes        | String (password) | _(None)_ | Password for API authentication. |
| **authentication.environment**    | âœ… Yes        | Enum (`dev` / `prod`) | `prod` | Selects the API environment (development or production). |
| **sync_options.dataset**          | âœ… Yes        | Enum (`xexport`) | `xexport` | Specifies the dataset for extraction. |
| **sync_options.nodes**            | âœ… Yes        | Array of Integers | `[]` | List of node IDs for data retrieval. |
| **sync_options.date_from**        | âœ… Yes        | Date (`YYYY-MM-DD`) | `2024-12-01` | Start date for data extraction. |
| **sync_options.date_to**          | âŒ No         | Date (`YYYY-MM-DD`) | _(Today)_ | End date for data extraction. If not set, defaults to today. |
| **sync_options.granularity**      | âœ… Yes        | Enum (`year`, `quarterYear`, `month`, `day`, `hour`, `quarterHour`, `minute`) | `day` | Defines data granularity for extraction. |
| **sync_options.reload_full_data** | âŒ No | Boolean| _False_      | When enabled, retrieves the complete dataset from 'date_from', bypassing incremental loading. |

## Debug Mode

> **Required:** âŒ No  
> **Description:** Enables debug logging for troubleshooting.
 
| **Property**        | **Required** | **Type**     | **Default** | **Description** |
|--------------------|-----------|------------|------------|---------------|
| **debug** | âŒ Yes | Boolean | `false` | Enables debug mode for additional logging. |


## Example configuration

```json
{
  "authentication": {
    "username": "your_username",
    "#password": "your_password",
    "environment": "prod"
  },
  "sync_options": {
    "dataset": "xexport",
    "nodes": [7090001, 7090002],
    "date_from": "2024-01-01",
    "date_to": "2024-12-31",
    "granularity": "hour"
  },
  "debug": false
}
```


================================================
FILE: component_config/documentationUrl.md
================================================
https://github.com/keboola/component-energis/blob/master/README.md


================================================
FILE: component_config/licenseUrl.md
================================================
https://github.com/keboola/component-energis/blob/master/LICENSE.md


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sourceCodeUrl.md
================================================
https://github.com/keboola/component-energis


================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi


================================================
FILE: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"


================================================
FILE: src/api_client.py
================================================
import io
import logging
import time
from datetime import datetime, timedelta
from typing import Iterator, Dict, Any

from lxml import etree
from requests import Session
from zeep.transports import Transport

from configuration import Configuration, DatasetEnum, GranularityEnum

from utils import (
    generate_logon_request,
    generate_xexport_request,
    mask_sensitive_data_in_body,
    granularity_to_short_code,
    convert_date_to_mmddyyyyhhmm,
    format_datetime
)


class EnergisClient:
    """API Client for Energis API using various WSDLs"""

    CHUNK_SIZE_DAYS = {
        GranularityEnum.year: 365 * 5,
        GranularityEnum.quarterYear: 365 * 2,
        GranularityEnum.month: 365,
        GranularityEnum.day: 180,
        GranularityEnum.hour: 30,
        GranularityEnum.quarterHour: 30,
        GranularityEnum.minute: 7,
    }

    def __init__(self, config: Configuration):
        self.config = config

        logging.getLogger("zeep").setLevel(logging.INFO)
        logging.getLogger("zeep.transport").setLevel(logging.WARNING)

        session = Session()
        session.verify = False

        self.transport = Transport(session=session, timeout=(30, 300))
        self.max_retries = 5
        self.retry_delay = 120
        self.auth_key = None

    def authenticate(self) -> str:
        """Calls the auth endpoint and retrieves the key for further requests."""
        body, headers = generate_logon_request(*self.config.authentication.credentials)
        auth_url = f"{self.config.authentication.api_base_url}?logon"

        if self.config.debug:
            masked_body = mask_sensitive_data_in_body(body)
            logging.debug("Request auth url: %s", auth_url)
            logging.debug("Request header: %s", headers)
            logging.debug("Request body: %s", masked_body)

        retries = 0

        while retries < self.max_retries:
            try:
                response = self.transport.post(address=auth_url, message=body, headers=headers)

                if response.status_code != 200:
                    logging.error("Authentication failed with status code %s", response.status_code)
                    raise Exception(f"Authentication failed: {response.status_code}")

                logging.debug("Authentication response: %s", response.text)

                xml_response = etree.fromstring(response.content)
                key = xml_response.xpath("//key/text()")

                if key:
                    logging.debug("Authentication successful, received key: %s", key[0][:4] + "****")
                    return key[0]

                raise Exception("Authentication failed: No key found in the response.")

            except Exception as e:
                logging.error("Authentication attempt %d failed: %s", retries + 1, str(e))

                xml_response = etree.fromstring(response.content)
                fault_string = xml_response.xpath("//faultstring/text()")

                if fault_string and "jiÅ¾ v systÃ©mu pÅ™ihlÃ¡Å¡en" in fault_string[0]:
                    logging.warning("User already logged in. Waiting 120 seconds before retrying...")
                    time.sleep(self.retry_delay)
                    retries += 1
                    continue

                raise e

        raise Exception("Maximum retries reached. Unable to authenticate.")

    def _generate_date_chunks(
        self, date_from: str, date_to: str, granularity: GranularityEnum
    ) -> Iterator[tuple[str, str]]:
        """
        Generates date range chunks based on the granularity to avoid large API payloads.

        Args:
            date_from: Start date in YYYY-MM-DD format
            date_to: End date in YYYY-MM-DD format
            granularity: The data granularity level

        Yields:
            Tuples of (chunk_start, chunk_end) dates in YYYY-MM-DD format
        """
        chunk_days = self.CHUNK_SIZE_DAYS.get(granularity, 30)
        start = datetime.strptime(date_from, "%Y-%m-%d")
        end = datetime.strptime(date_to, "%Y-%m-%d")

        current_start = start
        while current_start < end:
            current_end = min(current_start + timedelta(days=chunk_days), end)
            yield (current_start.strftime("%Y-%m-%d"), current_end.strftime("%Y-%m-%d"))
            current_start = current_end + timedelta(days=1)

    def fetch_data(self) -> Iterator[Dict[str, Any]]:
        """Fetches data from the Energis API using the xexport SOAP call and returns the data."""
        key = self.authenticate()
        nodes = self.config.sync_options.nodes
        dataset = self.config.sync_options.dataset
        date_from = self.config.sync_options.date_from
        date_to = self.config.sync_options.date_to
        data_url = f"{self.config.authentication.api_base_url}?data"

        if dataset == DatasetEnum.xexport:
            granularity = self.config.sync_options.granularity
            chunks = list(self._generate_date_chunks(date_from, date_to, granularity))
            total_chunks = len(chunks)

            logging.info(
                f"Splitting date range {date_from} to {date_to} into {total_chunks} chunk(s) "
                f"for granularity '{granularity.value}'"
            )

            for chunk_idx, (chunk_start, chunk_end) in enumerate(chunks, 1):
                logging.info(f"Processing chunk {chunk_idx}/{total_chunks}: {chunk_start} to {chunk_end}")

                body, headers = generate_xexport_request(
                    username=self.config.authentication.username,
                    key=key,
                    nodes=nodes,
                    date_from=convert_date_to_mmddyyyyhhmm(chunk_start),
                    date_to=convert_date_to_mmddyyyyhhmm(chunk_end),
                    granularity=granularity_to_short_code(granularity),
                )

                chunk_row_count = 0
                for row in self.send_request(data_url, body, headers):
                    chunk_row_count += 1
                    yield row
                logging.info(f"Chunk {chunk_idx}/{total_chunks} completed: {chunk_row_count} rows")

    def send_request(self, url: str, body: str, headers: dict) -> Iterator[Dict[str, Any]]:
        """Sends the SOAP request, parses the response, and stores data in memory."""
        if self.config.debug:
            masked_body = mask_sensitive_data_in_body(body)
            logging.debug("Request url: %s", url)
            logging.debug("Request header: %s", headers)
            logging.debug("Request body: %s", masked_body)

        response = self.transport.post(address=url, message=body, headers=headers)

        if response.status_code != 200:
            try:
                xml_response = etree.fromstring(response.content)
                fault_string = xml_response.find(".//faultstring")
                if fault_string is not None:
                    error_message = fault_string.text
                    logging.error("SOAP Fault: %s", error_message)
                    raise Exception(f"Data request failed: {error_message}")
            except Exception as e:
                logging.error("Failed to parse SOAP fault response: %s", str(e))
                raise Exception(f"Data request failed: {response.text}")

        logging.debug("Data request response received")

        try:
            dataset = self.config.sync_options.dataset
            context = etree.iterparse(io.BytesIO(response.content), events=("start", "end"))

            if dataset == DatasetEnum.xexport:
                for event, elem in context:
                    if event == "end" and elem.tag == "responseData":
                        uzel = elem.findtext("uzel")
                        hodnota = elem.findtext("hodnota")
                        cas = elem.findtext("cas")

                        if uzel and hodnota and cas:
                            yield {
                                "uzel": uzel,
                                "hodnota": hodnota,
                                "cas": format_datetime(cas, self.config.sync_options.granularity)
                            }

                        elem.clear()

        except Exception as e:
            logging.error("Failed to parse SOAP response: %s", str(e))
            raise



================================================
FILE: src/component.py
================================================
import logging
import os

from keboola.component.base import ComponentBase
from keboola.component.exceptions import UserException

from configuration import Configuration
from api_client import EnergisClient
from file_manager import FileManager
from manifest_manager import ManifestManager
from state_manager import StateManager


class Component(ComponentBase):
    def __init__(self):
        super().__init__()

    def run(self):
        """
        Main execution code
        """
        data_dir = self.configuration.data_dir
        state_manager = StateManager(os.path.join(data_dir, "."))
        config = Configuration(state_manager, **self.configuration.parameters)

        client = EnergisClient(config)
        output_dir = os.path.join(data_dir, "out", "tables")
        os.makedirs(output_dir, exist_ok=True)

        file_manager = FileManager(config, output_dir)
        manifest_manager = ManifestManager(self, config, file_manager)
        file_metadata = file_manager.get_file_metadata()

        data_stream = client.fetch_data()

        file_created = file_manager.save_to_csv(data_stream, file_metadata)

        if file_created:
            manifest_manager.create_manifest()
            state_manager.save_state(config.sync_options.date_to, data_dir)

        logging.info("Data processing completed!")


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)



================================================
FILE: src/configuration.py
================================================
import logging
from datetime import date
from enum import Enum
from typing import List, Optional

from pydantic import BaseModel, Field, ValidationError, field_validator
from keboola.component.exceptions import UserException
from state_manager import StateManager


class EnvironmentEnum(str, Enum):
    dev = "dev"
    prod = "prod"


ENVIRONMENT_URLS = {
    EnvironmentEnum.dev: "https://webenergis.eu/test/1.wsc/soap.r",
    EnvironmentEnum.prod: "https://bilance.c-energy.cz/cgi-bin/1.wsc/soap.r"
}


class DatasetEnum(str, Enum):
    xexport = "xexport"


DATASET_UNIQUE_FIELDS = {
    DatasetEnum.xexport: [
        "uzel",
        "cas"
    ]
}

DATASET_OUTPUT_FIELDS = {
    DatasetEnum.xexport: [
        "uzel",
        "hodnota",
        "cas"
    ]
}


class GranularityEnum(str, Enum):
    year = "year"
    quarterYear = "quarterYear"
    month = "month"
    day = "day"
    hour = "hour"
    quarterHour = "quarterHour"
    minute = "minute"


class Authentication(BaseModel):
    username: str
    password: str = Field(alias="#password")
    environment: EnvironmentEnum = Field(
        default=EnvironmentEnum.prod,
        description="Choose 'dev' for testing or 'prod' for production."
    )

    @field_validator("username", "password")
    def must_not_be_empty(cls, value: str, info) -> str:
        if not value.strip():
            raise ValueError(f"Field '{info.field_name}' cannot be empty")
        return value

    @property
    def credentials(self) -> tuple[str, str]:
        return self.username, self.password

    @property
    def api_base_url(self) -> str:
        """Returns the full API base URL based on the selected environment."""
        return ENVIRONMENT_URLS[self.environment]


class SyncOptions(BaseModel):
    dataset: DatasetEnum = Field(
        default=DatasetEnum.xexport,
        description="Source dataset for data extraction"
    )
    nodes: list[int] = Field(
        default=[],
        description="List of nodes to fetch, e.g. [7090001]"
    )
    date_from: str = Field(
        default="2020-01-01",
        description="Date from which to fetch data, default '2020-01-01'"
    )
    date_to: Optional[str] = Field(
        default=None,
        description="Date to which to fetch data."
    )
    granularity: GranularityEnum = Field(
        default=GranularityEnum.day,
        description="Granularity of fetched data, default 'day'"
    )
    reload_full_data: bool = Field(
        default=False,
        description="When enabled, retrieves the complete dataset from 'date_from', bypassing incremental loading"
    )

    @field_validator("nodes")
    def must_not_be_empty(cls, values: List[int], info) -> List[int]:
        if len(values) == 0:
            raise ValueError(f"Field '{info.field_name}' cannot be empty")
        return values

    @field_validator("granularity")
    def validate_granularity(cls, value: GranularityEnum) -> GranularityEnum:
        if value not in GranularityEnum:
            allowed_values = "', '".join([e.value for e in GranularityEnum])
            raise ValueError(f"Invalid value '{value}' for 'granularity'. Must be one of {allowed_values}")
        return value

    @property
    def resolved_date_to(self) -> str:
        """Ensures date_to is always a string."""
        return self.date_to if self.date_to else str(date.today())


class Configuration(BaseModel):
    authentication: Authentication
    sync_options: SyncOptions
    debug: bool = False

    def __init__(self, state_manager: StateManager, **data):
        try:
            super().__init__(**data)

            last_processed_date = state_manager.get_last_processed_date()
            if last_processed_date and not self.sync_options.reload_full_data:
                self.sync_options.date_from = last_processed_date

            if not self.sync_options.date_to:
                self.sync_options.date_to = self.sync_options.resolved_date_to

            logging.info(f"Using date_from: {self.sync_options.date_from}")
            logging.info(f"Using date_to: {self.sync_options.date_to}")
            logging.info(f"Using granularity: {self.sync_options.granularity.value}")
        except ValidationError as e:
            error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
            raise UserException(f"Validation Error: {', '.join(error_messages)}")

        if self.debug:
            logging.debug("Component will run in Debug mode")



================================================
FILE: src/file_manager.py
================================================
import os
import csv
import logging
from dataclasses import dataclass
from typing import Iterable

from utils import granularity_to_filename
from configuration import DATASET_OUTPUT_FIELDS


@dataclass
class FileMetadata:
    """Encapsulates output file information"""
    table_name: str
    file_name: str
    file_path: str


class FileManager:
    """Handles file path generation and saving data to CSV files."""

    def __init__(self, config, output_dir):
        self.config = config
        self.output_dir = output_dir

    def get_granularity(self) -> str:
        """Returns the granularity as a string."""
        return granularity_to_filename(self.config.sync_options.granularity)

    def get_file_metadata(self) -> FileMetadata:
        """Generates file metadata containing name and full path."""
        granularity = self.get_granularity()
        table_name = f"energis_{self.config.sync_options.dataset.value}_{granularity}_data"
        file_name = f"{table_name}.csv"
        file_path = os.path.join(self.output_dir, file_name)

        return FileMetadata(table_name, file_name, file_path)

    def save_to_csv(self, data: Iterable[dict[str, str]], file_metadata: FileMetadata) -> bool:
        """
        Saves the collected data to a CSV file and returns whether the file was created.

        Returns:
            bool: True if the file was created, False if skipped.
        """
        fieldnames = DATASET_OUTPUT_FIELDS.get(self.config.sync_options.dataset, [])
        row_count = 0

        try:
            with open(file_metadata.file_path, mode="w", newline="", encoding="utf-8") as csv_file:
                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
                writer.writeheader()
                for row in data:
                    writer.writerow(row)
                    row_count += 1

            if row_count == 0:
                logging.info("No data found")
                return False

            logging.info("Data successfully saved to %s (%d rows)", file_metadata.file_path, row_count)
            return True

        except Exception as e:
            logging.error("Failed to save data to CSV: %s", str(e))
            return False



================================================
FILE: src/manifest_manager.py
================================================
import logging
from keboola.component.base import ComponentBase
from file_manager import FileManager
from configuration import Configuration, DATASET_UNIQUE_FIELDS


class ManifestManager:
    """Handles the generation of Keboola manifest files."""

    def __init__(self, component: ComponentBase, config: Configuration, file_manager: FileManager):
        self.config = config
        self.component = component
        self.file_manager = file_manager

    def get_primary_keys(self) -> list[str]:
        """Returns the primary keys for a given dataset."""
        return DATASET_UNIQUE_FIELDS.get(self.config.sync_options.dataset, [])

    def create_manifest(self):
        """
        Generates a Keboola manifest file for a dataset.
        Uses FileManager to ensure consistent file naming.
        """
        file_metadata = self.file_manager.get_file_metadata()

        output_table = self.component.create_out_table_definition(
            file_metadata.file_name,
            incremental=True,
            primary_key=ManifestManager.get_primary_keys(self),
            destination=f"out.c-data.{file_metadata.table_name}",
        )

        self.component.write_manifest(output_table)
        logging.info(f"Manifest created for {file_metadata.file_name}")



================================================
FILE: src/state_manager.py
================================================
import json
import os
from datetime import datetime, timedelta
import logging


class StateManager:
    """Handles state persistence for incremental fetching in Keboola Connection."""

    def __init__(self, state_dir: str):
        """Initialize the state manager with the path to state.json"""
        self.state_file = os.path.join(state_dir, "in", "state.json")

    def load_state(self) -> dict:
        """Loads the state file if it exists, otherwise returns an empty dictionary."""
        if os.path.exists(self.state_file):
            with open(self.state_file, "r") as f:
                try:
                    return json.load(f)
                except json.JSONDecodeError:
                    logging.warning("State file is corrupted. Resetting state.")
                    return {}
        return {}

    def get_last_processed_date(self) -> str | None:
        """Returns the last processed date from the state file (adjusted by -1 day)."""
        state = self.load_state()
        last_processed_date = state.get("last_processed_date")

        if last_processed_date:
            try:
                last_date = datetime.strptime(last_processed_date, "%Y-%m-%d").date()
                adjusted_date = last_date - timedelta(days=1)
                return str(adjusted_date)
            except ValueError:
                logging.warning(f"Invalid date format in state file: {last_processed_date}")
        return None

    @staticmethod
    def save_state(last_date: str, state_dir: str):
        """Saves the last processed date to the state file."""
        state_out_file = os.path.join(state_dir, "out", "state.json")
        state = {"last_processed_date": last_date}

        with open(state_out_file, "w") as f:
            json.dump(state, f)

        logging.info(f"Saved last processed date: {last_date}")



================================================
FILE: src/utils.py
================================================
import re
from datetime import datetime, timedelta
from typing import Generator

from configuration import GranularityEnum

import logging


class MaskSensitiveDataFilter(logging.Filter):
    """
    A logging filter that masks sensitive fields in log messages.

    This filter scans log messages and replaces the values of specified XML fields
    with masked characters, ensuring that sensitive data is not logged.

    Attributes:
        fields_to_mask (list[str]): The list of XML tag names whose values should be masked.
        mask_char (str): The character used for masking sensitive values.
    """

    def __init__(self, fields_to_mask=None, mask_char="*"):
        """
        Initializes the filter with optional fields to mask and mask character.

        Args:
            fields_to_mask (list[str], optional): List of XML tag names to mask. Defaults to a predefined set.
            mask_char (str, optional): The character used for masking. Defaults to "*".
        """
        super().__init__()
        self.fields_to_mask = fields_to_mask or ["username", "password", "exuziv", "exklic"]
        self.mask_char = mask_char

    def filter(self, record):
        """
        Masks sensitive data in the log record message before it's logged.

        Args:
            record (logging.LogRecord): The log record object.

        Returns:
            bool: Always returns True to allow the log message to proceed.
        """
        if isinstance(record.msg, str):  # Ensure the message is a string before modifying
            record.msg = mask_sensitive_data_in_body(record.msg, self.fields_to_mask, self.mask_char)
        return True  # Allow the log message to pass through


def mask_sensitive_data_in_body(body: str, fields_to_mask: list[str] = None, mask_char: str = "*") -> str:
    """
    Masks sensitive fields in the SOAP XML body by replacing their values with masked characters.

    The function searches for XML tags that match the specified field names and replaces their
    contents with a masked version while preserving the first character (if available).

    Args:
        body (str): The raw SOAP XML body as a string.
        fields_to_mask (list[str], optional): The list of XML tag names to mask.
            Defaults to ["username", "password", "exuziv", "exklic"].
        mask_char (str, optional): The character to use for masking. Defaults to "*".

    Returns:
        str: The masked SOAP XML body with sensitive fields obfuscated.
    """
    if fields_to_mask is None:
        fields_to_mask = ["username", "password", "exuziv", "exklic"]

    for field in fields_to_mask:
        # Regex pattern to find <field>value</field> tags
        pattern = f"<{field}>(.*?)</{field}>"

        def mask_match(match: re.Match) -> str:
            value = match.group(1)
            if len(value) > 1:
                masked_value = f"{value[0].lower()}{mask_char * (len(value))}"
            else:
                masked_value = mask_char
            return f"<{field}>{masked_value}</{field}>"

        body = re.sub(pattern, mask_match, body, flags=re.IGNORECASE)

    return body


def generate_logon_request(username: str, password: str) -> tuple[str, dict[str, str]]:
    """
    Generates the SOAP request body and headers for the logonex operation.

    Args:
        username (str): The username for authentication.
        password (str): The password for authentication.

    Returns:
        tuple[str, dict[str, str]]: The SOAP request body and headers.
    """
    soap_body = f"""
    <soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/"
           soap:encodingStyle="http://schemas.xmlsoap.org/soap/encoding/"
           xmlns:ene="ENERGIS-URL">
        <soap:Body>
            <ene:logonex>
                <username>{username}</username>
                <password>{password}</password>
            </ene:logonex>
        </soap:Body>
    </soap:Envelope>
    """

    headers = {
        "Content-Type": "text/xml; charset=utf-8",
        "SOAPAction": "logonex"
    }

    return soap_body, headers


def generate_xexport_request(
    username: str,
    key: str,
    nodes: list[int],
    date_from: str,
    date_to: str,
    granularity: str,
) -> tuple[str, dict[str, str]]:
    """
    Generates the SOAP request body and headers for the xexport operation.

    Args:
        username (str): The username for authentication.
        key (str): The authentication key.
        nodes (list[int]): List of node IDs to fetch data for.
        date_from (str): Start date in MMDDYYYYHHMM format.
        date_to (str): End date in MMDDYYYYHHMM format.
        granularity (str): The granularity of the data ('m' for month, 'd' for day).

    Returns:
        tuple[str, dict[str, str]]: The SOAP request body and headers.
    """
    nodes_str = ",".join(map(str, nodes))

    soap_body = f"""<?xml version="1.0" encoding="UTF-8"?>
    <soap:Envelope xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/"
                   soap:encodingStyle="http://schemas.xmlsoap.org/soap/encoding/"
                   xmlns:ene="ENERGIS-URL">
        <soap:Header>
            <ene:Auth>
                <exuziv>{username}</exuziv>
                <exklic>{key}</exklic>
            </ene:Auth>
        </soap:Header>
        <soap:Body>
            <ene:xexport>
                <uzel>{nodes_str}</uzel>
                <typuz>2</typuz>
                <per>{granularity}</per>
                <cas>{date_from},{date_to}</cas>
                <typhodn>hodnota</typhodn>
            </ene:xexport>
        </soap:Body>
    </soap:Envelope>"""

    headers = {
        "Content-Type": "text/xml; charset=utf-8",
        "SOAPAction": "xexport"
    }

    return soap_body, headers


def granularity_to_short_code(granularity: GranularityEnum) -> str:
    """Returns a single-letter string based on the GranularityEnum value."""
    mapping = {
        GranularityEnum.year: "r",
        GranularityEnum.quarterYear: "v",
        GranularityEnum.month: "m",
        GranularityEnum.day: "d",
        GranularityEnum.hour: "h",
        GranularityEnum.quarterHour: "c",
        GranularityEnum.minute: "t"
    }

    return mapping.get(granularity)


def granularity_to_filename(granularity: GranularityEnum) -> str:
    """Returns a descriptive filename component based on the GranularityEnum value."""
    mapping = {
        GranularityEnum.year: "year",
        GranularityEnum.quarterYear: "quarter_year",
        GranularityEnum.month: "month",
        GranularityEnum.day: "day",
        GranularityEnum.hour: "hour",
        GranularityEnum.quarterHour: "quarter_hour",
        GranularityEnum.minute: "minute"
    }

    return mapping.get(granularity)


def convert_date_to_mmddyyyyhhmm(date_str: str) -> str:
    """
    Converts a date string from 'YYYY-MM-DD' format to 'MMDDYYYYHHMM'.

    Args:
        date_str (str): Date in 'YYYY-MM-DD' format.

    Returns:
        str: Converted date in 'MMDDYYYYHHMM' format.
    """
    try:
        date_obj = datetime.strptime(date_str, "%Y-%m-%d")
        return date_obj.strftime("%m%d%Y0000")
    except ValueError:
        raise ValueError(f"Invalid date format: {date_str}. Expected format: YYYY-MM-DD")


def generate_periods(granularity: GranularityEnum, date_from: str, date_to: str) -> Generator[str, None, None]:
    """
    Generates a sequence of period strings based on the given granularity and date range.

    Args:
        granularity (GranularityEnum): The granularity level (year, month, day, etc.).
        date_from (str): The start date in "YYYY-MM-DD" format.
        date_to (str): The end date in "YYYY-MM-DD" format.

    Returns:
        Generator[str, None, None]: A generator yielding period strings in the format `r-1, r-2, ...` or `r`.
    """
    date_from = datetime.strptime(date_from, "%Y-%m-%d")
    date_to = datetime.strptime(date_to, "%Y-%m-%d")

    granularity_steps = {
        GranularityEnum.year: (timedelta(days=365), "r-{index}"),
        GranularityEnum.quarterYear: (timedelta(days=91), "v-{index}"),
        GranularityEnum.month: (timedelta(days=30), "m-{index}"),  # Approximate
        GranularityEnum.day: (timedelta(days=1), "d-{index}"),
        GranularityEnum.hour: (timedelta(hours=1), "h-{index}"),
        GranularityEnum.quarterHour: (timedelta(minutes=15), "c-{index}"),
        GranularityEnum.minute: (timedelta(minutes=1), "t-{index}"),
    }

    if granularity not in granularity_steps:
        raise ValueError(f"Unsupported granularity: {granularity}")

    step, format_str = granularity_steps[granularity]

    if granularity == GranularityEnum.month:
        start_date = date_from.replace(day=1)
        end_date = date_to.replace(day=1)
        months_diff = (end_date.year - start_date.year) * 12 + end_date.month - start_date.month

        for i in range(months_diff + 1):
            yield f"m-{months_diff - i}" if i < months_diff else "m"
        return  # Exit early

    if granularity == GranularityEnum.year:
        start_year = date_from.year
        end_year = date_to.year
        years_diff = end_year - start_year

        for i in range(years_diff + 1):
            yield f"r-{years_diff - i}" if i < years_diff else "r"
        return

    if granularity == GranularityEnum.quarterYear:
        start_quarter = (date_from.year * 4) + (date_from.month - 1) // 3
        end_quarter = (date_to.year * 4) + (date_to.month - 1) // 3
        quarters_diff = end_quarter - start_quarter

        for i in range(quarters_diff + 1):
            yield f"v-{quarters_diff - i}" if i < quarters_diff else "v"
        return

    current_date = date_from
    steps_count = (date_to - date_from) // step

    for i in range(steps_count + 1):
        yield format_str.format(index=steps_count - i) if i < steps_count else format_str[0]
        current_date += step


def format_datetime(value: str, granularity: GranularityEnum) -> str:
    match granularity:
        case GranularityEnum.year:
            return value

        case GranularityEnum.quarterYear:
            quarter_map = {"I": "Q1", "II": "Q2", "III": "Q3", "IV": "Q4"}
            quarter, year = value.split("/")
            return f"{quarter_map.get(quarter, quarter)}/{year}"

        case GranularityEnum.month:
            return value

        case GranularityEnum.day:
            return datetime.strptime(value, "%d.%m.%Y").strftime("%Y-%m-%d")

        case GranularityEnum.hour:
            day_part, time_part = value.split(" ")
            day = datetime.strptime(day_part, "%d.%m.%Y").strftime("%Y-%m-%d")

            start_hour = time_part.split("-")[0]
            formatted_start = start_hour if ":" in start_hour else f"{start_hour}:00"

            return f"{day} {formatted_start}"

        case GranularityEnum.quarterHour | GranularityEnum.minute:
            day_part, time_part = value.split(" ")
            day = datetime.strptime(day_part, "%d.%m.%Y").strftime("%Y-%m-%d")

            start_time = time_part.split("-")[0]
            formatted_start = start_time if ":" in start_time else f"{start_time}:00"

            return f"{day} {formatted_start}"

        case _:
            raise ValueError(f"Unsupported granularity: {granularity}")



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
FILE: tests/test_api_client.py
================================================
import pytest
from unittest.mock import Mock, patch

from api_client import EnergisClient
from configuration import Configuration, DatasetEnum, GranularityEnum, SyncOptions
from utils import convert_date_to_mmddyyyyhhmm, granularity_to_short_code

@pytest.fixture
def mock_config():
    """Provides a mock Configuration object."""
    mock_config = Mock(spec=Configuration)

    mock_auth = Mock()
    mock_auth.credentials = ("testuser", "testpassword")
    mock_auth.api_base_url = "https://fake-api.com"
    mock_auth.username = "testuser"

    mock_config.authentication = mock_auth

    mock_sync_options = Mock()
    mock_sync_options.dataset = DatasetEnum.xexport
    mock_sync_options.nodes = [7090001]
    mock_sync_options.date_from = "2025-01-01"
    mock_sync_options.date_to = "2025-01-31"
    mock_sync_options.granularity = GranularityEnum.day
    mock_sync_options.resolved_date_to = "2025-01-31"

    mock_config.sync_options = mock_sync_options
    mock_config.debug = False

    return mock_config


@pytest.fixture
def mock_transport():
    """Mocks Zeep's Transport.post() method."""
    mock_transport = Mock()
    return mock_transport


@pytest.fixture
def client(mock_config, mock_transport):
    """Creates an EnergisClient instance with mocked transport."""
    client = EnergisClient(mock_config)
    client.transport = mock_transport
    return client


def create_mock_response(status_code, content):
    """Creates a mock HTTP response with given status code and content."""
    response = Mock()
    response.status_code = status_code
    response.content = content.encode("utf-8")
    response.text = content
    return response


def test_authenticate_success(client, mock_transport):
    """Tests successful authentication."""
    xml_response = """<response><key>test-api-key</key></response>"""
    mock_transport.post.return_value = create_mock_response(200, xml_response)

    auth_key = client.authenticate()

    assert auth_key == "test-api-key"
    mock_transport.post.assert_called_once()


def test_authenticate_failure(client, mock_transport):
    """Tests authentication failure with HTTP error."""
    mock_transport.post.return_value = create_mock_response(401, "<error>Unauthorized</error>")

    with pytest.raises(Exception, match="Authentication failed: 401"):
        client.authenticate()


def test_authenticate_retry_on_already_logged_in(client, mock_transport):
    """Tests authentication retry logic when user is already logged in."""
    xml_response = """<faultstring>UÅ¾ivatel jiÅ¾ v systÃ©mu pÅ™ihlÃ¡Å¡en</faultstring>"""
    mock_transport.post.return_value = create_mock_response(500, xml_response)

    with patch("time.sleep", return_value=None) as mock_sleep:
        with pytest.raises(Exception, match="Maximum retries reached"):
            client.authenticate()

        assert mock_transport.post.call_count == client.max_retries
        mock_sleep.assert_called_with(client.retry_delay)


def test_fetch_data_success(client, mock_transport, mock_config):
    """Tests fetching and parsing of data successfully."""
    auth_xml = """<response><key>test-api-key</key></response>"""
    data_xml = """
    <response>
        <responseData>
            <uzel>7090001</uzel>
            <hodnota>123.45</hodnota>
            <cas>06.03.2025</cas>
        </responseData>
    </response>
    """

    mock_transport.post.side_effect = [
        create_mock_response(200, auth_xml),
        create_mock_response(200, data_xml)
    ]

    results = list(client.fetch_data())

    assert len(results) == 1
    assert results[0] == {
        "uzel": "7090001",
        "hodnota": "123.45",
        "cas": "2025-03-06"
    }


def test_fetch_data_failure(client, mock_transport):
    """Tests handling of failed data request."""
    auth_xml = """<response><key>test-api-key</key></response>"""
    mock_transport.post.side_effect = [
        create_mock_response(200, auth_xml),
        create_mock_response(500, "Internal Server Error")
    ]

    with pytest.raises(Exception, match="Data request failed: Internal Server Error"):
        list(client.fetch_data())


def test_fetch_data_invalid_xml(client, mock_transport):
    """Tests handling of invalid XML response from API."""
    auth_xml = """<response><key>test-api-key</key></response>"""
    invalid_xml = "<response><invalid></invalid>"

    mock_transport.post.side_effect = [
        create_mock_response(200, auth_xml),
        create_mock_response(200, invalid_xml)
    ]

    with pytest.raises(Exception):
        list(client.fetch_data())


def test_send_request_success(client, mock_transport, mock_config):
    """Tests send_request with valid SOAP response."""
    mock_config.sync_options.granularity = GranularityEnum.hour

    xml_response = """
    <response>
        <responseData>
            <uzel>7090001</uzel>
            <hodnota>123.45</hodnota>
            <cas>06.03.2025 08-09</cas>
        </responseData>
    </response>
    """
    mock_transport.post.return_value = create_mock_response(200, xml_response)

    results = list(client.send_request("https://fake-api.com/data", "<soap_request>", {"Content-Type": "text/xml"}))

    assert len(results) == 1
    assert results[0]["cas"] == "2025-03-06 08:00"


def test_send_request_failure(client, mock_transport):
    """Tests handling of HTTP error response in send_request."""
    mock_transport.post.return_value = create_mock_response(500, "Internal Server Error")

    with pytest.raises(Exception, match="Data request failed: Internal Server Error"):
        list(client.send_request("https://fake-api.com/data", "<soap_request>", {"Content-Type": "text/xml"}))


def test_send_request_parsing_failure(client, mock_transport):
    """Tests handling of parsing errors in send_request."""
    xml_response = "<response><invalid></invalid>"
    mock_transport.post.return_value = create_mock_response(200, xml_response)

    with pytest.raises(Exception):
        list(client.send_request("https://fake-api.com/data", "<soap_request>", {"Content-Type": "text/xml"}))


def test_convert_date_to_mmddyyyyhhmm():
    """Tests correct conversion of date format."""
    assert convert_date_to_mmddyyyyhhmm("2025-03-06") == "030620250000"


def test_granularity_to_short_code():
    """Tests mapping of granularity enum to short codes."""
    assert granularity_to_short_code(GranularityEnum.year) == "r"
    assert granularity_to_short_code(GranularityEnum.minute) == "t"
    assert granularity_to_short_code(GranularityEnum.quarterHour) == "c"



================================================
FILE: tests/test_configuration.py
================================================
import pytest
import logging
from datetime import date
from unittest.mock import Mock
from configuration import (
    Configuration,
    Authentication,
    SyncOptions,
    GranularityEnum,
    DatasetEnum,
    EnvironmentEnum
)
from state_manager import StateManager


@pytest.fixture
def state_manager():
    """Creates a mock StateManager."""
    mock_state_manager = Mock(spec=StateManager)
    mock_state_manager.get_last_processed_date.return_value = "2025-01-01"
    return mock_state_manager


@pytest.fixture
def valid_auth():
    return Authentication(
        username="testuser",
        **{"#password": "securepassword"},
        environment=EnvironmentEnum.prod
    )

@pytest.fixture
def valid_sync_options():
    """Returns a valid SyncOptions instance."""
    return SyncOptions(
        dataset=DatasetEnum.xexport,
        nodes=[12345],
        date_from="2025-01-01",
        date_to="2025-01-31",
        granularity=GranularityEnum.day
    )


def test_authentication_properties(valid_auth):
    """Tests the computed properties of Authentication."""
    assert valid_auth.credentials == ("testuser", "securepassword")
    assert valid_auth.api_base_url == "https://bilance.c-energy.cz/cgi-bin/1.wsc/soap.r"


def test_authentication_validation():
    with pytest.raises(ValueError, match="Field 'username' cannot be empty"):
        Authentication(username="", **{"#password": "securepassword"})

    with pytest.raises(ValueError, match="Value error, Field 'password' cannot be empty"):
        Authentication(username="testuser", **{"#password": ""})


def test_sync_options_validation():
    """Tests that SyncOptions enforces correct values."""
    with pytest.raises(ValueError, match="Field 'nodes' cannot be empty"):
        SyncOptions(dataset=DatasetEnum.xexport, nodes=[], date_from="2025-01-01", date_to="2025-01-31", granularity=GranularityEnum.day)

    with pytest.raises(ValueError, match="Input should be 'year', 'quarterYear', 'month', 'day', 'hour', 'quarterHour' or 'minute'"):
        SyncOptions(dataset=DatasetEnum.xexport, nodes=[12345], date_from="2025-01-01", date_to="2025-01-31", granularity="invalid")


def test_sync_options_resolved_date_to(valid_sync_options):
    """Tests that resolved_date_to returns the correct value."""
    assert valid_sync_options.resolved_date_to == "2025-01-31"

    sync_options_without_date_to = SyncOptions(
        dataset=DatasetEnum.xexport,
        nodes=[12345],
        date_from="2025-01-01",
        date_to=None,
        granularity=GranularityEnum.day
    )
    assert sync_options_without_date_to.resolved_date_to == str(date.today())


def test_configuration_initialization(state_manager, valid_auth, valid_sync_options, caplog):
    """Tests Configuration initialization and state handling."""
    with caplog.at_level(logging.INFO):
        config = Configuration(state_manager=state_manager, authentication=valid_auth, sync_options=valid_sync_options)

    assert config.sync_options.date_from == "2025-01-01"
    assert config.sync_options.date_to == "2025-01-31"

    assert "Using date_from: 2025-01-01" in caplog.text
    assert f"Using date_to: {config.sync_options.date_to}" in caplog.text
    assert f"Using granularity: {config.sync_options.granularity.value}" in caplog.text


def test_configuration_validation_error(state_manager):
    """Tests that Configuration raises validation errors when required fields are missing."""
    with pytest.raises(ValueError, match="Field 'username' cannot be empty"):
        Configuration(
            state_manager=state_manager,
            authentication=Authentication(username="", password="password"),
            sync_options=SyncOptions(
                dataset=DatasetEnum.xexport,
                nodes=[12345],
                date_from="2025-01-01",
                date_to="2025-01-31",
                granularity=GranularityEnum.day
            )
        )



================================================
FILE: tests/test_file_manager.py
================================================
import pytest
import os
import logging
from unittest.mock import Mock, patch, mock_open
from file_manager import FileManager, FileMetadata
from configuration import Configuration, SyncOptions, DatasetEnum, GranularityEnum


@pytest.fixture
def config():
    """Creates a mock Configuration object."""
    mock_config = Mock(spec=Configuration)

    mock_config.sync_options = SyncOptions(
        dataset=DatasetEnum.xexport,
        nodes=[12345],
        date_from="2025-01-01",
        date_to="2025-01-31",
        granularity=GranularityEnum.day
    )

    return mock_config


@pytest.fixture
def file_manager(config, tmp_path):
    """Creates a FileManager instance using a temporary directory."""
    return FileManager(config, str(tmp_path))


def test_get_file_metadata(file_manager):
    """Tests that get_file_metadata() generates correct file metadata."""
    file_metadata = file_manager.get_file_metadata()

    expected_table_name = "energis_xexport_day_data"
    expected_file_name = f"{expected_table_name}.csv"
    expected_file_path = os.path.join(file_manager.output_dir, expected_file_name)

    assert file_metadata.table_name == expected_table_name
    assert file_metadata.file_name == expected_file_name
    assert file_metadata.file_path == expected_file_path


@patch("builtins.open", new_callable=mock_open)
@patch("csv.DictWriter")
def test_save_to_csv(mock_csv_writer, mock_open_func, file_manager, caplog):
    """Tests that save_to_csv() correctly writes data to a CSV file."""
    file_metadata = file_manager.get_file_metadata()
    test_data = iter([{"uzel": "12345", "hodnota": "100", "cas": "2025-03-05 08:00"}])  # Generator

    with caplog.at_level(logging.INFO):
        result = file_manager.save_to_csv(test_data, file_metadata)

    assert result is True

    mock_open_func.assert_called_once_with(file_metadata.file_path, mode="w", newline="", encoding="utf-8")

    mock_csv_writer.return_value.writeheader.assert_called_once()
    mock_csv_writer.return_value.writerow.assert_called_once()

    assert f"Data successfully saved to {file_metadata.file_path}" in caplog.text


def test_save_to_csv_empty_data(file_manager, caplog):
    """Tests that save_to_csv() handles an empty dataset correctly."""
    file_metadata = file_manager.get_file_metadata()

    with caplog.at_level(logging.INFO):
        result = file_manager.save_to_csv([], file_metadata)

    assert result is False
    assert "No data found" in caplog.text



================================================
FILE: tests/test_manifest_manager.py
================================================
import pytest
import logging
from unittest.mock import Mock
from manifest_manager import ManifestManager
from configuration import Configuration, DATASET_UNIQUE_FIELDS, SyncOptions, DatasetEnum


@pytest.fixture
def config():
    """Create a mock Configuration object with a sample dataset."""
    mock_config = Mock(spec=Configuration)

    mock_config.sync_options = SyncOptions(
        dataset=DatasetEnum.xexport,
        nodes=[12345],
        date_from="2025-01-01",
        date_to="2025-01-31",
        granularity="day"
    )

    return mock_config


@pytest.fixture
def mock_file_manager():
    """Create a mock FileManager."""
    mock_fm = Mock()
    mock_fm.get_file_metadata.return_value = Mock(
        file_name="dataset_20250306.csv",
        table_name="dataset"
    )
    return mock_fm


@pytest.fixture
def mock_component():
    """Create a mock ComponentBase object."""
    mock_comp = Mock()
    mock_comp.create_out_table_definition.return_value = Mock()
    return mock_comp


def test_get_primary_keys(config, mock_component, mock_file_manager):
    """Test retrieving primary keys from the dataset."""
    manifest_manager = ManifestManager(mock_component, config, mock_file_manager)

    expected_keys = DATASET_UNIQUE_FIELDS.get(config.sync_options.dataset, [])
    assert manifest_manager.get_primary_keys() == expected_keys


def test_create_manifest(config, mock_component, mock_file_manager, caplog):
    """Test that create_manifest() correctly generates a manifest."""
    manifest_manager = ManifestManager(mock_component, config, mock_file_manager)

    with caplog.at_level(logging.INFO):
        manifest_manager.create_manifest()

    mock_component.create_out_table_definition.assert_called_once_with(
        "dataset_20250306.csv",
        incremental=True,
        primary_key=DATASET_UNIQUE_FIELDS.get(config.sync_options.dataset, []),
        destination="out.c-data.dataset"
    )

    mock_component.write_manifest.assert_called_once()

    assert "Manifest created for dataset_20250306.csv" in caplog.text



================================================
FILE: tests/test_state_manager.py
================================================
import json
import os
import tempfile
import pytest
from datetime import datetime, timedelta
from state_manager import StateManager


@pytest.fixture
def temp_state_dir():
    """Creates a temporary directory to simulate state storage."""
    with tempfile.TemporaryDirectory() as temp_dir:
        os.makedirs(os.path.join(temp_dir, "in"), exist_ok=True)
        os.makedirs(os.path.join(temp_dir, "out"), exist_ok=True)
        yield temp_dir


def test_load_state_valid_file(temp_state_dir):
    """Test loading state when a valid state.json file exists."""
    state_file = os.path.join(temp_state_dir, "in", "state.json")
    state_data = {"last_processed_date": "2025-03-05"}

    with open(state_file, "w") as f:
        json.dump(state_data, f)

    state_manager = StateManager(temp_state_dir)
    loaded_state = state_manager.load_state()

    assert loaded_state == state_data


def test_load_state_no_file(temp_state_dir):
    """Test loading state when no state.json file exists."""
    state_manager = StateManager(temp_state_dir)
    loaded_state = state_manager.load_state()

    assert loaded_state == {}


def test_load_state_corrupt_file(temp_state_dir):
    """Test handling of a corrupted state.json file."""
    state_file = os.path.join(temp_state_dir, "in", "state.json")

    with open(state_file, "w") as f:
        f.write("{invalid json}")

    state_manager = StateManager(temp_state_dir)
    loaded_state = state_manager.load_state()

    assert loaded_state == {}


def test_get_last_processed_date_valid(temp_state_dir):
    """Test retrieving last processed date when a valid date is stored."""
    state_file = os.path.join(temp_state_dir, "in", "state.json")
    last_date = "2025-03-05"

    with open(state_file, "w") as f:
        json.dump({"last_processed_date": last_date}, f)

    state_manager = StateManager(temp_state_dir)
    expected_date = str(datetime.strptime(last_date, "%Y-%m-%d").date() - timedelta(days=1))

    assert state_manager.get_last_processed_date() == expected_date


def test_get_last_processed_date_missing(temp_state_dir):
    """Test retrieving last processed date when it's missing from state."""
    state_manager = StateManager(temp_state_dir)

    assert state_manager.get_last_processed_date() is None


def test_get_last_processed_date_invalid_format(temp_state_dir):
    """Test handling of invalid date format in state.json."""
    state_file = os.path.join(temp_state_dir, "in", "state.json")

    with open(state_file, "w") as f:
        json.dump({"last_processed_date": "invalid-date"}, f)

    state_manager = StateManager(temp_state_dir)

    assert state_manager.get_last_processed_date() is None


def test_save_state(temp_state_dir):
    """Test saving the last processed date."""
    state_manager = StateManager(temp_state_dir)
    last_date = "2025-03-06"

    StateManager.save_state(last_date, temp_state_dir)

    state_file = os.path.join(temp_state_dir, "out", "state.json")

    assert os.path.exists(state_file)

    with open(state_file, "r") as f:
        saved_state = json.load(f)

    assert saved_state == {"last_processed_date": last_date}



================================================
FILE: tests/test_utils.py
================================================
import pytest
import logging
from utils import (
    mask_sensitive_data_in_body,
    MaskSensitiveDataFilter,
    granularity_to_short_code,
    granularity_to_filename,
    convert_date_to_mmddyyyyhhmm,
    generate_periods,
    format_datetime,
    generate_logon_request,
    generate_xexport_request
)
from configuration import GranularityEnum


@pytest.mark.parametrize("input_text,expected_output", [
    ("<username>admin</username>", "<username>a*****</username>"),
    ("<password>secret123</password>", "<password>s*********</password>"),
    ("<exuziv>test</exuziv>", "<exuziv>t****</exuziv>"),
    ("<exklic>key</exklic>", "<exklic>k***</exklic>"),
])
def test_mask_sensitive_data(input_text, expected_output):
    assert mask_sensitive_data_in_body(input_text) == expected_output


def test_logging_filter():
    filter = MaskSensitiveDataFilter()
    log_record = logging.LogRecord(
        name="test", level=logging.INFO, pathname="", lineno=0, msg="<password>mysecret</password>", args=(), exc_info=None
    )
    filter.filter(log_record)
    assert log_record.msg == "<password>m********</password>"


@pytest.mark.parametrize("granularity,expected", [
    (GranularityEnum.year, "r"),
    (GranularityEnum.quarterYear, "v"),
    (GranularityEnum.month, "m"),
    (GranularityEnum.day, "d"),
    (GranularityEnum.hour, "h"),
    (GranularityEnum.quarterHour, "c"),
    (GranularityEnum.minute, "t"),
])
def test_granularity_to_short_code(granularity, expected):
    assert granularity_to_short_code(granularity) == expected


@pytest.mark.parametrize("granularity,expected", [
    (GranularityEnum.year, "year"),
    (GranularityEnum.quarterYear, "quarter_year"),
    (GranularityEnum.month, "month"),
    (GranularityEnum.day, "day"),
    (GranularityEnum.hour, "hour"),
    (GranularityEnum.quarterHour, "quarter_hour"),
    (GranularityEnum.minute, "minute"),
])
def test_granularity_to_filename(granularity, expected):
    assert granularity_to_filename(granularity) == expected


@pytest.mark.parametrize("date_input,expected_output", [
    ("2025-03-01", "030120250000"),
    ("1999-12-31", "123119990000"),
])
def test_convert_date_to_mmddyyyyhhmm(date_input, expected_output):
    assert convert_date_to_mmddyyyyhhmm(date_input) == expected_output


@pytest.mark.parametrize("granularity,start,end,expected_count", [
    (GranularityEnum.year, "2020-01-01", "2023-01-01", 4),
    (GranularityEnum.quarterYear, "2022-01-01", "2023-01-01", 5),
    (GranularityEnum.month, "2023-01-01", "2023-06-01", 6),
    (GranularityEnum.day, "2023-06-01", "2023-06-07", 7),
    (GranularityEnum.hour, "2023-06-01", "2023-06-01", 1),
])
def test_generate_periods(granularity, start, end, expected_count):
    periods = list(generate_periods(granularity, start, end))
    assert len(periods) == expected_count


@pytest.mark.parametrize("value,granularity,expected", [
    ("2025", GranularityEnum.year, "2025"),
    ("III/2025", GranularityEnum.quarterYear, "Q3/2025"),
    ("06.03.2025", GranularityEnum.day, "2025-03-06"),
    ("06.03.2025 08-09", GranularityEnum.hour, "2025-03-06 08:00"),
    ("06.03.2025 08:00-09:00", GranularityEnum.hour, "2025-03-06 08:00"),
    ("06.03.2025 08:00-15", GranularityEnum.quarterHour, "2025-03-06 08:00"),
    ("06.03.2025 08:00-01", GranularityEnum.minute, "2025-03-06 08:00"),
    ("06.03.2025 08:00-08:01", GranularityEnum.minute, "2025-03-06 08:00"),
])
def test_format_datetime(value, granularity, expected):
    assert format_datetime(value, granularity) == expected


def test_generate_logon_request():
    body, headers = generate_logon_request("user", "pass")
    assert "<username>user</username>" in body
    assert "<password>pass</password>" in body
    assert headers["Content-Type"] == "text/xml; charset=utf-8"
    assert headers["SOAPAction"] == "logonex"


def test_generate_xexport_request():
    body, headers = generate_xexport_request("user", "key", [1, 2, 3], "010120250000", "020120250000", "d")
    assert "<exuziv>user</exuziv>" in body
    assert "<exklic>key</exklic>" in body
    assert "<uzel>1,2,3</uzel>" in body
    assert "<cas>010120250000,020120250000</cas>" in body
    assert "<per>d</per>" in body
    assert headers["Content-Type"] == "text/xml; charset=utf-8"
    assert headers["SOAPAction"] == "xexport"



================================================
FILE: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on:
  push:  # skip the workflow on the main branch without tags
    branches-ignore:
      - main
    tags:
      - "*"

concurrency: ci-${{ github.ref }}  # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: "keboola.ex-energis"
  KBC_DEVELOPERPORTAL_VENDOR: "keboola"
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: ${{ vars.KBC_DEVELOPERPORTAL_USERNAME }}

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (Optional) Test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: "" # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }} # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo ${raw/*origin\//} | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh


