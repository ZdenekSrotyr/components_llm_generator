Directory structure:
└── keboola-component-zendesk/
    ├── README.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   ├── sourceCodeUrl.md
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run_kbc_tests.ps1
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── configuration.py
    │   └── dlt_zendesk/
    │       ├── __init__.py
    │       ├── zendesk_mapping.py
    │       ├── zendesk_objects.py
    │       └── helpers/
    │           ├── api_client.py
    │           └── credentials.py
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── .github/
        └── workflows/
            └── push.yml

================================================
FILE: README.md
================================================
# Zendesk Extractor

Zendesk is a customer service platform that helps businesses manage customer interactions across multiple channels. This component enables you to extract your Zendesk data into Keboola for analysis and reporting.

## Overview

This component enables you to extract various types of data from your Zendesk instance, including tickets, users, organizations, and custom fields. It supports both full and incremental data loading.

## Features

- Extracts data from multiple Zendesk endpoints
- Supports incremental loading for most tables
- Handles authentication via API token
- Provides detailed logging and debugging options
- Supports extraction of ticket details (comments and audits)

## Prerequisites

- Zendesk account with API access
- [API token](https://support.zendesk.com/hc/en-us/articles/4408889192858-Generating-a-new-API-token)

## Supported Endpoints

- `/api/v2/users.json`
- `/api/v2/groups.json`
- `/api/v2/group_memberships.json`
- `/api/v2/organizations.json`
- `/api/v2/tags.json`
- `/api/v2/ticket_fields.json`
- `/api/v2/incremental/tickets.json`
- `/api/v2/tickets/{ticket['id']}/comments.json`
- `/api/v2/tickets/{ticket['id']}/audits.json`

> Need more endpoints? Submit your request to [ideas.keboola.com](https://ideas.keboola.com/)

### Configuration

#### Authentication
```json
{
    "email": "your@email.com",
    "#api_token": "your_api_token",
    "sub_domain": "your_subdomain"
}
```

#### Sync Options
- **Full Sync**: Downloads all data from the source every run
- **Incremental Sync**: Downloads data (tickets, ticket_comments, ticket_audits) based on the `start_time` parameter. The start time is taken from the last successful run.

#### Destination
- **Full Load**: Destination table is overwritten every run
- **Incremental Load**: Data is upserted into the destination table. Tables with primary keys will have rows updated, tables without primary keys will have rows appended.

#### Available Details
- Comments
- Audits

> Note: Loading details has an impact on performance as they are loaded per ticket.

#### Debug
Enable detailed logging for troubleshooting purposes.

## Limitations

- API rate limits apply (see [Zendesk API documentation](https://developer.zendesk.com/api-reference/))

## Development

### Local Setup
1. Clone the repository:
```bash
git clone git@github.com:keboola/component-zendesk.git keboola.ex-zendesk-v2
cd keboola.ex-zendesk-v2
```

2. Configure local data folder in `docker-compose.yml`:
```yaml
volumes:
  - ./:/code
  - ./CUSTOM_FOLDER:/data
```

3. Build and run:
```bash
docker-compose build
docker-compose run --rm dev
```

### Testing
Run the test suite and lint check:
```bash
docker-compose run --rm test
```

## Integration

For deployment and integration with KBC, refer to the [deployment documentation](https://developers.keboola.com/extend/component/deployment/).

## Resources

- [Zendesk API Documentation](https://developer.zendesk.com/api-reference)
- [Keboola Documentation](https://help.keboola.com/)
- [Component Support](https://support.keboola.com/)



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "assigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "assigning travis tag"
	export TAG="$TRAVIS_TAG"
elif [ ! -z ${GITHUB_TAG} ]
then
	echo "assigning github tag"
	export TAG="$GITHUB_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.12-slim
ENV PYTHONIOENCODING utf-8

COPY /src /code/src/
COPY /tests /code/tests/
COPY /scripts /code/scripts/
COPY requirements.txt /code/requirements.txt
COPY flake8.cfg /code/flake8.cfg
COPY deploy.sh /code/deploy.sh

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/

CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: requirements.txt
================================================
keboola.component==1.6.6
keboola.utils==1.1.0
freezegun~=1.5.1
mock~=5.1.0
pydantic~=2.8.2
psutil~=5.8.0
dlt~=0.5.2
dlt[duckdb]
duckdb~=0.10.3
kbcstorage~=0.9.1


================================================
FILE: component_config/component_long_description.md
================================================
This component enables you to extract data from your Zendesk instance into Keboola. You can extract various types of data including tickets, users, organizations, and custom fields.

To use this component, you need to provide:
- Zendesk subdomain
- API token and email for authentication

The component supports incremental loading for most tables.

Available resources:
- [Zendesk API Documentation](https://developer.zendesk.com/api-reference)


================================================
FILE: component_config/component_short_description.md
================================================
Zendesk is a customer service platform that helps businesses manage customer interactions across multiple channels.


================================================
FILE: component_config/configRowSchema.json
================================================
{}


================================================
FILE: component_config/configSchema.json
================================================
{
    "type": "object",
    "title": "extractor configuration",
    "required": [
        "authentication"
    ],
    "properties": {
        "authentication": {
            "type": "object",
            "title": "Authorization",
            "required": [
                "email",
                "#api_token",
                "sub_domain"
            ],
            "propertyOrder": 10,
            "properties": {
                "email": {
                    "type": "string",
                    "title": "Email",
                    "propertyOrder": 10
                },
                "#api_token": {
                    "type": "string",
                    "title": "API token",
                    "description": "<a href='https://support.zendesk.com/hc/en-us/articles/4408889192858-Generating-a-new-API-token'>How to obtain API token</a>",
                    "format": "password",
                    "propertyOrder": 20
                },
                "sub_domain": {
                    "type": "string",
                    "title": "Sub-domain",
                    "description": "Sub-domain is a part of your Zendesk URL. For example: https://mysubdomain.zendesk.com.",
                    "propertyOrder": 30
                }
            }
        },
        "available_details": {
            "type": "object",
            "title": "Available Details",
            "description": "Choose details of tickets which will be loaded also. Details are loaded per ticket. It has an impact on performance.",
            "propertyOrder": 15,
            "properties": {
                "ticket_comments_raw": {
                    "type": "boolean",
                    "title": "Comments",
                    "default": true,
                    "format": "checkbox",
                    "propertyOrder": 10
                },
                "ticket_audits_raw": {
                    "type": "boolean",
                    "title": "Audits",
                    "default": true,
                    "format": "checkbox",
                    "propertyOrder": 20
                }
            }
        },
        "sync_options": {
            "type": "object",
            "title": "Sync Options",
            "propertyOrder": 20,
            "properties": {
                "sync_mode": {
                    "enum": [
                        "full_sync",
                        "incremental_sync"
                    ],
                    "type": "string",
                    "title": "Sync Mode",
                    "default": "incremental_sync",
                    "options": {
                        "enum_titles": [
                            "Full Sync",
                            "Incremental Sync"
                        ]
                    },
                    "required": true,
                    "description": "Full Sync downloads all data from the source every run, Incremental Sync downloads data (tickets, ticket_comments and ticket_audits) by parameter start_time described <a href='https://developer.zendesk.com/api-reference/ticketing/ticket-management/incremental_exports/#per_page'>here</a>. The start time is taken from the last successful run.",
                    "propertyOrder": 10
                },
                "date_from": {
                    "propertyOrder": 20,
                    "type": "string",
                    "title": "From date",
                    "description": "Date from. Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc. If left empty, all records from 2000-01-01 are downloaded.",
                    "options": {
                        "dependencies": {
                            "sync_mode": "incremental_sync"
                        }
                    }
                }
            }
        },
        "destination": {
            "type": "object",
            "title": "Destination",
            "propertyOrder": 30,
            "required": [
                "load_type"
            ],
            "properties": {
                "load_type": {
                    "enum": [
                        "full_load",
                        "incremental_load"
                    ],
                    "type": "string",
                    "title": "Load Type",
                    "format": "checkbox",
                    "default": "incremental_load",
                    "options": {
                        "enum_titles": [
                            "Full Load",
                            "Incremental Load"
                        ]
                    },
                    "description": "If Full load is used, the destination table will be overwritten every run. If incremental load is used, data will be upserted into the destination table. Tables with a primary key will have rows updated, tables without a primary key will have rows appended.",
                    "propertyOrder": 10
                },
                "custom_bucket": {
                    "type": "boolean",
                    "title": "Custom output bucket",
                    "format": "checkbox",
                    "description": "By default bucket will be created",
                    "propertyOrder": 20
                },
                "destination_bucket": {
                    "type": "string",
                    "title": "Destination bucket",
                    "description": "Choice of destination bucket",
                    "enum": [],
                    "items": {
                        "enum": [],
                        "type": "string"
                    },
                    "format": "select",
                    "options": {
                        "async": {
                            "label": "Load available buckets",
                            "action": "get_buckets",
                            "autoload": [
                                "parameters.custom_bucket"
                            ]
                        },
                        "dependencies": {
                            "custom_bucket": true
                        }
                    },
                    "uniqueItems": true,
                    "propertyOrder": 30
                }
            }
        },
        "debug": {
            "type": "boolean",
            "title": "Debug",
            "format": "checkbox",
            "propertyOrder": 90
        }
    }
}



================================================
FILE: component_config/configuration_description.md
================================================
Configuration description.


================================================
FILE: component_config/documentationUrl.md
================================================
https://github.com/keboola/component-zendesk/blob/master/README.md


================================================
FILE: component_config/licenseUrl.md
================================================
https://github.com/keboola/component-zendesk/blob/master/LICENSE.md


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sourceCodeUrl.md
================================================
https://github.com/keboola/component-zendesk


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}



================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi


================================================
FILE: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"


================================================
FILE: src/component.py
================================================
import os
import logging
from collections import OrderedDict
from typing import List

import dateparser
import dlt
from dlt.common import pendulum
from dlt.common.time import ensure_pendulum_datetime
from duckdb import duckdb
from keboola.component.base import ComponentBase, sync_action
from keboola.component.dao import ColumnDefinition, BaseType, SupportedDataTypes
from keboola.component.exceptions import UserException
from keboola.component.sync_actions import SelectElement
from kbcstorage.client import Client

from configuration import Configuration

from dlt_zendesk import zendesk_support, zendesk_mapping

DLT_TMP_DIR = "/tmp/.dlt"
DUCKDB_TMP_DIR = "/tmp/.dlt"
DATASET_NAME = "zendesk_data"
PIPELINE_NAME = "dlt_zendesk_pipeline"

DEFAULT_START_DATE: int = pendulum.datetime(year=2000, month=1, day=1).int_timestamp


class Component(ComponentBase):
    def __init__(self):
        super().__init__()
        self.params = None
        self.pipeline_destination = None
        self.connection = None
        self.pipeline_name = None
        self.dataset_name = None

    def run(self):
        """
        Main execution code
        """
        self.params = Configuration(**self.configuration.parameters)

        # create the actual start time here for elimination possible data gaps
        actual_start = pendulum.now().int_timestamp

        # get the previous start time
        if self.params.sync_options.is_incremental:
            previous_start = self._def_start_timestamp()
            logging.info("Incremental mode")
        else:
            previous_start = DEFAULT_START_DATE
            logging.info("Full sync mode load is disabled")
        load_from_iso: int = ensure_pendulum_datetime(previous_start).int_timestamp
        logging.info(f"Loading data from {pendulum.from_timestamp(load_from_iso)}")

        # set the DLT environment
        self._set_dlt()

        # run the pipeline
        loaded_tables = self._run_dlt_pipeline(load_from_iso)

        # initialize the connection
        self._init_connection(duck_db_file=self.duckdb_file)

        # prepare the views
        views_to_export = self._prepare_views(loaded_tables)

        # export views to the CSV
        self._export_views(views_to_export)

        # save the state
        logging.info(f"Saving the state file with the actual start date {actual_start}")
        self.write_state_file({"time": {"previousStart": actual_start}})

    def _def_start_timestamp(self):
        if self.params.sync_options.date_from:
            return self._parse_date(self.params.sync_options.date_from)
        else:
            return int(self.get_state_file().get("time", {}).get("previousStart", DEFAULT_START_DATE))

    @staticmethod
    def _parse_date(date_to_parse: str) -> int:
        try:
            parsed_date = dateparser.parse(date_to_parse)
            if parsed_date.tzinfo is None:
                parsed_date = parsed_date.replace(tzinfo=pendulum.UTC)
            return int(parsed_date.timestamp())
        except (AttributeError, TypeError) as err:
            raise UserException(f"Failed to parse date {date_to_parse}, make sure the date is either in YYYY-MM-DD "
                                f"format or relative date i.e. 5 days ago, 1 month ago, yesterday, etc.") from err

    def _set_dlt(self):
        # prepare the temporary directories
        os.makedirs(DLT_TMP_DIR, exist_ok=True)
        os.makedirs(DUCKDB_TMP_DIR, exist_ok=True)

        # set the environment variables
        os.environ["DLT_DATA_DIR"] = DLT_TMP_DIR
        os.environ["RUNTIME__DLTHUB_TELEMETRY"] = "false"
        os.environ["RUNTIME__LOG_LEVEL"] = "DEBUG" if self.params.debug else "CRITICAL"
        os.environ["SOURCES__CREDENTIALS__SUBDOMAIN"] = self.params.authentication.sub_domain
        os.environ["SOURCES__CREDENTIALS__EMAIL"] = self.params.authentication.email
        os.environ["SOURCES__CREDENTIALS__TOKEN"] = self.params.authentication.api_token
        os.environ["EXTRACT__WORKERS"] = "40"
        os.environ["EXTRACT__MAX_PARALLEL_ITEMS"] = "100"
        os.environ["NORMALIZE__WORKERS"] = "40"
        os.environ["LOAD__WORKERS"] = "40"

        # set the dataset and pipeline names
        self.dataset_name = DATASET_NAME
        self.pipeline_name = PIPELINE_NAME
        self.duckdb_file = f"{DUCKDB_TMP_DIR}/{self.pipeline_name}.duckdb"

        # check if the duckdb file exists delete it - especially for the local run
        if os.path.exists(self.duckdb_file):
            os.remove(self.duckdb_file)
        # set the duckdb connection
        config = dict(threads="6",
                      memory_limit="1024MB",
                      max_memory="1024MB")

        conn = duckdb.connect(self.duckdb_file, config=config)
        self.pipeline_destination = dlt.destinations.duckdb(conn)

    def _run_dlt_pipeline(self, start_date_iso) -> list:
        # prepare the pipeline
        logging.info("Preparing DLT pipeline")
        pipeline = dlt.pipeline(
            pipeline_name=self.pipeline_name,
            destination=self.pipeline_destination,
            dataset_name=self.dataset_name,
            progress="log",

        )

        # filter the source by selected details
        logging.info("Filtering the source by selected details")
        source = zendesk_support(start_date_iso)
        for key, value in self.params.available_details.dict().items():
            source.resources[key].selected = value

        # run the pipeline
        logging.info("Running the DLT pipeline")
        pipeline = pipeline.run(source, refresh="drop_sources")
        logging.info("Pipeline finished")
        pipeline.raise_on_failed_jobs()

        # get the loaded tables
        logging.debug("Getting the loaded tables")
        loaded_tables = []
        for package in pipeline.load_packages:
            jobs = package.jobs.get("completed_jobs", [])
            for job in jobs:
                table = job.job_file_info.table_name
                if not table.startswith("dlt_"):
                    loaded_tables.append(table)

        return loaded_tables

    def _prepare_views(self, loaded_tables):
        logging.info("Preparing output views")
        # set the database
        self.connection.execute(f"USE {self.dataset_name};")

        # create output views
        prepared_views = []
        for view in zendesk_mapping.views:
            # if is created all tables for view
            if all(t in loaded_tables for t in view.source_tables):
                logging.info(f"Creating output view {view.name}")
                self.connection.execute(view.query)
                prepared_views.append(view)
            else:
                logging.info(f"View {view.name} was not created due to missing tables probably due to an filter ")
        return prepared_views

    def _export_views(self, views):
        logging.info("Exporting views to CSV")
        for view in views:
            # get the schema of the view
            table_meta = self.connection.execute(f"""DESCRIBE {view.name};""").fetchall()
            schema = OrderedDict(
                (c[0], ColumnDefinition(data_types=BaseType(dtype=self.convert_base_types(c[1])))) for c in table_meta)

            # prepare the out table
            out_table = self.create_out_table_definition(f"{view.name}.csv",
                                                         schema=schema,
                                                         primary_key=view.primary_key,
                                                         incremental=self.params.destination.is_incremental_load_type,
                                                         destination=".".join(
                                                             filter(None, [self.params.destination.destination_bucket,
                                                                           view.name])),
                                                         has_header=True,
                                                         )
            # export the view
            logging.info(f"Exporting view {view.name}")
            try:
                # ../data/out/tables/{view.name}.csv
                export_query = f"""COPY '{view.name}' TO '{out_table.full_path}'
                                                (HEADER false, DELIMITER ',', FORCE_QUOTE *)"""
                self.connection.execute(export_query)
            except duckdb.ConversionException as e:
                raise Exception(f"Error during query execution: {e}")

            # write the manifest
            self.write_manifest(out_table)

    @staticmethod
    def convert_base_types(dtype: str) -> SupportedDataTypes:
        if dtype in ['TINYINT', 'SMALLINT', 'INTEGER', 'BIGINT', 'HUGEINT',
                     'UTINYINT', 'USMALLINT', 'UINTEGER', 'UBIGINT', 'UHUGEINT']:
            return SupportedDataTypes.INTEGER
        elif dtype in ['REAL', 'DECIMAL']:
            return SupportedDataTypes.NUMERIC
        elif dtype == 'DOUBLE':
            return SupportedDataTypes.FLOAT
        elif dtype == 'BOOLEAN':
            return SupportedDataTypes.BOOLEAN
        elif dtype in ['TIMESTAMP', 'TIMESTAMP WITH TIME ZONE']:
            return SupportedDataTypes.TIMESTAMP
        elif dtype == 'DATE':
            return SupportedDataTypes.DATE
        else:
            return SupportedDataTypes.STRING

    def _init_connection(self, duck_db_file):
        logging.debug(f"Initializing connection to DuckDB database {duck_db_file}")
        self.connection = duckdb.connect(duck_db_file)

    def _get_kbc_root_url(self):
        return f'https://{self.environment_variables.stack_id}'

    def _get_storage_token(self) -> str:
        return self.configuration.parameters.get('#storage_token') or self.environment_variables.token

    @sync_action('get_buckets')
    def get_buckets(self) -> List[SelectElement]:
        """
        Sync action for getting list of available buckets
        Returns:
            List[SelectElement]: List of available buckets
        """
        sapi_client = Client(self._get_kbc_root_url(), self._get_storage_token())

        buckets = sapi_client.buckets.list()
        return [SelectElement(value=b['id'], label=f'{b["id"]} ({b["name"]})') for b in buckets]


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)



================================================
FILE: src/configuration.py
================================================
import logging

from pydantic import BaseModel, Field, ValidationError, computed_field
from keboola.component.exceptions import UserException


class Authentication(BaseModel):
    email: str = Field()
    api_token: str = Field(alias="#api_token")
    sub_domain: str = Field()


class SyncOptions(BaseModel):
    sync_mode: str
    date_from: str = Field(default=None)

    @computed_field
    def is_incremental(self) -> bool:
        return self.sync_mode == "incremental_sync"


class Destination(BaseModel):
    load_type: str
    destination_bucket: str = Field(default=None)

    @computed_field
    def is_incremental_load_type(self) -> bool:
        return self.load_type == "incremental_load"


class AvailableDetails(BaseModel):
    ticket_comments_raw: bool
    ticket_audits_raw: bool


class Configuration(BaseModel):
    authentication: Authentication
    sync_options: SyncOptions
    destination: Destination
    available_details: AvailableDetails
    debug: bool = Field(default=False)

    def __init__(self, **data):
        try:
            super().__init__(**data)
        except ValidationError as e:
            error_messages = [f"{err['loc'][0]}: {err['msg']}" for err in e.errors()]
            raise UserException(f"Validation Error: {', '.join(error_messages)}")

        if self.debug:
            logging.debug("Component will run in Debug mode")



================================================
FILE: src/dlt_zendesk/__init__.py
================================================
import logging
from typing import Iterable, Iterator

import dlt
import pendulum

from dlt.common.typing import TDataItem
from dlt.sources import DltResource

from .helpers.credentials import TZendeskCredentials
from .helpers.api_client import ZendeskAPIClient, PaginationType

from .zendesk_objects import (Tags, Tickets, TicketComments, TicketAudits, Users, Groups, GroupMembership,
                              Organizations,
                              TicketsFields)


@dlt.source(max_table_nesting=0)
def zendesk_support(start_date_iso: int, credentials: TZendeskCredentials = dlt.secrets.value) \
        -> Iterable[DltResource]:
    supported_endpoints = [
        ("groups", "/api/v2/groups.json", Groups),
        ("group_memberships", "/api/v2/group_memberships.json", GroupMembership),
        ("tags", "/api/v2/tags.json", Tags),
        ("ticket_fields", "/api/v2/ticket_fields.json", TicketsFields),
    ]

    @dlt.resource(name="organizations_raw", parallelized=True, columns=Organizations, write_disposition="replace")
    def organizations() -> Iterator[TDataItem]:
        dt = pendulum.from_timestamp(start_date_iso)
        from_date = dt.format('YYYY-MM-DD')

        logging.info("Loading Organizations")
        user_pages = zendesk_client.get_pages(
            "/api/v2/search.json",
            "results",
            PaginationType.OFFSET,
            params={"query": f"type:organization updated>{from_date}"},
        )
        yield from user_pages

    @dlt.resource(name="users_raw", parallelized=True, columns=Users, write_disposition="replace")
    def users() -> Iterator[TDataItem]:
        dt = pendulum.from_timestamp(start_date_iso)
        from_date = dt.format('YYYY-MM-DD')

        logging.info("Loading users")
        user_pages = zendesk_client.get_pages(
            "/api/v2/users/search.json",
            "users",
            PaginationType.OFFSET,
            params={"query": f"type:user updated>{from_date}"},
        )
        yield from user_pages

    @dlt.resource(name="tickets_raw", parallelized=True, columns=Tickets, write_disposition="replace")
    def ticket_table() -> Iterator[TDataItem]:
        logging.info("Loading tickets")
        ticket_pages = zendesk_client.get_pages(
            "/api/v2/incremental/tickets.json",
            "tickets",
            PaginationType.STREAM,
            params={"include": "metric_sets,comment_count",
                    "start_time": start_date_iso},
        )
        yield from ticket_pages

    @dlt.transformer(name="ticket_comments_raw", parallelized=True, columns=TicketComments)
    def ticket_comments(tickets: Iterator[TDataItem]):
        logging.info("Loading ticket comments")
        for ticket in tickets:
            # try if page not found write to log
            try:
                comment_pages = zendesk_client.get_pages(
                    f"/api/v2/tickets/{ticket['id']}/comments.json",
                    "comments",
                    PaginationType.CURSOR,
                )
                for comments in comment_pages:
                    yield [dict(ticket_id=ticket['id'], **i) for i in comments]
            except Exception as e:
                logging.warning(f"Ticket {ticket['id']} comments not found {e}")

    @dlt.transformer(name="ticket_audits_raw", parallelized=True, columns=TicketAudits)
    def ticket_audits(tickets: Iterator[TDataItem]):
        logging.info("Loading ticket audits")
        for ticket in tickets:
            try:
                audit_pages = zendesk_client.get_pages(
                    f"/api/v2/tickets/{ticket['id']}/audits.json",
                    "audits",
                    PaginationType.CURSOR,
                )
                for audits in audit_pages:
                    yield audits
            except Exception as e:
                logging.warning(f"Ticket {ticket['id']} audits not found {e}")

    # Authenticate
    zendesk_client = ZendeskAPIClient(credentials)

    # loading base tables
    resource_list = [
        organizations,
        users,
        ticket_table,
        ticket_table | ticket_comments,
        ticket_table | ticket_audits
    ]
    # other tables to be loaded
    for resource, endpoint_url, columns in list(supported_endpoints):
        resource_list.append(
            dlt.resource(_basic_resource(zendesk_client, endpoint_url, resource), name=f"{resource}_raw",
                         columns=columns,
                         parallelized=True, )
        )
    return resource_list


def _basic_resource(zendesk_client: ZendeskAPIClient, endpoint_url: str, data_key: str) -> \
        Iterator[TDataItem]:
    logging.info(f"Loading {data_key}")
    pages = zendesk_client.get_pages(
        endpoint_url,
        data_key,
        PaginationType.CURSOR,
    )
    yield from pages



================================================
FILE: src/dlt_zendesk/zendesk_mapping.py
================================================
class TableMapping:
    def __init__(self, name: str, query: str, source_tables: list, primary_key: list = None):
        self.name = name
        self.query = query
        self.primary_key = primary_key
        self.source_tables = source_tables


views = {
    TableMapping("tags", """
        CREATE VIEW tags AS
        SELECT
            name AS name,
            count AS count
        FROM
            tags_raw;
    """, ["tags_raw"], ["name", "count"], ),

    TableMapping("users", """
        CREATE VIEW users AS
        SELECT
            id AS id,
            name AS name,
            email AS email,
            url AS url,
            deleted AS deleted,
            created_at AS created_at,
            updated_at AS updated_at,
            time_zone AS time_zone,
            phone AS phone,
            locale_id AS locale_id,
            locale AS locale,
            organization_id AS organizations_pk,
            role AS role,
            verified AS verified,
            external_id AS external_id,
            alias AS alias,
            active AS active,
            shared AS shared,
            shared_agent AS shared_agent,
            last_login_at AS last_login_at,
            two_factor_auth_enabled AS two_factor_auth_enabled,
            signature AS signature,
            details AS details,
            notes AS notes,
            custom_role_id AS custom_role_id,
            moderator AS moderator,
            ticket_restriction AS ticket_restriction,
            only_private_comments AS only_private_comments,
            restricted_agent AS restricted_agent,
            suspended AS suspended,
            chat_only AS chat_only,
            tags AS tags
        FROM
            users_raw;
    """, ["users_raw"], ["id"]),

    TableMapping("users_photos", """
        CREATE VIEW users_photos AS
        SELECT
            id AS users_pk,
            json(photo).id AS id,
            json_extract_string(photo,'$.file_name') AS file_name,
            json_extract_string(photo,'$.content_url') AS content_url,
            json_extract_string(photo,'$.content_type') AS content_type,
            json(photo).size AS size,
            json(photo).width AS width,
            json(photo).height AS height,
            json(photo).inline AS inline
        FROM
            users_raw
        WHERE json(photo) IS NOT NULL;
    """, ["users_raw"], ["id"]),

    TableMapping("users_groups", """
        CREATE VIEW users_groups AS
        SELECT
            user_id AS users_pk,
            group_id AS groups_pk,
            "default" AS "default",
            created_at AS created_at,
            updated_at AS updated_at
        FROM
            group_memberships_raw;
    """, ["group_memberships_raw"], ["users_pk", "groups_pk"]),

    TableMapping("groups", """
        CREATE VIEW groups AS
        SELECT
            id AS id,
            name AS name,
            url AS url,
            deleted AS deleted,
            created_at AS created_at,
            updated_at AS updated_at
        FROM
            groups_raw;
    """, ["groups_raw"], ["id"]),

    TableMapping("organizations", """
        CREATE VIEW organizations AS
        SELECT
            id AS id,
            name AS name,
            url AS url,
            shared_tickets AS shared_tickets,
            shared_comments AS shared_comments,
            external_id AS external_id,
            created_at AS created_at,
            updated_at AS updated_at,
            details AS details,
            notes AS notes,
            group_id AS groups_pk,
            tags AS tags
        FROM
            organizations_raw;
    """, ["organizations_raw"], ["id"]),

    TableMapping("organizations_domain_names", """
        CREATE VIEW organizations_domain_names AS
        SELECT
            id AS organizations_pk,
            unnest(domain_names->>'$[*]') AS "domain"
        FROM
            organizations_raw
        WHERE domain_names IS NOT NULL;
    """, ["organizations_raw"], ["organizations_pk", "domain"]),

    TableMapping("tickets", """
        CREATE VIEW tickets AS
        SELECT
            id AS id,
            url AS url,
            external_id AS external_id,
            type AS type,
            subject AS subject,
            priority AS priority,
            status AS status,
            recipient AS recipient,
            requester_id AS requester_users_pk,
            submitter_id AS submitter_users_pk,
            assignee_id AS assignee_users_pk,
            organization_id AS organizations_pk,
            group_id AS groups_pk,
            forum_topic_id AS topic_id,
            problem_id AS problem_id,
            has_incidents AS has_incidents,
            due_at AS due_at,
            json_extract_string(via,'$.channel') AS via_channel,
            ticket_form_id AS ticket_form_id,
            brand_id AS brand_pk,
            created_at AS created_at,
            updated_at AS updated_at,
            tags AS tags
        FROM
            tickets_raw;
    """, ["tickets_raw"], ["id"]),

    TableMapping("tickets_fields_values", """
        CREATE VIEW tickets_fields_values AS
        SELECT
            id AS tickets_pk,
            json_extract_string(cf.value,'$') AS value,
            cf.id AS tickets_fields_pk
        FROM (
        SELECT id, json(unnest(custom_fields->>'$[*]'))  cf
        FROM
            tickets_raw tr)
        WHERE cf.id IS NOT NULL ;
    """, ["tickets_raw"], ["tickets_fields_pk", "tickets_pk"]),

    TableMapping("tickets_ratings", """
        CREATE VIEW tickets_ratings AS
        SELECT
            id AS tickets_pk,
            json(satisfaction_rating).score AS score,
            json(satisfaction_rating).id AS id,
        FROM
            tickets_raw
        WHERE json(satisfaction_rating) IS NOT NULL;
    """, ["tickets_raw"], ["tickets_pk"]),

    TableMapping("tickets_metrics", """
        CREATE VIEW tickets_metrics AS
        SELECT
            id AS tickets_pk,
            json(metric_set).id AS id,
            json(metric_set).ticket_id AS ticket_id,
            json_extract_string(metric_set,'$.created_at') AS created_at,
            json_extract_string(metric_set,'$.updated_at') AS updated_at,
            json(metric_set).group_stations AS group_stations,
            json(metric_set).assignee_stations AS assignee_stations,
            json(metric_set).reopens AS reopens,
            json(metric_set).replies AS replies,
            json_extract_string(metric_set,'$.assignee_updated_at') AS assignee_updated_at,
            json_extract_string(metric_set,'$.requester_updated_at') AS requester_updated_at,
            json_extract_string(metric_set,'$.status_updated_at') AS status_updated_at,
            json_extract_string(metric_set,'$.initially_assigned_at') AS initially_assigned_at,
            json_extract_string(metric_set,'$.assigned_at') AS assigned_at,
            json_extract_string(metric_set,'$.solved_at') AS solved_at,
            json_extract_string(metric_set,'$.latest_comment_added_at') AS latest_comment_added_at,
            json(metric_set).reply_time_in_minutes_calendar AS reply_time_in_minutes_calendar,
            json(metric_set).reply_time_in_minutes_business AS reply_time_in_minutes_business,
            json(metric_set).first_resolution_time_in_minutes.calendar AS first_resolution_time_in_minutes_calendar,
            json(metric_set).first_resolution_time_in_minutes.business AS first_resolution_time_in_minutes_business,
            json(metric_set).full_resolution_time_in_minutes.calendar AS full_resolution_time_in_minutes_calendar,
            json(metric_set).full_resolution_time_in_minutes.business AS full_resolution_time_in_minutes_business,
            json(metric_set).agent_wait_time_in_minutes.calendar AS agent_wait_time_in_minutes_calendar,
            json(metric_set).agent_wait_time_in_minutes.business AS agent_wait_time_in_minutes_business,
            json(metric_set).requester_wait_time_in_minutes.calendar AS requester_wait_time_in_minutes_calendar,
            json(metric_set).requester_wait_time_in_minutes.business AS requester_wait_time_in_minutes_business,
            json(metric_set).on_hold_time_in_minutes.calendar AS on_hold_time_in_minutes_calendar,
            json(metric_set).on_hold_time_in_minutes.business AS on_hold_time_in_minutes_business
        FROM
            tickets_raw
        WHERE json(metric_set) IS NOT NULL;
    """, ["tickets_raw"], ["ticket_id", "id"]),

    TableMapping("tickets_fields", """
    CREATE VIEW tickets_fields AS
    SELECT
        id AS id,
        type AS type,
        title AS title,
        active AS active,
        tag AS tag
    FROM
        ticket_fields_raw;
""", ["ticket_fields_raw"], ["id"]),

    TableMapping("tickets_comments", """
        CREATE VIEW tickets_comments AS
        SELECT
            id AS id,
            type AS type,
            body AS body,
            "public" AS "public",
            author_id AS author_pk,
            audit_id AS tickets_audits_pk,
            json_extract_string(via,'$.channel') AS via_channel,
            json_extract_string(metadata,'$.system.ip_address') AS ip_address,
            created_at AS created_at,
            ticket_id AS tickets_pk
        FROM
            ticket_comments_raw;
    """, ["ticket_comments_raw"], ["id"]),

    TableMapping("tickets_comments_attachments", """
        CREATE VIEW tickets_comments_attachments AS
        SELECT
            id AS tickets_comments_pk,
            json_extract(att,'$.id') AS id,
            json_extract_string(att,'$.file_name') AS file_name,
            json_extract_string(att,'$.content_url') AS content_url,
            json_extract_string(att,'$.content_type') AS content_type,
            json_extract(att,'$.size') AS size,
            json_extract(att,'$.width') AS width,
            json_extract(att,'$.height') AS height,
            json_extract(att,'$.inline') AS inline
        FROM (
            SELECT id,
                   json(unnest(attachments->>'$[*]')) as att
            FROM ticket_comments_raw)
        WHERE att.id IS NOT NULL;
    """, ["ticket_comments_raw"], ["id"]),

    TableMapping("tickets_comments_attachments_thumbnails", """
        CREATE VIEW tickets_comments_attachments_thumbnails AS
        SELECT
            id AS tickets_comments_attachments_pk,
            json_extract(th,'$.id') AS id,
            json_extract_string(th,'$.file_name') AS file_name,
            json_extract_string(th,'$.content_url') AS content_url,
            json_extract_string(th,'$.content_type') AS content_type,
            json_extract(th,'$.size') AS size,
            json_extract(th,'$.width') AS width,
            json_extract(th,'$.height') AS height,
            json_extract(th,'$.inline') AS inline
        FROM
            (SELECT id,
                   json(unnest(json(att).thumbnails->>'$[*]')) as th
            FROM
               (SELECT id,
                       json(unnest(attachments->>'$[*]')) as att
                FROM ticket_comments_raw))
        WHERE th.id IS NOT NULL;
    """, ["ticket_comments_raw"], ["id"]),

    TableMapping("tickets_audits", """
        CREATE VIEW tickets_audits AS
        SELECT
            id AS id,
            ticket_id AS tickets_pk,
            created_at AS created_at,
            author_id AS author_pk,
            json_extract_string(via,'$.channel') AS via_channel
        FROM
            ticket_audits_raw;
    """, ["ticket_audits_raw"], ["id"])
}



================================================
FILE: src/dlt_zendesk/zendesk_objects.py
================================================
from typing import Optional

from dlt.common.libs.pydantic import BaseModel
from pydantic import Field


class Tags(BaseModel):
    name: Optional[str] = Field(default=None)
    count: Optional[int] = Field(default=None)


class TicketsFields(BaseModel):
    id: Optional[int]
    type: Optional[str] = Field(default=None)
    title: Optional[str] = Field(default=None)
    active: Optional[bool] = Field(default=None)
    tag: Optional[str] = Field(default=None)


class Users(BaseModel):
    id: int
    name: Optional[str] = Field(default=None)
    email: Optional[str] = Field(default=None)
    url: Optional[str] = Field(default=None)
    deleted: Optional[bool] = Field(default=None)
    created_at: Optional[str] = Field(default=None)
    updated_at: Optional[str] = Field(default=None)
    time_zone: Optional[str] = Field(default=None)
    phone: Optional[str] = Field(default=None)
    photo: Optional[object] = Field(default={})
    locale_id: Optional[int] = Field(default=None)
    locale: Optional[str] = Field(default=None)
    organization_id: Optional[int] = Field(default=None)
    role: Optional[str] = Field(default=None)
    role_type: Optional[int] = Field(default=None)
    verified: Optional[bool] = Field(default=None)
    external_id: Optional[str] = Field(default=None)
    alias: Optional[str] = Field(default=None)
    active: Optional[bool] = Field(default=None)
    shared: Optional[bool] = Field(default=None)
    shared_agent: Optional[bool] = Field(default=None)
    last_login_at: Optional[str] = Field(default=None)
    two_factor_auth_enabled: Optional[bool] = Field(default=None)
    signature: Optional[str] = Field(default=None)
    details: Optional[str] = Field(default=None)
    notes: Optional[str] = Field(default=None)
    custom_role_id: Optional[int] = Field(default=None)
    moderator: Optional[bool] = Field(default=None)
    ticket_restriction: Optional[str] = Field(default=None)
    only_private_comments: Optional[bool] = Field(default=None)
    restricted_agent: Optional[bool] = Field(default=None)
    suspended: Optional[bool] = Field(default=None)
    chat_only: Optional[bool] = Field(default=None)
    user_fields: Optional[object] = Field(default={})
    tags: Optional[list] = Field(default=[])


class GroupMembership(BaseModel):
    id: int
    default: Optional[bool]
    group_id: Optional[int]
    user_id: Optional[int]
    created_at: Optional[str]
    updated_at: Optional[str]


class Groups(BaseModel):
    id: int
    name: Optional[str]
    url: Optional[str]
    deleted: Optional[bool] = Field(default=None)
    created_at: Optional[str]
    updated_at: Optional[str]


class Organizations(BaseModel):
    id: int
    name: Optional[str]
    url: Optional[str]
    organization_fields: Optional[object]
    shared_tickets: Optional[bool]
    shared_comments: Optional[bool]
    external_id: Optional[str]
    created_at: Optional[str]
    updated_at: Optional[str]
    domain_names: Optional[list] = Field(default=[])
    details: Optional[str]
    notes: Optional[str]
    group_id: Optional[int]
    tags: Optional[list[str]]


class Tickets(BaseModel):
    id: int
    url: Optional[str] = Field(default=None)
    external_id: Optional[str] = Field(default=None)
    type: Optional[str] = Field(default=None)
    subject: Optional[str] = Field(default=None)
    priority: Optional[str] = Field(default=None)
    status: Optional[str] = Field(default=None)
    recipient: Optional[str] = Field(default=None)
    requester_id: Optional[int] = Field(default=None)
    submitter_id: Optional[int] = Field(default=None)
    assignee_id: Optional[int] = Field(default=None)
    organization_id: Optional[int] = Field(default=None)
    group_id: Optional[int] = Field(default=None)
    forum_topic_id: Optional[int] = Field(default=None)
    problem_id: Optional[int] = Field(default=None)
    has_incidents: Optional[bool] = Field(default=None)
    due_at: Optional[str] = Field(default=None)
    via: Optional[object] = Field(default={})
    custom_fields: Optional[list[object]] = Field(default=[])
    satisfaction_rating: Optional[object] = Field(default={})
    ticket_form_id: Optional[int] = Field(default=None)
    brand_id: Optional[int] = Field(default=None)
    created_at: Optional[str] = Field(default=None)
    updated_at: Optional[str] = Field(default=None)
    tags: Optional[list] = Field(default=[])
    metric_set: Optional[object] = Field(default={})


class TicketAudits(BaseModel):
    id: int
    ticket_id: int
    events: Optional[list[object]]
    author_id: Optional[int] = Field(default=None)
    created_at: Optional[str] = Field(default=None)
    metadata: Optional[object]
    via: Optional[object]


class TicketComments(BaseModel):
    id: int
    ticket_id: int
    audit_id: Optional[int] = Field(default=None)
    type: Optional[str] = Field(default=None)
    author_id: Optional[int] = Field(default=None)
    body: Optional[str] = Field(default=None)
    html_body: Optional[str] = Field(default=None)
    plain_body: Optional[str] = Field(default=None)
    public: Optional[bool] = Field(default=None)
    attachments: Optional[list[object]] = Field(default=None)
    via: Optional[object] = Field(default=None)
    created_at: Optional[str] = Field(default=None)
    uploads: Optional[list[object]] = Field(default=None)
    metadata: Optional[object] = Field(default=None)



================================================
FILE: src/dlt_zendesk/helpers/api_client.py
================================================
from enum import Enum
from typing import Dict, Iterator, Optional, Tuple, Any
from dlt.common.typing import DictStrStr, TDataItems, TSecretValue
from dlt.sources.helpers.requests import client

from .credentials import (
    ZendeskCredentialsEmailPass,
    ZendeskCredentialsOAuth,
    ZendeskCredentialsToken,
    TZendeskCredentials,
)

PAGE_SIZE = 100
INCREMENTAL_PAGE_SIZE = 1000


class PaginationType(Enum):
    OFFSET = 0
    CURSOR = 1
    STREAM = 2
    START_TIME = 3


class ZendeskAPIClient:
    """
    API client used to make requests to Zendesk talk, support and chat API
    """

    subdomain: str = ""
    url: str = ""
    headers: Optional[DictStrStr]
    auth: Optional[Tuple[str, TSecretValue]]

    def __init__(
            self, credentials: TZendeskCredentials, url_prefix: Optional[str] = None
    ) -> None:
        """
        Initializer for the API client which is then used to make API calls to the ZendeskAPI

        Args:
            credentials: ZendeskCredentials object
            which contains the necessary credentials to authenticate to ZendeskAPI
        """
        # oauth token is the preferred way to authenticate, followed by api token and then email + password combo
        # fill headers and auth for every possibility of credentials given,
        # raise error if credentials are of incorrect type
        if isinstance(credentials, ZendeskCredentialsOAuth):
            self.headers = {"Authorization": f"Bearer {credentials.oauth_token}"}
            self.auth = None
        elif isinstance(credentials, ZendeskCredentialsToken):
            self.headers = None
            self.auth = (f"{credentials.email}/token", credentials.token)
        elif isinstance(credentials, ZendeskCredentialsEmailPass):
            self.auth = (credentials.email, credentials.password)
            self.headers = None
        else:
            raise TypeError(
                "Wrong credentials type provided to ZendeskAPIClient. The credentials need to be of type:"
                " ZendeskCredentialsOAuth, ZendeskCredentialsToken or ZendeskCredentialsEmailPass"
            )

        # If url_prefix is set it overrides the default API URL (e.g. chat api uses zopim.com domain)
        if url_prefix:
            self.url = url_prefix
        else:
            self.subdomain = credentials.subdomain
            self.url = f"https://{self.subdomain}.zendesk.com"

    def get_pages(
            self,
            endpoint: str,
            data_point_name: str,
            pagination: PaginationType,
            params: Optional[Dict[str, Any]] = None,
    ) -> Iterator[TDataItems]:
        """
        Makes a request to a paginated endpoint and returns a generator of data items per page.

        Args:
            endpoint: The url to the endpoint, e.g. /api/v2/calls
            data_point_name: The key which data items are nested under in the response object (e.g. calls)
            params: Optional dict of query params to include in the request
            pagination: Type of pagination type used by endpoint

        Returns:
            Generator of pages, each page is a list of dict data items
        """
        # update the page size to enable cursor pagination
        params = params or {}
        if pagination == PaginationType.CURSOR:
            params["page[size]"] = PAGE_SIZE
        elif pagination == PaginationType.STREAM:
            params["per_page"] = INCREMENTAL_PAGE_SIZE
        elif pagination == PaginationType.START_TIME:
            params["limit"] = INCREMENTAL_PAGE_SIZE

        # make request and keep looping until there is no next page
        get_url = f"{self.url}{endpoint}"
        while get_url:
            response = client.get(
                url=get_url, headers=self.headers, auth=self.auth, params=params,
            )
            response.raise_for_status()
            response_json = response.json()
            result = response_json[data_point_name]
            yield result

            get_url = None
            if pagination == PaginationType.CURSOR:
                if response_json["meta"]["has_more"]:
                    get_url = response_json["links"]["next"]
            elif pagination == PaginationType.OFFSET:
                get_url = response_json.get("next_page", None)
            elif pagination == PaginationType.STREAM:
                # See
                # https://developer.zendesk.com/api-reference/ticketing/ticket-management/incremental_exports/#json-format
                if not response_json["end_of_stream"]:
                    get_url = response_json["next_page"]
            elif pagination == PaginationType.START_TIME:
                if response_json["count"] > 0:
                    get_url = response_json["next_page"]

            params = {}



================================================
FILE: src/dlt_zendesk/helpers/credentials.py
================================================
"""
This module handles how credentials are read in dlt sources
"""
import dlt
from typing import ClassVar, List, Union
from dlt.common.configuration import configspec
from dlt.common.configuration.specs import CredentialsConfiguration
from dlt.common.typing import TSecretValue


@configspec
class ZendeskCredentialsBase(CredentialsConfiguration):
    """
    The Base version of all the ZendeskCredential classes.
    """

    subdomain: str = dlt.config.value
    __config_gen_annotations__: ClassVar[List[str]] = []


@configspec
class ZendeskCredentialsEmailPass(ZendeskCredentialsBase):
    """
    This class is used to store credentials for Email + Password Authentication
    """

    email: str = dlt.config.value
    password: TSecretValue = dlt.secrets.value


@configspec
class ZendeskCredentialsOAuth(ZendeskCredentialsBase):
    """
    This class is used to store credentials for OAuth Token Authentication
    """

    oauth_token: TSecretValue = dlt.secrets.value


@configspec
class ZendeskCredentialsToken(ZendeskCredentialsBase):
    """
    This class is used to store credentials for Token Authentication
    """

    email: str = dlt.config.value
    token: TSecretValue = dlt.secrets.value


TZendeskCredentials = Union[
    ZendeskCredentialsEmailPass, ZendeskCredentialsToken, ZendeskCredentialsOAuth
]



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
FILE: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()



================================================
FILE: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on: [ push ]
concurrency: ci-${{ github.ref }} # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: "keboola.ex-zendesk-v2"
  KBC_DEVELOPERPORTAL_VENDOR: "keboola"
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: ${{ vars.KBC_DEVELOPERPORTAL_USERNAME }}

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }} # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (Optional) Test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: "" # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }} # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo ${raw//origin\//} | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh

