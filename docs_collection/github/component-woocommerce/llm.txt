Directory structure:
└── keboola-component-woocommerce/
    ├── README.md
    ├── Dockerfile
    ├── LICENSE.md
    ├── backup.sql
    ├── deploy.sh
    ├── docker-compose.yml
    ├── flake8.cfg
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configRowSchema.json
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   ├── sourceCodeUrl.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   ├── files/
    │       │   │   └── order1.xml
    │       │   └── tables/
    │       │       ├── test.csv
    │       │       └── test.csv.manifest
    │       └── out/
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               └── test.csv
    ├── docs/
    │   └── imgs/
    ├── local_testing/
    │   ├── instructions.txt
    │   ├── products_response.json
    │   └── wordpress_db.sql
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run_kbc_tests.ps1
    │   ├── update_dev_portal_properties.sh
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── result.py
    │   └── woocommerce_cli.py
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── .github/
        └── workflows/
            └── push.yml

================================================
File: README.md
================================================
# KBC Component

WooCommerce Extractor for Keboola Connection. Download all data under Orders, Products, and Customer hierarchies.

**Table of contents:**

[TOC]

## Background

WooCommerce is a customizable, open-source eCommerce platform built on WordPress that allows anyone to set up an online
store and sell their products.

## Requirements

To enable this application you need to:

- API Version: v3
- [Rest API Keys](https://woocommerce.github.io/woocommerce-rest-api-docs/#authentication)
- [Rest API Secret](https://woocommerce.github.io/woocommerce-rest-api-docs/#authentication)

## Generating API keys

To create or manage keys for a specific WordPress user, go to WooCommerce admin interface > Settings > API > Keys/Apps.
Click the "Add Key" button. In the next screen, add a description and select the WordPress user you would like to
generate the key for Choose the level of access for this REST API key, which can be Read access. Then click the "
Generate API Key" button and WooCommerce will generate REST API keys These two keys are your Consumer Key and Consumer
Secret.

## Configuration

To download data form WooCommerce we need to configure

### `Store_url`

Website Domain name where WooCommerce is hosted. e.g. https://myshop.com

### `consumer_key`

Rest API Consumer Key from WooCommerce Admin panel

### `consumer_secret`

Rest API Consumer Secret from WooCommerce Admin panel

### `query_string_auth`

Select this option if you receive 401 errors from your server. As per
official [documentation](https://woocommerce.github.io/woocommerce-rest-api-docs/#rest-api-keys), some servers may have
issues with standard Authorization header processing. Enabling this option should help in such scenarios.

### `date_from`

Inclusive Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc.

### `date_to`

Exclusive Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc.

### `load_type`

If set to Incremental update, the result tables will be updated based on primary key. Full load overwrites the
destination table each time. NOTE: If you wish to remove deleted records, this needs to be set to Full load and the
Period from attribute empty.

### `endpoint`

To fetch data from selected endpoints i.e. Customers, Orders and Products. Default selection is all i.e.

- Orders endpoint list all orders for give date range
- Products endpoint list all Products for given date range
- Customers endpoint list all customers

__NOTE:__ **The date selection does not affect Customers endpoint**

## Example JSON configuration

```json
{
  "parameters": {
    "store_url": "https://yourstore.com",
    "#consumer_key": "aaa",
    "#consumer_secret": "ccc",
    "query_string_auth": false,
    "load_type": 1,
    "date_from": "",
    "date_to": "",
    "endpoint": [
      "Customers",
      "Products",
      "Orders"
    ],
    "debug": false
  }
}
```

## Foreign Key Relations for nested objects

For example in the `product` hierarchy, each child table contains a `product_id` column as a foreign key reference to
the parent `product` table:

- In `products` table columns will
  be `["id", "name", "description", "dimensions__length", "dimensions__width", "related_ids"]`

- In `products__category` table columns will be `["id", "name", "slug", "product_id"]`

- In `products__images` table columns will be `["id", "src", "name", "product_id"]`

- In `products__attributes` table columns will be `["id", "name", "options", "product_id"]`

i.e. the `product` may be joined to its attributes by `product.id` = `products__attributes.id`.

### Hierarchy of output tables

```bash
tables
├── customer
│   └── customer__metadata
├── order
│   ├── order__coupon_lines
│   │   └── order__coupon_lines__meta_data
│   ├── order__fee_lines
│   │   ├── order__fee_lines__meta_data
│   │   └── order__fee_lines__taxes
│   ├── order__line_items
│   │   ├── order__line_items__meta_data
│   │   └── order__line_items__taxes
│   ├── order__metadata
│   ├── order__refunds
│   ├── order__shipping_lines
│   │   ├── order__shipping_lines__meta_data
│   │   └── order__shipping_lines__taxes
│   └── order__tax_lines
│       └── order__tax_lines__meta_data
└── product
    ├── product__attributes
    ├── product__categories
    ├── product__default_attributes
    ├── product__images
    ├── product__metadata
    └── product__tags
```

## Development

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path in the docker-compose
file:

```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```bash
git clone repo_path my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```bash
docker-compose run --rm test
```

## Integration

For information about deployment and integration with KBC, please refer to
the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/)

## Setup WooCommerce Test Server

1. create `docker-compose.yml` file on server

```bash

version: '3.3'

services:
  db:
    image: mysql:latest
    volumes:
      - data:/var/lib/mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: wordpress
      MYSQL_DATABASE: wordpress
      MYSQL_USER: wordpress
      MYSQL_PASSWORD: wordpress

  wordpress:
    depends_on:
      - db
    image: wordpress:latest
    ports:
      - "8000:80"
    restart: always
    environment:
      WORDPRESS_DB_HOST: db:3306
      WORDPRESS_DB_USER: wordpress
      WORDPRESS_DB_PASSWORD: wordpress
      WORDPRESS_DB_NAME: wordpress
volumes:
    data: {}
```


================================================
File: Dockerfile
================================================
FROM python:3.8.6-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install --upgrade pip

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]


================================================
File: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================================================
File: deploy.sh
================================================
#!/bin/sh
set -e

env

# compatibility with travis and bitbucket
if [ ! -z ${BITBUCKET_TAG} ]
then
	echo "asigning bitbucket tag"
	export TAG="$BITBUCKET_TAG"
elif [ ! -z ${TRAVIS_TAG} ]
then
	echo "asigning travis tag"
	export TAG="$TRAVIS_TAG"
else
	echo No Tag is set!
	exit 1
fi

echo "Tag is '${TAG}'"

#check if deployment is triggered only in master
if [ ${BITBUCKET_BRANCH} != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
echo "Obtain the component repository and log in"
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

echo "Set credentials"
eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
echo "Push to the repository"
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${TAG} is not allowed."
fi


================================================
File: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
    mem_limit:
      512MB
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh

================================================
File: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    example,
    venv
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.5.1.zip#egg=kbc
mock~=4.0.3
freezegun~=1.2.2
WooCommerce==2.1.1
backoff==1.10.0
kbc~=0.5.1
dateparser~=1.1.1
requests~=2.28.1

================================================
File: component_config/component_long_description.md
================================================
WooCommerce API Plugin

Download all data under `Orders`, `Products`, and `Customer` hierarchies

Requirements:

To enable this application you need to:

- API Version: v3
- [Rest API Keys](https://woocommerce.github.io/woocommerce-rest-api-docs/#authentication)
- [Rest API Secret](https://woocommerce.github.io/woocommerce-rest-api-docs/#authentication)

Generating API keys in the WordPress admin interface

To create or manage keys for a specific WordPress user:

- go to `WooCommerce > Settings > API > Keys/Apps`.
- Click the "Add Key" button. In the next screen, add a description and select the WordPress user you would like to generate the key for
- Choose the level of access for this REST API key, which can be Read access. 
- Then click the "Generate API Key" button and WooCommerce will generate REST API keys. These two keys are your Consumer Key and Consumer Secret.

================================================
File: component_config/component_short_description.md
================================================
WooCommerce is a customizable, open-source eCommerce platform built on WordPress that allows anyone to set up an online store and sell their products.


================================================
File: component_config/configRowSchema.json
================================================
{}

================================================
File: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "extractor configuration",
  "required": [
    "store_url",
    "#consumer_key",
    "#consumer_secret",
    "load_type",
    "endpoint"
  ],
  "properties": {
    "store_url": {
      "type": "string",
      "title": "Store URL",
      "description": "Your shop domain name, e.g. https://myshop.com",
      "propertyOrder": 100
    },
    "#consumer_key": {
      "type": "string",
      "title": "Consumer Key",
      "description": "Rest API Consumer Key from WooCommerce Admin panel",
      "format": "password",
      "propertyOrder": 200
    },
    "#consumer_secret": {
      "type": "string",
      "title": "Consumer secret",
      "description": "Rest API Consumer Secret from WooCommerce Admin panel",
      "format": "password",
      "propertyOrder": 300
    },
    "query_string_auth": {
      "type": "boolean",
      "title": "Include authorization keys in query string",
      "default": false,
      "description": "Select this option if you receive 401 errors from your server. Some servers may have issues with standard Authorization header processing.",
      "format": "checkbox",
      "propertyOrder": 350
    },
    "load_type": {
      "type": "number",
      "enum": [
        0,
        1
      ],
      "options": {
        "enum_titles": [
          "Full Load",
          "Incremental Update"
        ]
      },
      "default": 1,
      "title": "Load type",
      "description": "If set to Incremental update, the result tables will be updated based on primary key. Full load overwrites the destination table each time. NOTE: If you wish to remove deleted records, this needs to be set to Full load and the Period from attribute empty.",
      "propertyOrder": 400
    },
    "fetching_mode": {
      "type": "string",
      "required": true,
      "enum": [
        "Incremental Fetching with publish date",
        "Incremental Fetching with custom field",
        "Full Download"
      ],
      "default": "Incremental Fetching with publish date",
      "title": "Fetching mode",
      "description": "If set to Incremental Fetching with publish date, data will be fetched that has been published in a defined date range. Full Download downloads all data. Incremental Fetching with custom field allows the use of a custom field",
      "propertyOrder": 400
    },
    "date_from": {
      "type": "string",
      "title": "From date [inclusive]",
      "description": "Date from. Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc. If left empty, all records are downloaded.",
      "default": "1 week ago",
      "propertyOrder": 500,
      "options": {
        "dependencies": {
          "fetching_mode": "Incremental Fetching with publish date"
        }
      }
    },
    "date_to": {
      "type": "string",
      "title": "To date [exclusive]",
      "description": "Date to. Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc. If left empty, all records are downloaded.",
      "default": "now",
      "propertyOrder": 600,
      "options": {
        "dependencies": {
          "fetching_mode": "Incremental Fetching with publish date"
        }
      }
    },
    "custom_incremental_field": {
      "type": "string",
      "title": "Custom Incremental Parameter",
      "description": "Custom parameter in WooCommerce for incremental fetching",
      "default": "modified_after",
      "propertyOrder": 700,
      "options": {
        "dependencies": {
          "fetching_mode": "Incremental Fetching with custom field"
        }
      }
    },
    "custom_incremental_value": {
      "type": "string",
      "title": "Custom From date",
      "description": "Date from for the custom incremental fetching parameter. Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc.",
      "default": "2 days ago",
      "propertyOrder": 800,
      "options": {
        "dependencies": {
          "fetching_mode": "Incremental Fetching with custom field"
        }
      }
    },
    "endpoint": {
      "type": "array",
      "uniqueItems": true,
      "format": "select",
      "options": {
        "grid_columns": 12
      },
      "title": "Endpoint",
      "description": "To fetch data from selected endpoints i.e. Customers, Orders and Products. Note that the Customers endpoint is not affected by the selected period.",
      "items": {
        "type": "string",
        "enum": [
          "Customers",
          "Orders",
          "Products"
        ]
      },
      "default": [
        "Customers",
        "Orders",
        "Products"
      ],
      "propertyOrder": 900
    }
  }
}

================================================
File: component_config/configuration_description.md
================================================
To download data form WooCommerce we need to configure

- `store_url` Website Domain name where WooCommerce is hosted. e.g. https://myshop.com
- `consumer_key` Rest API Consumer Key from WooCommerce Admin panel
- `consumer_secret` Rest API Consumer Secret from WooCommerce Admin panel
- `fetching_mode` If set to Incremental Fetching with publish date, data will be fetched that has been published in the date range. Full Download downloads all data. Incremental Fetching with custom field allows the use of a custom field
- `custom_incremental_field` Custom parameter in WooCommerce for incremental fetching
- `custom_incremental_value` Inclusive Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc. which will be used for fetching with the custom fetching field
- `date_from` Inclusive Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc.
- `date_to` Exclusive Date in YYYY-MM-DD format or a string i.e. 5 days ago, 1 month ago, yesterday, etc.
- `load_type` If set to Incremental update, the result tables will be updated based on primary key. Full load overwrites the destination table each time. NOTE: If you wish to remove deleted records, this needs to be set to Full load and the Period from attribute empty.
- `endpoint` To fetch data from selected endpoints i.e. Customers, Orders and Products. Default selection is all
i.e.
- Orders endpoint list all orders for give date range
- Products endpoint list all Products for given date range
- Customers endpoint list all customers

__NOTE:__ *The date selection does not affect Customers endpoint*

**Getting the token:**

To create or manage keys for a specific WordPress user, go to `WooCommerce > Settings > API > Keys/Apps`.
Click the `Add Key` button. In the next screen, add a description and select the WordPress user you would like to generate the key for
Choose the level of access for this REST API key, which can be Read access. Then click the `Generate API Key` button and WooCommerce will generate REST API keys
These two keys are your Consumer Key and Consumer Secret.


**Custom Fetching:**

You can use [this plugin](https://wordpress.org/plugins/products-and-orders-last-modified-for-wc-rest-api/) or implement the custom incremental field by yourself.

================================================
File: component_config/logger
================================================
gelf

================================================
File: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}

================================================
File: component_config/stack_parameters.json
================================================
{}

================================================
File: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "store_url": "https://myshop.com",
    "#consumer_key": "XXXX",
    "#consumer_secret": "XXXX",
    "load_type": 1,
    "date_from": "1 week ago",
    "date_to": "now",
    "endpoint": [
      "Customers",
      "Orders",
      "Products"
    ],
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}


================================================
File: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: local_testing/instructions.txt
================================================
To Dump all table from Database:

Generic command:
docker exec CONTAINER sh -c 'exec mysqldump -u root -p "$MYSQL_ROOT_PASSWORD" "$MYSQL_DATABASE"' > backup.sql

To dump Only selected table:
docker exec CONTAINER sh -c 'exec mysqldump -u root --password=wordpress wordpress wp_users wp_wc_customer_lookup wp_wc_order_coupon_lookup wp_wc_order_product_lookup wp_wc_order_stats wp_wc_product_meta_lookup wp_wc_tax_rate_classes wp_woocommerce_attribute_taxonomies wp_woocommerce_order_itemmeta wp_woocommerce_order_items wp_woocommerce_shipping_zone_locations wp_woocommerce_shipping_zone_methods wp_woocommerce_shipping_zones wp_woocommerce_tax_rate_locations wp_woocommerce_tax_rates' > data.sql

To Restore Database:
Generic Command:
docker exec -i CONTAINER sh -c 'exec mysql -u root -p "$MYSQL_ROOT_PASSWORD" "MYSQL_DATABASE"' < data.sql

docker exec -i CONTAINER sh -c 'exec mysql -u root -p --password=wordpress wordpress' < data.sql

in this data.sql only selected table is dumped so

while importing you will get only selected table

make sure docker instance is running while restoring.

================================================
File: local_testing/products_response.json
================================================
[
  {
    "id": 799,
    "name": "Ship Your Idea",
    "description": "",
    "dimensions": {
      "length": "",
      "width": ""
    },
    "related_ids": [
      31,
      22
    ],
    "categories": [
      {
        "id": 9,
        "name": "Clothing",
        "slug": "clothing"
      },
      {
        "id": 14,
        "name": "T-shirts",
        "slug": "t-shirts"
      }
    ],
    "images": [
      {
        "id": 795,
        "src": "https://example.com/wp-content/uploads/2017/03/T_4_front-11.jpg",
        "name": ""
      },
      {
        "id": 796,
        "src": "https://example.com/wp-content/uploads/2017/03/T_4_back-10.jpg",
        "name": ""
      }
    ],
    "attributes": [
      {
        "id": 6,
        "name": "Color",
        "options": [
          "Black",
          "Green"
        ]
      },
      {
        "id": 0,
        "name": "Size",
        "options": [
          "S",
          "M"
        ]
      }
    ]
  }
]

================================================
File: local_testing/wordpress_db.sql
================================================
-- MySQL dump 10.13  Distrib 8.0.23, for Linux (x86_64)
--
-- Host: localhost    Database: wordpress
-- ------------------------------------------------------
-- Server version	8.0.23

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8mb4 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `wp_users`
--

DROP TABLE IF EXISTS `wp_users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_users` (
  `ID` bigint unsigned NOT NULL AUTO_INCREMENT,
  `user_login` varchar(60) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `user_pass` varchar(255) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `user_nicename` varchar(50) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `user_email` varchar(100) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `user_url` varchar(100) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `user_registered` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `user_activation_key` varchar(255) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `user_status` int NOT NULL DEFAULT '0',
  `display_name` varchar(250) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  PRIMARY KEY (`ID`),
  KEY `user_login_key` (`user_login`),
  KEY `user_nicename` (`user_nicename`),
  KEY `user_email` (`user_email`)
) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_users`
--

LOCK TABLES `wp_users` WRITE;
/*!40000 ALTER TABLE `wp_users` DISABLE KEYS */;
INSERT INTO `wp_users` VALUES (1,'admin','$P$BOsRbFUfY9zLkYaESOWREADWbjX8yU/','admin','purbey.santosh@hotmail.com','http://107.174.205.151:8000','2021-02-09 17:26:31','',0,'admin'),(2,'santosh1','$P$B8YXijc.aAJztDGNmLN9iRgYSA6wn/0','santosh1','santosh@gmail.com','','2021-02-09 17:36:57','',0,'Santosh1 Purbey1'),(3,'santosh2','$P$Bza.BatwbKDcIzyreKxBJ7DHLuvFNt1','santosh2','santosh2@hotmail.com','','2021-02-09 17:37:56','',0,'Santosh2 Purbey2'),(4,'santosh3','$P$BvEQmQyDe/T529Y3EzRDPRLK/hh3Hr.','santosh3','santosh3@gmail.com','','2021-02-09 17:38:56','',0,'Santosh3 Purbey'),(5,'santosh4','$P$Bfjx40n2SFT7RJ2YfaO0xfCFtjOkRx0','santosh4','santosh4@gmail.com','','2021-02-09 17:40:27','',0,'Santosh4 Purbey4'),(6,'santosh5','$P$BlQncO9CnyYLTtmIPF/twvcVnNr8dG0','santosh5','santosh5@gmail.com','','2021-02-09 17:41:14','',0,'santosh5 Purbey');
/*!40000 ALTER TABLE `wp_users` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_wc_customer_lookup`
--

DROP TABLE IF EXISTS `wp_wc_customer_lookup`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_wc_customer_lookup` (
  `customer_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `user_id` bigint unsigned DEFAULT NULL,
  `username` varchar(60) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `first_name` varchar(255) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `last_name` varchar(255) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `email` varchar(100) COLLATE utf8mb4_unicode_520_ci DEFAULT NULL,
  `date_last_active` timestamp NULL DEFAULT NULL,
  `date_registered` timestamp NULL DEFAULT NULL,
  `country` char(2) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `postcode` varchar(20) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `city` varchar(100) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `state` varchar(100) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  PRIMARY KEY (`customer_id`),
  UNIQUE KEY `user_id` (`user_id`),
  KEY `email` (`email`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_wc_customer_lookup`
--

LOCK TABLES `wp_wc_customer_lookup` WRITE;
/*!40000 ALTER TABLE `wp_wc_customer_lookup` DISABLE KEYS */;
INSERT INTO `wp_wc_customer_lookup` VALUES (1,2,'santosh1','Santosh1','Purbey1','santosh@gmail.com','2021-02-09 00:00:00','2021-02-09 17:36:57','NP','44600','Kathmandu','BAG'),(2,3,'santosh2','Santosh2','Purbey2','santosh2@hotmail.com','2021-02-09 00:00:00','2021-02-09 17:37:56','NP','44600','Lalitpur','BAG');
/*!40000 ALTER TABLE `wp_wc_customer_lookup` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_wc_order_coupon_lookup`
--

DROP TABLE IF EXISTS `wp_wc_order_coupon_lookup`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_wc_order_coupon_lookup` (
  `order_id` bigint unsigned NOT NULL,
  `coupon_id` bigint NOT NULL,
  `date_created` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `discount_amount` double NOT NULL DEFAULT '0',
  PRIMARY KEY (`order_id`,`coupon_id`),
  KEY `coupon_id` (`coupon_id`),
  KEY `date_created` (`date_created`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_wc_order_coupon_lookup`
--

LOCK TABLES `wp_wc_order_coupon_lookup` WRITE;
/*!40000 ALTER TABLE `wp_wc_order_coupon_lookup` DISABLE KEYS */;
INSERT INTO `wp_wc_order_coupon_lookup` VALUES (65,60,'2021-02-09 20:09:09',64.4),(66,62,'2021-02-09 20:14:21',100);
/*!40000 ALTER TABLE `wp_wc_order_coupon_lookup` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_wc_order_product_lookup`
--

DROP TABLE IF EXISTS `wp_wc_order_product_lookup`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_wc_order_product_lookup` (
  `order_item_id` bigint unsigned NOT NULL,
  `order_id` bigint unsigned NOT NULL,
  `product_id` bigint unsigned NOT NULL,
  `variation_id` bigint unsigned NOT NULL,
  `customer_id` bigint unsigned DEFAULT NULL,
  `date_created` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `product_qty` int NOT NULL,
  `product_net_revenue` double NOT NULL DEFAULT '0',
  `product_gross_revenue` double NOT NULL DEFAULT '0',
  `coupon_amount` double NOT NULL DEFAULT '0',
  `tax_amount` double NOT NULL DEFAULT '0',
  `shipping_amount` double NOT NULL DEFAULT '0',
  `shipping_tax_amount` double NOT NULL DEFAULT '0',
  PRIMARY KEY (`order_item_id`),
  KEY `order_id` (`order_id`),
  KEY `product_id` (`product_id`),
  KEY `customer_id` (`customer_id`),
  KEY `date_created` (`date_created`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_wc_order_product_lookup`
--

LOCK TABLES `wp_wc_order_product_lookup` WRITE;
/*!40000 ALTER TABLE `wp_wc_order_product_lookup` DISABLE KEYS */;
INSERT INTO `wp_wc_order_product_lookup` VALUES (1,65,16,0,1,'2021-02-09 20:09:09',1,14.4,14.4,3.6,0,0,0),(2,65,24,0,1,'2021-02-09 20:09:09',2,24,24,6,0,0,0),(3,65,33,0,1,'2021-02-09 20:09:09',3,43.2,43.2,10.8,0,0,0),(4,65,17,0,1,'2021-02-09 20:09:09',4,176,176,44,0,0,0),(7,66,21,0,2,'2021-02-09 20:14:21',1,37.93,37.93,7.07,0,0,0),(8,66,14,0,2,'2021-02-09 20:14:21',2,75.86,75.86,14.14,0,0,0),(9,66,22,0,2,'2021-02-09 20:14:21',4,71.69,71.69,28.31,0,0,0),(10,66,25,0,2,'2021-02-09 20:14:21',4,0,0,8,0,0,0),(11,66,19,0,2,'2021-02-09 20:14:21',6,497.52,497.52,42.48,0,0,0);
/*!40000 ALTER TABLE `wp_wc_order_product_lookup` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_wc_order_stats`
--

DROP TABLE IF EXISTS `wp_wc_order_stats`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_wc_order_stats` (
  `order_id` bigint unsigned NOT NULL,
  `parent_id` bigint unsigned NOT NULL DEFAULT '0',
  `date_created` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `date_created_gmt` datetime NOT NULL DEFAULT '0000-00-00 00:00:00',
  `num_items_sold` int NOT NULL DEFAULT '0',
  `total_sales` double NOT NULL DEFAULT '0',
  `tax_total` double NOT NULL DEFAULT '0',
  `shipping_total` double NOT NULL DEFAULT '0',
  `net_total` double NOT NULL DEFAULT '0',
  `returning_customer` tinyint(1) DEFAULT NULL,
  `status` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `customer_id` bigint unsigned NOT NULL,
  PRIMARY KEY (`order_id`),
  KEY `date_created` (`date_created`),
  KEY `customer_id` (`customer_id`),
  KEY `status` (`status`(191))
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_wc_order_stats`
--

LOCK TABLES `wp_wc_order_stats` WRITE;
/*!40000 ALTER TABLE `wp_wc_order_stats` DISABLE KEYS */;
INSERT INTO `wp_wc_order_stats` VALUES (65,0,'2021-02-09 20:09:09','2021-02-09 20:09:09',10,257.6,0,0,257.6,0,'wc-on-hold',1),(66,0,'2021-02-09 20:14:21','2021-02-09 20:14:21',17,683,0,0,683,0,'wc-on-hold',2);
/*!40000 ALTER TABLE `wp_wc_order_stats` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_wc_product_meta_lookup`
--

DROP TABLE IF EXISTS `wp_wc_product_meta_lookup`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_wc_product_meta_lookup` (
  `product_id` bigint NOT NULL,
  `sku` varchar(100) COLLATE utf8mb4_unicode_520_ci DEFAULT '',
  `virtual` tinyint(1) DEFAULT '0',
  `downloadable` tinyint(1) DEFAULT '0',
  `min_price` decimal(19,4) DEFAULT NULL,
  `max_price` decimal(19,4) DEFAULT NULL,
  `onsale` tinyint(1) DEFAULT '0',
  `stock_quantity` double DEFAULT NULL,
  `stock_status` varchar(100) COLLATE utf8mb4_unicode_520_ci DEFAULT 'instock',
  `rating_count` bigint DEFAULT '0',
  `average_rating` decimal(3,2) DEFAULT '0.00',
  `total_sales` bigint DEFAULT '0',
  `tax_status` varchar(100) COLLATE utf8mb4_unicode_520_ci DEFAULT 'taxable',
  `tax_class` varchar(100) COLLATE utf8mb4_unicode_520_ci DEFAULT '',
  PRIMARY KEY (`product_id`),
  KEY `virtual` (`virtual`),
  KEY `downloadable` (`downloadable`),
  KEY `stock_status` (`stock_status`),
  KEY `stock_quantity` (`stock_quantity`),
  KEY `onsale` (`onsale`),
  KEY `min_max_price` (`min_price`,`max_price`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_wc_product_meta_lookup`
--

LOCK TABLES `wp_wc_product_meta_lookup` WRITE;
/*!40000 ALTER TABLE `wp_wc_product_meta_lookup` DISABLE KEYS */;
INSERT INTO `wp_wc_product_meta_lookup` VALUES (12,'woo-vneck-tee',0,0,15.0000,20.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(13,'woo-hoodie',0,0,42.0000,45.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(14,'woo-hoodie-with-logo',0,0,45.0000,45.0000,0,NULL,'instock',0,0.00,2,'taxable',''),(15,'woo-tshirt',0,0,18.0000,18.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(16,'woo-beanie',0,0,18.0000,18.0000,1,NULL,'instock',0,0.00,1,'taxable',''),(17,'woo-belt',0,0,55.0000,55.0000,1,NULL,'instock',0,0.00,4,'taxable',''),(18,'woo-cap',0,0,16.0000,16.0000,1,NULL,'instock',0,0.00,0,'taxable',''),(19,'woo-sunglasses',0,0,90.0000,90.0000,0,NULL,'instock',0,0.00,6,'taxable',''),(20,'woo-hoodie-with-pocket',0,0,35.0000,35.0000,1,NULL,'instock',0,0.00,0,'taxable',''),(21,'woo-hoodie-with-zipper',0,0,45.0000,45.0000,0,NULL,'instock',0,0.00,1,'taxable',''),(22,'woo-long-sleeve-tee',0,0,25.0000,25.0000,0,NULL,'instock',0,0.00,4,'taxable',''),(23,'woo-polo',0,0,20.0000,20.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(24,'woo-album',1,1,15.0000,15.0000,0,NULL,'instock',0,0.00,2,'taxable',''),(25,'woo-single',1,1,2.0000,2.0000,1,NULL,'instock',0,0.00,4,'taxable',''),(26,'woo-vneck-tee-red',0,0,20.0000,20.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(27,'woo-vneck-tee-green',0,0,20.0000,20.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(28,'woo-vneck-tee-blue',0,0,15.0000,15.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(29,'woo-hoodie-red',0,0,42.0000,42.0000,1,NULL,'instock',0,0.00,0,'taxable',''),(30,'woo-hoodie-green',0,0,45.0000,45.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(31,'woo-hoodie-blue',0,0,45.0000,45.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(32,'Woo-tshirt-logo',0,0,18.0000,18.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(33,'Woo-beanie-logo',0,0,18.0000,18.0000,1,NULL,'instock',0,0.00,3,'taxable',''),(34,'logo-collection',0,0,18.0000,45.0000,0,NULL,'instock',0,0.00,0,'taxable',''),(35,'wp-pennant',0,0,11.0500,11.0500,0,NULL,'instock',0,0.00,0,'taxable',''),(36,'woo-hoodie-blue-logo',0,0,45.0000,45.0000,0,NULL,'instock',0,0.00,0,'taxable','');
/*!40000 ALTER TABLE `wp_wc_product_meta_lookup` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_wc_tax_rate_classes`
--

DROP TABLE IF EXISTS `wp_wc_tax_rate_classes`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_wc_tax_rate_classes` (
  `tax_rate_class_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `slug` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  PRIMARY KEY (`tax_rate_class_id`),
  UNIQUE KEY `slug` (`slug`(191))
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_wc_tax_rate_classes`
--

LOCK TABLES `wp_wc_tax_rate_classes` WRITE;
/*!40000 ALTER TABLE `wp_wc_tax_rate_classes` DISABLE KEYS */;
INSERT INTO `wp_wc_tax_rate_classes` VALUES (1,'Reduced rate','reduced-rate'),(2,'Zero rate','zero-rate');
/*!40000 ALTER TABLE `wp_wc_tax_rate_classes` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_woocommerce_attribute_taxonomies`
--

DROP TABLE IF EXISTS `wp_woocommerce_attribute_taxonomies`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_woocommerce_attribute_taxonomies` (
  `attribute_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `attribute_name` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `attribute_label` varchar(200) COLLATE utf8mb4_unicode_520_ci DEFAULT NULL,
  `attribute_type` varchar(20) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `attribute_orderby` varchar(20) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `attribute_public` int NOT NULL DEFAULT '1',
  PRIMARY KEY (`attribute_id`),
  KEY `attribute_name` (`attribute_name`(20))
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_woocommerce_attribute_taxonomies`
--

LOCK TABLES `wp_woocommerce_attribute_taxonomies` WRITE;
/*!40000 ALTER TABLE `wp_woocommerce_attribute_taxonomies` DISABLE KEYS */;
INSERT INTO `wp_woocommerce_attribute_taxonomies` VALUES (1,'color','Color','select','menu_order',0),(2,'size','Size','select','menu_order',0);
/*!40000 ALTER TABLE `wp_woocommerce_attribute_taxonomies` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_woocommerce_order_itemmeta`
--

DROP TABLE IF EXISTS `wp_woocommerce_order_itemmeta`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_woocommerce_order_itemmeta` (
  `meta_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `order_item_id` bigint unsigned NOT NULL,
  `meta_key` varchar(255) COLLATE utf8mb4_unicode_520_ci DEFAULT NULL,
  `meta_value` longtext COLLATE utf8mb4_unicode_520_ci,
  PRIMARY KEY (`meta_id`),
  KEY `order_item_id` (`order_item_id`),
  KEY `meta_key` (`meta_key`(32))
) ENGINE=InnoDB AUTO_INCREMENT=100 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_woocommerce_order_itemmeta`
--

LOCK TABLES `wp_woocommerce_order_itemmeta` WRITE;
/*!40000 ALTER TABLE `wp_woocommerce_order_itemmeta` DISABLE KEYS */;
INSERT INTO `wp_woocommerce_order_itemmeta` VALUES (1,1,'_product_id','16'),(2,1,'_variation_id','0'),(3,1,'_qty','1'),(4,1,'_tax_class',''),(5,1,'_line_subtotal','18'),(6,1,'_line_subtotal_tax','0'),(7,1,'_line_total','14.4'),(8,1,'_line_tax','0'),(9,1,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(10,2,'_product_id','24'),(11,2,'_variation_id','0'),(12,2,'_qty','2'),(13,2,'_tax_class',''),(14,2,'_line_subtotal','30'),(15,2,'_line_subtotal_tax','0'),(16,2,'_line_total','24'),(17,2,'_line_tax','0'),(18,2,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(19,3,'_product_id','33'),(20,3,'_variation_id','0'),(21,3,'_qty','3'),(22,3,'_tax_class',''),(23,3,'_line_subtotal','54'),(24,3,'_line_subtotal_tax','0'),(25,3,'_line_total','43.2'),(26,3,'_line_tax','0'),(27,3,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(28,4,'_product_id','17'),(29,4,'_variation_id','0'),(30,4,'_qty','4'),(31,4,'_tax_class',''),(32,4,'_line_subtotal','220'),(33,4,'_line_subtotal_tax','0'),(34,4,'_line_total','176'),(35,4,'_line_tax','0'),(36,4,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(37,5,'method_id','free_shipping'),(38,5,'instance_id','1'),(39,5,'cost','0.00'),(40,5,'total_tax','0'),(41,5,'taxes','a:1:{s:5:\"total\";a:0:{}}'),(42,5,'Items','Beanie &times; 1, Beanie with Logo &times; 3, Belt &times; 4'),(43,6,'discount_amount','64.4'),(44,6,'discount_amount_tax','0'),(45,6,'coupon_data','a:24:{s:2:\"id\";i:60;s:4:\"code\";s:8:\"2xfy3ffd\";s:6:\"amount\";s:2:\"20\";s:12:\"date_created\";O:11:\"WC_DateTime\":4:{s:13:\"\0*\0utc_offset\";i:0;s:4:\"date\";s:26:\"2021-02-09 17:46:51.000000\";s:13:\"timezone_type\";i:1;s:8:\"timezone\";s:6:\"+00:00\";}s:13:\"date_modified\";O:11:\"WC_DateTime\":4:{s:13:\"\0*\0utc_offset\";i:0;s:4:\"date\";s:26:\"2021-02-09 17:46:51.000000\";s:13:\"timezone_type\";i:1;s:8:\"timezone\";s:6:\"+00:00\";}s:12:\"date_expires\";N;s:13:\"discount_type\";s:7:\"percent\";s:11:\"description\";s:0:\"\";s:11:\"usage_count\";i:0;s:14:\"individual_use\";b:0;s:11:\"product_ids\";a:0:{}s:20:\"excluded_product_ids\";a:0:{}s:11:\"usage_limit\";i:0;s:20:\"usage_limit_per_user\";i:0;s:22:\"limit_usage_to_x_items\";N;s:13:\"free_shipping\";b:0;s:18:\"product_categories\";a:0:{}s:27:\"excluded_product_categories\";a:0:{}s:18:\"exclude_sale_items\";b:0;s:14:\"minimum_amount\";s:0:\"\";s:14:\"maximum_amount\";s:0:\"\";s:18:\"email_restrictions\";a:0:{}s:7:\"virtual\";b:0;s:9:\"meta_data\";a:0:{}}'),(46,7,'_product_id','21'),(47,7,'_variation_id','0'),(48,7,'_qty','1'),(49,7,'_tax_class',''),(50,7,'_line_subtotal','45'),(51,7,'_line_subtotal_tax','0'),(52,7,'_line_total','37.93'),(53,7,'_line_tax','0'),(54,7,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(55,8,'_product_id','14'),(56,8,'_variation_id','0'),(57,8,'_qty','2'),(58,8,'_tax_class',''),(59,8,'_line_subtotal','90'),(60,8,'_line_subtotal_tax','0'),(61,8,'_line_total','75.86'),(62,8,'_line_tax','0'),(63,8,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(64,9,'_product_id','22'),(65,9,'_variation_id','0'),(66,9,'_qty','4'),(67,9,'_tax_class',''),(68,9,'_line_subtotal','100'),(69,9,'_line_subtotal_tax','0'),(70,9,'_line_total','71.69'),(71,9,'_line_tax','0'),(72,9,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(73,10,'_product_id','25'),(74,10,'_variation_id','0'),(75,10,'_qty','4'),(76,10,'_tax_class',''),(77,10,'_line_subtotal','8'),(78,10,'_line_subtotal_tax','0'),(79,10,'_line_total','0'),(80,10,'_line_tax','0'),(81,10,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(82,11,'_product_id','19'),(83,11,'_variation_id','0'),(84,11,'_qty','6'),(85,11,'_tax_class',''),(86,11,'_line_subtotal','540'),(87,11,'_line_subtotal_tax','0'),(88,11,'_line_total','497.52'),(89,11,'_line_tax','0'),(90,11,'_line_tax_data','a:2:{s:5:\"total\";a:0:{}s:8:\"subtotal\";a:0:{}}'),(91,12,'method_id','free_shipping'),(92,12,'instance_id','1'),(93,12,'cost','0.00'),(94,12,'total_tax','0'),(95,12,'taxes','a:1:{s:5:\"total\";a:0:{}}'),(96,12,'Items','Hoodie with Zipper &times; 1, Hoodie with Logo &times; 2, Long Sleeve Tee &times; 4, Sunglasses &times; 6'),(97,13,'discount_amount','100'),(98,13,'discount_amount_tax','0'),(99,13,'coupon_data','a:24:{s:2:\"id\";i:62;s:4:\"code\";s:8:\"ra6nr9yw\";s:6:\"amount\";s:3:\"100\";s:12:\"date_created\";O:11:\"WC_DateTime\":4:{s:13:\"\0*\0utc_offset\";i:0;s:4:\"date\";s:26:\"2021-02-09 17:48:07.000000\";s:13:\"timezone_type\";i:1;s:8:\"timezone\";s:6:\"+00:00\";}s:13:\"date_modified\";O:11:\"WC_DateTime\":4:{s:13:\"\0*\0utc_offset\";i:0;s:4:\"date\";s:26:\"2021-02-09 17:48:07.000000\";s:13:\"timezone_type\";i:1;s:8:\"timezone\";s:6:\"+00:00\";}s:12:\"date_expires\";N;s:13:\"discount_type\";s:10:\"fixed_cart\";s:11:\"description\";s:0:\"\";s:11:\"usage_count\";i:0;s:14:\"individual_use\";b:0;s:11:\"product_ids\";a:0:{}s:20:\"excluded_product_ids\";a:0:{}s:11:\"usage_limit\";i:0;s:20:\"usage_limit_per_user\";i:0;s:22:\"limit_usage_to_x_items\";N;s:13:\"free_shipping\";b:1;s:18:\"product_categories\";a:0:{}s:27:\"excluded_product_categories\";a:0:{}s:18:\"exclude_sale_items\";b:0;s:14:\"minimum_amount\";s:0:\"\";s:14:\"maximum_amount\";s:0:\"\";s:18:\"email_restrictions\";a:0:{}s:7:\"virtual\";b:0;s:9:\"meta_data\";a:0:{}}');
/*!40000 ALTER TABLE `wp_woocommerce_order_itemmeta` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_woocommerce_order_items`
--

DROP TABLE IF EXISTS `wp_woocommerce_order_items`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_woocommerce_order_items` (
  `order_item_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `order_item_name` text COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `order_item_type` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `order_id` bigint unsigned NOT NULL,
  PRIMARY KEY (`order_item_id`),
  KEY `order_id` (`order_id`)
) ENGINE=InnoDB AUTO_INCREMENT=14 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_woocommerce_order_items`
--

LOCK TABLES `wp_woocommerce_order_items` WRITE;
/*!40000 ALTER TABLE `wp_woocommerce_order_items` DISABLE KEYS */;
INSERT INTO `wp_woocommerce_order_items` VALUES (1,'Beanie','line_item',65),(2,'Album','line_item',65),(3,'Beanie with Logo','line_item',65),(4,'Belt','line_item',65),(5,'Free shipping','shipping',65),(6,'2xfy3ffd','coupon',65),(7,'Hoodie with Zipper','line_item',66),(8,'Hoodie with Logo','line_item',66),(9,'Long Sleeve Tee','line_item',66),(10,'Single','line_item',66),(11,'Sunglasses','line_item',66),(12,'Free shipping','shipping',66),(13,'ra6nr9yw','coupon',66);
/*!40000 ALTER TABLE `wp_woocommerce_order_items` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_woocommerce_shipping_zone_locations`
--

DROP TABLE IF EXISTS `wp_woocommerce_shipping_zone_locations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_woocommerce_shipping_zone_locations` (
  `location_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `zone_id` bigint unsigned NOT NULL,
  `location_code` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `location_type` varchar(40) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  PRIMARY KEY (`location_id`),
  KEY `location_id` (`location_id`),
  KEY `location_type_code` (`location_type`(10),`location_code`(20))
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_woocommerce_shipping_zone_locations`
--

LOCK TABLES `wp_woocommerce_shipping_zone_locations` WRITE;
/*!40000 ALTER TABLE `wp_woocommerce_shipping_zone_locations` DISABLE KEYS */;
INSERT INTO `wp_woocommerce_shipping_zone_locations` VALUES (1,1,'NP','country');
/*!40000 ALTER TABLE `wp_woocommerce_shipping_zone_locations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_woocommerce_shipping_zone_methods`
--

DROP TABLE IF EXISTS `wp_woocommerce_shipping_zone_methods`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_woocommerce_shipping_zone_methods` (
  `zone_id` bigint unsigned NOT NULL,
  `instance_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `method_id` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `method_order` bigint unsigned NOT NULL,
  `is_enabled` tinyint(1) NOT NULL DEFAULT '1',
  PRIMARY KEY (`instance_id`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_woocommerce_shipping_zone_methods`
--

LOCK TABLES `wp_woocommerce_shipping_zone_methods` WRITE;
/*!40000 ALTER TABLE `wp_woocommerce_shipping_zone_methods` DISABLE KEYS */;
INSERT INTO `wp_woocommerce_shipping_zone_methods` VALUES (1,1,'free_shipping',1,1),(0,2,'flat_rate',1,1);
/*!40000 ALTER TABLE `wp_woocommerce_shipping_zone_methods` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_woocommerce_shipping_zones`
--

DROP TABLE IF EXISTS `wp_woocommerce_shipping_zones`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_woocommerce_shipping_zones` (
  `zone_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `zone_name` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `zone_order` bigint unsigned NOT NULL,
  PRIMARY KEY (`zone_id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_woocommerce_shipping_zones`
--

LOCK TABLES `wp_woocommerce_shipping_zones` WRITE;
/*!40000 ALTER TABLE `wp_woocommerce_shipping_zones` DISABLE KEYS */;
INSERT INTO `wp_woocommerce_shipping_zones` VALUES (1,'Nepal',0);
/*!40000 ALTER TABLE `wp_woocommerce_shipping_zones` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_woocommerce_tax_rate_locations`
--

DROP TABLE IF EXISTS `wp_woocommerce_tax_rate_locations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_woocommerce_tax_rate_locations` (
  `location_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `location_code` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  `tax_rate_id` bigint unsigned NOT NULL,
  `location_type` varchar(40) COLLATE utf8mb4_unicode_520_ci NOT NULL,
  PRIMARY KEY (`location_id`),
  KEY `tax_rate_id` (`tax_rate_id`),
  KEY `location_type_code` (`location_type`(10),`location_code`(20))
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_woocommerce_tax_rate_locations`
--

LOCK TABLES `wp_woocommerce_tax_rate_locations` WRITE;
/*!40000 ALTER TABLE `wp_woocommerce_tax_rate_locations` DISABLE KEYS */;
INSERT INTO `wp_woocommerce_tax_rate_locations` VALUES (1,'12345',5,'postcode'),(2,'123456',5,'postcode');
/*!40000 ALTER TABLE `wp_woocommerce_tax_rate_locations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `wp_woocommerce_tax_rates`
--

DROP TABLE IF EXISTS `wp_woocommerce_tax_rates`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `wp_woocommerce_tax_rates` (
  `tax_rate_id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `tax_rate_country` varchar(2) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `tax_rate_state` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `tax_rate` varchar(8) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `tax_rate_name` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  `tax_rate_priority` bigint unsigned NOT NULL,
  `tax_rate_compound` int NOT NULL DEFAULT '0',
  `tax_rate_shipping` int NOT NULL DEFAULT '1',
  `tax_rate_order` bigint unsigned NOT NULL,
  `tax_rate_class` varchar(200) COLLATE utf8mb4_unicode_520_ci NOT NULL DEFAULT '',
  PRIMARY KEY (`tax_rate_id`),
  KEY `tax_rate_country` (`tax_rate_country`),
  KEY `tax_rate_state` (`tax_rate_state`(2)),
  KEY `tax_rate_class` (`tax_rate_class`(10)),
  KEY `tax_rate_priority` (`tax_rate_priority`)
) ENGINE=InnoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_520_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `wp_woocommerce_tax_rates`
--

LOCK TABLES `wp_woocommerce_tax_rates` WRITE;
/*!40000 ALTER TABLE `wp_woocommerce_tax_rates` DISABLE KEYS */;
INSERT INTO `wp_woocommerce_tax_rates` VALUES (1,'GB','','20.0000','VAT',1,1,1,0,''),(2,'GB','','5.0000','VAT',1,1,1,1,'reduced-rate'),(3,'GB','','0.0000','VAT',1,1,1,2,'zero-rate'),(4,'US','','10.0000','US',1,1,1,3,''),(5,'US','AL','2.0000','US AL',2,1,1,4,'');
/*!40000 ALTER TABLE `wp_woocommerce_tax_rates` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2021-02-18 12:42:45


================================================
File: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover

================================================
File: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi

echo "Updating row config schema"
value=`cat component_config/configRowSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationRowSchema --value="$value"
else
    echo "configurationRowSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

================================================
File: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi

================================================
File: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"

================================================
File: src/component.py
================================================
"""
Template Component main class.

"""
import datetime
import logging
import os
import sys
import dateparser
from pathlib import Path

from kbc.env_handler import KBCEnvHandler

from result import OrdersWriter, CustomersWriter, ProductsWriter
from woocommerce_cli import WooCommerceClient, error_handling

# configuration variables
STORE_URL = "store_url"
CONSUMER_KEY = "#consumer_key"
CONSUMER_SECRET = "#consumer_secret"
KEY_QUERY_STRING_AUTH = "query_string_auth"
DATE_FROM = "date_from"
DATE_TO = "date_to"
ENDPOINT = "endpoint"
KEY_INCREMENTAL = "load_type"

KEY_FETCHING_MODE = "fetching_mode"
KEY_CUSTOM_INCREMENTAL_FIELD = "custom_incremental_field"
KEY_CUSTOM_INCREMENTAL_VALUE = "custom_incremental_value"
# params for compatibility with old version that had flatten_metadata option which could cause oom
KEY_ADDITIONAL_OPTIONS = "additional_options"
KEY_FLATTEN_METADATA = "flatten_metadata_values"
# #### Keep for debug
KEY_DEBUG = "debug"

# list of mandatory parameters => if some is missing, component will fail with readable message on initialization.
MANDATORY_PARS = [
    STORE_URL,
    CONSUMER_KEY,
    CONSUMER_SECRET,
    ENDPOINT,
]
# MANDATORY_PARS = [KEY_DEBUG]
MANDATORY_IMAGE_PARS = []


class UserException(Exception):
    pass


class Component(KBCEnvHandler):
    def __init__(self, debug=False):
        # for easier local project setup
        default_data_dir = (
            Path(__file__).resolve().parent.parent.joinpath("data").as_posix()
            if not os.environ.get("KBC_DATADIR")
            else None
        )

        KBCEnvHandler.__init__(
            self,
            MANDATORY_PARS,
            log_level=logging.DEBUG if debug else logging.INFO,
            data_path=default_data_dir,
        )
        # override debug from config
        if self.cfg_params.get(KEY_DEBUG):
            debug = True
        if debug:
            logging.getLogger().setLevel(logging.DEBUG)
        else:
            logging.getLogger("woocommerce.component").setLevel(logging.WARNING)
        logging.info("Loading configuration...")

        try:
            # validation of mandatory parameters. Produces ValueError
            self.validate_config(MANDATORY_PARS)
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as e:
            logging.exception(e)
            exit(1)

        self.client = WooCommerceClient(
            url=self.cfg_params.get("store_url"),
            consumer_key=self.cfg_params.get(CONSUMER_KEY),
            consumer_secret=self.cfg_params.get(CONSUMER_SECRET),
            version=self.cfg_params.get("version", "wc/v3"),
            query_string_auth=self.cfg_params.get(KEY_QUERY_STRING_AUTH, False)
        )
        self.extraction_time = datetime.datetime.now().isoformat()
        self.flatten_metadata = self.cfg_params.get(KEY_ADDITIONAL_OPTIONS, {}).get(KEY_FLATTEN_METADATA, False)
        if self.flatten_metadata:
            logging.warning("The component has been started with flatten metadata param set to true. This legacy "
                            "option can cause oom error in some cases. Please consider turning this off and processing "
                            "extracted data using Keboola transformations.")

    def run(self):
        """
        Main execution code
        """
        params = self.cfg_params  # noqa

        last_state = self.get_state_file()
        start_date = end_date = custom_incremental_date = None

        param_start_date = params.get(DATE_FROM, None)
        param_end_date = params.get(DATE_TO, None)
        fetching_mode = params.get(KEY_FETCHING_MODE, "Incremental Fetching with publish date")
        custom_incremental_value = params.get(KEY_CUSTOM_INCREMENTAL_VALUE, None)
        custom_incremental_field = params.get(KEY_CUSTOM_INCREMENTAL_FIELD, None)

        if param_start_date and param_end_date and fetching_mode == "Incremental Fetching with publish date":
            start_date, end_date = self.get_date_period_converted(param_start_date, param_end_date)
            start_date = start_date.replace(microsecond=0).isoformat()
            end_date = end_date.replace(microsecond=0).isoformat()
            logging.info(f"Getting data From: {start_date} To: {end_date}")

        elif custom_incremental_value and custom_incremental_field and \
                fetching_mode == "Incremental Fetching with custom field":
            try:
                custom_incremental_date = dateparser.parse(custom_incremental_value).replace(microsecond=0).isoformat()
            except ValueError as val_err:
                raise UserException("Failed to parse custom incremental date") from val_err
            logging.info(
                f"Getting data From: {custom_incremental_date} Till Now using '{custom_incremental_field}' param")
        else:
            logging.info("Getting all data")
        results = []
        endpoints = params.get("endpoint", ["Orders", "Products", "Customers"])
        for endpoint in endpoints:
            if endpoint.lower() == "orders":
                logging.info("Downloading Orders")
                results.extend(
                    self.download_orders(
                        start_date,
                        end_date,
                        last_state,
                        custom_incremental_field,
                        custom_incremental_date
                    )
                )
            if endpoint.lower() == "products":
                logging.info("Downloading Products")
                results.extend(
                    self.download_products(
                        start_date,
                        end_date,
                        last_state,
                        custom_incremental_field,
                        custom_incremental_date
                    )
                )
            if endpoint.lower() == "customers":
                logging.info("Downloading Customers")
                results.extend(self.download_customers(last_state))

        # get current columns and store in state
        headers = {}
        for r in results:
            file_name = os.path.basename(r.full_path)
            headers[file_name] = r.table_def.columns
        self.write_state_file(headers)

        self.create_manifests(results, incremental=params.get(KEY_INCREMENTAL, True))

    @error_handling
    def download_orders(self, start_date, end_date, file_headers, custom_incremental_field,
                        custom_incremental_date):
        with OrdersWriter(
                self.tables_out_path,
                "order",
                extraction_time=self.extraction_time,
                file_headers=file_headers,
                flatten_metadata=self.flatten_metadata
        ) as writer:
            for data in self.client.get_orders(date_from=start_date, date_to=end_date,
                                               custom_incremental_field=custom_incremental_field,
                                               custom_incremental_date=custom_incremental_date):
                try:
                    for obj in data:
                        writer.write(obj)
                except Exception as err:
                    logging.error(f"Fail to download orders: {err}")
        results = writer.collect_results()
        return results

    @error_handling
    def download_customers(self, file_headers):
        with CustomersWriter(
                self.tables_out_path,
                "customer",
                extraction_time=self.extraction_time,
                file_headers=file_headers,
                flatten_metadata=self.flatten_metadata
        ) as writer:
            for data in self.client.get_customers():
                try:
                    for customer in data:
                        writer.write(customer)
                except Exception as err:
                    logging.error(f"Fail to fetch customers {err}")
        results = writer.collect_results()
        return results

    @error_handling
    def download_products(self, start_date, end_date, file_headers, custom_incremental_field,
                          custom_incremental_date):
        with ProductsWriter(
                self.tables_out_path,
                "product",
                prefix="product__",
                extraction_time=self.extraction_time,
                file_headers=file_headers,
                client=self.client,
                flatten_metadata=self.flatten_metadata
        ) as writer:
            for data in self.client.get_products(
                    date_from=start_date, date_to=end_date, custom_incremental_field=custom_incremental_field,
                    custom_incremental_date=custom_incremental_date
            ):
                try:
                    for product in data:
                        writer.write(product)
                except Exception as err:
                    logging.error(f"Fail to fetch  products {err}")
        results = writer.collect_results()
        return results


"""
        Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug_arg = sys.argv[1]
    else:
        debug_arg = False
    try:
        comp = Component(debug_arg)
        comp.run()
    except Exception as exc:
        logging.exception(exc)
        exit(1)


================================================
File: src/result.py
================================================
from typing import List

from kbc.result import ResultWriter, KBCTableDef

EXTRACTION_TIME = "extraction_time"
KEY_ROW_NR = "row_nr"  # take row number


class MetadataWriter(ResultWriter):
    def __init__(
            self,
            result_dir_path,
            extraction_time,
            additional_pk: List[str] = None,
            prefix="",
            file_headers=None,
            flatten_metadata=True
    ) -> None:
        pk = ["id"]
        if additional_pk:
            pk.extend(additional_pk)
        result_name = f"{prefix}metadata"
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=result_name,
                pk=pk,
                columns=file_headers.get(f"{prefix}metadata.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=flatten_metadata,
            child_separator="__",
        )
        self.extration_time = extraction_time
        self.result_dir_path = result_dir_path


class FeeLinesWriter(ResultWriter):
    def __init__(
            self,
            result_dir_path,
            extraction_time,
            additional_pk: List[str] = None,
            prefix="",
            file_headers=None,
    ) -> None:
        pk = ["id"]
        if additional_pk:
            pk.extend(additional_pk)
        result_name = f"{prefix}fee_lines"
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=result_name,
                pk=pk,
                columns=file_headers.get(f"{prefix}fee_lines.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.extration_time = extraction_time
        self.result_dir_path = result_dir_path


class RefundsWriter(ResultWriter):
    def __init__(
            self,
            result_dir_path,
            extraction_time,
            additional_pk: List[str] = None,
            prefix="",
            file_headers=None,
    ) -> None:
        pk = ["id"]
        if additional_pk:
            pk.extend(additional_pk)
        result_name = f"{prefix}refunds"
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=result_name,
                pk=pk,
                columns=file_headers.get(f"{prefix}refunds.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.extration_time = extraction_time
        self.result_dir_path = result_dir_path


class LineItemsWriter(ResultWriter):
    def __init__(
            self,
            result_dir_path,
            extraction_time,
            additional_pk: List[str] = None,
            prefix: str = "",
            file_headers=None,
            flatten_metadata=True
    ):
        pk = ["id"]
        if additional_pk:
            pk.extend(additional_pk)
        file_name = f"{prefix}line_items"
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=file_name,
                pk=pk,
                columns=file_headers.get(f"{file_name}.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.extraction_time = extraction_time
        self.result_dir_path = result_dir_path
        primary_keys = pk + ["line_item_id"]
        # taxes writer
        self.taxes_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}line_items__taxes",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}line_items__taxes.csv", []),
                destination="",
            ),
            flatten_objects=True,
            fix_headers=True,
            child_separator="__",
        )
        # meta_data writer
        self.meta_data_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}line_items__meta_data",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}line_items__meta_data.csv", []),
                destination="",
            ),
            flatten_objects=flatten_metadata,
            fix_headers=True,
            child_separator="__",
        )

    def write(
            self,
            data,
            file_name=None,
            user_values=None,
            object_from_arrays=False,
            write_header=True,
    ):
        # flatten obj
        line_item_id = data["id"]
        taxes = data.pop("taxes", [])
        meta_data = data.pop("meta_data", [])
        self.taxes_writer.write_all(
            taxes,
            user_values={
                **user_values,
                "line_item_id": line_item_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        self.meta_data_writer.write_all(
            meta_data,
            user_values={
                **user_values,
                "line_item_id": line_item_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )

        super().write(data, file_name, user_values, object_from_arrays, write_header)

    def collect_results(self):
        results = []
        results.extend(self.taxes_writer.collect_results())
        results.extend(self.meta_data_writer.collect_results())
        results.extend(super().collect_results())
        return results

    def close(self):
        self.taxes_writer.close()
        self.meta_data_writer.close()
        super().close()


class TaxLinesWriter(ResultWriter):
    def __init__(
            self,
            result_dir_path,
            extraction_time,
            additional_pk: list = None,
            prefix="",
            file_headers=None,
            flatten_metadata=True
    ) -> None:
        pk = ["id"]
        if additional_pk:
            pk.extend(additional_pk)
        file_name = f"{prefix}tax_lines"
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=file_name,
                pk=pk,
                columns=file_headers.get(f"{file_name}.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.extraction_time = extraction_time
        self.result_dir_path = result_dir_path
        primary_keys = pk + ["tax_lines_id"]
        # meta_data writer
        self.meta_data_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}tax_lines__meta_data",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}tax_lines__meta_data.csv", []),
                destination="",
            ),
            flatten_objects=flatten_metadata,
            fix_headers=True,
            child_separator="__",
        )

    def write(
            self,
            data,
            file_name=None,
            user_values=None,
            object_from_arrays=False,
            write_header=True,
    ):
        # flatten obj
        tax_lines_id = data["id"]
        meta_data = data.pop("meta_data", [])
        self.meta_data_writer.write_all(
            meta_data,
            user_values={
                **user_values,
                "tax_lines_id": tax_lines_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )

        super().write(data, file_name, user_values, object_from_arrays, write_header)

    def collect_results(self):
        results = []
        results.extend(self.meta_data_writer.collect_results())
        results.extend(super().collect_results())
        return results

    def close(self):
        self.meta_data_writer.close()
        super().close()


class ShippingLinesWriter(ResultWriter):
    def __init__(
            self,
            result_dir_path,
            extraction_time,
            additional_pk: list = None,
            prefix="",
            file_headers=None,
            flatten_metadata=True
    ) -> None:
        pk = ["id"]
        if additional_pk:
            pk.extend(additional_pk)
        file_name = f"{prefix}shipping_lines"
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=file_name,
                pk=pk,
                columns=file_headers.get(f"{file_name}.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.extraction_time = extraction_time

        self.result_dir_path = result_dir_path

        # tax_lines writer
        primary_keys = pk + ["shipping_lines_id"]
        self.taxes_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}shipping_lines__taxes",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}shipping_lines__taxes.csv", []),
                destination="",
            ),
            flatten_objects=True,
            fix_headers=True,
            child_separator="__",
        )

        # meta_data writer
        self.meta_data_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}shipping_lines__meta_data",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}shipping_lines__meta_data.csv", []),
                destination="",
            ),
            flatten_objects=flatten_metadata,
            fix_headers=True,
            child_separator="__",
        )

    def write(
            self,
            data,
            file_name=None,
            user_values=None,
            object_from_arrays=False,
            write_header=True,
    ):
        # flatten obj
        shipping_line_id = data["id"]
        taxes = data.pop("taxes", [])
        meta_data = data.pop("meta_data", [])

        self.taxes_writer.write_all(
            taxes,
            user_values={
                **user_values,
                "shipping_lines_id": shipping_line_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        self.meta_data_writer.write_all(
            meta_data,
            user_values={
                **user_values,
                "shipping_lines_id": shipping_line_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        super().write(data, file_name, user_values, object_from_arrays, write_header)

    def collect_results(self):
        results = []
        results.extend(self.taxes_writer.collect_results())
        results.extend(self.meta_data_writer.collect_results())
        results.extend(super().collect_results())
        return results

    def close(self):
        self.taxes_writer.close()
        self.meta_data_writer.close()
        super().close()


class CouponLinesWriter(ResultWriter):
    def __init__(
            self,
            result_dir_path,
            extraction_time,
            additional_pk: list = None,
            prefix="",
            file_headers=None,
            flatten_metadata=True
    ):
        pk = ["id"]
        if additional_pk:
            pk.extend(additional_pk)
        file_name = f"{prefix}coupon_lines"
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=file_name,
                pk=pk,
                columns=file_headers.get(f"{file_name}.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.extraction_time = extraction_time

        self.result_dir_path = result_dir_path
        primary_keys = pk + ["coupon_lines_id"]
        self.meta_data_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}coupon_lines__meta_data",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}coupon_lines__meta_data.csv", []),
                destination="",
            ),
            flatten_objects=flatten_metadata,
            fix_headers=True,
            child_separator="__",
        )

    def write(
            self,
            data,
            file_name=None,
            user_values=None,
            object_from_arrays=False,
            write_header=True,
    ):
        # flatten obj
        coupon_lines_id = data["id"]
        meta_data = data.pop("meta_data", [])
        self.meta_data_writer.write_all(
            meta_data,
            user_values={
                **user_values,
                "coupon_lines_id": coupon_lines_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )

        super().write(data, file_name, user_values, object_from_arrays, write_header)

    def collect_results(self):
        results = []
        results.extend(self.meta_data_writer.collect_results())
        results.extend(super().collect_results())
        return results

    def close(self):
        self.meta_data_writer.close()
        super().close()


class OrdersWriter(ResultWriter):
    def __init__(
            self, result_dir_path, result_name, extraction_time, file_headers=None,
            flatten_metadata=True
    ):
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=result_name,
                pk=["id"],
                columns=file_headers.get("order.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.extraction_time = extraction_time
        self.user_value_cols = ["extraction_time"]
        self.result_dir_path = result_dir_path

        self.line_items_writer = LineItemsWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["order_id"],
            file_headers=file_headers,
            prefix="order__",
            flatten_metadata=flatten_metadata
        )

        self.tax_lines_writer = TaxLinesWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["order_id"],
            file_headers=file_headers,
            prefix="order__",
            flatten_metadata=flatten_metadata
        )

        self.shipping_lines_writer = ShippingLinesWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["order_id"],
            file_headers=file_headers,
            prefix="order__",
            flatten_metadata=flatten_metadata
        )
        self.order_meta_data_writer = MetadataWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["order_id"],
            file_headers=file_headers,
            prefix="order__",
            flatten_metadata=flatten_metadata
        )
        self.coupon_lines_writer = CouponLinesWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["order_id"],
            file_headers=file_headers,
            prefix="order__",
            flatten_metadata=flatten_metadata
        )
        self.fee_lines_writer = FeeLinesWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["order_id"],
            file_headers=file_headers,
            prefix="order__"
        )
        self.refunds_writer = RefundsWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["order_id"],
            file_headers=file_headers,
            prefix="order__",
        )

    def write(
            self,
            data,
            file_name=None,
            user_values=None,
            object_from_arrays=False,
            write_header=True,
    ):
        excludes = ["_links", "customer_user_agent"]
        for field in excludes:
            data.pop(field)
        order_id = data["id"]
        line_items = data.pop("line_items", [])
        tax_lines = data.pop("self.tax_lines_writer", [])
        shipping_lines = data.pop("shipping_lines", [])
        order_meta_data = data.pop("meta_data", [])
        coupon_lines = data.pop("coupon_lines", [])
        refunds = data.pop("fee_lines", [])
        fee_lines = data.pop("fee_lines", [])
        self.line_items_writer.write_all(
            line_items,
            user_values={"order_id": order_id, EXTRACTION_TIME: self.extraction_time},
        )
        self.tax_lines_writer.write_all(
            tax_lines,
            user_values={"order_id": order_id, EXTRACTION_TIME: self.extraction_time},
        )
        self.shipping_lines_writer.write_all(
            shipping_lines,
            user_values={"order_id": order_id, EXTRACTION_TIME: self.extraction_time},
        )
        self.order_meta_data_writer.write_all(
            order_meta_data,
            user_values={"order_id": order_id, EXTRACTION_TIME: self.extraction_time},
        )
        self.coupon_lines_writer.write_all(
            coupon_lines,
            user_values={"order_id": order_id, EXTRACTION_TIME: self.extraction_time},
        )
        self.fee_lines_writer.write_all(
            fee_lines,
            user_values={"order_id": order_id, EXTRACTION_TIME: self.extraction_time},
        )
        self.refunds_writer.write_all(
            refunds,
            user_values={"order_id": order_id, EXTRACTION_TIME: self.extraction_time},
        )
        super().write(
            data=data,
            file_name=file_name,
            user_values=user_values,
            object_from_arrays=object_from_arrays,
            write_header=write_header,
        )

    def collect_results(self):
        results = []
        results.extend(self.line_items_writer.collect_results())
        results.extend(self.tax_lines_writer.collect_results())
        results.extend(self.shipping_lines_writer.collect_results())
        results.extend(self.order_meta_data_writer.collect_results())
        results.extend(self.coupon_lines_writer.collect_results())
        results.extend(self.fee_lines_writer.collect_results())
        results.extend(self.refunds_writer.collect_results())
        results.extend(super().collect_results())
        return results

    def close(self):
        self.line_items_writer.close()
        self.tax_lines_writer.close()
        self.shipping_lines_writer.close()
        self.order_meta_data_writer.close()
        self.coupon_lines_writer.close()
        self.fee_lines_writer.close()
        self.refunds_writer.close()
        super().close()


class CustomersWriter(ResultWriter):
    def __init__(
            self, result_dir_path, result_name, extraction_time, file_headers=None,
            flatten_metadata=True
    ):
        super().__init__(
            result_dir_path,
            KBCTableDef(
                name=result_name,
                pk=["id"],
                columns=file_headers.get("customer.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.meta_data_writer = MetadataWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["customer_id"],
            file_headers=file_headers,
            prefix="customer__",
            flatten_metadata=flatten_metadata
        )
        self.extraction_time = extraction_time
        self.user_value_cols = ["extraction_time"]
        self.result_dir_path = result_dir_path

    def write(
            self,
            data,
            file_name=None,
            user_values=None,
            object_from_arrays=False,
            write_header=True,
    ):
        excludes = ["_links"]
        for field in excludes:
            data.pop(field)
        customer_id = data.get("id")
        meta_data = data.pop("meta_data", [])
        self.meta_data_writer.write_all(
            meta_data,
            user_values={
                "customer_id": customer_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        super().write(
            data,
            file_name=file_name,
            user_values=user_values,
            object_from_arrays=object_from_arrays,
            write_header=write_header,
        )

    def collect_results(self):
        results = []
        results.extend(self.meta_data_writer.collect_results())
        results.extend(super().collect_results())
        return results

    def close(self):
        self.meta_data_writer.close()
        super().close()


class ProductsWriter(ResultWriter):
    def __init__(
            self,
            result_dir_path: str,
            result_name: str,
            extraction_time,
            prefix="",
            file_headers=None,
            client=None,
            flatten_metadata=True
    ):
        self.client = client
        pk = ["id"]
        super().__init__(
            result_dir_path,
            table_def=KBCTableDef(
                name=result_name,
                pk=pk,
                columns=file_headers.get("product.csv", []),
                destination="",
            ),
            fix_headers=True,
            flatten_objects=True,
            child_separator="__",
        )
        self.extraction_time = extraction_time
        self.user_value_cols = ["extraction_time"]
        self.result_dir_path = result_dir_path
        primary_keys = pk + ["product_id"]
        self.categories_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}categories",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}categories.csv", []),
                destination="",
            ),
            flatten_objects=True,
            fix_headers=True,
            child_separator="__",
        )

        self.images_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}images",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}images.csv", []),
                destination="",
            ),
            flatten_objects=True,
            fix_headers=True,
            child_separator="__",
        )

        self.attributes_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}attributes",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}attributes.csv", []),
                destination="",
            ),
            flatten_objects=True,
            fix_headers=True,
            child_separator="__",
        )

        self.default_attributes_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}default_attributes",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}default_attributes.csv", []),
                destination="",
            ),
            flatten_objects=True,
            fix_headers=True,
            child_separator="__",
        )

        self.tags_writer = ResultWriter(
            result_dir_path,
            KBCTableDef(
                name=f"{prefix}tags",
                pk=primary_keys,
                columns=file_headers.get(f"{prefix}tags.csv", []),
                destination="",
            ),
            flatten_objects=True,
            fix_headers=True,
            child_separator="__",
        )
        self.meta_data_writer = MetadataWriter(
            result_dir_path,
            extraction_time,
            additional_pk=["product_id"],
            file_headers=file_headers,
            prefix="product__",
            flatten_metadata=flatten_metadata
        )

    def write(
            self,
            data,
            file_name=None,
            user_values=None,
            object_from_arrays=False,
            write_header=True,
    ):
        product_id = data.get("id", "Not found")
        excludes = ["_links", "downloads"]
        for field in excludes:
            data.pop(field)
        categories = data.pop("categories", [])
        images = data.pop("images", [])
        attributes = data.pop("attributes", [])
        default_attributes = data.pop("default_attributes", [])
        meta_data = data.pop("meta_data", [])
        tags = data.pop("tags", [])
        self.categories_writer.write_all(
            categories,
            user_values={
                "product_id": product_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        self.images_writer.write_all(
            images,
            user_values={
                "product_id": product_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        self.attributes_writer.write_all(
            attributes,
            user_values={
                "product_id": product_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        self.default_attributes_writer.write_all(
            default_attributes,
            user_values={
                "product_id": product_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        self.tags_writer.write_all(
            tags,
            user_values={
                "product_id": product_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        self.meta_data_writer.write_all(
            meta_data,
            user_values={
                "product_id": product_id,
                EXTRACTION_TIME: self.extraction_time,
            },
        )
        super().write(data, file_name, user_values, object_from_arrays, write_header)

    def collect_results(self):
        results = []
        results.extend(self.categories_writer.collect_results())
        results.extend(self.images_writer.collect_results())
        results.extend(self.attributes_writer.collect_results())
        results.extend(self.default_attributes_writer.collect_results())
        results.extend(self.tags_writer.collect_results())
        results.extend(self.meta_data_writer.collect_results())
        results.extend(super().collect_results())
        return results

    def close(self):
        self.categories_writer.close()
        self.images_writer.close()
        self.attributes_writer.close()
        self.default_attributes_writer.close()
        self.tags_writer.close()
        self.meta_data_writer.close()
        super().close()


================================================
File: src/woocommerce_cli.py
================================================
import datetime
import functools
import logging
import math
import sys

import backoff
import requests
from requests import Response
from woocommerce import API

RESULTS_PER_PAGE = 100

# We will retry a 500 error a maximum of 5 times before giving up
MAX_RETRIES = 5


class ConnectionError(Exception):
    pass


class WooCommerceClientError(Exception):
    pass


class HTTPSProtocolError(Exception):
    pass


class UnauthorizedError(Exception):
    pass


def is_not_status_code_fn(status_code):
    def gen_fn(exc):
        if getattr(exc, "code", None) and exc.code not in status_code or getattr(exc, "code", None) is None:
            return True
        # Retry other errors up to the max
        return False

    return gen_fn


def leaky_bucket_handler(details):
    logging.info("Received 429 -- sleeping for %s seconds", details["wait"])


def retry_handler(details):
    logging.info(
        "Received 500 or retryable error -- Retry %s/%s", details["tries"], MAX_RETRIES
    )


# pylint: disable=unused-argument
def retry_after_wait_gen(**kwargs):
    # This is called in an except block so we can retrieve the exception
    # and check it.
    exc_info = sys.exc_info()
    if not exc_info[1].response:
        return 0
    resp = exc_info[1].response
    # Retry-After is an undocumented header. But honoring
    # it was proven to work in our spikes.
    # It's been observed to come through as lowercase, so fallback if not present
    sleep_time_str = resp.headers.get("Retry-After", resp.headers.get("retry-after"))
    yield math.floor(float(sleep_time_str) * 2)


def error_handling(fnc):
    @backoff.on_exception(
        backoff.expo,
        (
                requests.exceptions.HTTPError,
                requests.exceptions.Timeout,
                requests.exceptions.ConnectionError
        ),
        giveup=is_not_status_code_fn(range(500, 599)),
        on_backoff=retry_handler,
        max_tries=MAX_RETRIES,
    )
    @backoff.on_exception(
        retry_after_wait_gen,
        requests.exceptions.ConnectionError,
        giveup=is_not_status_code_fn([429, None]),
        on_backoff=leaky_bucket_handler,
        # No jitter as we want a constant value
        jitter=None,
    )
    @functools.wraps(fnc)
    def wrapper(*args, **kwargs):
        return fnc(*args, **kwargs)

    return wrapper


def response_error_handling(func):
    """Function, that handles response handling of HTTP requests."""

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except requests.exceptions.HTTPError as e:
            logging.error(e, exc_info=True)
            # Handle different error codes
            raise WooCommerceClientError(
                "The resource was not found. Please check your store url!"
            ) from e
        except Exception as e:
            raise e

    return wrapper


class WooCommerceClient:
    def __init__(
            self,
            url: str,
            consumer_key: str,
            consumer_secret: str,
            version: str = "wc/v3",
            authenticate: bool = True,
            query_string_auth: bool = False
    ):
        self.session = API(
            url=url,
            consumer_key=consumer_key,
            timeout=120,
            consumer_secret=consumer_secret,
            version=version,
            query_string_auth=query_string_auth
        )
        if authenticate:
            response = self.session.get("")
            self._handle_response(response)

    def _handle_response(self, response: Response):
        try:
            if response.status_code == 401:
                r = response.json()
                msg = f"message: {r['message']} error: {r['code']} status: {r['data']['status']}"
                raise UnauthorizedError(msg)
            response.raise_for_status()
        except requests.exceptions.SSLError as err:
            logging.error(err)
            raise HTTPSProtocolError(
                "Verify the site has valid ssl certificates"
            ) from err
        except requests.exceptions.ConnectionError as err:
            logging.error(err)
            raise ConnectionError(
                "Failed to establish a connection, please correct and verify the store_url"
            ) from err

    @error_handling
    def _fetch_data(self, endpoint, params):
        """
        Fetch all data
        """
        page_count = 1
        # if any date_from or date_to is None then download all data for orders and products
        if endpoint in ["orders", "products"] and not (params.get("after") or params.get("before")):
            params.pop("after")
            params.pop("before")
        response = self.session.get(endpoint, params=params)
        self._handle_response(response)
        if response.status_code == 200:
            yield response.json()
            total_pages = int(response.headers.get("X-WP-TotalPages", 1))
            while page_count < total_pages:
                page_count += 1
                params["page"] = page_count
                response = self.session.get(endpoint, params=params)
                response.raise_for_status()
                if response.status_code == 200:
                    yield response.json()

    def get_orders(
            self,
            date_from: str = "",
            date_to: str = datetime.datetime.utcnow().replace(microsecond=0).isoformat(),
            status: str = "any",
            per_page: int = RESULTS_PER_PAGE,
            custom_incremental_field: str = None,
            custom_incremental_date: str = None
    ):
        if custom_incremental_field and custom_incremental_date:
            params = {
                "per_page": per_page,
                "status": status,
                "after": None,
                "before": None,
                custom_incremental_field: custom_incremental_date
            }
        else:
            params = {
                "per_page": per_page,
                "status": status,
                "after": date_from,
                "before": date_to,
            }
        return self._fetch_data("orders", params)

    def get_products(
            self,
            date_from: str = "",
            date_to: str = datetime.datetime.utcnow().replace(microsecond=0).isoformat(),
            status: str = "any",
            per_page: int = RESULTS_PER_PAGE,
            custom_incremental_field: str = None,
            custom_incremental_date: str = None
    ):
        if custom_incremental_field and custom_incremental_date:
            params = {
                "per_page": per_page,
                "status": status,
                "after": None,
                "before": None,
                custom_incremental_field: custom_incremental_date
            }
        else:
            params = {
                "per_page": per_page,
                "status": status,
                "after": date_from,
                "before": date_to,
            }
        data = self._fetch_data("products", params)
        return data

    def get_customers(self, per_page: int = RESULTS_PER_PAGE):
        """
        Get all customers
        """
        params = {"per_page": per_page, 'role': 'all'}
        data = self._fetch_data("customers", params)
        return data


================================================
File: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")

================================================
File: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on:
  push:
    branches:
      - 'feature/*'
      - 'bug/*'
    tags:
      - '*' # Skip the workflow on the main branch without tags

concurrency: ci-${{ github.ref }} # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: "kds-team.ex-woocommerce" # replace with your component id
  KBC_DEVELOPERPORTAL_VENDOR: "kds-team" # replace with your vendor
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: "kds-team+github"

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }} # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (Optional) Test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: "" # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }} # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo ${raw//origin\//} | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh

