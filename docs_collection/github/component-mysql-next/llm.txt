Directory structure:
└── keboola-component-mysql-next/
    ├── README.md
    ├── Dockerfile
    ├── LICENSE.md
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── flake8.cfg
    ├── requirements.txt
    ├── test-requirements.txt
    ├── test.csv
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   ├── sourceCodeUrl.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       ├── in/
    │       │   ├── state.json
    │       │   ├── files/
    │       │   │   └── order1.xml
    │       │   └── tables/
    │       │       ├── test.csv
    │       │       └── test.csv.manifest
    │       └── out/
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               └── test.csv
    ├── resource/
    │   ├── pypy3.8-v7.3.9-osx64.tar.bz2
    │   └── pypy3.8-v7.3.9-osx64/
    │       ├── LICENSE
    │       ├── README.rst
    │       ├── bin/
    │       │   ├── normalizer
    │       │   ├── pip3
    │       │   ├── pip3.8
    │       │   ├── pypy
    │       │   ├── pypy
    │       │   ├── pypy3
    │       │   ├── pypy3.8
    │       │   ├── pypy3.8
    │       │   ├── python
    │       │   ├── python
    │       │   ├── python3
    │       │   ├── python3
    │       │   ├── python3.8
    │       │   ├── python3.8
    │       │   ├── sqlformat
    │       │   ├── sshtunnel
    │       │   └── wheel
    │       └── include/
    │           ├── README
    │           └── pypy3.8/
    │               ├── Python.h
    │               ├── abstract.h
    │               ├── boolobject.h
    │               ├── bytearrayobject.h
    │               ├── bytesobject.h
    │               ├── cStringIO.h
    │               ├── ceval.h
    │               ├── code.h
    │               ├── compile.h
    │               ├── complexobject.h
    │               ├── cpyext_datetime.h
    │               ├── cpyext_descrobject.h
    │               ├── cpyext_genobject.h
    │               ├── cpyext_memoryobject.h
    │               ├── cpyext_moduleobject.h
    │               ├── cpyext_object.h
    │               ├── cpyext_unicodeobject.h
    │               ├── datetime.h
    │               ├── descrobject.h
    │               ├── dictobject.h
    │               ├── eval.h
    │               ├── exports.h
    │               ├── fileobject.h
    │               ├── floatobject.h
    │               ├── frameobject.h
    │               ├── funcobject.h
    │               ├── genobject.h
    │               ├── graminit.h
    │               ├── import.h
    │               ├── listobject.h
    │               ├── longintrepr.h
    │               ├── longobject.h
    │               ├── marshal.h
    │               ├── memoryobject.h
    │               ├── methodobject.h
    │               ├── missing.h
    │               ├── modsupport.h
    │               ├── moduleobject.h
    │               ├── object.h
    │               ├── patchlevel.h
    │               ├── pycapsule.h
    │               ├── pyconfig.h
    │               ├── pyerrors.h
    │               ├── pyhash.h
    │               ├── pylifecycle.h
    │               ├── pymacro.h
    │               ├── pymath.h
    │               ├── pymem.h
    │               ├── pyport.h
    │               ├── pypy_decl.h
    │               ├── pypy_macros.h
    │               ├── pypy_marshal_decl.h
    │               ├── pypy_structmember_decl.h
    │               ├── pysignals.h
    │               ├── pystate.h
    │               ├── pystrhex.h
    │               ├── pystrtod.h
    │               ├── pythonrun.h
    │               ├── pythread.h
    │               ├── pytime.h
    │               ├── setobject.h
    │               ├── sliceobject.h
    │               ├── structmember.h
    │               ├── structseq.h
    │               ├── sysmodule.h
    │               ├── traceback.h
    │               ├── tupleobject.h
    │               ├── typeslots.h
    │               ├── unicodeobject.h
    │               └── warnings.h
    ├── scripts/
    │   ├── build_and_run.ps1
    │   ├── build_and_test.sh
    │   ├── run_kbc_tests.ps1
    │   ├── run_pipeline.sh
    │   ├── update_dev_portal_properties.sh
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── table_metadata.py
    │   ├── core/
    │   │   ├── __init__.py
    │   │   ├── bookmarks.py
    │   │   ├── catalog.py
    │   │   ├── datatypes.py
    │   │   ├── env_handler.py
    │   │   ├── messages.py
    │   │   ├── metadata.py
    │   │   ├── metrics.py
    │   │   ├── schema.py
    │   │   ├── statediff.py
    │   │   ├── utils.py
    │   │   └── yaml_mappings.py
    │   └── mysql/
    │       ├── __init__.py
    │       ├── client.py
    │       └── replication/
    │           ├── __init__.py
    │           ├── binlog.py
    │           ├── common.py
    │           ├── ddl_parser.py
    │           ├── full_table.py
    │           ├── incremental.py
    │           └── stream_reader.py
    ├── tests/
    │   ├── __init__.py
    │   ├── test_binlogstreamreader.py
    │   ├── test_component.py
    │   ├── test_ddl_parser.py
    │   └── resources/
    │       ├── deduped.csv
    │       ├── drop_statements.sql
    │       ├── mock_show_binlog.py
    │       └── test.csv
    └── .github/
        └── workflows/
            └── push.yml

================================================
File: README.md
================================================
# MySQL Binlog CDC Extractor

[Keboola Connection](https://www.keboola.com/) component for MySQL databases log based replication.

This connector works with MySQL databases hosted on AWS RDS, Aurora MySQL, and standard non-hosted MySQL.

## Functionality notes

Log-based replication is a type of change data capture (CDC) where incremental changes made to a database are detected
by reading the binary logs (AKA binlogs in MySQL) to pick up only changes since the last execution of this pipeline.
More specifically, all INSERT, UPDATE, and DELETE statements are appropriately recorded for database change capture.
This replication style is actually the fastest method for identifying change (faster than key-based replication in
almost every case) and has the ability to capture hard deletes (so long as they are run as a DELETE, not a TRUNCATE or 
DROP statement), unlike key-based replication. 

Deleted records will be left with a "deletion marker", identified by a
timestamp for the time the record was deleted in the special `KBC_DELETED_AT` column.

### Schema Change handling

Unlike majority solutions on the market, this connector is capable of handling schema changes ADD/DROP COLUMN **without
the need to initiate full-sync**. The schema changes are handled in a following manner:

- **ADD column**
  - Such column is added to the destination table. Historic values will be empty (default not reflected).
  - The event will be logged in the resulting `SCHEMA_CHANGES` table as change type `ADD_COLUMN`
- **DROP column**
  - The column will remain in the destination table.
  - It's values will be NULL/EMPTY since the deletion.
  - `DROP_COLUMN` event will be emitted into the `SCHEMA_CHANGES` table.



### Important Limitations

- Only `INSERT`, `UPDATE`, and `DELETE`statements are collected. This means that any deletes that are results of `TRUNCATE` statement or `DROP` and `CREATE` statements will not be collected.
- On schema change (ADD COLUMN, ALTER) the default values **will not** be reflected to historical records. E.g. if you introduce new column, the replication will continue, but the all historical records prior the column addition will be empty.
- ALTER statements changing datatypes will not be reflected, the original value will be kept.
- If `Native Datatypes` are turned on in the Keboola project, ALTER statement changes may cause unexpected errors.




## MySQL Setup

### Enable row-level log based replication

In order for log-based replication to work, you must have enabled [row-based binary logging](https://dev.mysql.com/doc/refman/8.0/en/binary-log-setting.html)
on your MySQL instance/cluster. If you are using Aurora MySQL you will need to use a
[database cluster parameter group](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html)
with parameter '`binlog_format`' set to '`ROW`'.

By default, MySQL removes these binary logs as soon as possible. However, in order to read them effectively for
replication, we want to extend this retention period. You should set this to anywhere between 1 and 7 days. 

For example,  to set `3 day` retention period, run the following command:

```sql
call mysql.rds_set_configuration('binlog retention hours, 72);
```

The above only needs to be set up during initial setup for replication. You can also run
`call mysql.rds_show_configuration;` to see your existing binlog retention hours value, if any.

### User privileges 

The connector requires db user with privileges replication client privileges and read-only privileges to specified tables. 

Example SQL script to create the user:

```sql
create user keboola@'%' identified by '{insert strong password here}';
grant replication client, replication slave, select ON *.* TO keboola@'%';
```
If you follow the above, you will use Username 'keboola' during configuration, and the password you set for Password.

## Configuration 

### Connection and SSH Tunnel

For additional security you might need to whitelist Keboola IPs (see [here](https://help.keboola.com/components/ip-addresses)).

Fill in the database connection details:

- Hostname - url or hostname of your DB instance. In case you are usng SSH Tunnel, this should be the local network address.
- Port
- User
- Password


You may opt to connect through **SSH Tunnel**.  To do so, so select the `SSH Tunnel` option in the `Authorization` section and provide following parameters:

- SSH host IP
  - Note that this may be different from the actual MySQL Instance address. When using the tunnel the MySQL host is the address in the local network where your SSH tunnel resides, **NOT** the outside world IP
- Port - usually 22, but it is advised to change it to non-standard port 
- SSH user
- RSA SSH Private Key
  - The private key of the SSH user. Make sure that the public counterpart is properly added in the servers `ssh_keys`

#### Generating SSH Key pair

**Generate using UI**

You can let the component generate the SSH key pair for you. Click the `GENERATE SSH KEY PAIR` button to display the private and public key pair.

Make note of both, as they will be lost after you close the popup. Insert the private key to related configuration field and add the public key to ssh server's `ssh_keys`


**Generate key manually**

To generate SSH keypair you can use following bash command:

```shell
#!/bin/bash

# Generate the RSA key pair
ssh-keygen -t rsa -b 2048 -f /FOLDER_PATH/id_rsa -N ""
```
The generated private key will be saved in the `~/FOLDER_PATH/id_rsa` file, and the corresponding public key will be saved in the `~/FOLDER_PATH/id_rsa.pub` file.

### Advanced Options

- **Max Connection Time** - Optional parameter which sets `@@session.max_execution_time` to desired integer value. Use this when
  initial full syncs are failing due to the large size of the tables.
- **Show Binary Log Method** - Special parameter to force the connector to get the `SHOW BINLOG` result from a http
  endpoint instead.
  - This is a special edge-case option, in some cases our customers experienced very slow responses on Aurora databases of certain version
    when running `SHOW BINLOG` command. For this purpose they opted to expose the cached `SHOW BINLOG` result as an API endpoint.

### Sync Options

#### Replication Mode

- **Standard**
  - Performs full sync of the tables on the first run and then continues from the binlog.
- **Snapshot Only**
  - Performs full sync always.

#### Binary data handler

Binary data in most cases cannot be converted between databases 1:1. As our components result in CSV files as intermediate step, 
it is necessary to select Binary data handling strategy:

- **plain** - data in decoded from binary to string using Python's bytes.decode() method,
- **hex** - data is converted to hex representation of binary string using bytes.hex() method,
- **base64** - data is converted to a base64 string, using base64.b64encode() method.


### Data Source

Select schemas(databases) and tables you wish to sync. 

### Destination

- **Load Type**
  - Incremental Load - Events in each batch will be deduplicated and only the latest events will be _upserted_ into the
    destination table based on primary key. Resulting in fully replicated table
  - Append - Each event (INSERT, UPDATE, DELETE) will be appended to the resulting table as a separate row. The user is
    responsible for replication in the downstream processes.
- **Output Bucket** - (Optional) The name of bucket in Keboola storage where the resulting tables are stored. Keboola
  will create a bucket for you if not specified. The name is without the stage prefix e.g. `cdc-input`, the bucket stage
  will always be `IN`.

## Output

**The connector currently outputs all tables and columns in UPPERCASE**. The table names are prefixed with the schema(database) name. 
e.g. The result tables will be stored as `SCHEMA_TABLENAME`

### System columns

The connector generates additional system columns

| Column Name          | Descriptions                                                 |
|----------------------|--------------------------------------------------------------|
| **KBC_SYNCED_AT**    | UTC Timestamp of sync start                                  |
| **KBC_DELETED_AT**   | Epoch Timestamp of row deletion, otherwise NULL              |
| **BINLOG_CHANGE_AT** | Epoch Timestamp of last row change                           |
| **BINLOG_READ_AT**   | Time when the change was read from binlog as Epoch Timestamp |

### SCHEMA_CHANGES table

The schema changes will be logged into table `SCHEMA_CHANGES`:


| Column Name     | Descriptions                                  |
|-----------------|-----------------------------------------------|
| **schema**      | name of the schema                            |
| **table**       | name of the affected table                    |
| **change_type** | Typ of change (`DROP_COLUMN`,`ADD_COLUMN`)    |
| **column_name** | Name of affected column                       |
| **query**       | Full query that resulted in the schema change |
| **timestamp**   | Epoch Timestamp of the event                  |



## Legacy configuration format

For each table or view that you would like to replicate, you just need to add two
options to the "metadata" section of that table or view. First, specify `"selected": true`. Next, choose the replication 
method by setting `"replication-method": "INCREMENTAL"`. Allowed values are `FULL_TABLE`, `INCREMENTAL` and `LOG_BASED`.

If you choose `INCREMENTAL`, you also must specify a replication key, the field that will be used to determine if a 
given row in that table has changed. You specify this with an additional parameter, such as 
`"replication-key": "updated_at"`. `LOG_BASED` is only allowed if the server is set up to support it, and the database 
object is a table, not a view.

For any replication method, once you have chosen "selected" to True at the table level for each table/view you want to 
include, set `"selected": false` for any column(s) that you want to exclude from the replication (i.e. sensitive info),
by default all columns are included for selected tables.

By default all tables and views are excluded to protect potentially sensitive data.
However, if you choose to include a table, all columns are included by default (for ease of adding new tables); any
columns you would like to exclude must be explicitly set as such by including `selected: false` on that column.

#### Pulling Existing Schema Definitions
The extractor has the ability to pull existing schema definitions. If you set the parameter discover_schema to True, the
extractor will pull databases(schemas), tables and fields. You can then choose which to include or exclude.

#### Running Historical Syncs
Upon your first execution, all tables must run an initial first full sync. However, you need to set the configuration
file to do so, it will automatically. Full historical syncs on a particular table will be necessary in the future
whenever you update the schema you are pulling from that table. A full database re-sync will ONLY be necessary if you do
not run a sync for a long period of time, beyond your current 'binlog retention hours' setting.
 


## Development
 
This example contains runnable container with simple unittest. For local testing it is useful to include `data` folder in the root
and use docker-compose commands to run the container or execute tests. 

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path:
```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone https://bitbucket.org:kds_consulting_team/kbc-python-template.git my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

## Testing

The preset pipeline scripts contain sections allowing pushing testing image into the ECR repository and automatic 
testing in a dedicated project. These sections are by default commented out. 

**Running KBC tests on deploy step, before deployment**

Uncomment following section in the deployment step in `bitbucket-pipelines.yml` file:

```yaml
            # push test image to ECR - uncomment when initialised
            # - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
            # - docker tag $APP_IMAGE:latest $REPOSITORY:test
            # - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
            # - docker push $REPOSITORY:test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP BASE_KBC_CONFIG test
            # - docker run --rm -e KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP KBC_CONFIG_1 test
            - ./scripts/update_dev_portal_properties.sh
            - ./deploy.sh
```

Make sure that you have `KBC_STORAGE_TOKEN` env. variable set, containing appropriate storage token with access 
to your KBC project. Also make sure to create a functional testing configuration and replace the `BASE_KBC_CONFIG` placeholder with its id.

**Pushing testing image for manual KBC tests**

In some cases you may wish to execute a testing version of your component manually prior to publishing. For instance to test various
configurations on it. For that it may be convenient to push the `test` image on every push either to master, or any branch.

To achieve that simply uncomment appropriate sections in `bitbucket-pipelines.yml` file, either in master branch step or in `default` step.

```yaml
            # push test image to ecr - uncomment for testing before deployment
#            - echo 'Pushing test image to repo. [tag=test]'
#            - export REPOSITORY=`docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP`
#            - docker tag $APP_IMAGE:latest $REPOSITORY:test
#            - eval $(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)
#            - docker push $REPOSITORY:test
```
 
 Once the build is finished, you may run such configuration in any KBC project as many times as you want by using [run-job](https://kebooladocker.docs.apiary.io/#reference/run/create-a-job-with-image/run-job) API call, using the `test` image tag.

# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/) 

================================================
File: Dockerfile
================================================
FROM pypy:3.9-slim
ENV PYTHONIOENCODING utf-8
# ENV CRYPTOGRAPHY_DONT_BUILD_RUST=1

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
# Update default packages
RUN apt-get update

# Get Ubuntu packages
RUN apt-get install -y \
    build-essential \
    curl \
    libssl-dev \
    openssl

# Get Rust
RUN curl https://sh.rustup.rs -sSf | bash -s -- -y

ENV PATH="/root/.cargo/bin:${PATH}"

RUN pip install flake8


# requirements
RUN pypy -m ensurepip
RUN pypy -m pip install -U pip wheel
RUN pypy -m pip install --upgrade pip
RUN pypy -m pip install -r /code/requirements.txt

WORKDIR /code/

CMD ["pypy", "-u", "/code/src/component.py"]

================================================
File: LICENSE.md
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2020 BetterHelp

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


================================================
File: change_log.md
================================================
**0.5.4**
- Added option to specify `columns` for each table, marking column, which should be downloaded from MySQL binary log
- Added option to specify `columns_to_ignore`. If a change is made in these columns and there's no change in any other column, the event will not be written to storage.
- Added option to specify `columns_to_watch`. If a change happens outside of these columns, the event will not be written to storage.

**0.5.1**
- Fixed bug, where metadata was written to storage for a column, which was not yet occupied with columns

**0.5.0**
- Minor refactoring of code
- Added BINLOG_READ_AT column
- Fixed full load

**0.4.17**
- Output bucket option working correctly

**0.4.16**
- Append mode will not deduplicate based on primary key

**0.4.15**
- Added option for append mode only, which speeds up write to storage.

**0.4.14**
- Reduced traceback
- Fixed JSON mappings of database objects

**0.4.13**
- Added option to specify input mapping tables with JSON
- Binlog events are now filtered for specified tables only. This drastically improves replication speed.

**0.4.10**

- YAML support for input mapping specification
- Proper conversion to strings to avoid float/numeric conversion issues

**0.4.3**

- Allows replication-method to be specified in any case (not just all caps)
- Removed pandas for full table downloads for performance improvement
- Fixed output of ints and floats issue with pandas full sync
- Added docs on filling out mappings file

**0.3.4**

- Grabs only latest binlog event per primary key in extraction since data is uploaded by PK
- Supports sending column metadata to Keboola
- Full table syncs are marked with incremental false
- Added support for set MySQL data type
- Columns are capitalized for Snowflake transforms ease of use
- Primary keys and tables are capitalized

**0.2.25**

- Stable full sync and log-based incremental sync, with performance improvements to come
- Adds ability to manually specify state and outputs state as file mapping
- Proper manifest table handling

**0.2.14**

- Beta full sync that runs in chunks, should be much quicker, manifest file needs fixes
- Log-based still handled at row level

**0.1.1**

- SSH tunnel compatibility and initial full sync option ready.
- Fix for data path when running in container.

**0.1.0**

- Defined core libraries.
- Proof of concept execution on single MySQL table for full and log-based replication.

================================================
File: deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi


================================================
File: docker-compose.yml
================================================
version: "2"
services:
  app: &app
    build: .
    environment:
      MYSQL_DB_HOST: mysql
      MYSQL_DB_SSL_HOST: mysql-ssl
      MYSQL_DB_SSL_BAD_CN_HOST: mysql-different-cn
      MYSQL_DB_USER: root
      MYSQL_DB_PASSWORD: rootpassword
      MYSQL_DB_DATABASE: test
      MYSQL_DB_PORT: 3306
    links:
      - mysql
  # for development purposes
  dev:
    <<: *app
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
      - KBC_CONFIGID=12345
      - KBC_COMPONENTID=kds-team.ex-mysql-next
  mysql:
    image: mysql:${MYSQL_VERSION}
    command: mysqld --local-infile --port=3306 --default-authentication-plugin=mysql_native_password
    environment:
      MYSQL_DATABASE: test
      MYSQL_ROOT_PASSWORD: rootpassword
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_and_test.sh



================================================
File: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests,
    venv,
    core
ignore = F841, W504
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting


================================================
File: requirements.txt
================================================
PyYAML
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.5.2.zip#egg=kbc
backoff==1.10.0
ciso8601==2.1.3
dateparser==0.7.4
freezegun==0.3.15
logging-gelf==0.0.26
pygelf==0.3.6
mock==4.0.2
mysql-replication==0.43.0
paramiko==2.7.2
pendulum==1.2.0
PyMySQL==1.1.0
pytz==2020.1
simplejson==3.17.0
sshtunnel==0.4.0
sqlparse

================================================
File: test-requirements.txt
================================================
pytest
https://bitbucket.org/kds_consulting_team/datadirtest/get/1.5.1.zip#egg=datadirtest

================================================
File: test.csv
================================================
Type,Campaign_Name,Status,Start_Date,End_Date,Location,Eventbrite_link
Event,Are You Using Data to Understand Your Customers? ,Complete,2018-02-27,2018-02-27,United Kingdom2,https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611
Event,Are You Using Data to Understand Your Customers? ,Complete,2018-02-27,2018-02-27,United Kingdom1,https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611
Event,Conversion Rate Optimisation in Travel Industry,Complete,2018-01-30,2018-01-30,United Kingdom,https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719
Event,Becoming data driven in the high street fashion,Complete,2016-10-12,2016-10-12,United Kingdom,https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213
Event,Data Festival London 2016,Complete,2016-06-24,2016-06-26,United Kingdom,https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771
Event,Data Tools for Startups,Complete,2016-03-17,2016-03-17,United Kingdom,https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535
Event,How to become data driven startup,Complete,2015-11-04,2015-11-04,United Kingdom2,https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380
Event,How to become data driven startup,Complete,2015-11-04,2015-11-04,United Kingdom1,https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380
Event,How to become data driven startup,Complete,2015-10-13,2015-10-13,United Kingdom2,https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377
Event,"How to {""as"":""ssbecome
data driven startup""}",Complete,2015-10-13,2015-10-13,United Kingdom1,https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377


================================================
File: component_config/component_long_description.md
================================================
## MySQL Next

#### Summary
The open source database enables delivering high-performance and scalable web-based and embedded database applications.
This configuration works for MySQL databases hosted on AWS RDS, Aurora MySQL, and standard non-hosted MySQL. MySQL next
allows for choice of replication style at the table level, supporting log-based incremental replication.

#### Detail
Log-based replication is a type of change data capture (CDC) where incremental changes made to a database are detected
by reading the binary logs (AKA binlogs in MySQL) to pick up only changes since the last execution of this pipeline.
This replication style is actually the fastest method for identifying change (faster than key-based replication in
almost every case) and has the ability to capture hard deletes.

================================================
File: component_config/component_short_description.md
================================================
The open source database for high-performance and scalable web-based and embedded database applications. This
configuration supports log-based incremental loading and works for standard MySQL databases as well as those hosted
on AWS RDS and Aurora MySQL.

================================================
File: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "Connection Details",
  "required": [
    "fetchObjectsOnly",
    "host",
    "port",
    "username",
    "#password",
    "runIncrementalSync",
    "outputBucket",
    "ssl",
    "sslCa",
    "verifyCert",
    "sshTunnel",
    "storageMappingsFile",
    "inputMappingsJson",
    "handle_binary"
  ],
  "properties": {
    "fetchObjectsOnly": {
      "type": "boolean",
      "title": "Fetch Database Object Names and Schemas Only?",
      "default": false,
      "description": "Set to true to refresh (or fetch for the first time) database objects and schema options. NOTE: This step is necessary before first execution; setting to true does NOT run the extraction",
      "propertyOrder": 100
    },
    "host": {
      "type": "string",
      "title": "MySQL Host",
      "description": "The host name or IP for your MySQL instance or cluster",
      "propertyOrder": 200
    },
    "port": {
      "type": "number",
      "title": "Port",
      "description": "The port on which the MySQL database accepts connections, the default port is 3306",
      "default": 3306,
      "propertyOrder": 300
    },
    "databases": {
      "type": "array",
      "title": "MySQL Database(s)",
      "description": "MySQL Database(s) to Extract. If left empty, all non-system databases are pulled",
      "items": {
        "type": "string",
        "title": "Database"
      },
      "propertyOrder": 350
    },
    "username": {
      "type": "string",
      "title": "Username",
      "description": "The Keboola user configured for your MySQL database, for use only with your extractor",
      "propertyOrder": 400
    },
    "#password": {
      "type": "string",
      "title": "Password",
      "description": "Strong password you set for your Keboola MySQL user",
      "format": "password",
      "propertyOrder": 500
    },
    "runIncrementalSync": {
      "type": "boolean",
      "title": "Run Incremental Sync?",
      "default": true,
      "description": "Keep true to run incremental sync based on prior state (for all tables in your mappings specification where you have chosen incremental or log_based. If false, prior state is ignored and full database sync is run",
      "propertyOrder": 520
    },
    "storageMappingsFile": {
      "type": "string",
      "title": "Output Storage Mappings File Name",
      "default": "",
      "description": "The name of the output file (no extension necessary) for the YAML mappings in Keboola Storage",
      "propertyOrder": 530
    },
    "inputMappingsJson": {
      "type": "string",
      "title": "Table Mappings JSON Specification",
      "default": "",
      "description": "The JSON config for the tables, columns and corresponding replication methods to sync (see config documentation on how to do so)",
      "format": "json",
      "propertyOrder": 540,
      "options": {
        "input_height": "150px"
      }
    },
    "ssl": {
      "type": "boolean",
      "title": "Use SSL?",
      "default": true,
      "description": "Keep true to run incremental sync with SSL, uses self-signed certificate unless SSL Certificate Authority parameter is specified",
      "propertyOrder": 550
    },
    "sslCa": {
      "type": "string",
      "title": "SSL Certificate Authority (CA)",
      "default": "",
      "description": "Name of certificate authority implemented, if any (i.e. IdenTrust). If not specified, self-signed certificate is used",
      "propertyOrder": 550
    },
    "verifyCert": {
      "type": "boolean",
      "title": "Verify Server SSL Certificate?",
      "default": true,
      "description": "Keep true to connect to MySQL with verify mode set to true, which verifies the server and certificate",
      "propertyOrder": 550
    },
    "sshTunnel": {
      "type": "boolean",
      "title": "Use SSH Tunnel?",
      "description": "Set to true to connect using a configured SSH tunnel, otherwise keep false and be sure you have whitelisted Keboola's IPs. If false, other SSH parameters will be ignored and are not necessary.",
      "default": false,
      "propertyOrder": 600
    },
    "sshHost": {
      "type": "string",
      "title": "SSH Host",
      "description": "The host name or host IP associated with your SSH server (Note: Don't use load balancer as host)",
      "propertyOrder": 700
    },
    "sshUser": {
      "type": "string",
      "title": "SSH Username",
      "description": "The SSH User for connecting to your SSH server",
      "propertyOrder": 750
    },
    "sshPort": {
      "type": "number",
      "title": "SSH Port",
      "description": "The SSH server's port number, the default port is 22",
      "default": 22,
      "propertyOrder": 800
    },
    "sshPublicKey": {
      "type": "string",
      "title": "SSH Server Public Key",
      "description": "The public key for the key pair associated with your SSH server",
      "propertyOrder": 900
    },
    "#sshBase64PrivateKey": {
      "type": "string",
      "title": "Base64-Encoded SSH Server Private Key",
      "description": "The base64-encoded private key for the key pair associated with your SSH server (encrypted and secured)",
      "format": "password",
      "propertyOrder": 920
    },
    "outputBucket": {
      "type": "string",
      "title": "Output Bucket Destination",
      "default": "",
      "description": "(Optional) The name of bucket in Keboola storage where the data. Keboola will create a bucket for you if not specified",
      "propertyOrder": 930
    },
    "appendMode": {
      "type": "boolean",
      "title": "Append Mode",
      "default": false,
      "description": "If set to true, application will only insert new updates into the result table. The updates then have to be deduped in a transformation, to get the resulting table. This mode is faster, but requires extra work.",
      "propertyOrder": 1000
    },
    "handle_binary": {
      "type": "string",
      "title": "Binary data handler",
      "default": "plain",
      "enum": [
        "plain",
        "hex",
        "base64"
      ],
      "description": "Marks how to handle binary data:<ul><li><strong>plain</strong> - data in decoded from binary to string using Python's <i><a href='https://docs.python.org/3/library/stdtypes.html#bytes.decode' target='_blank'>bytes.decode()</a></i> method,</li><li><strong>hex</strong> - data is converted to hex representation of binary string using <i><a href='https://docs.python.org/3/library/stdtypes.html#bytes.hex' target='_blank'>bytes.hex()</a></i> method,</li><li><strong>base64</strong> - data is converted to a base64 string, using <i><a href='https://docs.python.org/3/library/base64.html#base64.b64encode' target='_blank'>base64.b64encode()</a></i> method.</li></ul>All trailing or leading null bytes are replaced.",
      "propertyOrder": 1100
    },
    "show_binary_log_config": {
      "type": "object",
      "title": "Show Binary Log Method",
      "required": [
        "method"
      ],
      "propertyOrder": 1300,
      "properties": {
        "method": {
          "type": "string",
          "title": "Method",
          "default": "direct",
          "enum": [
            "direct",
            "endpoint"
          ],
          "propertyOrder": 1
        },
        "endpoint_url": {
          "type": "string",
          "title": "Endpoint Url",
          "options": {
            "dependencies": {
              "method": "endpoint"
            }
          },
          "propertyOrder": 10
        },
        "authentication": {
          "type": "boolean",
          "title": "Enable Basic Authentication",
          "format": "checkbox",
          "default": false,
          "options": {
            "dependencies": {
              "method": "endpoint"
            }
          },
          "propertyOrder": 11
        },
        "user": {
          "type": "string",
          "title": "Username",
          "options": {
            "dependencies": {
              "authentication": true
            }
          },
          "propertyOrder": 13
        },
        "#password": {
          "type": "string",
          "title": "Password",
          "options": {
            "dependencies": {
              "authentication": true
            }
          },
          "propertyOrder": 15
        }
      }
    },
    "debug": {
      "type": "boolean",
      "propertyOrder": 1200
    },
    "maxExecutionTime": {
      "type": "string",
      "title": "Max connection time",
      "description": "Optional parameter which sets @@session.max_execution_time to desired integer value",
      "propertyOrder": 1210
    }
  }
}

================================================
File: component_config/configuration_description.md
================================================
## MySQL Next
#### Summary
The open source database enables delivering high-performance and scalable web-based and embedded database applications.
This configuration works for MySQL databases hosted on AWS RDS, Aurora MySQL, and standard non-hosted MySQL. MySQL next
allows for choice of replication style at the table level, supporting log-based incremental replication.

#### Log-Based Replication for Change Data Capture
Log-based replication is a type of change data capture (CDC) where incremental changes made to a database are detected
by reading the binary logs (AKA binlogs in MySQL) to pick up only changes since the last execution of this pipeline.
More specifically, all INSERT, UPDATE, and DELETE statements are appropriately recorded for database change capture.
This replication style is actually the fastest method for identifying change (faster than key-based replication in
almost every case) and has the ability to capture hard deletes (so long as they are run as a DELETE, not a TRUNCATE or 
DROP statement), unlike key-based replication. Deleted records will be left with a "deletion marker", identified by a
timestamp for the time the record was deleted in the special _KBC_DELETED_TIME column.

Generally speaking, a full sync is run during the first execution with log-based replication. From there, markers are
recorded to essentially keep track of the max of each table where the work was left off. From there, log-based
replication truly kicks in. All row-based binary logs for append, update and deletion events are captured and written to
Keboola storage. We record the place we last left off among those binary log files, and continue from there.

*Note*: A full table replication must be run if schema changes are necessary, i.e. adding a new column.

#### Setting Up the Database Connection
In order to connect to MySQL, you will supply your host name or IP, port (usually 3306), username and password. The
username and password you specify will essentially act as a Keboola service account. To create this user, you will need
to run the following SQL command against your instance:
```sql
create user keboola@'%' identified by '{insert strong password here}';
grant replication client, replication slave, select ON *.* TO keboola@'%';
```
If you follow the above, you will use Username 'keboola' during configuration, and the password you set for Password.

If you are authenticating without using an SSH tunnel, you will need to whitelist Keboola IPs (see [here](https://help.keboola.com/components/ip-addresses)).
If you are connecting via an SSH tunnel, specify 'ssh_tunnel' as True and supply the necessary parameters for your SSH
connection: SSH host IP, port (usually 22), SSH user and public key for accessing the SSH host.

#### Setting up Binary Logging
In order for log-based replication to work, you must have enabled [row-based binary logging](https://dev.mysql.com/doc/refman/8.0/en/binary-log-setting.html)
on your MySQL instance/cluster. If you are using Aurora MySQL you will need to use a
[database cluster parameter group](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html)
with parameter 'binlog_format' set to 'ROW'.

By default, MySQL removes these binary logs as soon as possible. However, in order to read them effectively for
replication, we want to extend this retention period. You should set this to anywhere between 1 and 7 days. For example,
if setting to a 3 day retention period, you would run the following:
```sql
call mysql.rds_set_configuration('binlog retention hours, 72);
```
The above only needs to be set up during initial setup for replication. You can also run
`call mysql.rds_show_configuration;` to see your existing binlog retention hours value, if any.

#### Setting Up the Configuration File
Setup is relatively straightforward! For each table or view that you would like to replicate, you just need to add two
options to the "metadata" section of that table or view. First, specify `"selected": true`. Next, choose the replication 
method by setting `"replication-method": "INCREMENTAL"`. Allowed values are `FULL_TABLE`, `INCREMENTAL` and `LOG_BASED`.

If you choose `INCREMENTAL`, you also must specify a replication key, the field that will be used to determine if a 
given row in that table has changed. You specify this with an additional parameter, such as 
`"replication-key": "updated_at"`. `LOG_BASED` is only allowed if the server is set up to support it, and the database 
object is a table, not a view.

For any replication method, once you have chosen "selected" to True at the table level for each table/view you want to 
include, set `"selected": false` for any column(s) that you want to exclude from the replication (i.e. sensitive info),
by default all columns are included for selected tables.

By default all tables and views are excluded to protect potentially sensitive data.
However, if you choose to include a table, all columns are included by default (for ease of adding new tables); any
columns you would like to exclude must be explicitly set as such by including `selected: false` on that column.

#### Pulling Existing Schema Definitions
The extractor has the ability to pull existing schema definitions. If you set the parameter discover_schema to True, the
extractor will pull databases(schemas), tables and fields. You can then choose which to include or exclude.

#### Running Historical Syncs
Upon your first execution, all tables must run an initial first full sync. However, you need to set the configuration
file to do so, it will automatically. Full historical syncs on a particular table will be necessary in the future
whenever you update the schema you are pulling from that table. A full database re-sync will ONLY be necessary if you do
not run a sync for a long period of time, beyond your current 'binlog retention hours' setting.

================================================
File: component_config/logger
================================================
gelf

================================================
File: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}

================================================
File: component_config/stack_parameters.json
================================================
{}

================================================
File: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}


================================================
File: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}

================================================
File: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}

================================================
File: component_config/sample-config/out/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>

================================================
File: component_config/sample-config/out/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"


================================================
File: resource/pypy3.8-v7.3.9-osx64/LICENSE
================================================
#encoding utf-8

License
=======

Except when otherwise stated (look for LICENSE files in directories
or information at the beginning of each file) all software and
documentation in the 'rpython', 'pypy', 'ctype_configure', 'dotviewer',
'demo', 'extra_tests', 'include', 'lib_pypy', 'py', and '_pytest'
directories is licensed as follows:

    The MIT License

    Permission is hereby granted, free of charge, to any person
    obtaining a copy of this software and associated documentation
    files (the "Software"), to deal in the Software without
    restriction, including without limitation the rights to use,
    copy, modify, merge, publish, distribute, sublicense, and/or
    sell copies of the Software, and to permit persons to whom the
    Software is furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included
    in all copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
    OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
    THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
    DEALINGS IN THE SOFTWARE.


PyPy Copyright holders 2003-2022
--------------------------------

Except when otherwise stated (look for LICENSE files or information at
the beginning of each file) the files in the 'pypy' directory are each
copyrighted by one or more of the following people and organizations:

  Armin Rigo
  Maciej Fijałkowski
  Matti Picus
  Carl Friedrich Bolz-Tereick
  Antonio Cuni
  Amaury Forgeot d'Arc
  Ronan Lamy
  Samuele Pedroni
  Alex Gaynor
  Philip Jenvey
  Richard Plangger
  Brian Kearns
  Manuel Jacob
  Michael Hudson-Doyle
  David Schneider
  Holger Krekel
  Christian Tismer
  Håkan Ardö
  Benjamin Peterson
  Wim Lavrijsen
  Anders Chrigstrom
  Eric van Riet Paap
  Dan Villiom Podlaski Christiansen
  Remi Meier
  Richard Emslie
  Alexander Schremmer
  Lukas Diekmann
  Sven Hager
  Anders Lehmann
  Edd Barrett
  Aurelien Campeas
  Niklaus Haldimann
  Camillo Bruni
  Laura Creighton
  Toon Verwaest
  Leonardo Santagada
  Seo Sanghyeon
  Romain Guillebert
  Ronny Pfannschmidt
  Justin Peel
  Raffael Tfirst
  David Edelsohn
  Anders Hammarquist
  Jakub Gustak
  Gregor Wegberg
  Guido Wesdorp
  Lawrence Oluyede
  Bartosz Skowron
  Stefano Rivera
  Daniel Roberts
  Adrien Di Mascio
  Niko Matsakis
  Alexander Hesse
  Ludovic Aubry
  Batuhan Taskaya
  stian
  Jacob Hallen
  Jason Creighton
  Mark Young
  Andrew Lawrence
  Alex Martelli
  Ondrej Baranovič
  Spenser Bauman
  Yusuke Izawa
  Michal Bendowski
  Jan de Mooij
  Stefan Beyer
  Tyler Wade
  Vincent Legoll
  Simon Cross
  Michael Foord
  muke101
  Stephan Diehl
  Jean-Paul Calderone
  Stefan Schwarzer
  Tomek Meka
  Valentino Volonghi
  Patrick Maupin
  Devin Jeanpierre
  Bob Ippolito
  Bruno Gola
  David Malcolm
  Yannick Jadoul
  Squeaky
  Timo Paulssen
  Marius Gedminas
  Laurence Tratt
  Alexandre Fayolle
  Nicolas Truessel
  Simon Burton
  Martin Matusiak
  Konstantin Lopuhin
  Wenzhu Man
  John Witulski
  Jeremy Thurgood
  Julian Berman
  Adrian Kuhn
  Dario Bertini
  Greg Price
  Ivan Sichmann Freitas
  Mark Pearse
  Tobias Pape
  Andreas Stührk
  Jean-Philippe St. Pierre
  Stian Andreassen
  Guido van Rossum
  Pavel Vinogradov
  William Leslie
  Paweł Piotr Przeradowski
  Paul deGrandis
  Ilya Osadchiy
  Tobias Oberstein
  marky1991
  Boris Feigin
  tav
  Taavi Burns
  Joannah Nanjekye
  Georg Brandl
  Michał Górny
  quejebo
  Vanessa Freudenberg
  Gerald Klix
  Wanja Saatkamp
  Mike Blume
  olliemath
  Oscar Nierstrasz
  Rami Chowdhury
  Stefan H. Muller
  Dodan Mihai
  Tim Felgentreff
  Eugene Oden
  Colin Valliant
  Henry Mason
  Jeff Terrace
  David Ripton
  Preston Timmons
  Vasily Kuznetsov
  Pieter Zieschang
  Lukas Renggli
  Dusty Phillips
  Guenter Jantzen
  Amit Regmi
  Ned Batchelder
  Jasper Schulz
  Anton Gulenko
  Ben Young
  Nicolas Chauvat
  Andrew Durdin
  Andrew Chambers
  Sergey Matyunin
  Łukasz Langa
  Nicholas Riley
  Michael Schneider
  Yusuke Tsutsumi
  Rocco Moretti
  Gintautas Miliauskas
  Michael Twomey
  Igor Trindade Oliveira
  Jason Chu
  Yichao Yu
  Lucian Branescu Mihaila
  anatoly techtonik
  Mariano Anaya
  Olivier Dormond
  Jared Grubb
  Karl Bartel
  Gabriel Lavoie
  Wouter van Heyst
  Alecsandru Patrascu
  Lin Cheng
  Brian Dorsey
  Victor Stinner
  Andrews Medina
  Sebastian Pawluś
  Stuart Williams
  Toby Watson
  Antoine Pitrou
  Aaron Iles
  Christian Hudon
  Daniel Patrick
  Ricky Zhou
  Justas Sadzevicius
  Gasper Zejn
  Neil Shepperd
  Mikael Schönenberg
  Michael Cheng
  Stanislaw Halik
  Berkin Ilbeyi
  Mihnea Saracin
  Matt Jackson
  Jonathan David Riehl
  Anders Qvist
  Beatrice During
  Elmo Mäntynen
  Corbin Simpson
  Chirag Jadwani
  Faye Zhao
  Pauli Virtanen
  Mike Pavone
  Alan McIntyre
  Alexander Sedov
  Alex Perry
  Floris Bruynooghe
  Christopher Pope
  Attila Gobi
  Vaibhav Sood
  Reuben Cummings
  Robert Zaremba
  David C Ellis
  cptpcrd
  Felix C. Stegerman
  Jens-Uwe Mager
  Dan Stromberg
  Carl Meyer
  Stefano Parmesan
  Alexis Daboville
  Christian Tismer 
  Marc Abramowitz
  Arjun Naik
  Valentina Mukhamedzhanova
  Florin Papa
  Aaron Gallagher
  touilleMan
  Tristan Arthur
  Anthony Sottile
  Arianna Avanzini
  Matt Billenstein
  Sebastian Berg
  Jacek Generowicz
  Sylvain Thenault
  Alejandro J. Cura
  Roberto De Ioris
  Andrew Dalke
  Gabriel
  Nathan Taylor
  Karl Ramm
  Vladimir Kryachko
  Lukas Vacek
  Jakub Stasiak
  Omer Katz
  Kunal Grover
  Mark Williams
  Thomas Hisch
  Barry Hart
  Tomasz Dziopa
  Lutz Paelike
  Ignas Mikalajunas
  Martin Blais
  Jacob Oscarson
  Lene Wagner
  Lucio Torre
  Henrik Vendelbo
  Artur Lisiecki
  Travis Francis Athougies
  Miguel de Val Borro
  Kristjan Valur Jonsson
  Christoph Gerum
  Yasir Suhail
  Tomo Cocoa
  Neil Blakey-Milner
  Dan Buch
  Lars Wassermann
  Sergey Kishchenko
  Ryan Gonzalez
  Ian Foote
  David Lievens
  Richard Lancaster
  Philipp Rustemeuer
  Logan Chien
  Catalin Gabriel Manciu
  Miro Hrončok
  Antoine Dupre
  Bernd Schoeller
  Catalin Fierut
  nimaje
  Pierre-Yves DAVID
  Gustavo Niemeyer
  Andrew Thompson
  Joshua Gilbert
  Yusei Tahara
  Christopher Armstrong
  Anders Sigfridsson
  Stephan Busemann
  Godefroid Chappelle
  Dan Colish
  Akira Li
  Bobby Impollonia
  timo
  Anna Katrina Dominguez
  Juan Francisco Cantero Hurtado
  Ben Darnell
  Rafał Gałczyński
  Yury V. Zaytsev
  Laurens Van Houtven
  rafalgalczynski@gmail.com
  Jason Michalski
  Toni Mattis
  Lucas Stadler
  Jeong YunWon
  Ruochen Huang
  Markus Holtermann
  Kim Jin Su
  Matt Bogosian
  Aaron Tubbs
  Amber Brown
  Nikolay Zinov
  florinpapa
  Vasantha Ganesh K
  Fabio Niephaus
  Nate Bragg
  afteryu
  Andrew Stepanov
  Radu Ciorba
  Carl Bordum Hansen
  Paul Ganssle
  Michal Kuffa
  joachim-ballmann@bitbucket.org
  Vincent Michel
  Ram Rachum
  Bystroushaak
  Ryan Hileman
  joserubiovidales@gmail.com
  dakarpov@gmail.com
  Sreepathi Pai
  Georges Racinet
  ashwinahuja
  Bolutife Ogunsola
  cjmcdonald@google.com
  Alex Orange
  alexprengere
  Dennis Sweeney
  Kevin Lee
  h-vertini
  Anna Ravencroft
  Dinu Gherman
  Michael Chermside
  Jim Baker
  Zooko Wilcox-O Hearn
  Daniel Neuhäuser
  Konrad Delong
  Rodrigo Araújo
  Armin Ronacher
  Jim Hunziker
  Christian Muirhead
  Brett Cannon
  Chris Lambacher
  Dan Loewenherz
  coolbutuseless@gmail.com
  Christopher Groskopf
  Buck Golemon
  soareschen
  Even Wiik Thomassen
  Antony Lee
  James Lan
  yrttyr
  Kristoffer Kleine
  Julien Phalip
  shoma hosaka
  Tomer Chachamu
  Flavio Percoco
  Markus Unterwaditzer
  Mike Bayer
  OlivierBlanvillain
  jiaaro
  James Robert
  aliceinwire
  Kurt Griffiths
  Matthew Miller
  Asmo Soinio
  Stefan Marr
  Boglarka Vezer
  Mads Kiilerich
  Dan Crosta
  Dan Sanders
  Ben Mather
  Chris Pressey
  halgari
  Berker Peksag
  Roman Podoliaka
  Nikolaos-Digenis Karagiannis
  Donald Stufft
  Volodymyr Vladymyrov
  Andrey Churin
  Niclas Olofsson
  Yaroslav Fedevych
  Zearin
  Tobias Diaz
  Jason Madden
  Jonas Pfannschmidt
  werat
  JohnDoe
  Diana Popa
  Eli Stevens
  pizi
  remarkablerocket
  reubano@gmail.com
  Daniil Yarancev
  PavloKapyshin
  Graham Markall
  Stanisław Halik
  Iraklis D.
  Petre Vijiac
  Min RK
  Caleb Hattingh
  Steve Papanik
  m@funkyhat.org
  Tomáš Pružina
  gabrielg@ec2-54-146-239-158.compute-1.amazonaws.com
  Filip Salomonsson
  Johan Forsberg
  Evgenii Gorinov
  John Aldis
  Hervé Beraud
  Paul Graydon
  whitequark
  DeVerne Jones
  Zsolt Cserna
  Yasen Kiprov
  mkuffa
  Ivan
  Jesdi
  paugier
  bernd.schoeller@inf.ethz.ch
  Sam Edwards
  Joannah Nanjekye nanjekyejoannah@gmail.com
  Alex Kashirin
  Ihar Shabes
  kotus9
  Mike Kaplinskiy
  Henri Tuhola
  mark doerr
  Tomas Hrnciar
  shaolo1
  Chris AtLee
  Christoph Reiter
  Chris Burr
  Brad Kish
  Michael Cho
  Ian Clester
  David Hewitt
  h-vetinari
  Isuru Fernando

  Heinrich-Heine University, Germany
  Open End AB (formerly AB Strakt), Sweden
  merlinux GmbH, Germany
  tismerysoft GmbH, Germany
  Logilab Paris, France
  DFKI GmbH, Germany
  Impara, Germany
  Change Maker, Sweden
  University of California Berkeley, USA
  Google Inc.
  King's College London

The PyPy Logo as used by http://speed.pypy.org and others was created
by Samuel Reis and is distributed on terms of Creative Commons Share Alike
License.

License for 'lib-python/2.7, lib-python/3'
==========================================

Except when otherwise stated (look for LICENSE files or copyright/license
information at the beginning of each file) the files in the 'lib-python'
directory are all copyrighted by the Python Software Foundation and licensed
under the terms that you can find here: https://docs.python.org/3/license.html

License for 'pypy/module/unicodedata/'
======================================

The following files are from the website of The Unicode Consortium
at http://www.unicode.org/.  For the terms of use of these files, see
http://www.unicode.org/terms_of_use.html .  Or they are derived from
files from the above website, and the same terms of use apply.

    CompositionExclusions-*.txt
    EastAsianWidth-*.txt
    LineBreak-*.txt
    UnicodeData-*.txt
    UnihanNumeric-*.txt

License for 'dotviewer/font/'
=============================

Copyright (C) 2008 The Android Open Source Project

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

Detailed license information is contained in the NOTICE file in the
directory.


Licenses and Acknowledgements for Incorporated Software
=======================================================

This section is an incomplete, but growing list of licenses and
acknowledgements for third-party software incorporated in the PyPy
distribution.

License for 'Tcl/Tk'
--------------------

This copy of PyPy contains library code that may, when used, result in
the Tcl/Tk library to be loaded.  PyPy also includes code that may be
regarded as being a copy of some parts of the Tcl/Tk header files.
You may see a copy of the License for Tcl/Tk in the file
`lib_pypy/_tkinter/license.terms` included here.

License for 'bzip2'
-------------------

This copy of PyPy may be linked (dynamically or statically) with the
bzip2 library.  You may see a copy of the License for bzip2/libbzip2 at

    http://www.bzip.org/1.0.5/bzip2-manual-1.0.5.html

License for 'openssl'
---------------------

This copy of PyPy may be linked (dynamically or statically) with the
openssl library.  You may see a copy of the License for OpenSSL at

    https://www.openssl.org/source/license.html

License for '_gdbm'
------------------

The _gdbm module includes code from gdbm.h, which is distributed under
the terms of the GPL license version 2 or any later version.  Thus the
_gdbm module, provided in the file lib_pypy/_gdbm.py, is redistributed
under the terms of the GPL license as well.

License for 'rpython/rlib/rvmprof/src'
--------------------------------------

The code is based on gperftools. You may see a copy of the License for it at

    https://github.com/gperftools/gperftools/blob/master/COPYING

License for 'liblzma and 'lzmaffi'
----------------------------------

This copy of PyPy may be linked (dynamically or statically) with the
liblzma library, which was put in the "public domain":

    http://tukaani.org/xz/

The cffi bindings to liblzma (in lib_pypy/_lzma.py) are derived from
the lzmaffi project which is distributed under a BSD license:

    https://pypi.python.org/pypi/lzmaffi/0.3.0


================================================
File: resource/pypy3.8-v7.3.9-osx64/README.rst
================================================
=====================================
PyPy: Python in Python Implementation
=====================================

Welcome to PyPy!

PyPy is an interpreter that implements the Python programming language, based
on the RPython compiler framework for dynamic language implementations.

The home page for the interpreter is:

    https://pypy.org/

If you want to help developing PyPy, this documentation might help you:

    https://doc.pypy.org/

More documentation about the RPython framework can be found here:

    https://rpython.readthedocs.io/

The source for the documentation is in the pypy/doc directory.


Using PyPy instead of CPython
-----------------------------

Please read the information at https://pypy.org/ to find the correct way to
download and use PyPy as an alternative to CPython. 


Building
--------

Building PyPy is not the recommended way to obtain the PyPy alternative python
interpreter. It is time-consuming and requires significant computing resources.
More information can be found here:

    https://doc.pypy.org/en/latest/build.html

Enjoy and send us feedback!

    the pypy-dev team <pypy-dev@python.org>



================================================
File: resource/pypy3.8-v7.3.9-osx64/bin/normalizer
================================================
#!/Users/esner/Documents/Prace/KBC/kds-team.ex-mysql-next/resource/pypy3.8-v7.3.9-osx64/bin/pypy
# -*- coding: utf-8 -*-
import re
import sys
from charset_normalizer.cli.normalizer import cli_detect
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(cli_detect())


================================================
File: resource/pypy3.8-v7.3.9-osx64/bin/pip3
================================================
#!/Users/esner/Documents/Prace/KBC/kds-team.ex-mysql-next/resource/pypy3.8-v7.3.9-osx64/bin/pypy
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


================================================
File: resource/pypy3.8-v7.3.9-osx64/bin/pip3.8
================================================
#!/Users/esner/Documents/Prace/KBC/kds-team.ex-mysql-next/resource/pypy3.8-v7.3.9-osx64/bin/pypy
# -*- coding: utf-8 -*-
import re
import sys
from pip._internal.cli.main import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


================================================
File: resource/pypy3.8-v7.3.9-osx64/bin/sqlformat
================================================
#!/Users/esner/Documents/Prace/KBC/kds-team.ex-mysql-next/resource/pypy3.8-v7.3.9-osx64/bin/pypy
# -*- coding: utf-8 -*-
import re
import sys
from sqlparse.__main__ import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


================================================
File: resource/pypy3.8-v7.3.9-osx64/bin/sshtunnel
================================================
#!/Users/esner/Documents/Prace/KBC/kds-team.ex-mysql-next/resource/pypy3.8-v7.3.9-osx64/bin/pypy
# -*- coding: utf-8 -*-
import re
import sys
from sshtunnel import _cli_main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(_cli_main())


================================================
File: resource/pypy3.8-v7.3.9-osx64/bin/wheel
================================================
#!/Users/esner/Documents/Prace/KBC/kds-team.ex-mysql-next/resource/pypy3.8-v7.3.9-osx64/bin/pypy
# -*- coding: utf-8 -*-
import re
import sys
from wheel.cli import main
if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/README
================================================
This directory contains all the include files needed to build cpython
extensions with PyPy.  Note that these are just copies of the original headers
that are in pypy/module/cpyext/{include,parse}: they are automatically copied
from there during translation.

Moreover, some pypy-specific files are automatically generated, also during
translation. Currently they are:
* pypy_decl.h
* pypy_macros.h
* pypy_numpy.h
* pypy_structmember_decl.h


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/Python.h
================================================
#ifndef Py_PYTHON_H
#define Py_PYTHON_H

#include "patchlevel.h"
#include <pyconfig.h>

/* Compat stuff */
#ifdef __GNUC__
#define _GNU_SOURCE 1
#endif
#ifndef _WIN32
# include <stddef.h>
# include <limits.h>
# include <math.h>
# include <errno.h>
# include <unistd.h>
#else
# ifdef _MSC_VER
#  include <crtdefs.h>
# endif
# ifdef __MINGW32__
#  include <limits.h>
# endif
# include <io.h>
# include <sys/types.h>   /* for 'off_t' */
#endif

/* Deprecated DL_IMPORT and DL_EXPORT macros */
#ifdef _WIN32
# if defined(Py_BUILD_CORE)
#  define DL_IMPORT(RTYPE) __declspec(dllexport) RTYPE
#  define DL_EXPORT(RTYPE) __declspec(dllexport) RTYPE
# else
#  define DL_IMPORT(RTYPE) __declspec(dllimport) RTYPE
#  define DL_EXPORT(RTYPE) __declspec(dllexport) RTYPE
# endif
#endif
#ifndef DL_EXPORT
#       define DL_EXPORT(RTYPE) PyAPI_FUNC(RTYPE)
#endif
#ifndef DL_IMPORT
#       define DL_IMPORT(RTYPE) RTYPE
#endif
#include <stdlib.h>

#define Py_SAFE_DOWNCAST(VALUE, WIDE, NARROW) (NARROW)(VALUE)

#define Py_USING_UNICODE

#define statichere static

#define Py_MEMCPY memcpy
#include "pyport.h"

#include "pypy_macros.h"
#include "pymacro.h"

#include "object.h"
#include "typeslots.h"
#include "abstract.h"
#include "pymath.h"
#include "pytime.h"
#include "warnings.h"

#include <stdarg.h>
#include <stdio.h>
#include <string.h>
#include <assert.h>
#include <locale.h>
#include <ctype.h>

#include "pyhash.h"
#include "boolobject.h"
#include "floatobject.h"
#include "complexobject.h"
#include "methodobject.h"
#include "funcobject.h"
#include "code.h"

#include "moduleobject.h"
#include "modsupport.h"
#include "pythonrun.h"
#include "pyerrors.h"
#include "sysmodule.h"
#include "bytearrayobject.h"
#include "descrobject.h"
#include "tupleobject.h"
#include "dictobject.h"
#include "longobject.h"
#include "setobject.h"
#include "listobject.h"
#include "longobject.h"
#include "unicodeobject.h"
#include "compile.h"
#include "frameobject.h"
#include "memoryobject.h"
#include "eval.h"
#include "pymem.h"
#include "pycapsule.h"
#include "bytesobject.h"
#include "sliceobject.h"
#include "genobject.h"
#include "datetime.h"
#include "structseq.h"
#include "pystate.h"
#include "fileobject.h"
#include "pysignals.h"
#include "pythread.h"
#include "traceback.h"
#include "pylifecycle.h"

/* Missing definitions */
#include "missing.h"

/* The declarations of most API functions are generated in a separate file */
/* Don't include them while building PyPy, RPython also generated signatures
 * which are similar but not identical. */
#ifndef PYPY_STANDALONE
#ifdef __cplusplus
extern "C" {
#endif
  #include "pypy_decl.h"
#ifdef __cplusplus
}
#endif
#endif  /* PYPY_STANDALONE */

/* Define macros for inline documentation. */
#define PyDoc_VAR(name) static char name[]
#define PyDoc_STRVAR(name,str) PyDoc_VAR(name) = PyDoc_STR(str)
#ifdef WITH_DOC_STRINGS
#define PyDoc_STR(str) str
#else
#define PyDoc_STR(str) ""
#endif

/* PyPy does not implement --with-fpectl */
#define PyFPE_START_PROTECT(err_string, leave_stmt)
#define PyFPE_END_PROTECT(v)

#include "pystrtod.h"

#endif


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/abstract.h
================================================
#ifndef Py_ABSTRACTOBJECT_H
#define Py_ABSTRACTOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

     PyAPI_FUNC(int) PyObject_DelItemString(PyObject *o, char *key);

       /*
     Remove the mapping for object, key, from the object *o.
     Returns -1 on failure.  This is equivalent to
     the Python statement: del o[key].
       */

#define PY_VECTORCALL_ARGUMENTS_OFFSET ((size_t)1 << (8 * sizeof(size_t) - 1))

static inline Py_ssize_t
PyVectorcall_NARGS(size_t n)
{
    return n & ~PY_VECTORCALL_ARGUMENTS_OFFSET;
}

/* Call "callable" (which must support vectorcall) with positional arguments
   "tuple" and keyword arguments "dict". "dict" may also be NULL */
PyAPI_FUNC(PyObject *) PyVectorcall_Call(PyObject *callable, PyObject *tuple, PyObject *dict);

    /* new buffer API */

#define PyObject_CheckBuffer(obj) \
    (((obj)->ob_type->tp_as_buffer != NULL) &&  \
     ((obj)->ob_type->tp_as_buffer->bf_getbuffer != NULL))

    /* Return 1 if the getbuffer function is available, otherwise
       return 0 */

     PyAPI_FUNC(int) PyObject_GetBuffer(PyObject *obj, Py_buffer *view,
                                        int flags);

    /* This is a C-API version of the getbuffer function call.  It checks
       to make sure object has the required function pointer and issues the
       call.  Returns -1 and raises an error on failure and returns 0 on
       success
    */

     PyAPI_FUNC(void) PyBuffer_Release(Py_buffer *view);

    /* Releases a Py_buffer obtained from getbuffer ParseTuple's s*.
    */

/*  Mapping protocol:*/

     /* implemented as a macro:

     int PyMapping_DelItemString(PyObject *o, char *key);

     Remove the mapping for object, key, from the object *o.
     Returns -1 on failure.  This is equivalent to
     the Python statement: del o[key].
       */
#define PyMapping_DelItemString(O,K) PyObject_DelItemString((O),(K))

     /* implemented as a macro:

     int PyMapping_DelItem(PyObject *o, PyObject *key);

     Remove the mapping for object, key, from the object *o.
     Returns -1 on failure.  This is equivalent to
     the Python statement: del o[key].
       */
#define PyMapping_DelItem(O,K) PyObject_DelItem((O),(K))

#ifdef __cplusplus
}
#endif
#endif /* Py_ABSTRACTOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/boolobject.h
================================================
/* Boolean object interface */

#ifndef Py_BOOLOBJECT_H
#define Py_BOOLOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

#define PyBool_Check(x) (Py_TYPE(x) == &PyBool_Type)

/* Py_False and Py_True are the only two bools in existence.
Don't forget to apply Py_INCREF() when returning either!!! */

/* Use these macros */
#define Py_False ((PyObject *) &_Py_FalseStruct)
#define Py_True ((PyObject *) &_Py_TrueStruct)

/* Macros for returning Py_True or Py_False, respectively */
#define Py_RETURN_TRUE return Py_INCREF(Py_True), Py_True
#define Py_RETURN_FALSE return Py_INCREF(Py_False), Py_False

#ifdef __cplusplus
}
#endif
#endif /* !Py_BOOLOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/bytearrayobject.h
================================================
/* ByteArray object interface */

#ifndef Py_BYTEARRAYOBJECT_H
#define Py_BYTEARRAYOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

#include <stdarg.h>

/* Type PyByteArrayObject represents a mutable array of bytes.
 * The Python API is that of a sequence;
 * the bytes are mapped to ints in [0, 256).
 * Bytes are not characters; they may be used to encode characters.
 * The only way to go between bytes and str/unicode is via encoding
 * and decoding.
 * While CPython exposes interfaces to this object, pypy does not
 */

#define PyByteArray_GET_SIZE(op) PyByteArray_Size((PyObject*)(op))
#define PyByteArray_AS_STRING(op) PyByteArray_AsString((PyObject*)(op))

/* Object layout */
typedef struct {
    PyObject_VAR_HEAD
#if 0
    int ob_exports; /* how many buffer exports */
    Py_ssize_t ob_alloc; /* How many bytes allocated */
    char *ob_bytes;
#endif
} PyByteArrayObject;

#ifdef __cplusplus
}
#endif
#endif /* !Py_BYTEARRAYOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/bytesobject.h
================================================
/* A copy of pypy2's PyStringObject */

#ifndef Py_BYTESOBJECT_H
#define Py_BYTESOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

#include <stdarg.h>

#define PyBytes_GET_SIZE(op) PyBytes_Size(op)

/*
Type PyStringObject represents a character string.  An extra zero byte is
reserved at the end to ensure it is zero-terminated, but a size is
present so strings with null bytes in them can be represented.  This
is an immutable object type.

There are functions to create new string objects, to test
an object for string-ness, and to get the
string value.  The latter function returns a null pointer
if the object is not of the proper type.
There is a variant that takes an explicit size as well as a
variant that assumes a zero-terminated string.  Note that none of the
functions should be applied to nil objects.
*/

/* Caching the hash (ob_shash) saves recalculation of a string's hash value.
   Interning strings (ob_sstate) tries to ensure that only one string
   object with a given value exists, so equality tests can be one pointer
   comparison.  This is generally restricted to strings that "look like"
   Python identifiers, although the intern() builtin can be used to force
   interning of any string.
   Together, these sped cpython up by up to 20%, and since they are part of the
   "public" interface PyPy must reimpliment them. */

typedef struct {
    PyObject_VAR_HEAD
    Py_hash_t ob_shash;
    int ob_sstate;
    char ob_sval[1]; 

    /* Invariants 
     *     ob_sval contains space for 'ob_size+1' elements.
     *     ob_sval[ob_size] == 0.
     *     ob_shash is the hash of the string or -1 if not computed yet.
     *     ob_sstate != 0 iff the string object is in stringobject.c's
     *       'interned' dictionary; in this case the two references
     *       from 'interned' to this object are *not counted* in ob_refcnt.
     */
} PyBytesObject;

#define SSTATE_NOT_INTERNED 0
#define SSTATE_INTERNED_MORTAL 1
#define SSTATE_INTERNED_IMMORTAL 2
#define PyString_CHECK_INTERNED(op) (((PyStringObject *)(op))->ob_sstate)

#define PyBytes_Check(op) \
                 PyType_FastSubclass(Py_TYPE(op), Py_TPFLAGS_BYTES_SUBCLASS)
#define PyBytes_CheckExact(op) (Py_TYPE(op) == &PyBytes_Type)

PyAPI_FUNC(PyObject *) PyBytes_FromFormatV(const char*, va_list);
PyAPI_FUNC(PyObject *) PyBytes_FromFormat(const char*, ...);

#ifdef __cplusplus
}
#endif
#endif


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/cStringIO.h
================================================
#ifndef Py_CSTRINGIO_H
#define Py_CSTRINGIO_H
#ifdef __cplusplus
extern "C" {
#endif
/*

  This header provides access to cStringIO objects from C.
  Functions are provided for calling cStringIO objects and
  macros are provided for testing whether you have cStringIO
  objects.

  Before calling any of the functions or macros, you must initialize
  the routines with:

    PycString_IMPORT

  This would typically be done in your init function.

*/

#define PycStringIO_CAPSULE_NAME "cStringIO.cStringIO_CAPI"

#define PycString_IMPORT \
  PycStringIO = ((struct PycStringIO_CAPI*)PyCapsule_Import(\
    PycStringIO_CAPSULE_NAME, 0))

/* Basic functions to manipulate cStringIO objects from C */

static struct PycStringIO_CAPI {

 /* Read a string from an input object.  If the last argument
    is -1, the remainder will be read.
    */
  int(*cread)(PyObject *, char **, Py_ssize_t);

 /* Read a line from an input object.  Returns the length of the read
    line as an int and a pointer inside the object buffer as char** (so
    the caller doesn't have to provide its own buffer as destination).
    */
  int(*creadline)(PyObject *, char **);

  /* Write a string to an output object*/
  int(*cwrite)(PyObject *, const char *, Py_ssize_t);

  /* Get the output object as a Python string (returns new reference). */
  PyObject *(*cgetvalue)(PyObject *);

  /* Create a new output object */
  PyObject *(*NewOutput)(int);

  /* Create an input object from a Python string
     (copies the Python string reference).
     */
  PyObject *(*NewInput)(PyObject *);

  /* The Python types for cStringIO input and output objects.
     Note that you can do input on an output object.
     */
  PyTypeObject *InputType, *OutputType;

} *PycStringIO;

/* These can be used to test if you have one */
#define PycStringIO_InputCheck(O) \
  (0) /* Py_TYPE(O)==PycStringIO->InputType) */
#define PycStringIO_OutputCheck(O) \
  (0) /* Py_TYPE(O)==PycStringIO->OutputType) */

#ifdef __cplusplus
}
#endif
#endif /* !Py_CSTRINGIO_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/ceval.h
================================================
/* empty */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/code.h
================================================
#ifndef Py_CODE_H
#define Py_CODE_H
#ifdef __cplusplus
extern "C" {
#endif

typedef struct {
    PyObject_HEAD
    PyObject *co_name;
    PyObject *co_filename;
    int co_argcount;
    int co_flags;
} PyCodeObject;

/* Masks for co_flags above */
/* These values are also in funcobject.py */
#define CO_OPTIMIZED    0x0001
#define CO_NEWLOCALS    0x0002
#define CO_VARARGS      0x0004
#define CO_VARKEYWORDS  0x0008
#define CO_NESTED       0x0010
#define CO_GENERATOR    0x0020
  
/* The CO_COROUTINE flag is set for coroutine functions (defined with
   ``async def`` keywords) */
#define CO_COROUTINE            0x0080
#define CO_ITERABLE_COROUTINE   0x0100

#define CO_FUTURE_DIVISION         0x020000
#define CO_FUTURE_ABSOLUTE_IMPORT  0x040000
#define CO_FUTURE_WITH_STATEMENT   0x080000
#define CO_FUTURE_PRINT_FUNCTION   0x100000
#define CO_FUTURE_UNICODE_LITERALS 0x200000

#ifdef __cplusplus
}
#endif
#endif /* !Py_CODE_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/compile.h
================================================
#ifndef Py_COMPILE_H
#define Py_COMPILE_H

#include "code.h"

#ifdef __cplusplus
extern "C" {
#endif

#ifdef __cplusplus
}
#endif
#endif /* !Py_COMPILE_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/complexobject.h
================================================
/* Complex object interface */

#ifndef Py_COMPLEXOBJECT_H
#define Py_COMPLEXOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

typedef struct Py_complex_t {
    double real;
    double imag;
} Py_complex;

typedef struct {
    PyObject_HEAD
    Py_complex cval;
} PyComplexObject;

PyAPI_FUNC(Py_complex) PyComplex_AsCComplex(PyObject *obj);
PyAPI_FUNC(PyObject *) PyComplex_FromCComplex(Py_complex c);

#ifdef __cplusplus
}
#endif
#endif /* !Py_COMPLEXOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/cpyext_datetime.h
================================================
/* Define structure for C API. */
typedef struct {
    /* type objects */
    PyTypeObject *DateType;
    PyTypeObject *DateTimeType;
    PyTypeObject *TimeType;
    PyTypeObject *DeltaType;
    PyTypeObject *TZInfoType;

    /* singletons */
    PyObject *TimeZone_UTC;

    /* constructors */
    PyObject *(*Date_FromDate)(int, int, int, PyTypeObject*);
    PyObject *(*DateTime_FromDateAndTime)(int, int, int, int, int, int, int,
        PyObject*, PyTypeObject*);
    PyObject *(*Time_FromTime)(int, int, int, int, PyObject*, PyTypeObject*);
    PyObject *(*Delta_FromDelta)(int, int, int, int, PyTypeObject*);
    PyObject *(*TimeZone_FromTimeZone)(PyObject*, PyObject*);

    /* constructors for the DB API */
    PyObject *(*DateTime_FromTimestamp)(PyObject*, PyObject*, PyObject*);
    PyObject *(*Date_FromTimestamp)(PyObject*, PyObject*);

    /* PEP 495 constructors */
    PyObject *(*DateTime_FromDateAndTimeAndFold)(int, int, int, int, int, int, int,
        PyObject*, int, PyTypeObject*);
    PyObject *(*Time_FromTimeAndFold)(int, int, int, int, PyObject*, int, PyTypeObject*);

} PyDateTime_CAPI;

typedef struct
{
    PyObject_HEAD
    int days;                   /* -MAX_DELTA_DAYS <= days <= MAX_DELTA_DAYS */
    int seconds;                /* 0 <= seconds < 24*3600 is invariant */
    int microseconds;           /* 0 <= microseconds < 1000000 is invariant */
} PyDateTime_Delta;

/* The datetime and time types have an optional tzinfo member,
 * PyNone if hastzinfo is false.
 */
typedef struct
{
    PyObject_HEAD
    unsigned char hastzinfo;
    PyObject *tzinfo;
} PyDateTime_Time;

typedef struct
{
    PyObject_HEAD
    unsigned char hastzinfo;
    PyObject *tzinfo;
} PyDateTime_DateTime;


typedef struct {
    PyObject_HEAD
} PyDateTime_Date;


typedef struct {
    PyObject_HEAD
} PyDateTime_TZInfo;



================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/cpyext_descrobject.h
================================================
typedef PyObject *(*getter)(PyObject *, void *);
typedef int (*setter)(PyObject *, PyObject *, void *);

typedef struct PyGetSetDef {
    const char *name;
    getter get;
    setter set;
    const char *doc;
    void *closure;
} PyGetSetDef;

typedef PyObject *(*wrapperfunc)(PyObject *self, PyObject *args,
                                 void *wrapped);

typedef PyObject *(*wrapperfunc_kwds)(PyObject *self, PyObject *args,
                                      void *wrapped, PyObject *kwds);

struct wrapperbase {
    const char *name;
    int offset;
    void *function;
    wrapperfunc wrapper;
    const char *doc;
    int flags;
    PyObject *name_strobj;
};

/* Flags for above struct */
#define PyWrapperFlag_KEYWORDS 1 /* wrapper function takes keyword args */

/* Various kinds of descriptor objects */

typedef struct {
    PyObject_HEAD
    PyTypeObject *d_type;
    PyObject *d_name;
    PyObject *d_qualname;
} PyDescrObject;

#define PyDescr_COMMON PyDescrObject d_common

#define PyDescr_TYPE(x) (((PyDescrObject *)(x))->d_type)
#define PyDescr_NAME(x) (((PyDescrObject *)(x))->d_name)

typedef struct {
    PyDescr_COMMON;
    PyMethodDef *d_method;
} PyMethodDescrObject;

typedef struct {
    PyDescr_COMMON;
    struct PyMemberDef *d_member;
} PyMemberDescrObject;

typedef struct {
    PyDescr_COMMON;
    PyGetSetDef *d_getset;
} PyGetSetDescrObject;

typedef struct {
    PyDescr_COMMON;
    struct wrapperbase *d_base;
    void *d_wrapped; /* This can be any function pointer */
} PyWrapperDescrObject;


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/cpyext_genobject.h
================================================
typedef struct {
    PyObject_HEAD
    PyObject* gi_code;
} PyGenObject;


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/cpyext_memoryobject.h
================================================
/* The struct is declared here but it shouldn't
   be considered public. Don't access those fields directly,
   use the functions instead! */


/* this is wrong, PyMemoryViewObject should use PyObject_VAR_HEAD, and use
   ob_data[1] to hold the shapes, strides, and offsets for the view. Then
   we should use specialized allocators (that break the cpyext model) to
   allocate ob_data = malloc(sizeof(Py_ssize_t) * view.ndims * 3) */
typedef struct {
    PyObject_HEAD
    Py_buffer view;
} PyMemoryViewObject;


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/cpyext_moduleobject.h
================================================
typedef struct PyModuleDef_Base {
  PyObject_HEAD
  PyObject* (*m_init)(void);
  Py_ssize_t m_index;
  PyObject* m_copy;
} PyModuleDef_Base;

#define PyModuleDef_HEAD_INIT { \
    PyObject_HEAD_INIT(NULL)    \
    NULL, /* m_init */          \
    0,    /* m_index */         \
    NULL, /* m_copy */          \
  }

struct PyModuleDef_Slot;
/* New in 3.5 */
typedef struct PyModuleDef_Slot{
    int slot;
    void *value;
} PyModuleDef_Slot;

#define Py_mod_create 1
#define Py_mod_exec 2

#define _Py_mod_LAST_SLOT 2


typedef struct PyModuleDef{
  PyModuleDef_Base m_base;
  const char* m_name;
  const char* m_doc;
  Py_ssize_t m_size;
  PyMethodDef *m_methods;
  struct PyModuleDef_Slot* m_slots;
  traverseproc m_traverse;
  inquiry m_clear;
  freefunc m_free;
} PyModuleDef;

typedef struct {
    PyObject_HEAD
    struct PyModuleDef *md_def;
    void *md_state;
} PyModuleObject;


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/cpyext_object.h
================================================
#pragma once

#define PyObject_HEAD     PyObject    ob_base;
#define PyObject_VAR_HEAD PyVarObject ob_base;

typedef struct _object {
    Py_ssize_t ob_refcnt;
    Py_ssize_t ob_pypy_link;
    struct _typeobject *ob_type;
} PyObject;

typedef struct {
    PyObject ob_base;
	Py_ssize_t ob_size; /* Number of items in variable part */
} PyVarObject;

struct _typeobject;
typedef void (*freefunc)(void *);
typedef void (*destructor)(PyObject *);
typedef int (*printfunc)(PyObject *, FILE *, int);
typedef PyObject *(*getattrfunc)(PyObject *, char *);
typedef PyObject *(*getattrofunc)(PyObject *, PyObject *);
typedef int (*setattrfunc)(PyObject *, char *, PyObject *);
typedef int (*setattrofunc)(PyObject *, PyObject *, PyObject *);
typedef int (*cmpfunc)(PyObject *, PyObject *);
typedef PyObject *(*reprfunc)(PyObject *);
typedef Py_hash_t (*hashfunc)(PyObject *);
typedef PyObject *(*richcmpfunc) (PyObject *, PyObject *, int);
typedef PyObject *(*getiterfunc) (PyObject *);
typedef PyObject *(*iternextfunc) (PyObject *);
typedef PyObject *(*descrgetfunc) (PyObject *, PyObject *, PyObject *);
typedef int (*descrsetfunc) (PyObject *, PyObject *, PyObject *);
typedef int (*initproc)(PyObject *, PyObject *, PyObject *);
typedef PyObject *(*newfunc)(struct _typeobject *, PyObject *, PyObject *);
typedef PyObject *(*allocfunc)(struct _typeobject *, Py_ssize_t);

typedef PyObject * (*unaryfunc)(PyObject *);
typedef PyObject * (*binaryfunc)(PyObject *, PyObject *);
typedef PyObject * (*ternaryfunc)(PyObject *, PyObject *, PyObject *);
typedef int (*inquiry)(PyObject *);
typedef Py_ssize_t (*lenfunc)(PyObject *);
typedef PyObject *(*ssizeargfunc)(PyObject *, Py_ssize_t);
typedef PyObject *(*ssizessizeargfunc)(PyObject *, Py_ssize_t, Py_ssize_t);
typedef int(*ssizeobjargproc)(PyObject *, Py_ssize_t, PyObject *);
typedef int(*ssizessizeobjargproc)(PyObject *, Py_ssize_t, Py_ssize_t, PyObject *);
typedef int(*objobjargproc)(PyObject *, PyObject *, PyObject *);


/* Py3k buffer interface, adapted for PyPy */
/* XXX remove this constant, us a PyObject_VAR_HEAD instead */
#define Py_MAX_NDIMS 36
typedef struct bufferinfo {
    void *buf;
    PyObject *obj;        /* owned reference */
    Py_ssize_t len;
    Py_ssize_t itemsize;  /* This is Py_ssize_t so it can be
                             pointed to by strides in simple case.*/
    int readonly;
    int ndim;
    char *format;
    Py_ssize_t *shape;
    Py_ssize_t *strides;
    Py_ssize_t *suboffsets; /* alway NULL for app-level objects*/
    void *internal; /* always NULL for app-level objects */
    /* PyPy extensions */
    int flags;
    Py_ssize_t _strides[Py_MAX_NDIMS];
    Py_ssize_t _shape[Py_MAX_NDIMS];
    /* static store for shape and strides of
       mono-dimensional buffers. */
    /* Py_ssize_t smalltable[2]; */
} Py_buffer;

typedef int (*getbufferproc)(PyObject *, Py_buffer *, int);
typedef void (*releasebufferproc)(PyObject *, Py_buffer *);
/* end Py3k buffer interface */
typedef PyObject *(*vectorcallfunc)(PyObject *callable, PyObject *const *args,
                                    size_t nargsf, PyObject *kwnames);

typedef int (*objobjproc)(PyObject *, PyObject *);
typedef int (*visitproc)(PyObject *, void *);
typedef int (*traverseproc)(PyObject *, visitproc, void *);


typedef struct {
    /* Number implementations must check *both*
       arguments for proper type and implement the necessary conversions
       in the slot functions themselves. */

    binaryfunc nb_add;
    binaryfunc nb_subtract;
    binaryfunc nb_multiply;
    binaryfunc nb_remainder;
    binaryfunc nb_divmod;
    ternaryfunc nb_power;
    unaryfunc nb_negative;
    unaryfunc nb_positive;
    unaryfunc nb_absolute;
    inquiry nb_bool;
    unaryfunc nb_invert;
    binaryfunc nb_lshift;
    binaryfunc nb_rshift;
    binaryfunc nb_and;
    binaryfunc nb_xor;
    binaryfunc nb_or;
    unaryfunc nb_int;
    void *nb_reserved;  /* the slot formerly known as nb_long */
    unaryfunc nb_float;

    binaryfunc nb_inplace_add;
    binaryfunc nb_inplace_subtract;
    binaryfunc nb_inplace_multiply;
    binaryfunc nb_inplace_remainder;
    ternaryfunc nb_inplace_power;
    binaryfunc nb_inplace_lshift;
    binaryfunc nb_inplace_rshift;
    binaryfunc nb_inplace_and;
    binaryfunc nb_inplace_xor;
    binaryfunc nb_inplace_or;

    binaryfunc nb_floor_divide;
    binaryfunc nb_true_divide;
    binaryfunc nb_inplace_floor_divide;
    binaryfunc nb_inplace_true_divide;

    unaryfunc nb_index;

    binaryfunc nb_matrix_multiply;
    binaryfunc nb_inplace_matrix_multiply;
} PyNumberMethods;

typedef struct {
    lenfunc sq_length;
    binaryfunc sq_concat;
    ssizeargfunc sq_repeat;
    ssizeargfunc sq_item;
    void *was_sq_slice;
    ssizeobjargproc sq_ass_item;
    void *was_sq_ass_slice;
    objobjproc sq_contains;

    binaryfunc sq_inplace_concat;
    ssizeargfunc sq_inplace_repeat;
} PySequenceMethods;

typedef struct {
    lenfunc mp_length;
    binaryfunc mp_subscript;
    objobjargproc mp_ass_subscript;
} PyMappingMethods;

typedef struct {
    unaryfunc am_await;
    unaryfunc am_aiter;
    unaryfunc am_anext;
} PyAsyncMethods;

typedef struct {
     getbufferproc bf_getbuffer;
     releasebufferproc bf_releasebuffer;
} PyBufferProcs;

/* from methodobject.h (the `PyObject **` are `PyObject *const *` in CPython) */
typedef PyObject *(*PyCFunction)(PyObject *, PyObject *);
typedef PyObject *(*_PyCFunctionFast) (PyObject *, PyObject **, Py_ssize_t);
typedef PyObject *(*PyCFunctionWithKeywords)(PyObject *, PyObject *,
                                             PyObject *);
typedef PyObject *(*_PyCFunctionFastWithKeywords) (PyObject *,
                                                   PyObject **, Py_ssize_t,
                                                   PyObject *);

typedef PyObject *(*PyNoArgsFunction)(PyObject *);

struct PyMethodDef {
    const char  *ml_name;   /* The name of the built-in function/method */
    PyCFunction  ml_meth;   /* The C function that implements it */
    int          ml_flags;  /* Combination of METH_xxx flags, which mostly
                               describe the args expected by the C func */
    const char  *ml_doc;    /* The __doc__ attribute, or NULL */
};
typedef struct PyMethodDef PyMethodDef;

typedef struct {
    PyObject_HEAD
    PyMethodDef *m_ml; /* Description of the C function to call */
    PyObject    *m_self; /* Passed as 'self' arg to the C func, can be NULL */
    PyObject    *m_module; /* The __module__ attribute, can be anything */
    PyObject    *m_weakreflist; /* List of weak references */
} PyCFunctionObject;

/* from structmember.h */
typedef struct PyMemberDef {
    /* Current version, use this */
    const char *name;
    int type;
    Py_ssize_t offset;
    int flags;
    const char *doc;
} PyMemberDef;


typedef struct _typeobject {
    PyObject_VAR_HEAD
    const char *tp_name; /* For printing, in format "<module>.<name>" */
    Py_ssize_t tp_basicsize, tp_itemsize; /* For allocation */

    /* Methods to implement standard operations */

    destructor tp_dealloc;
    Py_ssize_t tp_vectorcall_offset;
    getattrfunc tp_getattr;
    setattrfunc tp_setattr;
    PyAsyncMethods *tp_as_async; /* formerly known as tp_compare (Python 2)
                                    or tp_reserved (Python 3) */
    reprfunc tp_repr;

    /* Method suites for standard classes */

    PyNumberMethods *tp_as_number;
    PySequenceMethods *tp_as_sequence;
    PyMappingMethods *tp_as_mapping;

    /* More standard operations (here for binary compatibility) */

    hashfunc tp_hash;
    ternaryfunc tp_call;
    reprfunc tp_str;
    getattrofunc tp_getattro;
    setattrofunc tp_setattro;

    /* Functions to access object as input/output buffer */
    PyBufferProcs *tp_as_buffer;

    /* Flags to define presence of optional/expanded features */
    unsigned long tp_flags;

    const char *tp_doc; /* Documentation string */

    /* Assigned meaning in release 2.0 */
    /* call function for all accessible objects */
    traverseproc tp_traverse;

    /* delete references to contained objects */
    inquiry tp_clear;

    /* Assigned meaning in release 2.1 */
    /* rich comparisons */
    richcmpfunc tp_richcompare;

    /* weak reference enabler */
    Py_ssize_t tp_weaklistoffset;

    /* Iterators */
    getiterfunc tp_iter;
    iternextfunc tp_iternext;

    /* Attribute descriptor and subclassing stuff */
    struct PyMethodDef *tp_methods;
    struct PyMemberDef *tp_members;
    struct PyGetSetDef *tp_getset;
    struct _typeobject *tp_base;
    PyObject *tp_dict;
    descrgetfunc tp_descr_get;
    descrsetfunc tp_descr_set;
    Py_ssize_t tp_dictoffset;
    initproc tp_init;
    allocfunc tp_alloc;
    newfunc tp_new;
    freefunc tp_free; /* Low-level free-memory routine */
    inquiry tp_is_gc; /* For PyObject_IS_GC */
    PyObject *tp_bases;
    PyObject *tp_mro; /* method resolution order */
    PyObject *tp_cache;
    PyObject *tp_subclasses;
    PyObject *tp_weaklist;
    destructor tp_del;

    /* Type attribute cache version tag. Added in version 2.6 */
    unsigned int tp_version_tag;

    destructor tp_finalize;
    vectorcallfunc tp_vectorcall;

    printfunc tp_print; // deprecated, but stays around for compatibility
} PyTypeObject;

typedef struct{
    int slot;    /* slot id, see below */
    void *pfunc; /* function pointer */
} PyType_Slot;

typedef struct{
    const char* name;
    int basicsize;
    int itemsize;
    unsigned int flags;
    PyType_Slot *slots; /* terminated by slot==0. */
} PyType_Spec;

typedef struct _heaptypeobject {
    PyTypeObject ht_type;
    PyAsyncMethods as_async;
    PyNumberMethods as_number;
    PyMappingMethods as_mapping;
    PySequenceMethods as_sequence;
    PyBufferProcs as_buffer;
    PyObject *ht_name, *ht_slots, *ht_qualname;
} PyHeapTypeObject;


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/cpyext_unicodeobject.h
================================================
/* --- Internal Unicode Format -------------------------------------------- */


/* Py_UNICODE was the native Unicode storage format (code unit) used by
   Python and represents a single Unicode element in the Unicode type.
   With PEP 393, Py_UNICODE is deprecated and replaced with a
   typedef to wchar_t. */

#define PY_UNICODE_TYPE wchar_t
typedef wchar_t Py_UNICODE;

/* Py_UCS4 and Py_UCS2 are typedefs for the respective
   unicode representations. */
typedef unsigned int Py_UCS4;
typedef unsigned short Py_UCS2;
typedef unsigned char Py_UCS1;

/* --- Unicode Type ------------------------------------------------------- */

typedef struct {
        /*
           SSTATE_NOT_INTERNED (0)
           SSTATE_INTERNED_MORTAL (1)
           SSTATE_INTERNED_IMMORTAL (2)

           If interned != SSTATE_NOT_INTERNED, the two references from the
           dictionary to this object are *not* counted in ob_refcnt.
         */
        unsigned char interned;
        /* Character size:

           - PyUnicode_WCHAR_KIND (0):

             * character type = wchar_t (16 or 32 bits, depending on the
               platform)

           - PyUnicode_1BYTE_KIND (1):

             * character type = Py_UCS1 (8 bits, unsigned)
             * all characters are in the range U+0000-U+00FF (latin1)
             * if ascii is set, all characters are in the range U+0000-U+007F
               (ASCII), otherwise at least one character is in the range
               U+0080-U+00FF

           - PyUnicode_2BYTE_KIND (2):

             * character type = Py_UCS2 (16 bits, unsigned)
             * all characters are in the range U+0000-U+FFFF (BMP)
             * at least one character is in the range U+0100-U+FFFF

           - PyUnicode_4BYTE_KIND (4):

             * character type = Py_UCS4 (32 bits, unsigned)
             * all characters are in the range U+0000-U+10FFFF
             * at least one character is in the range U+10000-U+10FFFF
         */
        unsigned char kind;
        /* Compact is with respect to the allocation scheme. Compact unicode
           objects only require one memory block while non-compact objects use
           one block for the PyUnicodeObject struct and another for its data
           buffer. */
        unsigned char compact;
        /* The string only contains characters in the range U+0000-U+007F (ASCII)
           and the kind is PyUnicode_1BYTE_KIND. If ascii is set and compact is
           set, use the PyASCIIObject structure. */
        unsigned char ascii;
        /* The ready flag indicates whether the object layout is initialized
           completely. This means that this is either a compact object, or
           the data pointer is filled out. The bit is redundant, and helps
           to minimize the test in PyUnicode_IS_READY(). */
        unsigned char ready;
        /* Padding to ensure that PyUnicode_DATA() is always aligned to
           4 bytes (see issue #19537 on m68k). */
        /* not on PyPy */
    } _PyASCIIObject_state_t;

/* ASCII-only strings created through PyUnicode_New use the PyASCIIObject
   structure. state.ascii and state.compact are set, and the data
   immediately follow the structure. utf8_length and wstr_length can be found
   in the length field; the utf8 pointer is equal to the data pointer. */
typedef struct {
    /* There are 4 forms of Unicode strings:

       - compact ascii:

         * structure = PyASCIIObject
         * test: PyUnicode_IS_COMPACT_ASCII(op)
         * kind = PyUnicode_1BYTE_KIND
         * compact = 1
         * ascii = 1
         * ready = 1
         * (length is the length of the utf8 and wstr strings)
         * (data starts just after the structure)
         * (since ASCII is decoded from UTF-8, the utf8 string are the data)

       - compact:

         * structure = PyCompactUnicodeObject
         * test: PyUnicode_IS_COMPACT(op) && !PyUnicode_IS_ASCII(op)
         * kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND or
           PyUnicode_4BYTE_KIND
         * compact = 1
         * ready = 1
         * ascii = 0
         * utf8 is not shared with data
         * utf8_length = 0 if utf8 is NULL
         * wstr is shared with data and wstr_length=length
           if kind=PyUnicode_2BYTE_KIND and sizeof(wchar_t)=2
           or if kind=PyUnicode_4BYTE_KIND and sizeof(wchar_t)=4
         * wstr_length = 0 if wstr is NULL
         * (data starts just after the structure)

       - legacy string, not ready:

         * structure = PyUnicodeObject
         * test: kind == PyUnicode_WCHAR_KIND
         * length = 0 (use wstr_length)
         * hash = -1
         * kind = PyUnicode_WCHAR_KIND
         * compact = 0
         * ascii = 0
         * ready = 0
         * interned = SSTATE_NOT_INTERNED
         * wstr is not NULL
         * data.any is NULL
         * utf8 is NULL
         * utf8_length = 0

       - legacy string, ready:

         * structure = PyUnicodeObject structure
         * test: !PyUnicode_IS_COMPACT(op) && kind != PyUnicode_WCHAR_KIND
         * kind = PyUnicode_1BYTE_KIND, PyUnicode_2BYTE_KIND or
           PyUnicode_4BYTE_KIND
         * compact = 0
         * ready = 1
         * data.any is not NULL
         * utf8 is shared and utf8_length = length with data.any if ascii = 1
         * utf8_length = 0 if utf8 is NULL
         * wstr is shared with data.any and wstr_length = length
           if kind=PyUnicode_2BYTE_KIND and sizeof(wchar_t)=2
           or if kind=PyUnicode_4BYTE_KIND and sizeof(wchar_4)=4
         * wstr_length = 0 if wstr is NULL

       Compact strings use only one memory block (structure + characters),
       whereas legacy strings use one block for the structure and one block
       for characters.

       Legacy strings are created by PyUnicode_FromUnicode() and
       PyUnicode_FromStringAndSize(NULL, size) functions. They become ready
       when PyUnicode_READY() is called.

       See also _PyUnicode_CheckConsistency().
    */
    PyObject_HEAD
    Py_ssize_t length;          /* Number of code points in the string */
    //Py_hash_t hash;             /* Hash value; -1 if not set */
    _PyASCIIObject_state_t state;
    wchar_t *wstr;              /* wchar_t representation (null-terminated) */
} PyASCIIObject;

/* Non-ASCII strings allocated through PyUnicode_New use the
   PyCompactUnicodeObject structure. state.compact is set, and the data
   immediately follow the structure. */
typedef struct {
    PyASCIIObject _base;
    Py_ssize_t utf8_length;     /* Number of bytes in utf8, excluding the
                                 * terminating \0. */
    char *utf8;                 /* UTF-8 representation (null-terminated) */
    Py_ssize_t wstr_length;     /* Number of code points in wstr, possible
                                 * surrogates count as two code points. */
} PyCompactUnicodeObject;

/* Strings allocated through PyUnicode_FromUnicode(NULL, len) use the
   PyUnicodeObject structure. The actual string data is initially in the wstr
   block, and copied into the data block using _PyUnicode_Ready. */
typedef struct {
    PyCompactUnicodeObject _base;
    void* data;                     /* Canonical, smallest-form Unicode buffer */
} PyUnicodeObject;


/* --- Flexible String Representation Helper Macros (PEP 393) -------------- */

/* Values for PyASCIIObject.state: */

/* Interning state. */
#define SSTATE_NOT_INTERNED 0
#define SSTATE_INTERNED_MORTAL 1
#define SSTATE_INTERNED_IMMORTAL 2

/* --- Constants ---------------------------------------------------------- */

/* This Unicode character will be used as replacement character during
   decoding if the errors argument is set to "replace". Note: the
   Unicode character U+FFFD is the official REPLACEMENT CHARACTER in
   Unicode 3.0. */

#define Py_UNICODE_REPLACEMENT_CHARACTER ((Py_UCS4) 0xFFFD)


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/datetime.h
================================================
#ifndef DATETIME_H
#define DATETIME_H
#ifdef __cplusplus
extern "C" {
#endif

#include "cpyext_datetime.h"

PyAPI_DATA(PyDateTime_CAPI*) PyDateTimeAPI;

#define PyDateTime_IMPORT (PyDateTimeAPI = _PyDateTime_Import())

/* Macro for access to the UTC singleton */
#define PyDateTime_TimeZone_UTC PyDateTimeAPI->TimeZone_UTC

/* Macros for accessing constructors in a simplified fashion. */
#define PyDate_FromDate(year, month, day) \
    PyDateTimeAPI->Date_FromDate(year, month, day, PyDateTimeAPI->DateType)

#define PyDateTime_FromDateAndTime(year, month, day, hour, min, sec, usec) \
    PyDateTimeAPI->DateTime_FromDateAndTime(year, month, day, hour, \
        min, sec, usec, Py_None, PyDateTimeAPI->DateTimeType)

#define PyDateTime_FromDateAndTimeAndFold(year, month, day, hour, min, sec, usec, fold) \
    PyDateTimeAPI->DateTime_FromDateAndTimeAndFold(year, month, day, hour, \
        min, sec, usec, Py_None, fold, PyDateTimeAPI->DateTimeType)

#define PyTime_FromTime(hour, minute, second, usecond) \
    PyDateTimeAPI->Time_FromTime(hour, minute, second, usecond, \
        Py_None, PyDateTimeAPI->TimeType)

#define PyTime_FromTimeAndFold(hour, minute, second, usecond, fold) \
    PyDateTimeAPI->Time_FromTimeAndFold(hour, minute, second, usecond, \
        Py_None, fold, PyDateTimeAPI->TimeType)

#define PyDelta_FromDSU(days, seconds, useconds) \
    PyDateTimeAPI->Delta_FromDelta(days, seconds, useconds, 1, \
        PyDateTimeAPI->DeltaType)

#define PyTimeZone_FromOffset(offset) \
    PyDateTimeAPI->TimeZone_FromTimeZone(offset, NULL)

#define PyTimeZone_FromOffsetAndName(offset, name) \
    PyDateTimeAPI->TimeZone_FromTimeZone(offset, name)

#define PyDateTime_TimeZone_UTC PyDateTimeAPI->TimeZone_UTC

/* Issue 3627: PEP 495 defines PyDateTime_GET_FOLD but CPython implemented
 * PyDateTime_DATE_GET_FOLD
 */
#define PyDateTime_DATE_GET_FOLD PyDateTime_GET_FOLD

#ifdef __cplusplus
}
#endif
#endif


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/descrobject.h
================================================
#ifndef Py_DESCROBJECT_H
#define Py_DESCROBJECT_H

#include "cpyext_descrobject.h"

#endif


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/dictobject.h
================================================

/* dict object interface */

#ifndef Py_DICTOBJECT_H
#define Py_DICTOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

typedef struct {
    PyObject_HEAD
    PyObject *_tmpkeys; /* a private place to put keys during PyDict_Next */
} PyDictObject;

#define PyDict_Check(op) \
		 PyType_FastSubclass(Py_TYPE(op), Py_TPFLAGS_DICT_SUBCLASS)
#define PyDict_CheckExact(op) (Py_TYPE(op) == &PyDict_Type)
#define PyDict_GET_SIZE(op)  PyObject_Length(op)

#ifdef __cplusplus
}
#endif
#endif /* !Py_DICTOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/eval.h
================================================

/* Int object interface */

#ifndef Py_EVAL_H
#define Py_EVAL_H
#ifdef __cplusplus
extern "C" {
#endif

#include "Python.h"

#ifdef PY_SSIZE_T_CLEAN
#undef PyObject_CallFunction
#undef PyObject_CallMethod
#define PyObject_CallFunction _PyObject_CallFunction_SizeT
#define PyObject_CallMethod _PyObject_CallMethod_SizeT
#endif

#define PyEval_CallObject(func,arg) \
        PyEval_CallObjectWithKeywords(func, arg, (PyObject *)NULL)

PyAPI_FUNC(PyObject *) PyEval_CallFunction(PyObject *obj, const char *format, ...);
PyAPI_FUNC(PyObject *) PyEval_CallMethod(PyObject *obj, const char *name, const char *format, ...);
PyAPI_FUNC(PyObject *) PyObject_CallFunction(PyObject *obj, const char *format, ...);
PyAPI_FUNC(PyObject *) PyObject_CallMethod(PyObject *obj, const char *name, const char *format, ...);
PyAPI_FUNC(PyObject *) _PyObject_CallFunction_SizeT(PyObject *obj, const char *format, ...);
PyAPI_FUNC(PyObject *) _PyObject_CallMethod_SizeT(PyObject *obj, const char *name, const char *format, ...);
PyAPI_FUNC(PyObject *) PyObject_CallFunctionObjArgs(PyObject *callable, ...);
PyAPI_FUNC(PyObject *) PyObject_CallMethodObjArgs(PyObject *callable, PyObject *name, ...);

/* These constants are also defined in cpyext/eval.py */
#define Py_single_input 256
#define Py_file_input 257
#define Py_eval_input 258
#define Py_func_type_input 345

#ifdef __cplusplus
}
#endif
#endif /* !Py_EVAL_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/exports.h
================================================
#ifndef Py_EXPORTS_H
#define Py_EXPORTS_H

#if defined(_WIN32) || defined(__CYGWIN__)
    #define Py_IMPORTED_SYMBOL __declspec(dllimport)
    #define Py_EXPORTED_SYMBOL __declspec(dllexport)
    #define Py_LOCAL_SYMBOL
#else
/*
 * If we only ever used gcc >= 5, we could use __has_attribute(visibility)
 * as a cross-platform way to determine if visibility is supported. However,
 * we may still need to support gcc >= 4, as some Ubuntu LTS and Centos versions
 * have 4 < gcc < 5.
 */
    #ifndef __has_attribute
      #define __has_attribute(x) 0  // Compatibility with non-clang compilers.
    #endif
    #if (defined(__GNUC__) && (__GNUC__ >= 4)) ||\
        (defined(__clang__) && __has_attribute(visibility))
        #define Py_IMPORTED_SYMBOL __attribute__ ((visibility ("default")))
        #define Py_EXPORTED_SYMBOL __attribute__ ((visibility ("default")))
        #define Py_LOCAL_SYMBOL  __attribute__ ((visibility ("hidden")))
    #else
        #define Py_IMPORTED_SYMBOL
        #define Py_EXPORTED_SYMBOL
        #define Py_LOCAL_SYMBOL
    #endif
#endif

#endif /* Py_EXPORTS_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/fileobject.h
================================================
PyAPI_DATA(const char *) Py_FileSystemDefaultEncoding;
PyAPI_FUNC(void) _Py_setfilesystemdefaultencoding(const char *);


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/floatobject.h
================================================

/* Float object interface */

#ifndef Py_FLOATOBJECT_H
#define Py_FLOATOBJECT_H

#ifdef _MSC_VER
#include <math.h>
#include <float.h>
#define copysign _copysign
#endif

#ifdef __cplusplus
extern "C" {
#endif

typedef struct {
    PyObject_HEAD
    double ob_fval;
} PyFloatObject;

#define PyFloat_STR_PRECISION 12

#ifdef Py_NAN
#define Py_RETURN_NAN return PyFloat_FromDouble(Py_NAN)
#endif

#define Py_RETURN_INF(sign) do                                  \
        if (copysign(1., sign) == 1.) {                         \
                return PyFloat_FromDouble(Py_HUGE_VAL); \
        } else {                                                \
                return PyFloat_FromDouble(-Py_HUGE_VAL);        \
        } while(0)

#define PyFloat_Check(op) PyObject_TypeCheck(op, &PyFloat_Type)
#define PyFloat_CheckExact(op) (Py_TYPE(op) == &PyFloat_Type)


#ifdef __cplusplus
}
#endif
#endif /* !Py_FLOATOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/frameobject.h
================================================
#ifndef Py_FRAMEOBJECT_H
#define Py_FRAMEOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

typedef struct _frame {
    PyObject_HEAD
    struct _frame *f_back;      /* previous frame, or NULL */
    PyCodeObject *f_code;
    PyObject *f_globals;
    PyObject *f_locals;
    int f_lineno;
} PyFrameObject;

#ifdef __cplusplus
}
#endif
#endif /* !Py_FRAMEOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/funcobject.h
================================================

/* Function object interface */

#ifndef Py_FUNCOBJECT_H
#define Py_FUNCOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

typedef struct {
    PyObject_HEAD
    PyObject *func_name;	/* The __name__ attribute, a string object */
} PyFunctionObject;

PyAPI_DATA(PyTypeObject) PyFunction_Type;

#define PyFunction_GET_CODE(obj) PyFunction_GetCode((PyObject*)(obj))

#define PyMethod_GET_FUNCTION(obj) PyMethod_Function((PyObject*)(obj))
#define PyMethod_GET_SELF(obj) PyMethod_Self((PyObject*)(obj))

#ifdef __cplusplus
}
#endif
#endif /* !Py_FUNCOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/genobject.h
================================================
#ifndef Py_GENOBJECT_H
#define Py_GENOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

#include "cpyext_genobject.h"

#ifdef __cplusplus
}
#endif
#endif /* !Py_GENOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/graminit.h
================================================
#define yield_stmt 347

================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/import.h
================================================

/* Module definition and import interface */

#ifndef Py_IMPORT_H
#define Py_IMPORT_H
#ifdef __cplusplus
extern "C" {
#endif

PyAPI_FUNC(PyObject *) PyImport_ImportModuleLevel(
    const char *name,           /* UTF-8 encoded string */
    PyObject *globals,
    PyObject *locals,
    PyObject *fromlist,
    int level
    );

#define PyImport_ImportModuleEx(n, g, l, f) \
    PyImport_ImportModuleLevel(n, g, l, f, 0)

#ifdef __cplusplus
}
#endif
#endif /* !Py_IMPORT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/listobject.h
================================================
/* empty */
#define PyList_Check(op) \
		 PyType_FastSubclass(Py_TYPE(op), Py_TPFLAGS_LIST_SUBCLASS)
#define PyList_CheckExact(op) (Py_TYPE(op) == &PyList_Type)


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/longintrepr.h
================================================
/* empty */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/longobject.h
================================================
#ifndef Py_LONGOBJECT_H
#define Py_LONGOBJECT_H

#include <stdlib.h>

#ifdef __cplusplus
extern "C" {
#endif

/* why does cpython redefine these, and even supply an implementation in mystrtoul.c?
PyAPI_FUNC(unsigned long) PyOS_strtoul(const char *, char **, int);
PyAPI_FUNC(long) PyOS_strtol(const char *, char **, int);
*/

#define PyOS_strtoul strtoul
#define PyOS_strtol strtoul
#define PyLong_Check(op) \
		 PyType_FastSubclass(Py_TYPE(op), Py_TPFLAGS_LONG_SUBCLASS)
#define PyLong_CheckExact(op) (Py_TYPE(op) == &PyLong_Type)

#define PyLong_AS_LONG(op) PyLong_AsLong(op)

#define _PyLong_AsByteArray(v, bytes, n, little_endian, is_signed)   \
    _PyLong_AsByteArrayO((PyObject *)(v), bytes, n, little_endian, is_signed)

#ifdef __cplusplus
}
#endif
#endif /* !Py_LONGOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/marshal.h
================================================
#ifndef Py_MARSHAL_H
#define Py_MARSHAL_H
#ifdef __cplusplus
extern "C" {
#endif

#define Py_MARSHAL_VERSION 2
#include "pypy_marshal_decl.h"

#ifdef __cplusplus
}
#endif
#endif /* !Py_MARSHAL_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/memoryobject.h
================================================
#ifndef Py_MEMORYOBJECT_H
#define Py_MEMORYOBJECT_H

#ifdef __cplusplus
extern "C" {
#endif

#include "cpyext_memoryobject.h"

/* Get a pointer to the memoryview's private copy of the exporter's buffer. */
#define PyMemoryView_GET_BUFFER(op) (&((PyMemoryViewObject *)(op))->view)
/* Get a pointer to the exporting object (this may be NULL!). */
#define PyMemoryView_GET_BASE(op) (((PyMemoryViewObject *)(op))->view.obj)


#ifdef __cplusplus
}
#endif
#endif /* !Py_MEMORYOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/methodobject.h
================================================

/* Method object interface */

#ifndef Py_METHODOBJECT_H
#define Py_METHODOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

/* Flag passed to newmethodobject */
#define METH_VARARGS  0x0001
#define METH_KEYWORDS 0x0002
/* METH_NOARGS and METH_O must not be combined with the flags above. */
#define METH_NOARGS   0x0004
#define METH_O        0x0008

/* METH_CLASS and METH_STATIC are a little different; these control
   the construction of methods for a class.  These cannot be used for
   functions in modules. */
#define METH_CLASS    0x0010
#define METH_STATIC   0x0020

/* METH_COEXIST allows a method to be entered eventhough a slot has
   already filled the entry.  When defined, the flag allows a separate
   method, "__contains__" for example, to coexist with a defined 
   slot like sq_contains. */

#define METH_COEXIST   0x0040

#if !defined(Py_LIMITED_API) || Py_LIMITED_API+0 >= 0x03100000
#define METH_FASTCALL  0x0080
#endif

#define PyCFunction_New(ml, self) PyCFunction_NewEx((ml), (self), NULL)

/* Macros for direct access to these values. Type checks are *not*
   done, so use with care. */
#define PyCFunction_GET_FUNCTION(func) \
        (((PyCFunctionObject *)func) -> m_ml -> ml_meth)
#define PyCFunction_GET_SELF(func) \
	(((PyCFunctionObject *)func) -> m_self)
#define PyCFunction_GET_FLAGS(func) \
	(((PyCFunctionObject *)func) -> m_ml -> ml_flags)

#ifdef __cplusplus
}
#endif
#endif /* !Py_METHODOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/missing.h
================================================

/* Definitions from missing header files */

#ifndef Py_MISSING_H
#define Py_MISSING_H
#ifdef __cplusplus
extern "C" {
#endif

PyAPI_DATA(PyTypeObject) PyMethod_Type;
PyAPI_DATA(PyTypeObject) PyRange_Type;
PyAPI_DATA(PyTypeObject) PyTraceBack_Type;

#ifdef __cplusplus
}
#endif
#endif /* !Py_MISSING_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/modsupport.h
================================================

#ifndef Py_MODSUPPORT_H
#define Py_MODSUPPORT_H
#ifdef __cplusplus
extern "C" {
#endif

/* Module support interface */

#include <stdarg.h>

/* If PY_SSIZE_T_CLEAN is defined, each functions treats #-specifier
   to mean Py_ssize_t */
#ifdef PY_SSIZE_T_CLEAN
#undef PyArg_Parse
#undef PyArg_ParseTuple
#undef PyArg_ParseTupleAndKeywords
#undef PyArg_VaParse
#undef PyArg_VaParseTupleAndKeywords
#undef Py_BuildValue
#undef Py_VaBuildValue
#define PyArg_Parse                     _PyArg_Parse_SizeT
#define PyArg_ParseTuple                _PyArg_ParseTuple_SizeT
#define PyArg_ParseTupleAndKeywords     _PyArg_ParseTupleAndKeywords_SizeT
#define PyArg_VaParse                   _PyArg_VaParse_SizeT
#define PyArg_VaParseTupleAndKeywords   _PyArg_VaParseTupleAndKeywords_SizeT
#define Py_BuildValue                   _Py_BuildValue_SizeT
#define Py_VaBuildValue                 _Py_VaBuildValue_SizeT
#ifndef Py_LIMITED_API
#define _Py_VaBuildStack                _Py_VaBuildStack_SizeT
#endif
#else
#endif

PyAPI_FUNC(int) PyArg_Parse(PyObject *, const char *, ...);
PyAPI_FUNC(int) PyArg_ParseTuple(PyObject *, const char *, ...);
PyAPI_FUNC(int) PyArg_ParseTupleAndKeywords(PyObject *, PyObject *,
                                                  const char *, char **, ...);
PyAPI_FUNC(int) PyArg_VaParse(PyObject *, const char *, va_list);
PyAPI_FUNC(int) PyArg_VaParseTupleAndKeywords(PyObject *, PyObject *,
                                                  const char *, char **, va_list);
PyAPI_FUNC(PyObject *) Py_BuildValue(const char *, ...);
PyAPI_FUNC(PyObject *) _Py_BuildValue_SizeT(const char *, ...);
PyAPI_FUNC(PyObject *) _Py_VaBuildValue_SizeT(const char *, va_list);
PyAPI_FUNC(int) _PyArg_NoKeywords(const char *funcname, PyObject *kw);

PyAPI_FUNC(int) PyArg_UnpackTuple(PyObject *args, const char *name, Py_ssize_t min, Py_ssize_t max, ...);

PyAPI_FUNC(PyObject *) Py_VaBuildValue(const char *, va_list);

PyAPI_FUNC(int) PyModule_AddObject(PyObject *, const char *, PyObject *);
PyAPI_FUNC(int) PyModule_AddIntConstant(PyObject *, const char *, long);
PyAPI_FUNC(int) PyModule_AddStringConstant(PyObject *, const char *, const char *);
#define PyModule_AddIntMacro(m, c) PyModule_AddIntConstant(m, #c, c)
#define PyModule_AddStringMacro(m, c) PyModule_AddStringConstant(m, #c, c)
#define Py_CLEANUP_SUPPORTED 0x20000

#define PYTHON_API_VERSION 1013
#define PYTHON_API_STRING "1013"
/* The PYTHON_ABI_VERSION is introduced in PEP 384. For the lifetime of
   Python 3, it will stay at the value of 3; changes to the limited API
   must be performed in a strictly backwards-compatible manner. */
#define PYTHON_ABI_VERSION 3
#define PYTHON_ABI_STRING "3"


PyAPI_FUNC(int) _PyArg_Parse_SizeT(PyObject *, const char *, ...);
PyAPI_FUNC(int) _PyArg_ParseTuple_SizeT(PyObject *, const char *, ...);
PyAPI_FUNC(int) _PyArg_VaParse_SizeT(PyObject *, const char *, va_list);

PyAPI_FUNC(int) _PyArg_ParseTupleAndKeywords_SizeT(PyObject *, PyObject *,
				const char *, char **, ...);
PyAPI_FUNC(int) _PyArg_VaParseTupleAndKeywords_SizeT(PyObject *, PyObject *,
				const char *, char **, va_list);
  
PyAPI_FUNC(PyObject *) PyModule_Create2(struct PyModuleDef*,
                                     int apiver);
#ifdef Py_LIMITED_API
#define PyModule_Create(module) \
        PyModule_Create2(module, PYTHON_ABI_VERSION)
#else
#define PyModule_Create(module) \
        PyModule_Create2(module, PYTHON_API_VERSION)
#endif



PyAPI_DATA(const char *) _Py_PackageContext;

#ifdef __cplusplus
}
#endif
#endif /* !Py_MODSUPPORT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/moduleobject.h
================================================
/* Module object interface */

#ifndef Py_MODULEOBJECT_H
#define Py_MODULEOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

#include "cpyext_moduleobject.h"

PyAPI_FUNC(struct PyModuleDef*) PyModule_GetDef(PyObject*);
PyAPI_FUNC(void*) PyModule_GetState(PyObject*);


PyAPI_FUNC(PyObject *) PyModuleDef_Init(struct PyModuleDef*);

#ifdef __cplusplus
}
#endif
#endif /* !Py_MODULEOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/object.h
================================================
#ifndef Py_OBJECT_H
#define Py_OBJECT_H

#include <stdio.h>

#ifdef __cplusplus
extern "C" {
#endif

#include "cpyext_object.h"

#define PY_SSIZE_T_MAX ((Py_ssize_t)(((size_t)-1)>>1))
#define PY_SSIZE_T_MIN (-PY_SSIZE_T_MAX-1)

#define Py_RETURN_NONE return Py_INCREF(Py_None), Py_None
#define Py_RETURN_NOTIMPLEMENTED \
    return Py_INCREF(Py_NotImplemented), Py_NotImplemented

/*
CPython has this for backwards compatibility with really old extensions, and now
we have it for compatibility with CPython.
*/
#define staticforward static

#define PyObject_HEAD_INIT(type)	\
	{ 1, 0, type },

#define PyVarObject_HEAD_INIT(type, size)	\
	{ PyObject_HEAD_INIT(type) size },

#ifdef PYPY_DEBUG_REFCOUNT
/* Slow version, but useful for debugging */
#define Py_INCREF(ob)   (Py_IncRef((PyObject *)(ob)))
#define Py_DECREF(ob)   (Py_DecRef((PyObject *)(ob)))
#define Py_XINCREF(ob)  (Py_IncRef((PyObject *)(ob)))
#define Py_XDECREF(ob)  (Py_DecRef((PyObject *)(ob)))
#else
/* Fast version */
#define Py_INCREF(ob)   (((PyObject *)(ob))->ob_refcnt++)
#define Py_DECREF(op)                                   \
    do {                                                \
        PyObject *_py_decref_tmp = (PyObject *)(op);    \
        if (--(_py_decref_tmp)->ob_refcnt != 0)         \
            ;                                           \
        else                                            \
            _Py_Dealloc(_py_decref_tmp);                \
    } while (0)

#define Py_XINCREF(op)                                \
    do {                                              \
        PyObject *_py_xincref_tmp = (PyObject *)(op); \
        if (_py_xincref_tmp != NULL)                  \
            Py_INCREF(_py_xincref_tmp);               \
    } while (0)

#define Py_XDECREF(op)                                \
    do {                                              \
        PyObject *_py_xdecref_tmp = (PyObject *)(op); \
        if (_py_xdecref_tmp != NULL)                  \
            Py_DECREF(_py_xdecref_tmp);               \
    } while (0)

#endif
PyAPI_FUNC(void) Py_IncRef(PyObject *);
PyAPI_FUNC(void) Py_DecRef(PyObject *);
extern void *_pypy_rawrefcount_w_marker_deallocating;
PyAPI_FUNC(void) _Py_Dealloc(PyObject *);


#define Py_CLEAR(op)                            \
    do {                                        \
        PyObject *_py_tmp = (PyObject *)(op);   \
        if (_py_tmp != NULL) {                  \
            (op) = NULL;                        \
            Py_DECREF(_py_tmp);                 \
        }                                       \
    } while (0)

#define Py_SETREF(op, op2)                      \
    do {                                        \
        PyObject *_py_tmp = (PyObject *)(op);   \
        (op) = (op2);                           \
        Py_DECREF(_py_tmp);                     \
    } while (0)

#define Py_XSETREF(op, op2)                     \
    do {                                        \
        PyObject *_py_tmp = (PyObject *)(op);   \
        (op) = (op2);                           \
        Py_XDECREF(_py_tmp);                    \
    } while (0)

#define Py_REFCNT(ob)		(((PyObject*)(ob))->ob_refcnt)
#define Py_TYPE(ob)		(((PyObject*)(ob))->ob_type)
#define Py_SIZE(ob)		(((PyVarObject*)(ob))->ob_size)

#define _Py_NewReference(op)                                        \
    ( ((PyObject *)(op))->ob_refcnt = 1,                            \
      ((PyObject *)(op))->ob_pypy_link = 0 )

#define _Py_ForgetReference(ob) /* nothing */

#define Py_None (&_Py_NoneStruct)

/*
Py_NotImplemented is a singleton used to signal that an operation is
not implemented for a given type combination.
*/
#define Py_NotImplemented (&_Py_NotImplementedStruct)

/* Rich comparison opcodes */
/*
    XXX: Also defined in slotdefs.py
*/
#define Py_LT 0
#define Py_LE 1
#define Py_EQ 2
#define Py_NE 3
#define Py_GT 4
#define Py_GE 5

/* Py3k buffer interface, adapted for PyPy */
    /* Flags for getting buffers */
#define PyBUF_SIMPLE 0
#define PyBUF_WRITABLE 0x0001
/*  we used to include an E, backwards compatible alias  */
#define PyBUF_WRITEABLE PyBUF_WRITABLE
#define PyBUF_FORMAT 0x0004
#define PyBUF_ND 0x0008
#define PyBUF_STRIDES (0x0010 | PyBUF_ND)
#define PyBUF_C_CONTIGUOUS (0x0020 | PyBUF_STRIDES)
#define PyBUF_F_CONTIGUOUS (0x0040 | PyBUF_STRIDES)
#define PyBUF_ANY_CONTIGUOUS (0x0080 | PyBUF_STRIDES)
#define PyBUF_INDIRECT (0x0100 | PyBUF_STRIDES)

#define PyBUF_CONTIG (PyBUF_ND | PyBUF_WRITABLE)
#define PyBUF_CONTIG_RO (PyBUF_ND)

#define PyBUF_STRIDED (PyBUF_STRIDES | PyBUF_WRITABLE)
#define PyBUF_STRIDED_RO (PyBUF_STRIDES)

#define PyBUF_RECORDS (PyBUF_STRIDES | PyBUF_WRITABLE | PyBUF_FORMAT)
#define PyBUF_RECORDS_RO (PyBUF_STRIDES | PyBUF_FORMAT)

#define PyBUF_FULL (PyBUF_INDIRECT | PyBUF_WRITABLE | PyBUF_FORMAT)
#define PyBUF_FULL_RO (PyBUF_INDIRECT | PyBUF_FORMAT)


#define PyBUF_READ  0x100
#define PyBUF_WRITE 0x200
#define PyBUF_SHADOW 0x400
/* end Py3k buffer interface */


PyAPI_FUNC(PyObject*) PyType_FromSpec(PyType_Spec*);


/* Flag bits for printing: */
#define Py_PRINT_RAW    1       /* No string quotes etc. */

/*
`Type flags (tp_flags)

These flags are used to extend the type structure in a backwards-compatible
fashion. Extensions can use the flags to indicate (and test) when a given
type structure contains a new feature. The Python core will use these when
introducing new functionality between major revisions (to avoid mid-version
changes in the PYTHON_API_VERSION).

Arbitration of the flag bit positions will need to be coordinated among
all extension writers who publically release their extensions (this will
be fewer than you might expect!)..

Most flags were removed as of Python 3.0 to make room for new flags.  (Some
flags are not for backwards compatibility but to indicate the presence of an
optional feature; these flags remain of course.)

Type definitions should use Py_TPFLAGS_DEFAULT for their tp_flags value.

Code can use PyType_HasFeature(type_ob, flag_value) to test whether the
given type object has a specified feature.
*/

/* Set if the type object is dynamically allocated */
#define Py_TPFLAGS_HEAPTYPE (1L<<9)

/* Set if the type allows subclassing */
#define Py_TPFLAGS_BASETYPE (1L<<10)

/* Set if the type implements the vectorcall protocol (PEP 590) */
#define _Py_TPFLAGS_HAVE_VECTORCALL (1UL << 11)

/* Set if the type is 'ready' -- fully initialized */
#define Py_TPFLAGS_READY (1L<<12)

/* Set while the type is being 'readied', to prevent recursive ready calls */
#define Py_TPFLAGS_READYING (1L<<13)

/* Objects support garbage collection (see objimp.h) */
#define Py_TPFLAGS_HAVE_GC (1L<<14)

/* These two bits are preserved for Stackless Python, next after this is 17 */
#ifdef STACKLESS
#define Py_TPFLAGS_HAVE_STACKLESS_EXTENSION (3L<<15)
#else
#define Py_TPFLAGS_HAVE_STACKLESS_EXTENSION 0
#endif

/* Objects behave like an unbound method */
#define Py_TPFLAGS_METHOD_DESCRIPTOR (1UL << 17)

/* Objects support type attribute cache */
#define Py_TPFLAGS_HAVE_VERSION_TAG   (1L<<18)
#define Py_TPFLAGS_VALID_VERSION_TAG  (1L<<19)

/* Type is abstract and cannot be instantiated */
#define Py_TPFLAGS_IS_ABSTRACT (1L<<20)

/* These flags are used to determine if a type is a subclass. */
#define Py_TPFLAGS_LONG_SUBCLASS        (1UL << 24)
#define Py_TPFLAGS_LIST_SUBCLASS        (1UL << 25)
#define Py_TPFLAGS_TUPLE_SUBCLASS       (1UL << 26)
#define Py_TPFLAGS_BYTES_SUBCLASS       (1UL << 27)
#define Py_TPFLAGS_UNICODE_SUBCLASS     (1UL << 28)
#define Py_TPFLAGS_DICT_SUBCLASS        (1UL << 29)
#define Py_TPFLAGS_BASE_EXC_SUBCLASS    (1UL << 30)
#define Py_TPFLAGS_TYPE_SUBCLASS        (1UL << 31)



    
#define Py_TPFLAGS_DEFAULT  ( \
                 Py_TPFLAGS_HAVE_STACKLESS_EXTENSION | \
                 Py_TPFLAGS_HAVE_VERSION_TAG | \
                0)

/* NOTE: The following flags reuse lower bits (removed as part of the
 * Python 3.0 transition). */

/* Type structure has tp_finalize member (3.4) */
#define Py_TPFLAGS_HAVE_FINALIZE (1UL << 0)

PyAPI_FUNC(long) PyType_GetFlags(PyTypeObject*);

#ifdef Py_LIMITED_API
#define PyType_HasFeature(t,f)  ((PyType_GetFlags(t) & (f)) != 0)
#else
#define PyType_HasFeature(t,f)  (((t)->tp_flags & (f)) != 0)
#endif
#define PyType_FastSubclass(t,f)  PyType_HasFeature(t,f)

#if !defined(Py_LIMITED_API)
PyAPI_FUNC(void*) PyType_GetSlot(PyTypeObject*, int);
#endif
    
#define PyType_Check(op) \
    PyType_FastSubclass(Py_TYPE(op), Py_TPFLAGS_TYPE_SUBCLASS)
#define PyType_CheckExact(op) (Py_TYPE(op) == &PyType_Type)

/* objimpl.h ----------------------------------------------*/
#define PyObject_New(type, typeobj) \
		( (type *) _PyObject_New(typeobj) )
#define PyObject_NewVar(type, typeobj, n) \
		( (type *) _PyObject_NewVar((typeobj), (n)) )

#define _PyObject_SIZE(typeobj) ( (typeobj)->tp_basicsize )
#define _PyObject_VAR_SIZE(typeobj, nitems)	\
	(size_t)				\
	( ( (typeobj)->tp_basicsize +		\
	    (nitems)*(typeobj)->tp_itemsize +	\
	    (SIZEOF_VOID_P - 1)			\
	  ) & ~(SIZEOF_VOID_P - 1)		\
	)

        
#define PyObject_INIT(op, typeobj) \
    ( Py_TYPE(op) = (typeobj), ((PyObject *)(op))->ob_refcnt = 1,\
      ((PyObject *)(op))->ob_pypy_link = 0, (op) )
#define PyObject_INIT_VAR(op, typeobj, size) \
    ( Py_SIZE(op) = (size), PyObject_INIT((op), (typeobj)) )


PyAPI_FUNC(PyObject *) PyType_GenericAlloc(PyTypeObject *, Py_ssize_t);

/*
#define PyObject_NEW(type, typeobj) \
( (type *) PyObject_Init( \
	(PyObject *) PyObject_MALLOC( _PyObject_SIZE(typeobj) ), (typeobj)) )
*/
#define PyObject_NEW PyObject_New
#define PyObject_NEW_VAR PyObject_NewVar

/*
#define PyObject_NEW_VAR(type, typeobj, n) \
( (type *) PyObject_InitVar( \
      (PyVarObject *) PyObject_MALLOC(_PyObject_VAR_SIZE((typeobj),(n)) ),\
      (typeobj), (n)) )
*/

#define PyObject_GC_New(type, typeobj) \
                ( (type *) _PyObject_GC_New(typeobj) )
#define PyObject_GC_NewVar(type, typeobj, size) \
                ( (type *) _PyObject_GC_NewVar(typeobj, size) )

/* A dummy PyGC_Head, just to please some tests. Don't use it! */
typedef union _gc_head {
    char dummy;
} PyGC_Head;

/* dummy GC macros */
#define _PyGC_FINALIZED(o) 1
#define PyType_IS_GC(tp) 1

#define PyObject_GC_Track(o)      do { } while(0)
#define PyObject_GC_UnTrack(o)    do { } while(0)
#define _PyObject_GC_TRACK(o)     do { } while(0)
#define _PyObject_GC_UNTRACK(o)   do { } while(0)

/* Utility macro to help write tp_traverse functions.
 * To use this macro, the tp_traverse function must name its arguments
 * "visit" and "arg".  This is intended to keep tp_traverse functions
 * looking as much alike as possible.
 */
#define Py_VISIT(op)                                                    \
        do {                                                            \
                if (op) {                                               \
                        int vret = visit((PyObject *)(op), arg);        \
                        if (vret)                                       \
                                return vret;                            \
                }                                                       \
        } while (0)

#define PyObject_TypeCheck(ob, tp) \
    (Py_TYPE(ob) == (tp) || PyType_IsSubtype(Py_TYPE(ob), (tp)))

#define Py_TRASHCAN_SAFE_BEGIN(pyObj) do {
#define Py_TRASHCAN_SAFE_END(pyObj)   ; } while(0);
/* note: the ";" at the start of Py_TRASHCAN_SAFE_END is needed
   if the code has a label in front of the macro call */

/* Copied from CPython ----------------------------- */
PyAPI_FUNC(int) PyObject_AsReadBuffer(PyObject *, const void **, Py_ssize_t *);
PyAPI_FUNC(int) PyObject_AsWriteBuffer(PyObject *, void **, Py_ssize_t *);
PyAPI_FUNC(int) PyObject_CheckReadBuffer(PyObject *);
PyAPI_FUNC(void *) PyBuffer_GetPointer(Py_buffer *view, Py_ssize_t *indices);
/* Get the memory area pointed to by the indices for the buffer given.
   Note that view->ndim is the assumed size of indices
*/

PyAPI_FUNC(int) PyBuffer_ToContiguous(void *buf, Py_buffer *view,
                                   Py_ssize_t len, char fort);
PyAPI_FUNC(int) PyBuffer_FromContiguous(Py_buffer *view, void *buf,
                                     Py_ssize_t len, char fort);
/* Copy len bytes of data from the contiguous chunk of memory
   pointed to by buf into the buffer exported by obj.  Return
   0 on success and return -1 and raise a PyBuffer_Error on
   error (i.e. the object does not have a buffer interface or
   it is not working).

   If fort is 'F' and the object is multi-dimensional,
   then the data will be copied into the array in
   Fortran-style (first dimension varies the fastest).  If
   fort is 'C', then the data will be copied into the array
   in C-style (last dimension varies the fastest).  If fort
   is 'A', then it does not matter and the copy will be made
   in whatever way is more efficient.

*/


/* on CPython, these are in objimpl.h */

PyAPI_FUNC(void) PyObject_Free(void *);
PyAPI_FUNC(void) PyObject_GC_Del(void *);

#define PyObject_MALLOC         PyObject_Malloc
#define PyObject_REALLOC        PyObject_Realloc
#define PyObject_FREE           PyObject_Free
#define PyObject_Del            PyObject_Free
#define PyObject_DEL            PyObject_Free

PyAPI_FUNC(PyObject *) _PyObject_New(PyTypeObject *);
PyAPI_FUNC(PyVarObject *) _PyObject_NewVar(PyTypeObject *, Py_ssize_t);
PyAPI_FUNC(PyObject *) _PyObject_GC_Malloc(size_t);
PyAPI_FUNC(PyObject *) _PyObject_GC_New(PyTypeObject *);
PyAPI_FUNC(PyVarObject *) _PyObject_GC_NewVar(PyTypeObject *, Py_ssize_t);

PyAPI_FUNC(PyObject *) PyObject_Init(PyObject *, PyTypeObject *);
PyAPI_FUNC(PyVarObject *) PyObject_InitVar(PyVarObject *,
                                           PyTypeObject *, Py_ssize_t);

#ifndef Py_LIMITED_API
PyAPI_FUNC(int) PyObject_CallFinalizerFromDealloc(PyObject *);
#endif

/*
 * On CPython with Py_REF_DEBUG these use _PyRefTotal, _Py_NegativeRefcount,
 * _Py_GetRefTotal, ...
 * So far we ignore Py_REF_DEBUG
 */

#define _Py_INC_REFTOTAL
#define _Py_DEC_REFTOTAL
#define _Py_REF_DEBUG_COMMA
#define _Py_CHECK_REFCNT(OP)    /* a semicolon */;


/* PyPy internal ----------------------------------- */
PyAPI_FUNC(int) PyPyType_Register(PyTypeObject *);
#define PyObject_Length PyObject_Size
#define _PyObject_GC_Del PyObject_GC_Del
PyAPI_FUNC(void) _PyPy_subtype_dealloc(PyObject *);
PyAPI_FUNC(void) _PyPy_object_dealloc(PyObject *);


#ifdef __cplusplus
}
#endif
#endif /* !Py_OBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/patchlevel.h
================================================

/* Newfangled version identification scheme.

   This scheme was added in Python 1.5.2b2; before that time, only PATCHLEVEL
   was available.  To test for presence of the scheme, test for
   defined(PY_MAJOR_VERSION).

   When the major or minor version changes, the VERSION variable in
   configure.in must also be changed.

   There is also (independent) API version information in modsupport.h.
*/

/* Values for PY_RELEASE_LEVEL */
#define PY_RELEASE_LEVEL_ALPHA	0xA
#define PY_RELEASE_LEVEL_BETA	0xB
#define PY_RELEASE_LEVEL_GAMMA	0xC     /* For release candidates */
#define PY_RELEASE_LEVEL_FINAL	0xF	/* Serial should be 0 here */
					/* Higher for patch releases */

/* Version parsed out into numeric values */
#define PY_MAJOR_VERSION	3
#define PY_MINOR_VERSION	8
#define PY_MICRO_VERSION	13
#define PY_RELEASE_LEVEL	PY_RELEASE_LEVEL_FINAL
#define PY_RELEASE_SERIAL	0

/* Version as a string */
#define PY_VERSION		"3.8.13"

/* PyPy version as a string: make sure to keep this in sync with:
 *     module/sys/version.py
 *     doc/conf.py
 */
#define PYPY_VERSION "7.3.9"
#define PYPY_VERSION_NUM  0x07030900
/* Defined to mean a PyPy where cpyext holds more regular references
   to PyObjects, e.g. staying alive as long as the internal PyPy object
   stays alive. */
#define PYPY_CPYEXT_GC      1
#define PyPy_Borrow(a, b)   ((void) 0)

/* Subversion Revision number of this file (not of the repository).
 * Empty since Mercurial migration. */
#define PY_PATCHLEVEL_REVISION  ""

/* Version as a single 4-byte hex number, e.g. 0x010502B2 == 1.5.2b2.
   Use this for numeric comparisons, e.g. #if PY_VERSION_HEX >= ... */
#define PY_VERSION_HEX ((PY_MAJOR_VERSION << 24) | \
			(PY_MINOR_VERSION << 16) | \
			(PY_MICRO_VERSION <<  8) | \
			(PY_RELEASE_LEVEL <<  4) | \
			(PY_RELEASE_SERIAL << 0))


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pycapsule.h
================================================

/* Capsule objects let you wrap a C "void *" pointer in a Python
   object.  They're a way of passing data through the Python interpreter
   without creating your own custom type.

   Capsules are used for communication between extension modules.
   They provide a way for an extension module to export a C interface
   to other extension modules, so that extension modules can use the
   Python import mechanism to link to one another.

   For more information, please see "c-api/capsule.html" in the
   documentation.
*/

#ifndef Py_CAPSULE_H
#define Py_CAPSULE_H
#ifdef __cplusplus
extern "C" {
#endif

PyAPI_DATA(PyTypeObject) PyCapsule_Type;

typedef void (*PyCapsule_Destructor)(PyObject *);

#define PyCapsule_CheckExact(op) (Py_TYPE(op) == &PyCapsule_Type)


PyAPI_FUNC(PyObject *) PyCapsule_New(
    void *pointer,
    const char *name,
    PyCapsule_Destructor destructor);

PyAPI_FUNC(void *) PyCapsule_GetPointer(PyObject *capsule, const char *name);

PyAPI_FUNC(PyCapsule_Destructor) PyCapsule_GetDestructor(PyObject *capsule);

PyAPI_FUNC(const char *) PyCapsule_GetName(PyObject *capsule);

PyAPI_FUNC(void *) PyCapsule_GetContext(PyObject *capsule);

PyAPI_FUNC(int) PyCapsule_IsValid(PyObject *capsule, const char *name);

PyAPI_FUNC(int) PyCapsule_SetPointer(PyObject *capsule, void *pointer);

PyAPI_FUNC(int) PyCapsule_SetDestructor(PyObject *capsule, PyCapsule_Destructor destructor);

PyAPI_FUNC(int) PyCapsule_SetName(PyObject *capsule, const char *name);

PyAPI_FUNC(int) PyCapsule_SetContext(PyObject *capsule, void *context);

PyAPI_FUNC(void *) PyCapsule_Import(const char *name, int no_block);

PyAPI_FUNC(PyTypeObject *) _Py_get_capsule_type(void);

#ifdef __cplusplus
}
#endif
#endif /* !Py_CAPSULE_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pyconfig.h
================================================
#ifndef Py_PYCONFIG_H
#define Py_PYCONFIG_H
#ifdef __cplusplus
extern "C" {
#endif

#define HAVE_PROTOTYPES 1
#define STDC_HEADERS 1

#define HAVE_LONG_LONG 1
#define HAVE_STDARG_PROTOTYPES 1
#define PY_FORMAT_LONG_LONG "ll"
#define PY_FORMAT_SIZE_T "z"
#define WITH_DOC_STRINGS
#define HAVE_UNICODE
#define WITHOUT_COMPLEX
#define HAVE_WCHAR_H 1
#define HAVE_SYS_TYPES_H 1
#define HAVE_SYS_STAT_H 1

/* PyPy supposes Py_UNICODE == wchar_t */
#define HAVE_USABLE_WCHAR_T 1
#ifndef _WIN32
#define SIZEOF_WCHAR_T 4
#else
#define SIZEOF_WCHAR_T 2
#endif

#ifndef _WIN32
#define VA_LIST_IS_ARRAY
#ifndef __APPLE__
#define HAVE_CLOCK_GETTIME
#endif
#endif


#ifdef __cplusplus
}
#endif
#endif


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pyerrors.h
================================================

/* Exception interface */

#ifndef Py_PYERRORS_H
#define Py_PYERRORS_H
#ifdef __cplusplus
extern "C" {
#endif

#define PyExceptionClass_Check(x)                                       \
    ((PyType_Check((x)) &&                                              \
      PyType_FastSubclass((PyTypeObject*)(x), Py_TPFLAGS_BASE_EXC_SUBCLASS)))

#define PyExceptionInstance_Check(x)                                    \
    (PyObject_IsSubclass((PyObject *)Py_TYPE(x), PyExc_BaseException))

#define PyExc_EnvironmentError PyExc_OSError
#define PyExc_IOError PyExc_OSError

#ifdef MS_WINDOWS
#define PyExc_WindowsError PyExc_OSError
#endif

PyAPI_FUNC(PyObject *) PyErr_NewException(const char *name, PyObject *base, PyObject *dict);
PyAPI_FUNC(PyObject *) PyErr_NewExceptionWithDoc(const char *name, const char *doc, PyObject *base, PyObject *dict);
PyAPI_FUNC(PyObject *) PyErr_Format(PyObject *exception, const char *format, ...);
PyAPI_FUNC(PyObject *) _PyErr_FormatFromCause(PyObject *exception, const char *format, ...);

/* These APIs aren't really part of the error implementation, but
   often needed to format error messages; the native C lib APIs are
   not available on all platforms, which is why we provide emulations
   for those platforms in Python/mysnprintf.c,
   WARNING:  The return value of snprintf varies across platforms; do
   not rely on any particular behavior; eventually the C99 defn may
   be reliable.
*/
#if defined(MS_WIN32) && !defined(HAVE_SNPRINTF)
# define HAVE_SNPRINTF
# define snprintf _snprintf
# define vsnprintf _vsnprintf
#endif

#include <stdarg.h>
PyAPI_FUNC(int) PyOS_snprintf(char *str, size_t size, const  char  *format, ...);
PyAPI_FUNC(int) PyOS_vsnprintf(char *str, size_t size, const char  *format, va_list va);

typedef struct {
    PyObject_HEAD       /* xxx PyException_HEAD in CPython */
    PyObject *value;
} PyStopIterationObject;

#ifdef __cplusplus
}
#endif
#endif /* !Py_PYERRORS_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pyhash.h
================================================
#ifndef Py_HASH_H

#define Py_HASH_H
#ifdef __cplusplus
extern "C" {
#endif

/* Keep synced with rpython/rlib/objectmodel.py */

/* Prime multiplier used in string and various other hashes. */
#define _PyHASH_MULTIPLIER 1000003UL  /* 0xf4243 */

/* Parameters used for the numeric hash implementation.  See notes for
   _Py_HashDouble in Python/pyhash.c.  Numeric hashes are based on
   reduction modulo the prime 2**_PyHASH_BITS - 1. */

#if SIZEOF_VOID_P >= 8
#  define _PyHASH_BITS 61
#else
#  define _PyHASH_BITS 31
#endif

#define _PyHASH_MODULUS (((size_t)1 << _PyHASH_BITS) - 1)
#define _PyHASH_INF 314159
#define _PyHASH_NAN 0
#define _PyHASH_IMAG _PyHASH_MULTIPLIER


#ifdef __cplusplus
}
#endif

#endif /* !Py_HASH_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pylifecycle.h
================================================
#ifndef Py_PYLIFECYCLE_H
#define Py_PYLIFECYCLE_H
#ifdef __cplusplus
extern "C" {
#endif


/* Restore signals that the interpreter has called SIG_IGN on to SIG_DFL. */
#ifndef Py_LIMITED_API
PyAPI_FUNC(void) _Py_RestoreSignals(void);
#endif

/* In Python <= 3.6 there is a variable _Py_Finalizing of type
   'PyThreadState *'.  Emulate it with a macro. */
#define _Py_Finalizing  \
    (_Py_IsFinalizing() ? _PyThreadState_UncheckedGet() : NULL)


#ifdef __cplusplus
}
#endif
#endif /* !Py_PYLIFECYCLE_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pymacro.h
================================================
#ifndef Py_PYMACRO_H
#define Py_PYMACRO_H

/* Minimum value between x and y */
#define Py_MIN(x, y) (((x) > (y)) ? (y) : (x))

/* Maximum value between x and y */
#define Py_MAX(x, y) (((x) > (y)) ? (x) : (y))

/* Absolute value of the number x */
#define Py_ABS(x) ((x) < 0 ? -(x) : (x))

#define _Py_XSTRINGIFY(x) #x

/* Convert the argument to a string. For example, Py_STRINGIFY(123) is replaced
   with "123" by the preprocessor. Defines are also replaced by their value.
   For example Py_STRINGIFY(__LINE__) is replaced by the line number, not
   by "__LINE__". */
#define Py_STRINGIFY(x) _Py_XSTRINGIFY(x)

/* Get the size of a structure member in bytes */
#define Py_MEMBER_SIZE(type, member) sizeof(((type *)0)->member)

/* Argument must be a char or an int in [-128, 127] or [0, 255]. */
#define Py_CHARMASK(c) ((unsigned char)((c) & 0xff))

/* Assert a build-time dependency, as an expression.

   Your compile will fail if the condition isn't true, or can't be evaluated
   by the compiler. This can be used in an expression: its value is 0.

   Example:

   #define foo_to_char(foo)  \
       ((char *)(foo)        \
        + Py_BUILD_ASSERT_EXPR(offsetof(struct foo, string) == 0))

   Written by Rusty Russell, public domain, http://ccodearchive.net/ */
#define Py_BUILD_ASSERT_EXPR(cond) \
    (sizeof(char [1 - 2*!(cond)]) - 1)

#define Py_BUILD_ASSERT(cond)  do {         \
        (void)Py_BUILD_ASSERT_EXPR(cond);   \
    } while(0)

/* Get the number of elements in a visible array

   This does not work on pointers, or arrays declared as [], or function
   parameters. With correct compiler support, such usage will cause a build
   error (see Py_BUILD_ASSERT_EXPR).

   Written by Rusty Russell, public domain, http://ccodearchive.net/

   Requires at GCC 3.1+ */
#if (defined(__GNUC__) && !defined(__STRICT_ANSI__) && \
    (((__GNUC__ == 3) && (__GNU_MINOR__ >= 1)) || (__GNUC__ >= 4)))
/* Two gcc extensions.
   &a[0] degrades to a pointer: a different type from an array */
#define Py_ARRAY_LENGTH(array) \
    (sizeof(array) / sizeof((array)[0]) \
     + Py_BUILD_ASSERT_EXPR(!__builtin_types_compatible_p(typeof(array), \
                                                          typeof(&(array)[0]))))
#else
#define Py_ARRAY_LENGTH(array) \
    (sizeof(array) / sizeof((array)[0]))
#endif


/* Define macros for inline documentation. */
#define PyDoc_VAR(name) static char name[]
#define PyDoc_STRVAR(name,str) PyDoc_VAR(name) = PyDoc_STR(str)
#ifdef WITH_DOC_STRINGS
#define PyDoc_STR(str) str
#else
#define PyDoc_STR(str) ""
#endif

/* Below "a" is a power of 2. */
/* Round down size "n" to be a multiple of "a". */
#define _Py_SIZE_ROUND_DOWN(n, a) ((size_t)(n) & ~(size_t)((a) - 1))
/* Round up size "n" to be a multiple of "a". */
#define _Py_SIZE_ROUND_UP(n, a) (((size_t)(n) + \
        (size_t)((a) - 1)) & ~(size_t)((a) - 1))
/* Round pointer "p" down to the closest "a"-aligned address <= "p". */
#define _Py_ALIGN_DOWN(p, a) ((void *)((uintptr_t)(p) & ~(uintptr_t)((a) - 1)))
/* Round pointer "p" up to the closest "a"-aligned address >= "p". */
#define _Py_ALIGN_UP(p, a) ((void *)(((uintptr_t)(p) + \
        (uintptr_t)((a) - 1)) & ~(uintptr_t)((a) - 1)))
/* Check if pointer "p" is aligned to "a"-bytes boundary. */
#define _Py_IS_ALIGNED(p, a) (!((uintptr_t)(p) & (uintptr_t)((a) - 1)))

#ifdef __GNUC__
#define Py_UNUSED(name) _unused_ ## name __attribute__((unused))
#else
#define Py_UNUSED(name) _unused_ ## name
#endif

#endif /* Py_PYMACRO_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pymath.h
================================================
#ifndef Py_PYMATH_H
#define Py_PYMATH_H

/**************************************************************************
Symbols and macros to supply platform-independent interfaces to mathematical
functions and constants
**************************************************************************/

/* High precision definition of pi and e (Euler)
 * The values are taken from libc6's math.h.
 */
#ifndef Py_MATH_PIl
#define Py_MATH_PIl 3.1415926535897932384626433832795029L
#endif
#ifndef Py_MATH_PI
#define Py_MATH_PI 3.[REDACTED]
#endif

#ifndef Py_MATH_El
#define Py_MATH_El 2.7182818284590452353602874713526625L
#endif

#ifndef Py_MATH_E
#define Py_MATH_E 2.7182818284590452354
#endif

/* Tau (2pi) to 40 digits, taken from tauday.com/tau-digits. */
#ifndef Py_MATH_TAU
#define Py_MATH_TAU 6.2831853071795864769252867665590057683943L
#endif

/* Py_IS_NAN(X)
 * Return 1 if float or double arg is a NaN, else 0.
 * Caution:
 *     X is evaluated more than once.
 *     This may not work on all platforms.  Each platform has *some*
 *     way to spell this, though -- override in pyconfig.h if you have
 *     a platform where it doesn't work.
 */
#ifndef Py_IS_NAN
#if __STDC_VERSION__ >= 199901L || __cplusplus >= 201103L
#define Py_IS_NAN(X) isnan(X)
#else
#define Py_IS_NAN(X) ((X) != (X))
#endif
#endif

/* Py_IS_INFINITY(X)
 * Return 1 if float or double arg is an infinity, else 0.
 * Caution:
 *    X is evaluated more than once.
 *    This implementation may set the underflow flag if |X| is very small;
 *    it really can't be implemented correctly (& easily) before C99.
 *    Override in pyconfig.h if you have a better spelling on your platform.
 */
#ifndef Py_IS_INFINITY
#  if __STDC_VERSION__ >= 199901L || __cplusplus >= 201103L
#    define Py_IS_INFINITY(X) isinf(X)
#  else
#    define Py_IS_INFINITY(X) ((X) && ((X)*0.5 == (X)))
#  endif
#endif

/* Py_IS_FINITE(X)
 * Return 1 if float or double arg is neither infinite nor NAN, else 0.
 * Some compilers (e.g. VisualStudio) have intrinsics for this, so a special
 * macro for this particular test is useful
 */
#ifndef Py_IS_FINITE
#if __STDC_VERSION__ >= 199901L || __cplusplus >= 201103L
#define Py_IS_FINITE(X) isfinite(X)
#else
#define Py_IS_FINITE(X) (!Py_IS_INFINITY(X) && !Py_IS_NAN(X))
#endif
#endif

/* HUGE_VAL is supposed to expand to a positive double infinity.  Python
 * uses Py_HUGE_VAL instead because some platforms are broken in this
 * respect.  We used to embed code in pyport.h to try to worm around that,
 * but different platforms are broken in conflicting ways.  If you're on
 * a platform where HUGE_VAL is defined incorrectly, fiddle your Python
 * config to #define Py_HUGE_VAL to something that works on your platform.
 */
#ifndef Py_HUGE_VAL
#define Py_HUGE_VAL HUGE_VAL
#endif

/* Py_NAN
 * A value that evaluates to a NaN. On IEEE 754 platforms INF*0 or
 * INF/INF works. Define Py_NO_NAN in pyconfig.h if your platform
 * doesn't support NaNs.
 */
#if !defined(Py_NAN) && !defined(Py_NO_NAN)
#if !defined(__INTEL_COMPILER)
    #define Py_NAN (Py_HUGE_VAL * 0.)
#else /* __INTEL_COMPILER */
    #if defined(ICC_NAN_STRICT)
        #pragma float_control(push)
        #pragma float_control(precise, on)
        #pragma float_control(except,  on)
        #if defined(_MSC_VER)
            __declspec(noinline)
        #else /* Linux */
            __attribute__((noinline))
        #endif /* _MSC_VER */
        static double __icc_nan()
        {
            return sqrt(-1.0);
        }
        #pragma float_control (pop)
        #define Py_NAN __icc_nan()
    #else /* ICC_NAN_RELAXED as default for Intel Compiler */
        static const union { unsigned char buf[8]; double __icc_nan; } __nan_store = {0,0,0,0,0,0,0xf8,0x7f};
        #define Py_NAN (__nan_store.__icc_nan)
    #endif /* ICC_NAN_STRICT */
#endif /* __INTEL_COMPILER */
#endif
/* Return whether integral type *type* is signed or not. */
#define _Py_IntegralTypeSigned(type) ((type)(-1) < 0)
/* Return the maximum value of integral type *type*. */
#define _Py_IntegralTypeMax(type) ((_Py_IntegralTypeSigned(type)) ? (((((type)1 << (sizeof(type)*CHAR_BIT - 2)) - 1) << 1) + 1) : ~(type)0)
/* Return the minimum value of integral type *type*. */
#define _Py_IntegralTypeMin(type) ((_Py_IntegralTypeSigned(type)) ? -_Py_IntegralTypeMax(type) - 1 : 0)
/* Check whether *v* is in the range of integral type *type*. This is most
 * useful if *v* is floating-point, since demoting a floating-point *v* to an
 * integral type that cannot represent *v*'s integral part is undefined
 * behavior. */
#define _Py_InIntegralTypeRange(type, v) (_Py_IntegralTypeMin(type) <= v && v <= _Py_IntegralTypeMax(type))

#endif /* Py_PYMATH_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pymem.h
================================================
#include <stdlib.h>

#ifndef Py_PYMEM_H
#define Py_PYMEM_H

#ifdef __cplusplus
extern "C" {
#endif

#ifndef Py_LIMITED_API
PyAPI_FUNC(void *) PyMem_RawMalloc(size_t size);
PyAPI_FUNC(void *) PyMem_RawCalloc(size_t nelem, size_t elsize);
PyAPI_FUNC(void *) PyMem_RawRealloc(void *ptr, size_t new_size);
PyAPI_FUNC(void) PyMem_RawFree(void *ptr);
#endif

PyAPI_FUNC(void *) PyMem_Malloc(size_t size);
PyAPI_FUNC(void *) PyMem_Calloc(size_t nelem, size_t elsize);
PyAPI_FUNC(void *) PyMem_Realloc(void *ptr, size_t new_size);
PyAPI_FUNC(void) PyMem_Free(void *ptr);

#define PyMem_MALLOC(n)         PyMem_Malloc(n)
#define PyMem_REALLOC(p, n)     PyMem_Realloc(p, n)
#define PyMem_FREE(p)           PyMem_Free(p)


/*
 * Type-oriented memory interface
 * ==============================
 *
 * Allocate memory for n objects of the given type.  Returns a new pointer
 * or NULL if the request was too large or memory allocation failed.  Use
 * these macros rather than doing the multiplication yourself so that proper
 * overflow checking is always done.
 */

#define PyMem_New(type, n) \
  ( ((n) > PY_SSIZE_T_MAX / sizeof(type)) ? NULL : \
        ( (type *) PyMem_Malloc((n) * sizeof(type)) ) )
#define PyMem_NEW(type, n) \
  ( ((n) > PY_SSIZE_T_MAX / sizeof(type)) ? NULL : \
        ( (type *) PyMem_MALLOC((n) * sizeof(type)) ) )

/*
 * The value of (p) is always clobbered by this macro regardless of success.
 * The caller MUST check if (p) is NULL afterwards and deal with the memory
 * error if so.  This means the original value of (p) MUST be saved for the
 * caller's memory error handler to not lose track of it.
 */
#define PyMem_Resize(p, type, n) \
  ( (p) = ((n) > PY_SSIZE_T_MAX / sizeof(type)) ? NULL : \
        (type *) PyMem_Realloc((p), (n) * sizeof(type)) )
#define PyMem_RESIZE(p, type, n) \
  ( (p) = ((n) > PY_SSIZE_T_MAX / sizeof(type)) ? NULL : \
        (type *) PyMem_REALLOC((p), (n) * sizeof(type)) )

/* PyMem{Del,DEL} are left over from ancient days, and shouldn't be used
 * anymore.  They're just confusing aliases for PyMem_{Free,FREE} now.
 */
#define PyMem_Del               PyMem_Free
#define PyMem_DEL               PyMem_FREE


/* From CPython 3.6, with a different goal.  PyTraceMalloc_Track()
 * is equivalent to __pypy__.add_memory_pressure(size); it works with
 * or without the GIL.  PyTraceMalloc_Untrack() is an empty stub.
 * From CPython 3.7 these are public API functions
 *
 *    #if defined(PYPY_TRACEMALLOC) || \
 *         (PY_VERSION_HEX >= 0x03060000 && !defined(Py_LIMITED_API))
 */
#define PYPY_TRACEMALLOC        1

PyAPI_FUNC(int) PyTraceMalloc_Track(
    unsigned int domain,
    uintptr_t ptr,
    size_t size);

PyAPI_FUNC(int) PyTraceMalloc_Untrack(
    unsigned int domain,
    uintptr_t ptr);


#ifdef __cplusplus
}
#endif

#endif /* !Py_PYMEM_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pyport.h
================================================
#ifndef Py_PYPORT_H
#define Py_PYPORT_H

#include <pyconfig.h> /* include for defines */

#include <inttypes.h>

/**************************************************************************
Symbols and macros to supply platform-independent interfaces to basic
C language & library operations whose spellings vary across platforms.

Please try to make documentation here as clear as possible:  by definition,
the stuff here is trying to illuminate C's darkest corners.

Config #defines referenced here:

SIGNED_RIGHT_SHIFT_ZERO_FILLS
Meaning:  To be defined iff i>>j does not extend the sign bit when i is a
          signed integral type and i < 0.
Used in:  Py_ARITHMETIC_RIGHT_SHIFT

Py_DEBUG
Meaning:  Extra checks compiled in for debug mode.
Used in:  Py_SAFE_DOWNCAST

**************************************************************************/

/* typedefs for some C9X-defined synonyms for integral types.
 *
 * The names in Python are exactly the same as the C9X names, except with a
 * Py_ prefix.  Until C9X is universally implemented, this is the only way
 * to ensure that Python gets reliable names that don't conflict with names
 * in non-Python code that are playing their own tricks to define the C9X
 * names.
 *
 * NOTE: don't go nuts here!  Python has no use for *most* of the C9X
 * integral synonyms.  Only define the ones we actually need.
 */

/* typedefs for some C9X-defined synonyms for integral types. */
#ifndef PY_LONG_LONG
#define PY_LONG_LONG long long
/* If LLONG_MAX is defined in limits.h, use that. */
#define PY_LLONG_MIN LLONG_MIN
#define PY_LLONG_MAX LLONG_MAX
#define PY_ULLONG_MAX ULLONG_MAX
#endif

#define PY_UINT32_T uint32_t
#define PY_UINT64_T uint64_t

/* Signed variants of the above */
#define PY_INT32_T int32_t
#define PY_INT64_T int64_t


/* uintptr_t is the C9X name for an unsigned integral type such that a
 * legitimate void* can be cast to uintptr_t and then back to void* again
 * without loss of information.  Similarly for intptr_t, wrt a signed
 * integral type.
 */
typedef uintptr_t       Py_uintptr_t;
typedef intptr_t        Py_intptr_t;

/* CPython does this differently */
#ifdef _WIN64
typedef long long Py_ssize_t;
typedef long long Py_hash_t;
typedef unsigned long long Py_uhash_t;
#else
typedef long Py_ssize_t;
typedef long Py_hash_t;
typedef unsigned long Py_uhash_t;
#endif


/* Py_hash_t is the same size as a pointer. */
#define SIZEOF_PY_HASH_T SIZEOF_SIZE_T
/* Py_uhash_t is the unsigned equivalent needed to calculate numeric hash. */
#define SIZEOF_PY_UHASH_T SIZEOF_SIZE_T

/* Largest possible value of size_t. */
#define PY_SIZE_MAX SIZE_MAX

/* Largest positive value of type Py_ssize_t. */
#define PY_SSIZE_T_MAX ((Py_ssize_t)(((size_t)-1)>>1))
/* Smallest negative value of type Py_ssize_t. */
#define PY_SSIZE_T_MIN (-PY_SSIZE_T_MAX-1)

#include <stdarg.h>

/* Py_LOCAL can be used instead of static to get the fastest possible calling
 * convention for functions that are local to a given module.
 *
 * Py_LOCAL_INLINE does the same thing, and also explicitly requests inlining,
 * for platforms that support that.
 *
 * If PY_LOCAL_AGGRESSIVE is defined before python.h is included, more
 * "aggressive" inlining/optimization is enabled for the entire module.  This
 * may lead to code bloat, and may slow things down for those reasons.  It may
 * also lead to errors, if the code relies on pointer aliasing.  Use with
 * care.
 *
 * NOTE: You can only use this for functions that are entirely local to a
 * module; functions that are exported via method tables, callbacks, etc,
 * should keep using static.
 */

#if defined(_MSC_VER)
#  if defined(PY_LOCAL_AGGRESSIVE)
   /* enable more aggressive optimization for MSVC */
   /* active in both release and debug builds - see bpo-43271 */
#  pragma optimize("gt", on)
#endif
   /* ignore warnings if the compiler decides not to inline a function */
#  pragma warning(disable: 4710)
   /* fastest possible local call under MSVC */
#  define Py_LOCAL(type) static type __fastcall
#  define Py_LOCAL_INLINE(type) static __inline type __fastcall
#else
#  define Py_LOCAL(type) static type
#  define Py_LOCAL_INLINE(type) static inline type
#endif



/* CPython needs this for the c-extension datetime, which is pure python on PyPy
   downstream packages assume it is here (Pandas for instance) */
#include <time.h>

/*******************************
 * stat() and fstat() fiddling *
 *******************************/

/* We expect that stat and fstat exist on most systems.
 *  It's confirmed on Unix, Mac and Windows.
 *  If you don't have them, add
 *      #define DONT_HAVE_STAT
 * and/or
 *      #define DONT_HAVE_FSTAT
 * to your pyconfig.h. Python code beyond this should check HAVE_STAT and
 * HAVE_FSTAT instead.
 * Also
 *      #define HAVE_SYS_STAT_H
 * if <sys/stat.h> exists on your platform, and
 *      #define HAVE_STAT_H
 * if <stat.h> does.
 */
#ifndef DONT_HAVE_STAT
#define HAVE_STAT
#endif

#ifndef DONT_HAVE_FSTAT
#define HAVE_FSTAT
#endif

#ifdef RISCOS
#include <sys/types.h>
#include "unixstuff.h"
#endif

#ifdef HAVE_SYS_STAT_H
#if defined(PYOS_OS2) && defined(PYCC_GCC)
#include <sys/types.h>
#endif
#include <sys/stat.h>
#elif defined(HAVE_STAT_H)
#include <stat.h>
#else
#endif

/* Py_DEPRECATED(version)
 * Declare a variable, type, or function deprecated.
 * The macro must be placed before the declaration.
 * Usage:
 *    Py_DEPRECATED(3.3) extern int old_var;
 *    Py_DEPRECATED(3.4) typedef int T1;
 *    Py_DEPRECATED(3.8) PyAPI_FUNC(int) Py_OldFunction(void);
 */
#if defined(__GNUC__) \
    && ((__GNUC__ >= 4) || (__GNUC__ == 3) && (__GNUC_MINOR__ >= 1))
#define Py_DEPRECATED(VERSION_UNUSED) __attribute__((__deprecated__))
#elif defined(_MSC_VER)
#define Py_DEPRECATED(VERSION) __declspec(deprecated( \
                                          "deprecated in " #VERSION))
#else
#define Py_DEPRECATED(VERSION_UNUSED)
#endif

/* Declarations for symbol visibility.

  PyAPI_FUNC(type): Declares a public Python API function and return type
  PyAPI_DATA(type): Declares public Python data and its type
  PyMODINIT_FUNC:   A Python module init function.  If these functions are
                    inside the Python core, they are private to the core.
                    If in an extension module, it may be declared with
                    external linkage depending on the platform.

  As a number of platforms support/require "__declspec(dllimport/dllexport)",
  we support a HAVE_DECLSPEC_DLL macro to save duplication.
*/

/*
  All windows ports, except cygwin, are handled in PC/pyconfig.h.

  Cygwin is the only other autoconf platform requiring special
  linkage handling and it uses __declspec().
*/
#if defined(__CYGWIN__)
#       define HAVE_DECLSPEC_DLL
#endif

#include "exports.h"

/* only get special linkage if built as shared or platform is Cygwin */
#if defined(Py_ENABLE_SHARED) || defined(__CYGWIN__)
#       if defined(HAVE_DECLSPEC_DLL)
#               if defined(Py_BUILD_CORE) && !defined(Py_BUILD_CORE_MODULE)
#                       define PyAPI_FUNC(RTYPE) Py_EXPORTED_SYMBOL RTYPE
#                       define PyAPI_DATA(RTYPE) extern Py_EXPORTED_SYMBOL RTYPE
        /* module init functions inside the core need no external linkage */
        /* except for Cygwin to handle embedding */
#                       if defined(__CYGWIN__)
#                               define PyMODINIT_FUNC Py_EXPORTED_SYMBOL PyObject*
#                       else /* __CYGWIN__ */
#                               define PyMODINIT_FUNC PyObject*
#                       endif /* __CYGWIN__ */
#               else /* Py_BUILD_CORE */
        /* Building an extension module, or an embedded situation */
        /* public Python functions and data are imported */
        /* Under Cygwin, auto-import functions to prevent compilation */
        /* failures similar to those described at the bottom of 4.1: */
        /* http://docs.python.org/extending/windows.html#a-cookbook-approach */
#                       if !defined(__CYGWIN__)
#                               define PyAPI_FUNC(RTYPE) Py_IMPORTED_SYMBOL RTYPE
#                       endif /* !__CYGWIN__ */
#                       define PyAPI_DATA(RTYPE) extern Py_IMPORTED_SYMBOL RTYPE
        /* module init functions outside the core must be exported */
#                       if defined(__cplusplus)
#                               define PyMODINIT_FUNC extern "C" Py_EXPORTED_SYMBOL PyObject*
#                       else /* __cplusplus */
#                               define PyMODINIT_FUNC Py_EXPORTED_SYMBOL PyObject*
#                       endif /* __cplusplus */
#               endif /* Py_BUILD_CORE */
#       endif /* HAVE_DECLSPEC_DLL */
#endif /* Py_ENABLE_SHARED */

/* If no external linkage macros defined by now, create defaults */
#ifndef PyAPI_FUNC
#       define PyAPI_FUNC(RTYPE) Py_EXPORTED_SYMBOL RTYPE
#endif
#ifndef PyAPI_DATA
#       define PyAPI_DATA(RTYPE) extern Py_EXPORTED_SYMBOL RTYPE
#endif
#ifndef PyMODINIT_FUNC
#       if defined(__cplusplus)
#               define PyMODINIT_FUNC extern "C" Py_EXPORTED_SYMBOL PyObject*
#       else /* __cplusplus */
#               define PyMODINIT_FUNC Py_EXPORTED_SYMBOL PyObject*
#       endif /* __cplusplus */
#endif


/*
 * Hide GCC attributes from compilers that don't support them.
 */
#if (!defined(__GNUC__) || __GNUC__ < 2 || \
     (__GNUC__ == 2 && __GNUC_MINOR__ < 7) )
#define Py_GCC_ATTRIBUTE(x)
#else
#define Py_GCC_ATTRIBUTE(x) __attribute__(x)
#endif

/*
 * Specify alignment on compilers that support it.
 */
#if defined(__GNUC__) && __GNUC__ >= 3
#define Py_ALIGNED(x) __attribute__((aligned(x)))
#else
#define Py_ALIGNED(x)
#endif

/* Eliminate end-of-loop code not reached warnings from SunPro C
 * when using do{...}while(0) macros
 */
#ifdef __SUNPRO_C
#pragma error_messages (off,E_END_OF_LOOP_CODE_NOT_REACHED)
#endif

#ifndef Py_LL
#define Py_LL(x) x##LL
#endif

#ifndef Py_ULL
#define Py_ULL(x) Py_LL(x##U)
#endif

#define Py_VA_COPY va_copy

/* Mark a function which cannot return. Example:
   PyAPI_FUNC(void) _Py_NO_RETURN PyThread_exit_thread(void);

   XLC support is intentionally omitted due to bpo-40244 */
#if defined(__clang__) || \
    (defined(__GNUC__) && \
     ((__GNUC__ >= 3) || \
      (__GNUC__ == 2) && (__GNUC_MINOR__ >= 5)))
#  define _Py_NO_RETURN __attribute__((__noreturn__))
#elif defined(_MSC_VER)
#  define _Py_NO_RETURN __declspec(noreturn)
#else
#  define _Py_NO_RETURN
#endif


// Preprocessor check for a builtin preprocessor function. Always return 0
// if __has_builtin() macro is not defined.
//
// __has_builtin() is available on clang and GCC 10.
#ifdef __has_builtin
#  define _Py__has_builtin(x) __has_builtin(x)
#else
#  define _Py__has_builtin(x) 0
#endif


#endif /* Py_PYPORT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pypy_decl.h
================================================
#include "cpyext_object.h"

#ifdef _WIN64
#define Signed   Py_ssize_t          /* xxx temporary fix */
#define Unsigned unsigned long long  /* xxx temporary fix */
#else
#define Signed   Py_ssize_t     /* xxx temporary fix */
#define Unsigned unsigned long  /* xxx temporary fix */
#endif
        
typedef struct { PyObject_HEAD } PyMethodObject;
typedef struct { PyObject_HEAD } PyListObject;
typedef struct { PyObject_HEAD } PyLongObject;
typedef struct { PyObject_HEAD } PyBaseExceptionObject;

/* hack for https://bugs.python.org/issue29943 */

PyAPI_FUNC(int) PyPySlice_GetIndicesEx(PyObject *arg0,
                    Signed arg1, Signed *arg2,
                    Signed *arg3, Signed *arg4, Signed *arg5);
#ifdef __GNUC__
__attribute__((__unused__))
#endif
static int PySlice_GetIndicesEx(PyObject *arg0, Py_ssize_t arg1,
        Py_ssize_t *arg2, Py_ssize_t *arg3, Py_ssize_t *arg4,
        Py_ssize_t *arg5) {
    return PyPySlice_GetIndicesEx(arg0, arg1, arg2, arg3,
                arg4, arg5);
}
#define PyAnySet_Check PyPyAnySet_Check
PyAPI_FUNC(int) PyAnySet_Check(struct _object *arg0);
#define PyAnySet_CheckExact PyPyAnySet_CheckExact
PyAPI_FUNC(int) PyAnySet_CheckExact(struct _object *arg0);
#define PyBool_FromLong PyPyBool_FromLong
PyAPI_FUNC(struct _object *) PyBool_FromLong(Signed arg0);
#define PyBuffer_FillInfo PyPyBuffer_FillInfo
PyAPI_FUNC(int) PyBuffer_FillInfo(struct bufferinfo *arg0, struct _object *arg1, void *arg2, Signed arg3, int arg4, int arg5);
#define PyBuffer_IsContiguous PyPyBuffer_IsContiguous
PyAPI_FUNC(int) PyBuffer_IsContiguous(struct bufferinfo *arg0, char arg1);
#define PyByteArray_AsString PyPyByteArray_AsString
PyAPI_FUNC(char *) PyByteArray_AsString(struct _object *arg0);
#define PyByteArray_Check PyPyByteArray_Check
PyAPI_FUNC(int) PyByteArray_Check(void * arg0);
#define PyByteArray_CheckExact PyPyByteArray_CheckExact
PyAPI_FUNC(int) PyByteArray_CheckExact(void * arg0);
#define PyByteArray_Concat PyPyByteArray_Concat
PyAPI_FUNC(struct _object *) PyByteArray_Concat(struct _object *arg0, struct _object *arg1);
#define PyByteArray_FromObject PyPyByteArray_FromObject
PyAPI_FUNC(struct _object *) PyByteArray_FromObject(struct _object *arg0);
#define PyByteArray_FromStringAndSize PyPyByteArray_FromStringAndSize
PyAPI_FUNC(struct _object *) PyByteArray_FromStringAndSize(const char *arg0, Signed arg1);
#define PyByteArray_Resize PyPyByteArray_Resize
PyAPI_FUNC(int) PyByteArray_Resize(struct _object *arg0, Signed arg1);
#define PyByteArray_Size PyPyByteArray_Size
PyAPI_FUNC(Signed) PyByteArray_Size(struct _object *arg0);
#define PyBytes_AS_STRING PyPyBytes_AS_STRING
PyAPI_FUNC(char *) PyBytes_AS_STRING(void *arg0);
#define PyBytes_AsString PyPyBytes_AsString
PyAPI_FUNC(char *) PyBytes_AsString(struct _object *arg0);
#define PyBytes_AsStringAndSize PyPyBytes_AsStringAndSize
PyAPI_FUNC(int) PyBytes_AsStringAndSize(struct _object *arg0, char **arg1, Signed *arg2);
#define PyBytes_Concat PyPyBytes_Concat
PyAPI_FUNC(void) PyBytes_Concat(struct _object **arg0, struct _object *arg1);
#define PyBytes_ConcatAndDel PyPyBytes_ConcatAndDel
PyAPI_FUNC(void) PyBytes_ConcatAndDel(struct _object **arg0, struct _object *arg1);
#define PyBytes_FromObject PyPyBytes_FromObject
PyAPI_FUNC(struct _object *) PyBytes_FromObject(struct _object *arg0);
#define PyBytes_FromString PyPyBytes_FromString
PyAPI_FUNC(struct _object *) PyBytes_FromString(const char *arg0);
#define PyBytes_FromStringAndSize PyPyBytes_FromStringAndSize
PyAPI_FUNC(struct _object *) PyBytes_FromStringAndSize(const char *arg0, Signed arg1);
#define PyBytes_Size PyPyBytes_Size
PyAPI_FUNC(Signed) PyBytes_Size(struct _object *arg0);
#define PyCFunction_Call PyPyCFunction_Call
PyAPI_FUNC(struct _object *) PyCFunction_Call(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyCFunction_Check PyPyCFunction_Check
PyAPI_FUNC(int) PyCFunction_Check(struct _object *arg0);
#define PyCFunction_GetFunction PyPyCFunction_GetFunction
PyAPI_FUNC(PyCFunction) PyCFunction_GetFunction(PyObject * arg0);
#define PyCFunction_NewEx PyPyCFunction_NewEx
PyAPI_FUNC(struct _object *) PyCFunction_NewEx(struct PyMethodDef *arg0, struct _object *arg1, struct _object *arg2);
#define PyCallIter_New PyPyCallIter_New
PyAPI_FUNC(struct _object *) PyCallIter_New(struct _object *arg0, struct _object *arg1);
#define PyCallable_Check PyPyCallable_Check
PyAPI_FUNC(int) PyCallable_Check(struct _object *arg0);
#define PyClassMethod_New PyPyClassMethod_New
PyAPI_FUNC(struct _object *) PyClassMethod_New(struct _object *arg0);
#define PyCode_Addr2Line PyPyCode_Addr2Line
PyAPI_FUNC(int) PyCode_Addr2Line(PyCodeObject *arg0, int arg1);
#define PyCode_Check PyPyCode_Check
PyAPI_FUNC(int) PyCode_Check(void * arg0);
#define PyCode_CheckExact PyPyCode_CheckExact
PyAPI_FUNC(int) PyCode_CheckExact(void * arg0);
#define PyCode_GetNumFree PyPyCode_GetNumFree
PyAPI_FUNC(Signed) PyCode_GetNumFree(PyCodeObject *arg0);
#define PyCode_New PyPyCode_New
PyAPI_FUNC(PyCodeObject *) PyCode_New(int arg0, int arg1, int arg2, int arg3, int arg4, struct _object *arg5, struct _object *arg6, struct _object *arg7, struct _object *arg8, struct _object *arg9, struct _object *arg10, struct _object *arg11, struct _object *arg12, int arg13, struct _object *arg14);
#define PyCode_NewEmpty PyPyCode_NewEmpty
PyAPI_FUNC(PyCodeObject *) PyCode_NewEmpty(const char *arg0, const char *arg1, int arg2);
#define PyCodec_Decode PyPyCodec_Decode
PyAPI_FUNC(struct _object *) PyCodec_Decode(struct _object *arg0, const char *arg1, const char *arg2);
#define PyCodec_Decoder PyPyCodec_Decoder
PyAPI_FUNC(struct _object *) PyCodec_Decoder(const char *arg0);
#define PyCodec_Encode PyPyCodec_Encode
PyAPI_FUNC(struct _object *) PyCodec_Encode(struct _object *arg0, const char *arg1, const char *arg2);
#define PyCodec_Encoder PyPyCodec_Encoder
PyAPI_FUNC(struct _object *) PyCodec_Encoder(const char *arg0);
#define PyCodec_IncrementalDecoder PyPyCodec_IncrementalDecoder
PyAPI_FUNC(struct _object *) PyCodec_IncrementalDecoder(const char *arg0, const char *arg1);
#define PyCodec_IncrementalEncoder PyPyCodec_IncrementalEncoder
PyAPI_FUNC(struct _object *) PyCodec_IncrementalEncoder(const char *arg0, const char *arg1);
#define PyComplex_Check PyPyComplex_Check
PyAPI_FUNC(int) PyComplex_Check(void * arg0);
#define PyComplex_CheckExact PyPyComplex_CheckExact
PyAPI_FUNC(int) PyComplex_CheckExact(void * arg0);
#define PyComplex_FromDoubles PyPyComplex_FromDoubles
PyAPI_FUNC(struct _object *) PyComplex_FromDoubles(double arg0, double arg1);
#define PyComplex_ImagAsDouble PyPyComplex_ImagAsDouble
PyAPI_FUNC(double) PyComplex_ImagAsDouble(struct _object *arg0);
#define PyComplex_RealAsDouble PyPyComplex_RealAsDouble
PyAPI_FUNC(double) PyComplex_RealAsDouble(struct _object *arg0);
#define PyContextVar_Get PyPyContextVar_Get
PyAPI_FUNC(int) PyContextVar_Get(struct _object *arg0, struct _object *arg1, struct _object **arg2);
#define PyContextVar_New PyPyContextVar_New
PyAPI_FUNC(struct _object *) PyContextVar_New(const char *arg0, struct _object *arg1);
#define PyContextVar_Set PyPyContextVar_Set
PyAPI_FUNC(struct _object *) PyContextVar_Set(struct _object *arg0, struct _object *arg1);
#define PyCoro_Check PyPyCoro_Check
PyAPI_FUNC(int) PyCoro_Check(void * arg0);
#define PyCoro_CheckExact PyPyCoro_CheckExact
PyAPI_FUNC(int) PyCoro_CheckExact(void * arg0);
#define PyDateTime_Check PyPyDateTime_Check
PyAPI_FUNC(int) PyDateTime_Check(struct _object *arg0);
#define PyDateTime_CheckExact PyPyDateTime_CheckExact
PyAPI_FUNC(int) PyDateTime_CheckExact(struct _object *arg0);
#define PyDateTime_DATE_GET_HOUR PyPyDateTime_DATE_GET_HOUR
PyAPI_FUNC(int) PyDateTime_DATE_GET_HOUR(void *arg0);
#define PyDateTime_DATE_GET_MICROSECOND PyPyDateTime_DATE_GET_MICROSECOND
PyAPI_FUNC(int) PyDateTime_DATE_GET_MICROSECOND(void *arg0);
#define PyDateTime_DATE_GET_MINUTE PyPyDateTime_DATE_GET_MINUTE
PyAPI_FUNC(int) PyDateTime_DATE_GET_MINUTE(void *arg0);
#define PyDateTime_DATE_GET_SECOND PyPyDateTime_DATE_GET_SECOND
PyAPI_FUNC(int) PyDateTime_DATE_GET_SECOND(void *arg0);
#define PyDateTime_DELTA_GET_DAYS PyPyDateTime_DELTA_GET_DAYS
PyAPI_FUNC(int) PyDateTime_DELTA_GET_DAYS(void *arg0);
#define PyDateTime_DELTA_GET_MICROSECONDS PyPyDateTime_DELTA_GET_MICROSECONDS
PyAPI_FUNC(int) PyDateTime_DELTA_GET_MICROSECONDS(void *arg0);
#define PyDateTime_DELTA_GET_SECONDS PyPyDateTime_DELTA_GET_SECONDS
PyAPI_FUNC(int) PyDateTime_DELTA_GET_SECONDS(void *arg0);
#define PyDateTime_FromTimestamp PyPyDateTime_FromTimestamp
PyAPI_FUNC(struct _object *) PyDateTime_FromTimestamp(struct _object *arg0);
#define PyDateTime_GET_DAY PyPyDateTime_GET_DAY
PyAPI_FUNC(int) PyDateTime_GET_DAY(void *arg0);
#define PyDateTime_GET_FOLD PyPyDateTime_GET_FOLD
PyAPI_FUNC(int) PyDateTime_GET_FOLD(void *arg0);
#define PyDateTime_GET_MONTH PyPyDateTime_GET_MONTH
PyAPI_FUNC(int) PyDateTime_GET_MONTH(void *arg0);
#define PyDateTime_GET_YEAR PyPyDateTime_GET_YEAR
PyAPI_FUNC(int) PyDateTime_GET_YEAR(void *arg0);
#define PyDateTime_TIME_GET_FOLD PyPyDateTime_TIME_GET_FOLD
PyAPI_FUNC(int) PyDateTime_TIME_GET_FOLD(void *arg0);
#define PyDateTime_TIME_GET_HOUR PyPyDateTime_TIME_GET_HOUR
PyAPI_FUNC(int) PyDateTime_TIME_GET_HOUR(void *arg0);
#define PyDateTime_TIME_GET_MICROSECOND PyPyDateTime_TIME_GET_MICROSECOND
PyAPI_FUNC(int) PyDateTime_TIME_GET_MICROSECOND(void *arg0);
#define PyDateTime_TIME_GET_MINUTE PyPyDateTime_TIME_GET_MINUTE
PyAPI_FUNC(int) PyDateTime_TIME_GET_MINUTE(void *arg0);
#define PyDateTime_TIME_GET_SECOND PyPyDateTime_TIME_GET_SECOND
PyAPI_FUNC(int) PyDateTime_TIME_GET_SECOND(void *arg0);
#define PyDate_Check PyPyDate_Check
PyAPI_FUNC(int) PyDate_Check(struct _object *arg0);
#define PyDate_CheckExact PyPyDate_CheckExact
PyAPI_FUNC(int) PyDate_CheckExact(struct _object *arg0);
#define PyDate_FromTimestamp PyPyDate_FromTimestamp
PyAPI_FUNC(struct _object *) PyDate_FromTimestamp(struct _object *arg0);
#define PyDelta_Check PyPyDelta_Check
PyAPI_FUNC(int) PyDelta_Check(struct _object *arg0);
#define PyDelta_CheckExact PyPyDelta_CheckExact
PyAPI_FUNC(int) PyDelta_CheckExact(struct _object *arg0);
#define PyDescr_NewClassMethod PyPyDescr_NewClassMethod
PyAPI_FUNC(PyObject *) PyDescr_NewClassMethod(PyTypeObject * arg0, PyMethodDef * arg1);
#define PyDescr_NewGetSet PyPyDescr_NewGetSet
PyAPI_FUNC(struct _object *) PyDescr_NewGetSet(struct _typeobject *arg0, struct PyGetSetDef *arg1);
#define PyDescr_NewMethod PyPyDescr_NewMethod
PyAPI_FUNC(PyObject *) PyDescr_NewMethod(PyTypeObject * arg0, PyMethodDef * arg1);
#define PyDictProxy_Check PyPyDictProxy_Check
PyAPI_FUNC(int) PyDictProxy_Check(void * arg0);
#define PyDictProxy_CheckExact PyPyDictProxy_CheckExact
PyAPI_FUNC(int) PyDictProxy_CheckExact(void * arg0);
#define PyDictProxy_New PyPyDictProxy_New
PyAPI_FUNC(struct _object *) PyDictProxy_New(struct _object *arg0);
#define PyDict_Clear PyPyDict_Clear
PyAPI_FUNC(void) PyDict_Clear(struct _object *arg0);
#define PyDict_Contains PyPyDict_Contains
PyAPI_FUNC(int) PyDict_Contains(struct _object *arg0, struct _object *arg1);
#define PyDict_Copy PyPyDict_Copy
PyAPI_FUNC(struct _object *) PyDict_Copy(struct _object *arg0);
#define PyDict_DelItem PyPyDict_DelItem
PyAPI_FUNC(int) PyDict_DelItem(struct _object *arg0, struct _object *arg1);
#define PyDict_DelItemString PyPyDict_DelItemString
PyAPI_FUNC(int) PyDict_DelItemString(struct _object *arg0, const char *arg1);
#define PyDict_GetItem PyPyDict_GetItem
PyAPI_FUNC(struct _object *) PyDict_GetItem(struct _object *arg0, struct _object *arg1);
#define PyDict_GetItemString PyPyDict_GetItemString
PyAPI_FUNC(struct _object *) PyDict_GetItemString(struct _object *arg0, const char *arg1);
#define PyDict_GetItemWithError PyPyDict_GetItemWithError
PyAPI_FUNC(struct _object *) PyDict_GetItemWithError(struct _object *arg0, struct _object *arg1);
#define PyDict_Items PyPyDict_Items
PyAPI_FUNC(struct _object *) PyDict_Items(struct _object *arg0);
#define PyDict_Keys PyPyDict_Keys
PyAPI_FUNC(struct _object *) PyDict_Keys(struct _object *arg0);
#define PyDict_Merge PyPyDict_Merge
PyAPI_FUNC(int) PyDict_Merge(struct _object *arg0, struct _object *arg1, int arg2);
#define PyDict_New PyPyDict_New
PyAPI_FUNC(struct _object *) PyDict_New(void);
#define PyDict_Next PyPyDict_Next
PyAPI_FUNC(int) PyDict_Next(struct _object *arg0, Signed *arg1, struct _object **arg2, struct _object **arg3);
#define PyDict_SetDefault PyPyDict_SetDefault
PyAPI_FUNC(PyObject *) PyDict_SetDefault(PyObject * arg0, PyObject * arg1, PyObject * arg2);
#define PyDict_SetItem PyPyDict_SetItem
PyAPI_FUNC(int) PyDict_SetItem(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyDict_SetItemString PyPyDict_SetItemString
PyAPI_FUNC(int) PyDict_SetItemString(struct _object *arg0, const char *arg1, struct _object *arg2);
#define PyDict_Size PyPyDict_Size
PyAPI_FUNC(Signed) PyDict_Size(struct _object *arg0);
#define PyDict_Update PyPyDict_Update
PyAPI_FUNC(int) PyDict_Update(struct _object *arg0, struct _object *arg1);
#define PyDict_Values PyPyDict_Values
PyAPI_FUNC(struct _object *) PyDict_Values(struct _object *arg0);
#define PyErr_BadArgument PyPyErr_BadArgument
PyAPI_FUNC(int) PyErr_BadArgument(void);
#define PyErr_BadInternalCall PyPyErr_BadInternalCall
PyAPI_FUNC(void) PyErr_BadInternalCall(void);
#define PyErr_CheckSignals PyPyErr_CheckSignals
PyAPI_FUNC(int) PyErr_CheckSignals(void);
#define PyErr_Clear PyPyErr_Clear
PyAPI_FUNC(void) PyErr_Clear(void);
#define PyErr_Display PyPyErr_Display
PyAPI_FUNC(void) PyErr_Display(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyErr_ExceptionMatches PyPyErr_ExceptionMatches
PyAPI_FUNC(int) PyErr_ExceptionMatches(struct _object *arg0);
#define PyErr_Fetch PyPyErr_Fetch
PyAPI_FUNC(void) PyErr_Fetch(struct _object **arg0, struct _object **arg1, struct _object **arg2);
#define PyErr_GetExcInfo PyPyErr_GetExcInfo
PyAPI_FUNC(void) PyErr_GetExcInfo(struct _object **arg0, struct _object **arg1, struct _object **arg2);
#define PyErr_GivenExceptionMatches PyPyErr_GivenExceptionMatches
PyAPI_FUNC(int) PyErr_GivenExceptionMatches(struct _object *arg0, struct _object *arg1);
#define PyErr_NoMemory PyPyErr_NoMemory
PyAPI_FUNC(struct _object *) PyErr_NoMemory(void);
#define PyErr_NormalizeException PyPyErr_NormalizeException
PyAPI_FUNC(void) PyErr_NormalizeException(struct _object **arg0, struct _object **arg1, struct _object **arg2);
#define PyErr_Occurred PyPyErr_Occurred
PyAPI_FUNC(struct _object *) PyErr_Occurred(void);
#define PyErr_Print PyPyErr_Print
PyAPI_FUNC(void) PyErr_Print(void);
#define PyErr_PrintEx PyPyErr_PrintEx
PyAPI_FUNC(void) PyErr_PrintEx(int arg0);
#define PyErr_Restore PyPyErr_Restore
PyAPI_FUNC(void) PyErr_Restore(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyErr_SetExcInfo PyPyErr_SetExcInfo
PyAPI_FUNC(void) PyErr_SetExcInfo(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyErr_SetFromErrno PyPyErr_SetFromErrno
PyAPI_FUNC(struct _object *) PyErr_SetFromErrno(struct _object *arg0);
#define PyErr_SetFromErrnoWithFilename PyPyErr_SetFromErrnoWithFilename
PyAPI_FUNC(struct _object *) PyErr_SetFromErrnoWithFilename(struct _object *arg0, const char *arg1);
#define PyErr_SetFromErrnoWithFilenameObject PyPyErr_SetFromErrnoWithFilenameObject
PyAPI_FUNC(struct _object *) PyErr_SetFromErrnoWithFilenameObject(struct _object *arg0, struct _object *arg1);
#define PyErr_SetInterrupt PyPyErr_SetInterrupt
PyAPI_FUNC(void) PyErr_SetInterrupt(void);
#define PyErr_SetNone PyPyErr_SetNone
PyAPI_FUNC(void) PyErr_SetNone(struct _object *arg0);
#define PyErr_SetObject PyPyErr_SetObject
PyAPI_FUNC(void) PyErr_SetObject(struct _object *arg0, struct _object *arg1);
#define PyErr_SetString PyPyErr_SetString
PyAPI_FUNC(void) PyErr_SetString(struct _object *arg0, const char *arg1);
#define PyErr_Warn PyPyErr_Warn
PyAPI_FUNC(int) PyErr_Warn(struct _object *arg0, const char *arg1);
#define PyErr_WarnEx PyPyErr_WarnEx
PyAPI_FUNC(int) PyErr_WarnEx(struct _object *arg0, const char *arg1, int arg2);
#define PyErr_WarnExplicit PyPyErr_WarnExplicit
PyAPI_FUNC(int) PyErr_WarnExplicit(struct _object *arg0, const char *arg1, const char *arg2, int arg3, const char *arg4, struct _object *arg5);
#define PyErr_WriteUnraisable PyPyErr_WriteUnraisable
PyAPI_FUNC(void) PyErr_WriteUnraisable(struct _object *arg0);
#define PyEval_AcquireThread PyPyEval_AcquireThread
PyAPI_FUNC(void) PyEval_AcquireThread(PyThreadState *arg0);
#define PyEval_CallObjectWithKeywords PyPyEval_CallObjectWithKeywords
PyAPI_FUNC(struct _object *) PyEval_CallObjectWithKeywords(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyEval_EvalCode PyPyEval_EvalCode
PyAPI_FUNC(struct _object *) PyEval_EvalCode(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyEval_GetBuiltins PyPyEval_GetBuiltins
PyAPI_FUNC(struct _object *) PyEval_GetBuiltins(void);
#define PyEval_GetFrame PyPyEval_GetFrame
PyAPI_FUNC(PyFrameObject *) PyEval_GetFrame(void);
#define PyEval_GetGlobals PyPyEval_GetGlobals
PyAPI_FUNC(struct _object *) PyEval_GetGlobals(void);
#define PyEval_GetLocals PyPyEval_GetLocals
PyAPI_FUNC(struct _object *) PyEval_GetLocals(void);
#define PyEval_InitThreads PyPyEval_InitThreads
PyAPI_FUNC(void) PyEval_InitThreads(void);
#define PyEval_MergeCompilerFlags PyPyEval_MergeCompilerFlags
PyAPI_FUNC(int) PyEval_MergeCompilerFlags(PyCompilerFlags *arg0);
#define PyEval_ReleaseThread PyPyEval_ReleaseThread
PyAPI_FUNC(void) PyEval_ReleaseThread(PyThreadState *arg0);
#define PyEval_RestoreThread PyPyEval_RestoreThread
PyAPI_FUNC(void) PyEval_RestoreThread(PyThreadState *arg0);
#define PyEval_SaveThread PyPyEval_SaveThread
PyAPI_FUNC(PyThreadState *) PyEval_SaveThread(void);
#define PyEval_ThreadsInitialized PyPyEval_ThreadsInitialized
PyAPI_FUNC(int) PyEval_ThreadsInitialized(void);
#define PyExceptionInstance_Class PyPyExceptionInstance_Class
PyAPI_FUNC(struct _object *) PyExceptionInstance_Class(struct _object *arg0);
#define PyException_GetCause PyPyException_GetCause
PyAPI_FUNC(struct _object *) PyException_GetCause(struct _object *arg0);
#define PyException_GetContext PyPyException_GetContext
PyAPI_FUNC(struct _object *) PyException_GetContext(struct _object *arg0);
#define PyException_GetTraceback PyPyException_GetTraceback
PyAPI_FUNC(struct _object *) PyException_GetTraceback(struct _object *arg0);
#define PyException_SetCause PyPyException_SetCause
PyAPI_FUNC(void) PyException_SetCause(struct _object *arg0, struct _object *arg1);
#define PyException_SetContext PyPyException_SetContext
PyAPI_FUNC(void) PyException_SetContext(struct _object *arg0, struct _object *arg1);
#define PyException_SetTraceback PyPyException_SetTraceback
PyAPI_FUNC(int) PyException_SetTraceback(struct _object *arg0, struct _object *arg1);
#define PyFile_FromFd PyPyFile_FromFd
PyAPI_FUNC(struct _object *) PyFile_FromFd(int arg0, const char *arg1, const char *arg2, int arg3, const char *arg4, const char *arg5, const char *arg6, int arg7);
#define PyFile_FromString PyPyFile_FromString
PyAPI_FUNC(struct _object *) PyFile_FromString(const char *arg0, const char *arg1);
#define PyFile_GetLine PyPyFile_GetLine
PyAPI_FUNC(struct _object *) PyFile_GetLine(struct _object *arg0, int arg1);
#define PyFile_WriteObject PyPyFile_WriteObject
PyAPI_FUNC(int) PyFile_WriteObject(struct _object *arg0, struct _object *arg1, int arg2);
#define PyFile_WriteString PyPyFile_WriteString
PyAPI_FUNC(int) PyFile_WriteString(const char *arg0, struct _object *arg1);
#define PyFloat_AS_DOUBLE PyPyFloat_AS_DOUBLE
PyAPI_FUNC(double) PyFloat_AS_DOUBLE(void *arg0);
#define PyFloat_AsDouble PyPyFloat_AsDouble
PyAPI_FUNC(double) PyFloat_AsDouble(struct _object *arg0);
#define PyFloat_FromDouble PyPyFloat_FromDouble
PyAPI_FUNC(struct _object *) PyFloat_FromDouble(double arg0);
#define PyFloat_FromString PyPyFloat_FromString
PyAPI_FUNC(struct _object *) PyFloat_FromString(struct _object *arg0);
#define PyFrame_New PyPyFrame_New
PyAPI_FUNC(PyFrameObject *) PyFrame_New(PyThreadState *arg0, PyCodeObject *arg1, struct _object *arg2, struct _object *arg3);
#define PyFrozenSet_Check PyPyFrozenSet_Check
PyAPI_FUNC(int) PyFrozenSet_Check(void * arg0);
#define PyFrozenSet_CheckExact PyPyFrozenSet_CheckExact
PyAPI_FUNC(int) PyFrozenSet_CheckExact(void * arg0);
#define PyFrozenSet_New PyPyFrozenSet_New
PyAPI_FUNC(struct _object *) PyFrozenSet_New(struct _object *arg0);
#define PyFunction_Check PyPyFunction_Check
PyAPI_FUNC(int) PyFunction_Check(void * arg0);
#define PyFunction_CheckExact PyPyFunction_CheckExact
PyAPI_FUNC(int) PyFunction_CheckExact(void * arg0);
#define PyFunction_GetCode PyPyFunction_GetCode
PyAPI_FUNC(struct _object *) PyFunction_GetCode(struct _object *arg0);
#define PyGILState_Check PyPyGILState_Check
PyAPI_FUNC(int) PyGILState_Check(void);
#define PyGILState_Ensure PyPyGILState_Ensure
PyAPI_FUNC(int) PyGILState_Ensure(void);
#define PyGILState_Release PyPyGILState_Release
PyAPI_FUNC(void) PyGILState_Release(int arg0);
#define PyGen_Check PyPyGen_Check
PyAPI_FUNC(int) PyGen_Check(void * arg0);
#define PyGen_CheckExact PyPyGen_CheckExact
PyAPI_FUNC(int) PyGen_CheckExact(void * arg0);
#define PyImport_AddModule PyPyImport_AddModule
PyAPI_FUNC(struct _object *) PyImport_AddModule(const char *arg0);
#define PyImport_ExecCodeModule PyPyImport_ExecCodeModule
PyAPI_FUNC(struct _object *) PyImport_ExecCodeModule(const char *arg0, struct _object *arg1);
#define PyImport_ExecCodeModuleEx PyPyImport_ExecCodeModuleEx
PyAPI_FUNC(struct _object *) PyImport_ExecCodeModuleEx(const char *arg0, struct _object *arg1, const char *arg2);
#define PyImport_GetModule PyPyImport_GetModule
PyAPI_FUNC(struct _object *) PyImport_GetModule(struct _object *arg0);
#define PyImport_GetModuleDict PyPyImport_GetModuleDict
PyAPI_FUNC(struct _object *) PyImport_GetModuleDict(void);
#define PyImport_Import PyPyImport_Import
PyAPI_FUNC(struct _object *) PyImport_Import(struct _object *arg0);
#define PyImport_ImportModule PyPyImport_ImportModule
PyAPI_FUNC(struct _object *) PyImport_ImportModule(const char *arg0);
#define PyImport_ImportModuleLevelObject PyPyImport_ImportModuleLevelObject
PyAPI_FUNC(PyObject *) PyImport_ImportModuleLevelObject(PyObject * arg0, PyObject * arg1, PyObject * arg2, PyObject * arg3, int arg4);
#define PyImport_ImportModuleNoBlock PyPyImport_ImportModuleNoBlock
PyAPI_FUNC(struct _object *) PyImport_ImportModuleNoBlock(const char *arg0);
#define PyImport_ReloadModule PyPyImport_ReloadModule
PyAPI_FUNC(struct _object *) PyImport_ReloadModule(struct _object *arg0);
#define PyIndex_Check PyPyIndex_Check
PyAPI_FUNC(int) PyIndex_Check(struct _object *arg0);
#define PyInstanceMethod_Check PyPyInstanceMethod_Check
PyAPI_FUNC(int) PyInstanceMethod_Check(struct _object *arg0);
#define PyInstanceMethod_Function PyPyInstanceMethod_Function
PyAPI_FUNC(struct _object *) PyInstanceMethod_Function(struct _object *arg0);
#define PyInstanceMethod_GET_FUNCTION PyPyInstanceMethod_GET_FUNCTION
PyAPI_FUNC(struct _object *) PyInstanceMethod_GET_FUNCTION(struct _object *arg0);
#define PyInstanceMethod_New PyPyInstanceMethod_New
PyAPI_FUNC(struct _object *) PyInstanceMethod_New(struct _object *arg0);
#define PyInterpreterState_Head PyPyInterpreterState_Head
PyAPI_FUNC(PyInterpreterState *) PyInterpreterState_Head(void);
#define PyInterpreterState_Next PyPyInterpreterState_Next
PyAPI_FUNC(PyInterpreterState *) PyInterpreterState_Next(PyInterpreterState *arg0);
#define PyIter_Check PyPyIter_Check
PyAPI_FUNC(int) PyIter_Check(struct _object *arg0);
#define PyIter_Next PyPyIter_Next
PyAPI_FUNC(struct _object *) PyIter_Next(struct _object *arg0);
#define PyList_Append PyPyList_Append
PyAPI_FUNC(int) PyList_Append(struct _object *arg0, struct _object *arg1);
#define PyList_AsTuple PyPyList_AsTuple
PyAPI_FUNC(struct _object *) PyList_AsTuple(struct _object *arg0);
#define PyList_GET_ITEM PyPyList_GET_ITEM
PyAPI_FUNC(struct _object *) PyList_GET_ITEM(void *arg0, Signed arg1);
#define PyList_GET_SIZE PyPyList_GET_SIZE
PyAPI_FUNC(Signed) PyList_GET_SIZE(void *arg0);
#define PyList_GetItem PyPyList_GetItem
PyAPI_FUNC(struct _object *) PyList_GetItem(struct _object *arg0, Signed arg1);
#define PyList_GetSlice PyPyList_GetSlice
PyAPI_FUNC(struct _object *) PyList_GetSlice(struct _object *arg0, Signed arg1, Signed arg2);
#define PyList_Insert PyPyList_Insert
PyAPI_FUNC(int) PyList_Insert(struct _object *arg0, Signed arg1, struct _object *arg2);
#define PyList_New PyPyList_New
PyAPI_FUNC(struct _object *) PyList_New(Signed arg0);
#define PyList_Reverse PyPyList_Reverse
PyAPI_FUNC(int) PyList_Reverse(struct _object *arg0);
#define PyList_SET_ITEM PyPyList_SET_ITEM
PyAPI_FUNC(void) PyList_SET_ITEM(void *arg0, Signed arg1, struct _object *arg2);
#define PyList_SetItem PyPyList_SetItem
PyAPI_FUNC(int) PyList_SetItem(struct _object *arg0, Signed arg1, struct _object *arg2);
#define PyList_SetSlice PyPyList_SetSlice
PyAPI_FUNC(int) PyList_SetSlice(struct _object *arg0, Signed arg1, Signed arg2, struct _object *arg3);
#define PyList_Size PyPyList_Size
PyAPI_FUNC(Signed) PyList_Size(struct _object *arg0);
#define PyList_Sort PyPyList_Sort
PyAPI_FUNC(int) PyList_Sort(struct _object *arg0);
#define PyLong_AsDouble PyPyLong_AsDouble
PyAPI_FUNC(double) PyLong_AsDouble(struct _object *arg0);
#define PyLong_AsLong PyPyLong_AsLong
PyAPI_FUNC(Signed) PyLong_AsLong(struct _object *arg0);
#define PyLong_AsLongAndOverflow PyPyLong_AsLongAndOverflow
PyAPI_FUNC(Signed) PyLong_AsLongAndOverflow(struct _object *arg0, int *arg1);
#define PyLong_AsLongLong PyPyLong_AsLongLong
PyAPI_FUNC(Signed) PyLong_AsLongLong(struct _object *arg0);
#define PyLong_AsLongLongAndOverflow PyPyLong_AsLongLongAndOverflow
PyAPI_FUNC(Signed) PyLong_AsLongLongAndOverflow(struct _object *arg0, int *arg1);
#define PyLong_AsSize_t PyPyLong_AsSize_t
PyAPI_FUNC(Unsigned) PyLong_AsSize_t(struct _object *arg0);
#define PyLong_AsSsize_t PyPyLong_AsSsize_t
PyAPI_FUNC(Signed) PyLong_AsSsize_t(struct _object *arg0);
#define PyLong_AsUnsignedLong PyPyLong_AsUnsignedLong
PyAPI_FUNC(Unsigned) PyLong_AsUnsignedLong(struct _object *arg0);
#define PyLong_AsUnsignedLongLong PyPyLong_AsUnsignedLongLong
PyAPI_FUNC(Unsigned) PyLong_AsUnsignedLongLong(struct _object *arg0);
#define PyLong_AsUnsignedLongLongMask PyPyLong_AsUnsignedLongLongMask
PyAPI_FUNC(Unsigned) PyLong_AsUnsignedLongLongMask(struct _object *arg0);
#define PyLong_AsUnsignedLongMask PyPyLong_AsUnsignedLongMask
PyAPI_FUNC(Unsigned) PyLong_AsUnsignedLongMask(struct _object *arg0);
#define PyLong_AsVoidPtr PyPyLong_AsVoidPtr
PyAPI_FUNC(void *) PyLong_AsVoidPtr(struct _object *arg0);
#define PyLong_FromDouble PyPyLong_FromDouble
PyAPI_FUNC(struct _object *) PyLong_FromDouble(double arg0);
#define PyLong_FromLong PyPyLong_FromLong
PyAPI_FUNC(struct _object *) PyLong_FromLong(Signed arg0);
#define PyLong_FromLongLong PyPyLong_FromLongLong
PyAPI_FUNC(struct _object *) PyLong_FromLongLong(Signed arg0);
#define PyLong_FromSize_t PyPyLong_FromSize_t
PyAPI_FUNC(struct _object *) PyLong_FromSize_t(Unsigned arg0);
#define PyLong_FromSsize_t PyPyLong_FromSsize_t
PyAPI_FUNC(struct _object *) PyLong_FromSsize_t(Signed arg0);
#define PyLong_FromString PyPyLong_FromString
PyAPI_FUNC(struct _object *) PyLong_FromString(const char *arg0, char **arg1, int arg2);
#define PyLong_FromUnicode PyPyLong_FromUnicode
PyAPI_FUNC(struct _object *) PyLong_FromUnicode(wchar_t *arg0, Signed arg1, int arg2);
#define PyLong_FromUnicodeObject PyPyLong_FromUnicodeObject
PyAPI_FUNC(struct _object *) PyLong_FromUnicodeObject(struct _object *arg0, int arg1);
#define PyLong_FromUnsignedLong PyPyLong_FromUnsignedLong
PyAPI_FUNC(struct _object *) PyLong_FromUnsignedLong(Unsigned arg0);
#define PyLong_FromUnsignedLongLong PyPyLong_FromUnsignedLongLong
PyAPI_FUNC(struct _object *) PyLong_FromUnsignedLongLong(Unsigned arg0);
#define PyLong_FromVoidPtr PyPyLong_FromVoidPtr
PyAPI_FUNC(struct _object *) PyLong_FromVoidPtr(void *arg0);
#define PyMapping_Check PyPyMapping_Check
PyAPI_FUNC(int) PyMapping_Check(struct _object *arg0);
#define PyMapping_GetItemString PyPyMapping_GetItemString
PyAPI_FUNC(struct _object *) PyMapping_GetItemString(struct _object *arg0, const char *arg1);
#define PyMapping_HasKey PyPyMapping_HasKey
PyAPI_FUNC(int) PyMapping_HasKey(struct _object *arg0, struct _object *arg1);
#define PyMapping_HasKeyString PyPyMapping_HasKeyString
PyAPI_FUNC(int) PyMapping_HasKeyString(struct _object *arg0, const char *arg1);
#define PyMapping_Items PyPyMapping_Items
PyAPI_FUNC(struct _object *) PyMapping_Items(struct _object *arg0);
#define PyMapping_Keys PyPyMapping_Keys
PyAPI_FUNC(struct _object *) PyMapping_Keys(struct _object *arg0);
#define PyMapping_Length PyPyMapping_Length
PyAPI_FUNC(Signed) PyMapping_Length(struct _object *arg0);
#define PyMapping_SetItemString PyPyMapping_SetItemString
PyAPI_FUNC(int) PyMapping_SetItemString(struct _object *arg0, const char *arg1, struct _object *arg2);
#define PyMapping_Size PyPyMapping_Size
PyAPI_FUNC(Signed) PyMapping_Size(struct _object *arg0);
#define PyMapping_Values PyPyMapping_Values
PyAPI_FUNC(struct _object *) PyMapping_Values(struct _object *arg0);
#define PyMemoryView_Check PyPyMemoryView_Check
PyAPI_FUNC(int) PyMemoryView_Check(void * arg0);
#define PyMemoryView_CheckExact PyPyMemoryView_CheckExact
PyAPI_FUNC(int) PyMemoryView_CheckExact(void * arg0);
#define PyMemoryView_FromBuffer PyPyMemoryView_FromBuffer
PyAPI_FUNC(struct _object *) PyMemoryView_FromBuffer(struct bufferinfo *arg0);
#define PyMemoryView_FromMemory PyPyMemoryView_FromMemory
PyAPI_FUNC(PyObject *) PyMemoryView_FromMemory(char * arg0, Py_ssize_t arg1, int arg2);
#define PyMemoryView_FromObject PyPyMemoryView_FromObject
PyAPI_FUNC(struct _object *) PyMemoryView_FromObject(struct _object *arg0);
#define PyMemoryView_GetContiguous PyPyMemoryView_GetContiguous
PyAPI_FUNC(struct _object *) PyMemoryView_GetContiguous(struct _object *arg0, int arg1, char arg2);
#define PyMethodDescr_Check PyPyMethodDescr_Check
PyAPI_FUNC(int) PyMethodDescr_Check(void * arg0);
#define PyMethodDescr_CheckExact PyPyMethodDescr_CheckExact
PyAPI_FUNC(int) PyMethodDescr_CheckExact(void * arg0);
#define PyMethod_Check PyPyMethod_Check
PyAPI_FUNC(int) PyMethod_Check(void * arg0);
#define PyMethod_CheckExact PyPyMethod_CheckExact
PyAPI_FUNC(int) PyMethod_CheckExact(void * arg0);
#define PyMethod_Function PyPyMethod_Function
PyAPI_FUNC(struct _object *) PyMethod_Function(struct _object *arg0);
#define PyMethod_New PyPyMethod_New
PyAPI_FUNC(struct _object *) PyMethod_New(struct _object *arg0, struct _object *arg1);
#define PyMethod_Self PyPyMethod_Self
PyAPI_FUNC(struct _object *) PyMethod_Self(struct _object *arg0);
#define PyModule_AddFunctions PyPyModule_AddFunctions
PyAPI_FUNC(int) PyModule_AddFunctions(struct _object *arg0, struct PyMethodDef *arg1);
#define PyModule_Check PyPyModule_Check
PyAPI_FUNC(int) PyModule_Check(void * arg0);
#define PyModule_CheckExact PyPyModule_CheckExact
PyAPI_FUNC(int) PyModule_CheckExact(void * arg0);
#define PyModule_Create2 PyPyModule_Create2
PyAPI_FUNC(struct _object *) PyModule_Create2(struct PyModuleDef *arg0, int arg1);
#define PyModule_ExecDef PyPyModule_ExecDef
PyAPI_FUNC(int) PyModule_ExecDef(struct _object *arg0, struct PyModuleDef *arg1);
#define PyModule_GetDict PyPyModule_GetDict
PyAPI_FUNC(struct _object *) PyModule_GetDict(struct _object *arg0);
#define PyModule_GetName PyPyModule_GetName
PyAPI_FUNC(char *) PyModule_GetName(struct _object *arg0);
#define PyModule_New PyPyModule_New
PyAPI_FUNC(struct _object *) PyModule_New(const char *arg0);
#define PyModule_NewObject PyPyModule_NewObject
PyAPI_FUNC(struct _object *) PyModule_NewObject(struct _object *arg0);
#define PyNumber_Absolute PyPyNumber_Absolute
PyAPI_FUNC(struct _object *) PyNumber_Absolute(struct _object *arg0);
#define PyNumber_Add PyPyNumber_Add
PyAPI_FUNC(struct _object *) PyNumber_Add(struct _object *arg0, struct _object *arg1);
#define PyNumber_And PyPyNumber_And
PyAPI_FUNC(struct _object *) PyNumber_And(struct _object *arg0, struct _object *arg1);
#define PyNumber_AsSsize_t PyPyNumber_AsSsize_t
PyAPI_FUNC(Signed) PyNumber_AsSsize_t(struct _object *arg0, struct _object *arg1);
#define PyNumber_Check PyPyNumber_Check
PyAPI_FUNC(int) PyNumber_Check(struct _object *arg0);
#define PyNumber_Divide PyPyNumber_Divide
PyAPI_FUNC(struct _object *) PyNumber_Divide(struct _object *arg0, struct _object *arg1);
#define PyNumber_Divmod PyPyNumber_Divmod
PyAPI_FUNC(struct _object *) PyNumber_Divmod(struct _object *arg0, struct _object *arg1);
#define PyNumber_Float PyPyNumber_Float
PyAPI_FUNC(struct _object *) PyNumber_Float(struct _object *arg0);
#define PyNumber_FloorDivide PyPyNumber_FloorDivide
PyAPI_FUNC(struct _object *) PyNumber_FloorDivide(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceAdd PyPyNumber_InPlaceAdd
PyAPI_FUNC(struct _object *) PyNumber_InPlaceAdd(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceAnd PyPyNumber_InPlaceAnd
PyAPI_FUNC(struct _object *) PyNumber_InPlaceAnd(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceDivide PyPyNumber_InPlaceDivide
PyAPI_FUNC(struct _object *) PyNumber_InPlaceDivide(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceFloorDivide PyPyNumber_InPlaceFloorDivide
PyAPI_FUNC(struct _object *) PyNumber_InPlaceFloorDivide(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceLshift PyPyNumber_InPlaceLshift
PyAPI_FUNC(struct _object *) PyNumber_InPlaceLshift(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceMatrixMultiply PyPyNumber_InPlaceMatrixMultiply
PyAPI_FUNC(struct _object *) PyNumber_InPlaceMatrixMultiply(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceMultiply PyPyNumber_InPlaceMultiply
PyAPI_FUNC(struct _object *) PyNumber_InPlaceMultiply(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceOr PyPyNumber_InPlaceOr
PyAPI_FUNC(struct _object *) PyNumber_InPlaceOr(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlacePower PyPyNumber_InPlacePower
PyAPI_FUNC(struct _object *) PyNumber_InPlacePower(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyNumber_InPlaceRemainder PyPyNumber_InPlaceRemainder
PyAPI_FUNC(struct _object *) PyNumber_InPlaceRemainder(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceRshift PyPyNumber_InPlaceRshift
PyAPI_FUNC(struct _object *) PyNumber_InPlaceRshift(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceSubtract PyPyNumber_InPlaceSubtract
PyAPI_FUNC(struct _object *) PyNumber_InPlaceSubtract(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceTrueDivide PyPyNumber_InPlaceTrueDivide
PyAPI_FUNC(struct _object *) PyNumber_InPlaceTrueDivide(struct _object *arg0, struct _object *arg1);
#define PyNumber_InPlaceXor PyPyNumber_InPlaceXor
PyAPI_FUNC(struct _object *) PyNumber_InPlaceXor(struct _object *arg0, struct _object *arg1);
#define PyNumber_Index PyPyNumber_Index
PyAPI_FUNC(struct _object *) PyNumber_Index(struct _object *arg0);
#define PyNumber_Invert PyPyNumber_Invert
PyAPI_FUNC(struct _object *) PyNumber_Invert(struct _object *arg0);
#define PyNumber_Long PyPyNumber_Long
PyAPI_FUNC(struct _object *) PyNumber_Long(struct _object *arg0);
#define PyNumber_Lshift PyPyNumber_Lshift
PyAPI_FUNC(struct _object *) PyNumber_Lshift(struct _object *arg0, struct _object *arg1);
#define PyNumber_MatrixMultiply PyPyNumber_MatrixMultiply
PyAPI_FUNC(struct _object *) PyNumber_MatrixMultiply(struct _object *arg0, struct _object *arg1);
#define PyNumber_Multiply PyPyNumber_Multiply
PyAPI_FUNC(struct _object *) PyNumber_Multiply(struct _object *arg0, struct _object *arg1);
#define PyNumber_Negative PyPyNumber_Negative
PyAPI_FUNC(struct _object *) PyNumber_Negative(struct _object *arg0);
#define PyNumber_Or PyPyNumber_Or
PyAPI_FUNC(struct _object *) PyNumber_Or(struct _object *arg0, struct _object *arg1);
#define PyNumber_Positive PyPyNumber_Positive
PyAPI_FUNC(struct _object *) PyNumber_Positive(struct _object *arg0);
#define PyNumber_Power PyPyNumber_Power
PyAPI_FUNC(struct _object *) PyNumber_Power(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyNumber_Remainder PyPyNumber_Remainder
PyAPI_FUNC(struct _object *) PyNumber_Remainder(struct _object *arg0, struct _object *arg1);
#define PyNumber_Rshift PyPyNumber_Rshift
PyAPI_FUNC(struct _object *) PyNumber_Rshift(struct _object *arg0, struct _object *arg1);
#define PyNumber_Subtract PyPyNumber_Subtract
PyAPI_FUNC(struct _object *) PyNumber_Subtract(struct _object *arg0, struct _object *arg1);
#define PyNumber_ToBase PyPyNumber_ToBase
PyAPI_FUNC(struct _object *) PyNumber_ToBase(struct _object *arg0, int arg1);
#define PyNumber_TrueDivide PyPyNumber_TrueDivide
PyAPI_FUNC(struct _object *) PyNumber_TrueDivide(struct _object *arg0, struct _object *arg1);
#define PyNumber_Xor PyPyNumber_Xor
PyAPI_FUNC(struct _object *) PyNumber_Xor(struct _object *arg0, struct _object *arg1);
#define PyOS_AfterFork PyPyOS_AfterFork
PyAPI_FUNC(void) PyOS_AfterFork(void);
#define PyOS_FSPath PyPyOS_FSPath
PyAPI_FUNC(struct _object *) PyOS_FSPath(struct _object *arg0);
#define PyOS_InterruptOccurred PyPyOS_InterruptOccurred
PyAPI_FUNC(int) PyOS_InterruptOccurred(void);
#define PyOS_double_to_string PyPyOS_double_to_string
PyAPI_FUNC(char *) PyOS_double_to_string(double arg0, char arg1, int arg2, int arg3, int *arg4);
#define PyOS_string_to_double PyPyOS_string_to_double
PyAPI_FUNC(double) PyOS_string_to_double(const char *arg0, char **arg1, struct _object *arg2);
#define PyObject_ASCII PyPyObject_ASCII
PyAPI_FUNC(struct _object *) PyObject_ASCII(struct _object *arg0);
#define PyObject_AsCharBuffer PyPyObject_AsCharBuffer
PyAPI_FUNC(int) PyObject_AsCharBuffer(struct _object *arg0, const char **arg1, Signed *arg2);
#define PyObject_AsFileDescriptor PyPyObject_AsFileDescriptor
PyAPI_FUNC(int) PyObject_AsFileDescriptor(struct _object *arg0);
#define PyObject_Bytes PyPyObject_Bytes
PyAPI_FUNC(PyObject *) PyObject_Bytes(PyObject * arg0);
#define PyObject_Call PyPyObject_Call
PyAPI_FUNC(struct _object *) PyObject_Call(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyObject_CallObject PyPyObject_CallObject
PyAPI_FUNC(struct _object *) PyObject_CallObject(struct _object *arg0, struct _object *arg1);
#define PyObject_Calloc PyPyObject_Calloc
PyAPI_FUNC(void *) PyObject_Calloc(Unsigned arg0, Unsigned arg1);
#define PyObject_ClearWeakRefs PyPyObject_ClearWeakRefs
PyAPI_FUNC(void) PyObject_ClearWeakRefs(struct _object *arg0);
#define PyObject_DelAttr PyPyObject_DelAttr
PyAPI_FUNC(int) PyObject_DelAttr(struct _object *arg0, struct _object *arg1);
#define PyObject_DelAttrString PyPyObject_DelAttrString
PyAPI_FUNC(int) PyObject_DelAttrString(struct _object *arg0, const char *arg1);
#define PyObject_DelItem PyPyObject_DelItem
PyAPI_FUNC(int) PyObject_DelItem(struct _object *arg0, struct _object *arg1);
#define PyObject_Dir PyPyObject_Dir
PyAPI_FUNC(struct _object *) PyObject_Dir(struct _object *arg0);
#define PyObject_Format PyPyObject_Format
PyAPI_FUNC(struct _object *) PyObject_Format(struct _object *arg0, struct _object *arg1);
#define PyObject_GenericGetAttr PyPyObject_GenericGetAttr
PyAPI_FUNC(struct _object *) PyObject_GenericGetAttr(struct _object *arg0, struct _object *arg1);
#define PyObject_GenericGetDict PyPyObject_GenericGetDict
PyAPI_FUNC(struct _object *) PyObject_GenericGetDict(struct _object *arg0, void *arg1);
#define PyObject_GenericSetAttr PyPyObject_GenericSetAttr
PyAPI_FUNC(int) PyObject_GenericSetAttr(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyObject_GenericSetDict PyPyObject_GenericSetDict
PyAPI_FUNC(int) PyObject_GenericSetDict(struct _object *arg0, struct _object *arg1, void *arg2);
#define PyObject_GetAttr PyPyObject_GetAttr
PyAPI_FUNC(struct _object *) PyObject_GetAttr(struct _object *arg0, struct _object *arg1);
#define PyObject_GetAttrString PyPyObject_GetAttrString
PyAPI_FUNC(struct _object *) PyObject_GetAttrString(struct _object *arg0, const char *arg1);
#define PyObject_GetItem PyPyObject_GetItem
PyAPI_FUNC(struct _object *) PyObject_GetItem(struct _object *arg0, struct _object *arg1);
#define PyObject_GetIter PyPyObject_GetIter
PyAPI_FUNC(struct _object *) PyObject_GetIter(struct _object *arg0);
#define PyObject_HasAttr PyPyObject_HasAttr
PyAPI_FUNC(int) PyObject_HasAttr(struct _object *arg0, struct _object *arg1);
#define PyObject_HasAttrString PyPyObject_HasAttrString
PyAPI_FUNC(int) PyObject_HasAttrString(struct _object *arg0, const char *arg1);
#define PyObject_Hash PyPyObject_Hash
PyAPI_FUNC(Signed) PyObject_Hash(struct _object *arg0);
#define PyObject_HashNotImplemented PyPyObject_HashNotImplemented
PyAPI_FUNC(Signed) PyObject_HashNotImplemented(struct _object *arg0);
#define PyObject_IsInstance PyPyObject_IsInstance
PyAPI_FUNC(int) PyObject_IsInstance(struct _object *arg0, struct _object *arg1);
#define PyObject_IsSubclass PyPyObject_IsSubclass
PyAPI_FUNC(int) PyObject_IsSubclass(struct _object *arg0, struct _object *arg1);
#define PyObject_IsTrue PyPyObject_IsTrue
PyAPI_FUNC(int) PyObject_IsTrue(struct _object *arg0);
#define PyObject_LengthHint PyPyObject_LengthHint
PyAPI_FUNC(Py_ssize_t) PyObject_LengthHint(PyObject * arg0, Py_ssize_t arg1);
#define PyObject_Malloc PyPyObject_Malloc
PyAPI_FUNC(void *) PyObject_Malloc(Unsigned arg0);
#define PyObject_Not PyPyObject_Not
PyAPI_FUNC(int) PyObject_Not(struct _object *arg0);
#define PyObject_Print PyPyObject_Print
PyAPI_FUNC(int) PyObject_Print(struct _object *arg0, FILE *arg1, int arg2);
#define PyObject_Realloc PyPyObject_Realloc
PyAPI_FUNC(void *) PyObject_Realloc(void *arg0, Unsigned arg1);
#define PyObject_Repr PyPyObject_Repr
PyAPI_FUNC(struct _object *) PyObject_Repr(struct _object *arg0);
#define PyObject_RichCompare PyPyObject_RichCompare
PyAPI_FUNC(struct _object *) PyObject_RichCompare(struct _object *arg0, struct _object *arg1, int arg2);
#define PyObject_RichCompareBool PyPyObject_RichCompareBool
PyAPI_FUNC(int) PyObject_RichCompareBool(struct _object *arg0, struct _object *arg1, int arg2);
#define PyObject_SelfIter PyPyObject_SelfIter
PyAPI_FUNC(struct _object *) PyObject_SelfIter(struct _object *arg0);
#define PyObject_SetAttr PyPyObject_SetAttr
PyAPI_FUNC(int) PyObject_SetAttr(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyObject_SetAttrString PyPyObject_SetAttrString
PyAPI_FUNC(int) PyObject_SetAttrString(struct _object *arg0, const char *arg1, struct _object *arg2);
#define PyObject_SetItem PyPyObject_SetItem
PyAPI_FUNC(int) PyObject_SetItem(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PyObject_Size PyPyObject_Size
PyAPI_FUNC(Signed) PyObject_Size(struct _object *arg0);
#define PyObject_Str PyPyObject_Str
PyAPI_FUNC(struct _object *) PyObject_Str(struct _object *arg0);
#define PyObject_Type PyPyObject_Type
PyAPI_FUNC(struct _object *) PyObject_Type(struct _object *arg0);
#define PyObject_Unicode PyPyObject_Unicode
PyAPI_FUNC(struct _object *) PyObject_Unicode(struct _object *arg0);
#define PyPyUnicode_Check PyPyUnicode_Check
PyAPI_FUNC(int) PyPyUnicode_Check(void * arg0);
#define PyPyUnicode_CheckExact PyPyUnicode_CheckExact
PyAPI_FUNC(int) PyPyUnicode_CheckExact(void * arg0);
#define PyRun_File PyPyRun_File
PyAPI_FUNC(struct _object *) PyRun_File(FILE *arg0, const char *arg1, int arg2, struct _object *arg3, struct _object *arg4);
#define PyRun_SimpleString PyPyRun_SimpleString
PyAPI_FUNC(int) PyRun_SimpleString(const char *arg0);
#define PyRun_String PyPyRun_String
PyAPI_FUNC(struct _object *) PyRun_String(const char *arg0, int arg1, struct _object *arg2, struct _object *arg3);
#define PyRun_StringFlags PyPyRun_StringFlags
PyAPI_FUNC(struct _object *) PyRun_StringFlags(const char *arg0, int arg1, struct _object *arg2, struct _object *arg3, PyCompilerFlags *arg4);
#define PySeqIter_New PyPySeqIter_New
PyAPI_FUNC(struct _object *) PySeqIter_New(struct _object *arg0);
#define PySequence_Check PyPySequence_Check
PyAPI_FUNC(int) PySequence_Check(struct _object *arg0);
#define PySequence_Concat PyPySequence_Concat
PyAPI_FUNC(struct _object *) PySequence_Concat(struct _object *arg0, struct _object *arg1);
#define PySequence_Contains PyPySequence_Contains
PyAPI_FUNC(int) PySequence_Contains(struct _object *arg0, struct _object *arg1);
#define PySequence_DelItem PyPySequence_DelItem
PyAPI_FUNC(int) PySequence_DelItem(struct _object *arg0, Signed arg1);
#define PySequence_DelSlice PyPySequence_DelSlice
PyAPI_FUNC(int) PySequence_DelSlice(struct _object *arg0, Signed arg1, Signed arg2);
#define PySequence_Fast PyPySequence_Fast
PyAPI_FUNC(struct _object *) PySequence_Fast(struct _object *arg0, const char *arg1);
#define PySequence_Fast_GET_ITEM PyPySequence_Fast_GET_ITEM
PyAPI_FUNC(struct _object *) PySequence_Fast_GET_ITEM(void *arg0, Signed arg1);
#define PySequence_Fast_GET_SIZE PyPySequence_Fast_GET_SIZE
PyAPI_FUNC(Signed) PySequence_Fast_GET_SIZE(void *arg0);
#define PySequence_Fast_ITEMS PyPySequence_Fast_ITEMS
PyAPI_FUNC(struct _object **) PySequence_Fast_ITEMS(void *arg0);
#define PySequence_GetItem PyPySequence_GetItem
PyAPI_FUNC(struct _object *) PySequence_GetItem(struct _object *arg0, Signed arg1);
#define PySequence_GetSlice PyPySequence_GetSlice
PyAPI_FUNC(struct _object *) PySequence_GetSlice(struct _object *arg0, Signed arg1, Signed arg2);
#define PySequence_ITEM PyPySequence_ITEM
PyAPI_FUNC(struct _object *) PySequence_ITEM(void *arg0, Signed arg1);
#define PySequence_InPlaceConcat PyPySequence_InPlaceConcat
PyAPI_FUNC(struct _object *) PySequence_InPlaceConcat(struct _object *arg0, struct _object *arg1);
#define PySequence_InPlaceRepeat PyPySequence_InPlaceRepeat
PyAPI_FUNC(struct _object *) PySequence_InPlaceRepeat(struct _object *arg0, Signed arg1);
#define PySequence_Index PyPySequence_Index
PyAPI_FUNC(Signed) PySequence_Index(struct _object *arg0, struct _object *arg1);
#define PySequence_Length PyPySequence_Length
PyAPI_FUNC(Signed) PySequence_Length(struct _object *arg0);
#define PySequence_List PyPySequence_List
PyAPI_FUNC(struct _object *) PySequence_List(struct _object *arg0);
#define PySequence_Repeat PyPySequence_Repeat
PyAPI_FUNC(struct _object *) PySequence_Repeat(struct _object *arg0, Signed arg1);
#define PySequence_SetItem PyPySequence_SetItem
PyAPI_FUNC(int) PySequence_SetItem(struct _object *arg0, Signed arg1, struct _object *arg2);
#define PySequence_SetSlice PyPySequence_SetSlice
PyAPI_FUNC(int) PySequence_SetSlice(struct _object *arg0, Signed arg1, Signed arg2, struct _object *arg3);
#define PySequence_Size PyPySequence_Size
PyAPI_FUNC(Signed) PySequence_Size(struct _object *arg0);
#define PySequence_Tuple PyPySequence_Tuple
PyAPI_FUNC(struct _object *) PySequence_Tuple(struct _object *arg0);
#define PySet_Add PyPySet_Add
PyAPI_FUNC(int) PySet_Add(struct _object *arg0, struct _object *arg1);
#define PySet_Check PyPySet_Check
PyAPI_FUNC(int) PySet_Check(void * arg0);
#define PySet_CheckExact PyPySet_CheckExact
PyAPI_FUNC(int) PySet_CheckExact(void * arg0);
#define PySet_Clear PyPySet_Clear
PyAPI_FUNC(int) PySet_Clear(struct _object *arg0);
#define PySet_Contains PyPySet_Contains
PyAPI_FUNC(int) PySet_Contains(struct _object *arg0, struct _object *arg1);
#define PySet_Discard PyPySet_Discard
PyAPI_FUNC(int) PySet_Discard(struct _object *arg0, struct _object *arg1);
#define PySet_GET_SIZE PyPySet_GET_SIZE
PyAPI_FUNC(Signed) PySet_GET_SIZE(void *arg0);
#define PySet_New PyPySet_New
PyAPI_FUNC(struct _object *) PySet_New(struct _object *arg0);
#define PySet_Pop PyPySet_Pop
PyAPI_FUNC(struct _object *) PySet_Pop(struct _object *arg0);
#define PySet_Size PyPySet_Size
PyAPI_FUNC(Signed) PySet_Size(struct _object *arg0);
#define PySlice_GetIndices PyPySlice_GetIndices
PyAPI_FUNC(int) PySlice_GetIndices(struct _object *arg0, Signed arg1, Signed *arg2, Signed *arg3, Signed *arg4);
#define PySlice_GetIndicesEx PyPySlice_GetIndicesEx
PyAPI_FUNC(int) PySlice_GetIndicesEx(struct _object *arg0, Signed arg1, Signed *arg2, Signed *arg3, Signed *arg4, Signed *arg5);
#define PySlice_New PyPySlice_New
PyAPI_FUNC(struct _object *) PySlice_New(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define PySlice_Unpack PyPySlice_Unpack
PyAPI_FUNC(int) PySlice_Unpack(struct _object *arg0, Signed *arg1, Signed *arg2, Signed *arg3);
#define PyStaticMethod_New PyPyStaticMethod_New
PyAPI_FUNC(struct _object *) PyStaticMethod_New(struct _object *arg0);
#define PySys_GetObject PyPySys_GetObject
PyAPI_FUNC(struct _object *) PySys_GetObject(const char *arg0);
#define PySys_SetObject PyPySys_SetObject
PyAPI_FUNC(int) PySys_SetObject(const char *arg0, struct _object *arg1);
#define PyTZInfo_Check PyPyTZInfo_Check
PyAPI_FUNC(int) PyTZInfo_Check(struct _object *arg0);
#define PyTZInfo_CheckExact PyPyTZInfo_CheckExact
PyAPI_FUNC(int) PyTZInfo_CheckExact(struct _object *arg0);
#define PyThreadState_Clear PyPyThreadState_Clear
PyAPI_FUNC(void) PyThreadState_Clear(PyThreadState *arg0);
#define PyThreadState_Delete PyPyThreadState_Delete
PyAPI_FUNC(void) PyThreadState_Delete(PyThreadState *arg0);
#define PyThreadState_DeleteCurrent PyPyThreadState_DeleteCurrent
PyAPI_FUNC(void) PyThreadState_DeleteCurrent(void);
#define PyThreadState_Get PyPyThreadState_Get
PyAPI_FUNC(PyThreadState *) PyThreadState_Get(void);
#define PyThreadState_GetDict PyPyThreadState_GetDict
PyAPI_FUNC(struct _object *) PyThreadState_GetDict(void);
#define PyThreadState_New PyPyThreadState_New
PyAPI_FUNC(PyThreadState *) PyThreadState_New(PyInterpreterState *arg0);
#define PyThreadState_Swap PyPyThreadState_Swap
PyAPI_FUNC(PyThreadState *) PyThreadState_Swap(PyThreadState *arg0);
#define PyThread_exit_thread PyPyThread_exit_thread
PyAPI_FUNC(struct _object *) PyThread_exit_thread(void);
#define PyTime_Check PyPyTime_Check
PyAPI_FUNC(int) PyTime_Check(struct _object *arg0);
#define PyTime_CheckExact PyPyTime_CheckExact
PyAPI_FUNC(int) PyTime_CheckExact(struct _object *arg0);
#define PyTraceBack_Check PyPyTraceBack_Check
PyAPI_FUNC(int) PyTraceBack_Check(struct _object *arg0);
#define PyTraceBack_Here PyPyTraceBack_Here
PyAPI_FUNC(int) PyTraceBack_Here(PyFrameObject *arg0);
#define PyTraceBack_Print PyPyTraceBack_Print
PyAPI_FUNC(int) PyTraceBack_Print(struct _object *arg0, struct _object *arg1);
#define PyTuple_GetItem PyPyTuple_GetItem
PyAPI_FUNC(struct _object *) PyTuple_GetItem(struct _object *arg0, Signed arg1);
#define PyTuple_GetSlice PyPyTuple_GetSlice
PyAPI_FUNC(struct _object *) PyTuple_GetSlice(struct _object *arg0, Signed arg1, Signed arg2);
#define PyTuple_SetItem PyPyTuple_SetItem
PyAPI_FUNC(int) PyTuple_SetItem(struct _object *arg0, Signed arg1, struct _object *arg2);
#define PyTuple_Size PyPyTuple_Size
PyAPI_FUNC(Signed) PyTuple_Size(struct _object *arg0);
#define PyType_FromSpecWithBases PyPyType_FromSpecWithBases
PyAPI_FUNC(PyObject *) PyType_FromSpecWithBases(PyType_Spec * arg0, PyObject * arg1);
#define PyType_GenericNew PyPyType_GenericNew
PyAPI_FUNC(struct _object *) PyType_GenericNew(struct _typeobject *arg0, struct _object *arg1, struct _object *arg2);
#define PyType_GetSlot PyPyType_GetSlot
PyAPI_FUNC(void *) PyType_GetSlot(struct _typeobject *arg0, int arg1);
#define PyType_IsSubtype PyPyType_IsSubtype
PyAPI_FUNC(int) PyType_IsSubtype(struct _typeobject *arg0, struct _typeobject *arg1);
#define PyType_Modified PyPyType_Modified
PyAPI_FUNC(void) PyType_Modified(struct _typeobject *arg0);
#define PyType_Ready PyPyType_Ready
PyAPI_FUNC(int) PyType_Ready(struct _typeobject *arg0);
#define PyUnicode_AsASCIIString PyPyUnicode_AsASCIIString
PyAPI_FUNC(struct _object *) PyUnicode_AsASCIIString(struct _object *arg0);
#define PyUnicode_AsEncodedObject PyPyUnicode_AsEncodedObject
PyAPI_FUNC(struct _object *) PyUnicode_AsEncodedObject(struct _object *arg0, const char *arg1, const char *arg2);
#define PyUnicode_AsEncodedString PyPyUnicode_AsEncodedString
PyAPI_FUNC(struct _object *) PyUnicode_AsEncodedString(struct _object *arg0, const char *arg1, const char *arg2);
#define PyUnicode_AsLatin1String PyPyUnicode_AsLatin1String
PyAPI_FUNC(struct _object *) PyUnicode_AsLatin1String(struct _object *arg0);
#define PyUnicode_AsUCS4 PyPyUnicode_AsUCS4
PyAPI_FUNC(Py_UCS4 *) PyUnicode_AsUCS4(PyObject * arg0, Py_UCS4 * arg1, Py_ssize_t arg2, int arg3);
#define PyUnicode_AsUCS4Copy PyPyUnicode_AsUCS4Copy
PyAPI_FUNC(Py_UCS4 *) PyUnicode_AsUCS4Copy(PyObject * arg0);
#define PyUnicode_AsUTF16String PyPyUnicode_AsUTF16String
PyAPI_FUNC(struct _object *) PyUnicode_AsUTF16String(struct _object *arg0);
#define PyUnicode_AsUTF32String PyPyUnicode_AsUTF32String
PyAPI_FUNC(struct _object *) PyUnicode_AsUTF32String(struct _object *arg0);
#define PyUnicode_AsUTF8 PyPyUnicode_AsUTF8
PyAPI_FUNC(char *) PyUnicode_AsUTF8(PyObject * arg0);
#define PyUnicode_AsUTF8AndSize PyPyUnicode_AsUTF8AndSize
PyAPI_FUNC(char *) PyUnicode_AsUTF8AndSize(PyObject * arg0, Py_ssize_t * arg1);
#define PyUnicode_AsUTF8String PyPyUnicode_AsUTF8String
PyAPI_FUNC(struct _object *) PyUnicode_AsUTF8String(struct _object *arg0);
#define PyUnicode_AsUnicode PyPyUnicode_AsUnicode
PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicode(PyObject * arg0);
#define PyUnicode_AsUnicodeAndSize PyPyUnicode_AsUnicodeAndSize
PyAPI_FUNC(Py_UNICODE *) PyUnicode_AsUnicodeAndSize(PyObject * arg0, Py_ssize_t * arg1);
#define PyUnicode_AsUnicodeEscapeString PyPyUnicode_AsUnicodeEscapeString
PyAPI_FUNC(struct _object *) PyUnicode_AsUnicodeEscapeString(struct _object *arg0);
#define PyUnicode_AsWideChar PyPyUnicode_AsWideChar
PyAPI_FUNC(Signed) PyUnicode_AsWideChar(struct _object *arg0, wchar_t *arg1, Signed arg2);
#define PyUnicode_Compare PyPyUnicode_Compare
PyAPI_FUNC(int) PyUnicode_Compare(struct _object *arg0, struct _object *arg1);
#define PyUnicode_CompareWithASCIIString PyPyUnicode_CompareWithASCIIString
PyAPI_FUNC(int) PyUnicode_CompareWithASCIIString(struct _object *arg0, const char *arg1);
#define PyUnicode_Concat PyPyUnicode_Concat
PyAPI_FUNC(struct _object *) PyUnicode_Concat(struct _object *arg0, struct _object *arg1);
#define PyUnicode_Contains PyPyUnicode_Contains
PyAPI_FUNC(int) PyUnicode_Contains(struct _object *arg0, struct _object *arg1);
#define PyUnicode_Count PyPyUnicode_Count
PyAPI_FUNC(Signed) PyUnicode_Count(struct _object *arg0, struct _object *arg1, Signed arg2, Signed arg3);
#define PyUnicode_Decode PyPyUnicode_Decode
PyAPI_FUNC(struct _object *) PyUnicode_Decode(const char *arg0, Signed arg1, const char *arg2, const char *arg3);
#define PyUnicode_DecodeASCII PyPyUnicode_DecodeASCII
PyAPI_FUNC(struct _object *) PyUnicode_DecodeASCII(const char *arg0, Signed arg1, const char *arg2);
#define PyUnicode_DecodeFSDefault PyPyUnicode_DecodeFSDefault
PyAPI_FUNC(struct _object *) PyUnicode_DecodeFSDefault(const char *arg0);
#define PyUnicode_DecodeFSDefaultAndSize PyPyUnicode_DecodeFSDefaultAndSize
PyAPI_FUNC(struct _object *) PyUnicode_DecodeFSDefaultAndSize(const char *arg0, Signed arg1);
#define PyUnicode_DecodeLatin1 PyPyUnicode_DecodeLatin1
PyAPI_FUNC(struct _object *) PyUnicode_DecodeLatin1(const char *arg0, Signed arg1, const char *arg2);
#define PyUnicode_DecodeLocale PyPyUnicode_DecodeLocale
PyAPI_FUNC(struct _object *) PyUnicode_DecodeLocale(const char *arg0, const char *arg1);
#define PyUnicode_DecodeLocaleAndSize PyPyUnicode_DecodeLocaleAndSize
PyAPI_FUNC(struct _object *) PyUnicode_DecodeLocaleAndSize(const char *arg0, Signed arg1, const char *arg2);
#define PyUnicode_DecodeUTF16 PyPyUnicode_DecodeUTF16
PyAPI_FUNC(struct _object *) PyUnicode_DecodeUTF16(const char *arg0, Signed arg1, const char *arg2, int *arg3);
#define PyUnicode_DecodeUTF32 PyPyUnicode_DecodeUTF32
PyAPI_FUNC(struct _object *) PyUnicode_DecodeUTF32(const char *arg0, Signed arg1, const char *arg2, int *arg3);
#define PyUnicode_DecodeUTF8 PyPyUnicode_DecodeUTF8
PyAPI_FUNC(struct _object *) PyUnicode_DecodeUTF8(const char *arg0, Signed arg1, const char *arg2);
#define PyUnicode_EncodeASCII PyPyUnicode_EncodeASCII
PyAPI_FUNC(struct _object *) PyUnicode_EncodeASCII(const wchar_t *arg0, Signed arg1, const char *arg2);
#define PyUnicode_EncodeDecimal PyPyUnicode_EncodeDecimal
PyAPI_FUNC(int) PyUnicode_EncodeDecimal(wchar_t *arg0, Signed arg1, char *arg2, const char *arg3);
#define PyUnicode_EncodeFSDefault PyPyUnicode_EncodeFSDefault
PyAPI_FUNC(struct _object *) PyUnicode_EncodeFSDefault(struct _object *arg0);
#define PyUnicode_EncodeLatin1 PyPyUnicode_EncodeLatin1
PyAPI_FUNC(struct _object *) PyUnicode_EncodeLatin1(const wchar_t *arg0, Signed arg1, const char *arg2);
#define PyUnicode_EncodeLocale PyPyUnicode_EncodeLocale
PyAPI_FUNC(struct _object *) PyUnicode_EncodeLocale(struct _object *arg0, const char *arg1);
#define PyUnicode_EncodeUTF8 PyPyUnicode_EncodeUTF8
PyAPI_FUNC(struct _object *) PyUnicode_EncodeUTF8(const wchar_t *arg0, Signed arg1, const char *arg2);
#define PyUnicode_FSConverter PyPyUnicode_FSConverter
PyAPI_FUNC(int) PyUnicode_FSConverter(struct _object *arg0, struct _object **arg1);
#define PyUnicode_FSDecoder PyPyUnicode_FSDecoder
PyAPI_FUNC(int) PyUnicode_FSDecoder(struct _object *arg0, struct _object **arg1);
#define PyUnicode_Find PyPyUnicode_Find
PyAPI_FUNC(Signed) PyUnicode_Find(struct _object *arg0, struct _object *arg1, Signed arg2, Signed arg3, int arg4);
#define PyUnicode_FindChar PyPyUnicode_FindChar
PyAPI_FUNC(Py_ssize_t) PyUnicode_FindChar(PyObject * arg0, Py_UCS4 arg1, Py_ssize_t arg2, Py_ssize_t arg3, int arg4);
#define PyUnicode_Format PyPyUnicode_Format
PyAPI_FUNC(struct _object *) PyUnicode_Format(struct _object *arg0, struct _object *arg1);
#define PyUnicode_FromEncodedObject PyPyUnicode_FromEncodedObject
PyAPI_FUNC(struct _object *) PyUnicode_FromEncodedObject(struct _object *arg0, const char *arg1, const char *arg2);
#define PyUnicode_FromKindAndData PyPyUnicode_FromKindAndData
PyAPI_FUNC(PyObject *) PyUnicode_FromKindAndData(int arg0, void const * arg1, Py_ssize_t arg2);
#define PyUnicode_FromObject PyPyUnicode_FromObject
PyAPI_FUNC(struct _object *) PyUnicode_FromObject(struct _object *arg0);
#define PyUnicode_FromOrdinal PyPyUnicode_FromOrdinal
PyAPI_FUNC(struct _object *) PyUnicode_FromOrdinal(int arg0);
#define PyUnicode_FromString PyPyUnicode_FromString
PyAPI_FUNC(struct _object *) PyUnicode_FromString(const char *arg0);
#define PyUnicode_FromStringAndSize PyPyUnicode_FromStringAndSize
PyAPI_FUNC(struct _object *) PyUnicode_FromStringAndSize(const char *arg0, Signed arg1);
#define PyUnicode_FromUnicode PyPyUnicode_FromUnicode
PyAPI_FUNC(struct _object *) PyUnicode_FromUnicode(const wchar_t *arg0, Signed arg1);
#define PyUnicode_GetDefaultEncoding PyPyUnicode_GetDefaultEncoding
PyAPI_FUNC(char *) PyUnicode_GetDefaultEncoding(void);
#define PyUnicode_GetMax PyPyUnicode_GetMax
PyAPI_FUNC(wchar_t) PyUnicode_GetMax(void);
#define PyUnicode_InternFromString PyPyUnicode_InternFromString
PyAPI_FUNC(struct _object *) PyUnicode_InternFromString(const char *arg0);
#define PyUnicode_InternInPlace PyPyUnicode_InternInPlace
PyAPI_FUNC(void) PyUnicode_InternInPlace(struct _object **arg0);
#define PyUnicode_Join PyPyUnicode_Join
PyAPI_FUNC(struct _object *) PyUnicode_Join(struct _object *arg0, struct _object *arg1);
#define PyUnicode_New PyPyUnicode_New
PyAPI_FUNC(PyObject *) PyUnicode_New(Py_ssize_t arg0, Py_UCS4 arg1);
#define PyUnicode_ReadChar PyPyUnicode_ReadChar
PyAPI_FUNC(Py_UCS4) PyUnicode_ReadChar(PyObject * arg0, Py_ssize_t arg1);
#define PyUnicode_Replace PyPyUnicode_Replace
PyAPI_FUNC(struct _object *) PyUnicode_Replace(struct _object *arg0, struct _object *arg1, struct _object *arg2, Signed arg3);
#define PyUnicode_Resize PyPyUnicode_Resize
PyAPI_FUNC(int) PyUnicode_Resize(struct _object **arg0, Signed arg1);
#define PyUnicode_Split PyPyUnicode_Split
PyAPI_FUNC(struct _object *) PyUnicode_Split(struct _object *arg0, struct _object *arg1, Signed arg2);
#define PyUnicode_Splitlines PyPyUnicode_Splitlines
PyAPI_FUNC(struct _object *) PyUnicode_Splitlines(struct _object *arg0, int arg1);
#define PyUnicode_Substring PyPyUnicode_Substring
PyAPI_FUNC(struct _object *) PyUnicode_Substring(struct _object *arg0, Signed arg1, Signed arg2);
#define PyUnicode_Tailmatch PyPyUnicode_Tailmatch
PyAPI_FUNC(int) PyUnicode_Tailmatch(struct _object *arg0, struct _object *arg1, Signed arg2, Signed arg3, int arg4);
#define PyUnicode_TransformDecimalToASCII PyPyUnicode_TransformDecimalToASCII
PyAPI_FUNC(struct _object *) PyUnicode_TransformDecimalToASCII(wchar_t *arg0, Signed arg1);
#define PyUnicode_WriteChar PyPyUnicode_WriteChar
PyAPI_FUNC(int) PyUnicode_WriteChar(PyObject * arg0, Py_ssize_t arg1, Py_UCS4 arg2);
#define PyWeakref_Check PyPyWeakref_Check
PyAPI_FUNC(int) PyWeakref_Check(struct _object *arg0);
#define PyWeakref_CheckProxy PyPyWeakref_CheckProxy
PyAPI_FUNC(int) PyWeakref_CheckProxy(struct _object *arg0);
#define PyWeakref_CheckRef PyPyWeakref_CheckRef
PyAPI_FUNC(int) PyWeakref_CheckRef(struct _object *arg0);
#define PyWeakref_CheckRefExact PyPyWeakref_CheckRefExact
PyAPI_FUNC(int) PyWeakref_CheckRefExact(struct _object *arg0);
#define PyWeakref_GET_OBJECT PyPyWeakref_GET_OBJECT
PyAPI_FUNC(struct _object *) PyWeakref_GET_OBJECT(void *arg0);
#define PyWeakref_GetObject PyPyWeakref_GetObject
PyAPI_FUNC(struct _object *) PyWeakref_GetObject(struct _object *arg0);
#define PyWeakref_LockObject PyPyWeakref_LockObject
PyAPI_FUNC(struct _object *) PyWeakref_LockObject(struct _object *arg0);
#define PyWeakref_NewProxy PyPyWeakref_NewProxy
PyAPI_FUNC(struct _object *) PyWeakref_NewProxy(struct _object *arg0, struct _object *arg1);
#define PyWeakref_NewRef PyPyWeakref_NewRef
PyAPI_FUNC(struct _object *) PyWeakref_NewRef(struct _object *arg0, struct _object *arg1);
#define Py_AddPendingCall PyPy_AddPendingCall
PyAPI_FUNC(int) Py_AddPendingCall(int (*arg0)(void *), void *arg1);
#define Py_AtExit PyPy_AtExit
PyAPI_FUNC(int) Py_AtExit(void (*arg0)(void));
#define Py_CompileStringFlags PyPy_CompileStringFlags
PyAPI_FUNC(struct _object *) Py_CompileStringFlags(const char *arg0, const char *arg1, int arg2, PyCompilerFlags *arg3);
#define Py_DecRef PyPy_DecRef
PyAPI_FUNC(void) Py_DecRef(struct _object *arg0);
#define Py_EnterRecursiveCall PyPy_EnterRecursiveCall
PyAPI_FUNC(int) Py_EnterRecursiveCall(const char *arg0);
#define Py_FindMethod PyPy_FindMethod
PyAPI_FUNC(struct _object *) Py_FindMethod(struct PyMethodDef *arg0, struct _object *arg1, const char *arg2);
#define Py_GetProgramName PyPy_GetProgramName
PyAPI_FUNC(wchar_t *) Py_GetProgramName(void);
#define Py_GetRecursionLimit PyPy_GetRecursionLimit
PyAPI_FUNC(int) Py_GetRecursionLimit(void);
#define Py_GetVersion PyPy_GetVersion
PyAPI_FUNC(char *) Py_GetVersion(void);
#define Py_IncRef PyPy_IncRef
PyAPI_FUNC(void) Py_IncRef(struct _object *arg0);
#define Py_IsInitialized PyPy_IsInitialized
PyAPI_FUNC(int) Py_IsInitialized(void);
#define Py_LeaveRecursiveCall PyPy_LeaveRecursiveCall
PyAPI_FUNC(void) Py_LeaveRecursiveCall(void);
#define Py_MakePendingCalls PyPy_MakePendingCalls
PyAPI_FUNC(int) Py_MakePendingCalls(void);
#define Py_ReprEnter PyPy_ReprEnter
PyAPI_FUNC(int) Py_ReprEnter(struct _object *arg0);
#define Py_ReprLeave PyPy_ReprLeave
PyAPI_FUNC(void) Py_ReprLeave(struct _object *arg0);
#define Py_SetRecursionLimit PyPy_SetRecursionLimit
PyAPI_FUNC(void) Py_SetRecursionLimit(int arg0);
#define Py_UNICODE_COPY PyPy_UNICODE_COPY
PyAPI_FUNC(void) Py_UNICODE_COPY(wchar_t *arg0, wchar_t *arg1, Signed arg2);
#define Py_UNICODE_ISALNUM PyPy_UNICODE_ISALNUM
PyAPI_FUNC(int) Py_UNICODE_ISALNUM(wchar_t arg0);
#define Py_UNICODE_ISALPHA PyPy_UNICODE_ISALPHA
PyAPI_FUNC(int) Py_UNICODE_ISALPHA(wchar_t arg0);
#define Py_UNICODE_ISDECIMAL PyPy_UNICODE_ISDECIMAL
PyAPI_FUNC(int) Py_UNICODE_ISDECIMAL(wchar_t arg0);
#define Py_UNICODE_ISDIGIT PyPy_UNICODE_ISDIGIT
PyAPI_FUNC(int) Py_UNICODE_ISDIGIT(wchar_t arg0);
#define Py_UNICODE_ISLINEBREAK PyPy_UNICODE_ISLINEBREAK
PyAPI_FUNC(int) Py_UNICODE_ISLINEBREAK(wchar_t arg0);
#define Py_UNICODE_ISLOWER PyPy_UNICODE_ISLOWER
PyAPI_FUNC(int) Py_UNICODE_ISLOWER(wchar_t arg0);
#define Py_UNICODE_ISNUMERIC PyPy_UNICODE_ISNUMERIC
PyAPI_FUNC(int) Py_UNICODE_ISNUMERIC(wchar_t arg0);
#define Py_UNICODE_ISSPACE PyPy_UNICODE_ISSPACE
PyAPI_FUNC(int) Py_UNICODE_ISSPACE(wchar_t arg0);
#define Py_UNICODE_ISTITLE PyPy_UNICODE_ISTITLE
PyAPI_FUNC(int) Py_UNICODE_ISTITLE(wchar_t arg0);
#define Py_UNICODE_ISUPPER PyPy_UNICODE_ISUPPER
PyAPI_FUNC(int) Py_UNICODE_ISUPPER(wchar_t arg0);
#define Py_UNICODE_TODECIMAL PyPy_UNICODE_TODECIMAL
PyAPI_FUNC(int) Py_UNICODE_TODECIMAL(wchar_t arg0);
#define Py_UNICODE_TODIGIT PyPy_UNICODE_TODIGIT
PyAPI_FUNC(int) Py_UNICODE_TODIGIT(wchar_t arg0);
#define Py_UNICODE_TOLOWER PyPy_UNICODE_TOLOWER
PyAPI_FUNC(wchar_t) Py_UNICODE_TOLOWER(wchar_t arg0);
#define Py_UNICODE_TONUMERIC PyPy_UNICODE_TONUMERIC
PyAPI_FUNC(double) Py_UNICODE_TONUMERIC(wchar_t arg0);
#define Py_UNICODE_TOTITLE PyPy_UNICODE_TOTITLE
PyAPI_FUNC(wchar_t) Py_UNICODE_TOTITLE(wchar_t arg0);
#define Py_UNICODE_TOUPPER PyPy_UNICODE_TOUPPER
PyAPI_FUNC(wchar_t) Py_UNICODE_TOUPPER(wchar_t arg0);
#define _PyBytes_Eq _PyPyBytes_Eq
PyAPI_FUNC(int) _PyBytes_Eq(struct _object *arg0, struct _object *arg1);
#define _PyBytes_Join _PyPyBytes_Join
PyAPI_FUNC(struct _object *) _PyBytes_Join(struct _object *arg0, struct _object *arg1);
#define _PyBytes_Resize _PyPyBytes_Resize
PyAPI_FUNC(int) _PyBytes_Resize(struct _object **arg0, Signed arg1);
#define _PyComplex_AsCComplex _PyPyComplex_AsCComplex
PyAPI_FUNC(int) _PyComplex_AsCComplex(struct _object *arg0, struct Py_complex_t *arg1);
#define _PyComplex_FromCComplex _PyPyComplex_FromCComplex
PyAPI_FUNC(struct _object *) _PyComplex_FromCComplex(struct Py_complex_t *arg0);
#define _PyDateTime_FromDateAndTime _PyPyDateTime_FromDateAndTime
PyAPI_FUNC(struct _object *) _PyDateTime_FromDateAndTime(int arg0, int arg1, int arg2, int arg3, int arg4, int arg5, int arg6, struct _object *arg7, struct _typeobject *arg8);
#define _PyDateTime_FromDateAndTimeAndFold _PyPyDateTime_FromDateAndTimeAndFold
PyAPI_FUNC(struct _object *) _PyDateTime_FromDateAndTimeAndFold(int arg0, int arg1, int arg2, int arg3, int arg4, int arg5, int arg6, struct _object *arg7, int arg8, struct _typeobject *arg9);
#define _PyDateTime_FromTimestamp _PyPyDateTime_FromTimestamp
PyAPI_FUNC(struct _object *) _PyDateTime_FromTimestamp(struct _object *arg0, struct _object *arg1, struct _object *arg2);
#define _PyDateTime_Import _PyPyDateTime_Import
PyAPI_FUNC(PyDateTime_CAPI *) _PyDateTime_Import(void);
#define _PyDate_FromDate _PyPyDate_FromDate
PyAPI_FUNC(struct _object *) _PyDate_FromDate(int arg0, int arg1, int arg2, struct _typeobject *arg3);
#define _PyDate_FromTimestamp _PyPyDate_FromTimestamp
PyAPI_FUNC(struct _object *) _PyDate_FromTimestamp(struct _object *arg0, struct _object *arg1);
#define _PyDelta_FromDelta _PyPyDelta_FromDelta
PyAPI_FUNC(struct _object *) _PyDelta_FromDelta(int arg0, int arg1, int arg2, int arg3, struct _typeobject *arg4);
#define _PyDict_GetItemStringWithError _PyPyDict_GetItemStringWithError
PyAPI_FUNC(struct _object *) _PyDict_GetItemStringWithError(struct _object *arg0, const char *arg1);
#define _PyDict_HasOnlyStringKeys _PyPyDict_HasOnlyStringKeys
PyAPI_FUNC(int) _PyDict_HasOnlyStringKeys(struct _object *arg0);
#define _PyErr_WriteUnraisableMsg _PyPyErr_WriteUnraisableMsg
PyAPI_FUNC(void) _PyErr_WriteUnraisableMsg(const char *arg0, struct _object *arg1);
#define _PyEval_SliceIndex _PyPyEval_SliceIndex
PyAPI_FUNC(int) _PyEval_SliceIndex(struct _object *arg0, Signed *arg1);
#define _PyFloat_Unpack4 _PyPyFloat_Unpack4
PyAPI_FUNC(double) _PyFloat_Unpack4(const unsigned char *arg0, int arg1);
#define _PyFloat_Unpack8 _PyPyFloat_Unpack8
PyAPI_FUNC(double) _PyFloat_Unpack8(const unsigned char *arg0, int arg1);
#define _PyImport_AcquireLock _PyPyImport_AcquireLock
PyAPI_FUNC(void) _PyImport_AcquireLock(void);
#define _PyImport_ReleaseLock _PyPyImport_ReleaseLock
PyAPI_FUNC(int) _PyImport_ReleaseLock(void);
#define _PyList_Extend _PyPyList_Extend
PyAPI_FUNC(struct _object *) _PyList_Extend(struct _object *arg0, struct _object *arg1);
#define _PyLong_AsByteArrayO _PyPyLong_AsByteArrayO
PyAPI_FUNC(int) _PyLong_AsByteArrayO(struct _object *arg0, unsigned char *arg1, Unsigned arg2, int arg3, int arg4);
#define _PyLong_FromByteArray _PyPyLong_FromByteArray
PyAPI_FUNC(struct _object *) _PyLong_FromByteArray(const unsigned char *arg0, Unsigned arg1, int arg2, int arg3);
#define _PyLong_NumBits _PyPyLong_NumBits
PyAPI_FUNC(Unsigned) _PyLong_NumBits(struct _object *arg0);
#define _PyLong_Sign _PyPyLong_Sign
PyAPI_FUNC(int) _PyLong_Sign(struct _object *arg0);
#define _PyNamespace_New _PyPyNamespace_New
PyAPI_FUNC(PyObject *) _PyNamespace_New(PyObject * arg0);
#define _PyObject_CallNoArg _PyPyObject_CallNoArg
PyAPI_FUNC(struct _object *) _PyObject_CallNoArg(struct _object *arg0);
#define _PyObject_FastCall _PyPyObject_FastCall
PyAPI_FUNC(struct _object *) _PyObject_FastCall(struct _object *arg0, struct _object **arg1, Signed arg2);
#define _PyObject_FastCallDict _PyPyObject_FastCallDict
PyAPI_FUNC(struct _object *) _PyObject_FastCallDict(struct _object *arg0, struct _object **arg1, Signed arg2, struct _object *arg3);
#define _PyObject_GetDictPtr _PyPyObject_GetDictPtr
PyAPI_FUNC(struct _object **) _PyObject_GetDictPtr(struct _object *arg0);
#define _PyObject_Vectorcall _PyPyObject_Vectorcall
PyAPI_FUNC(struct _object *) _PyObject_Vectorcall(struct _object *arg0, struct _object **arg1, Signed arg2, struct _object *arg3);
#define _PyPyGC_AddMemoryPressure _PyPyPyGC_AddMemoryPressure
PyAPI_FUNC(void) _PyPyGC_AddMemoryPressure(Signed arg0);
#define _PyPy_Free _PyPyPy_Free
extern void _PyPy_Free(void *arg0);
#define _PyPy_Malloc _PyPyPy_Malloc
extern void * _PyPy_Malloc(Signed arg0);
#define _PySet_Next _PyPySet_Next
PyAPI_FUNC(int) _PySet_Next(struct _object *arg0, Signed *arg1, struct _object **arg2);
#define _PySet_NextEntry _PyPySet_NextEntry
PyAPI_FUNC(int) _PySet_NextEntry(struct _object *arg0, Signed *arg1, struct _object **arg2, Signed *arg3);
#define _PyThreadState_UncheckedGet _PyPyThreadState_UncheckedGet
PyAPI_FUNC(PyThreadState *) _PyThreadState_UncheckedGet(void);
#define _PyTimeZone_FromTimeZone _PyPyTimeZone_FromTimeZone
PyAPI_FUNC(struct _object *) _PyTimeZone_FromTimeZone(struct _object *arg0, struct _object *arg1);
#define _PyTime_FromTime _PyPyTime_FromTime
PyAPI_FUNC(struct _object *) _PyTime_FromTime(int arg0, int arg1, int arg2, int arg3, struct _object *arg4, struct _typeobject *arg5);
#define _PyTime_FromTimeAndFold _PyPyTime_FromTimeAndFold
PyAPI_FUNC(struct _object *) _PyTime_FromTimeAndFold(int arg0, int arg1, int arg2, int arg3, struct _object *arg4, int arg5, struct _typeobject *arg6);
#define _PyTuple_Resize _PyPyTuple_Resize
PyAPI_FUNC(int) _PyTuple_Resize(struct _object **arg0, Signed arg1);
#define _PyType_Lookup _PyPyType_Lookup
PyAPI_FUNC(struct _object *) _PyType_Lookup(struct _typeobject *arg0, struct _object *arg1);
#define _PyUnicode_Ready _PyPyUnicode_Ready
PyAPI_FUNC(int) _PyUnicode_Ready(PyObject * arg0);
#define _Py_HashDouble _PyPy_HashDouble
PyAPI_FUNC(Signed) _Py_HashDouble(double arg0);
#define _Py_HashPointer _PyPy_HashPointer
PyAPI_FUNC(Signed) _Py_HashPointer(void *arg0);
#define _Py_IsFinalizing _PyPy_IsFinalizing
PyAPI_FUNC(int) _Py_IsFinalizing(void);
#define _Py_strhex _PyPy_strhex
PyAPI_FUNC(PyObject *) _Py_strhex(char const * arg0, Py_ssize_t arg1);
#define _Py_strhex_bytes _PyPy_strhex_bytes
PyAPI_FUNC(PyObject *) _Py_strhex_bytes(char const * arg0, Py_ssize_t arg1);
#define _Py_NoneStruct _PyPy_NoneStruct
PyAPI_DATA(PyObject) _Py_NoneStruct;
#define _Py_TrueStruct _PyPy_TrueStruct
PyAPI_DATA(PyObject) _Py_TrueStruct;
#define _Py_FalseStruct _PyPy_FalseStruct
PyAPI_DATA(PyObject) _Py_FalseStruct;
#define _Py_NotImplementedStruct _PyPy_NotImplementedStruct
PyAPI_DATA(PyObject) _Py_NotImplementedStruct;
#define _Py_EllipsisObject _PyPy_EllipsisObject
PyAPI_DATA(PyObject) _Py_EllipsisObject;
#define PyDateTimeAPI PyPyDateTimeAPI
PyAPI_DATA(PyDateTime_CAPI*) PyDateTimeAPI;
#define PyExc_ArithmeticError PyPyExc_ArithmeticError
PyAPI_DATA(PyObject*) PyExc_ArithmeticError;
#define PyExc_AssertionError PyPyExc_AssertionError
PyAPI_DATA(PyObject*) PyExc_AssertionError;
#define PyExc_AttributeError PyPyExc_AttributeError
PyAPI_DATA(PyObject*) PyExc_AttributeError;
#define PyExc_BaseException PyPyExc_BaseException
PyAPI_DATA(PyObject*) PyExc_BaseException;
#define PyExc_BlockingIOError PyPyExc_BlockingIOError
PyAPI_DATA(PyObject*) PyExc_BlockingIOError;
#define PyExc_BrokenPipeError PyPyExc_BrokenPipeError
PyAPI_DATA(PyObject*) PyExc_BrokenPipeError;
#define PyExc_BufferError PyPyExc_BufferError
PyAPI_DATA(PyObject*) PyExc_BufferError;
#define PyExc_BytesWarning PyPyExc_BytesWarning
PyAPI_DATA(PyObject*) PyExc_BytesWarning;
#define PyExc_ChildProcessError PyPyExc_ChildProcessError
PyAPI_DATA(PyObject*) PyExc_ChildProcessError;
#define PyExc_ConnectionAbortedError PyPyExc_ConnectionAbortedError
PyAPI_DATA(PyObject*) PyExc_ConnectionAbortedError;
#define PyExc_ConnectionError PyPyExc_ConnectionError
PyAPI_DATA(PyObject*) PyExc_ConnectionError;
#define PyExc_ConnectionRefusedError PyPyExc_ConnectionRefusedError
PyAPI_DATA(PyObject*) PyExc_ConnectionRefusedError;
#define PyExc_ConnectionResetError PyPyExc_ConnectionResetError
PyAPI_DATA(PyObject*) PyExc_ConnectionResetError;
#define PyExc_DeprecationWarning PyPyExc_DeprecationWarning
PyAPI_DATA(PyObject*) PyExc_DeprecationWarning;
#define PyExc_EOFError PyPyExc_EOFError
PyAPI_DATA(PyObject*) PyExc_EOFError;
#define PyExc_Exception PyPyExc_Exception
PyAPI_DATA(PyObject*) PyExc_Exception;
#define PyExc_FileExistsError PyPyExc_FileExistsError
PyAPI_DATA(PyObject*) PyExc_FileExistsError;
#define PyExc_FileNotFoundError PyPyExc_FileNotFoundError
PyAPI_DATA(PyObject*) PyExc_FileNotFoundError;
#define PyExc_FloatingPointError PyPyExc_FloatingPointError
PyAPI_DATA(PyObject*) PyExc_FloatingPointError;
#define PyExc_FutureWarning PyPyExc_FutureWarning
PyAPI_DATA(PyObject*) PyExc_FutureWarning;
#define PyExc_GeneratorExit PyPyExc_GeneratorExit
PyAPI_DATA(PyObject*) PyExc_GeneratorExit;
#define PyExc_ImportError PyPyExc_ImportError
PyAPI_DATA(PyObject*) PyExc_ImportError;
#define PyExc_ImportWarning PyPyExc_ImportWarning
PyAPI_DATA(PyObject*) PyExc_ImportWarning;
#define PyExc_IndentationError PyPyExc_IndentationError
PyAPI_DATA(PyObject*) PyExc_IndentationError;
#define PyExc_IndexError PyPyExc_IndexError
PyAPI_DATA(PyObject*) PyExc_IndexError;
#define PyExc_InterruptedError PyPyExc_InterruptedError
PyAPI_DATA(PyObject*) PyExc_InterruptedError;
#define PyExc_IsADirectoryError PyPyExc_IsADirectoryError
PyAPI_DATA(PyObject*) PyExc_IsADirectoryError;
#define PyExc_KeyError PyPyExc_KeyError
PyAPI_DATA(PyObject*) PyExc_KeyError;
#define PyExc_KeyboardInterrupt PyPyExc_KeyboardInterrupt
PyAPI_DATA(PyObject*) PyExc_KeyboardInterrupt;
#define PyExc_LookupError PyPyExc_LookupError
PyAPI_DATA(PyObject*) PyExc_LookupError;
#define PyExc_MemoryError PyPyExc_MemoryError
PyAPI_DATA(PyObject*) PyExc_MemoryError;
#define PyExc_ModuleNotFoundError PyPyExc_ModuleNotFoundError
PyAPI_DATA(PyObject*) PyExc_ModuleNotFoundError;
#define PyExc_NameError PyPyExc_NameError
PyAPI_DATA(PyObject*) PyExc_NameError;
#define PyExc_NotADirectoryError PyPyExc_NotADirectoryError
PyAPI_DATA(PyObject*) PyExc_NotADirectoryError;
#define PyExc_NotImplementedError PyPyExc_NotImplementedError
PyAPI_DATA(PyObject*) PyExc_NotImplementedError;
#define PyExc_OSError PyPyExc_OSError
PyAPI_DATA(PyObject*) PyExc_OSError;
#define PyExc_OverflowError PyPyExc_OverflowError
PyAPI_DATA(PyObject*) PyExc_OverflowError;
#define PyExc_PendingDeprecationWarning PyPyExc_PendingDeprecationWarning
PyAPI_DATA(PyObject*) PyExc_PendingDeprecationWarning;
#define PyExc_PermissionError PyPyExc_PermissionError
PyAPI_DATA(PyObject*) PyExc_PermissionError;
#define PyExc_ProcessLookupError PyPyExc_ProcessLookupError
PyAPI_DATA(PyObject*) PyExc_ProcessLookupError;
#define PyExc_RecursionError PyPyExc_RecursionError
PyAPI_DATA(PyObject*) PyExc_RecursionError;
#define PyExc_ReferenceError PyPyExc_ReferenceError
PyAPI_DATA(PyObject*) PyExc_ReferenceError;
#define PyExc_ResourceWarning PyPyExc_ResourceWarning
PyAPI_DATA(PyObject*) PyExc_ResourceWarning;
#define PyExc_RuntimeError PyPyExc_RuntimeError
PyAPI_DATA(PyObject*) PyExc_RuntimeError;
#define PyExc_RuntimeWarning PyPyExc_RuntimeWarning
PyAPI_DATA(PyObject*) PyExc_RuntimeWarning;
#define PyExc_StopAsyncIteration PyPyExc_StopAsyncIteration
PyAPI_DATA(PyObject*) PyExc_StopAsyncIteration;
#define PyExc_StopIteration PyPyExc_StopIteration
PyAPI_DATA(PyObject*) PyExc_StopIteration;
#define PyExc_SyntaxError PyPyExc_SyntaxError
PyAPI_DATA(PyObject*) PyExc_SyntaxError;
#define PyExc_SyntaxWarning PyPyExc_SyntaxWarning
PyAPI_DATA(PyObject*) PyExc_SyntaxWarning;
#define PyExc_SystemError PyPyExc_SystemError
PyAPI_DATA(PyObject*) PyExc_SystemError;
#define PyExc_SystemExit PyPyExc_SystemExit
PyAPI_DATA(PyObject*) PyExc_SystemExit;
#define PyExc_TabError PyPyExc_TabError
PyAPI_DATA(PyObject*) PyExc_TabError;
#define PyExc_TimeoutError PyPyExc_TimeoutError
PyAPI_DATA(PyObject*) PyExc_TimeoutError;
#define PyExc_TypeError PyPyExc_TypeError
PyAPI_DATA(PyObject*) PyExc_TypeError;
#define PyExc_UnboundLocalError PyPyExc_UnboundLocalError
PyAPI_DATA(PyObject*) PyExc_UnboundLocalError;
#define PyExc_UnicodeDecodeError PyPyExc_UnicodeDecodeError
PyAPI_DATA(PyObject*) PyExc_UnicodeDecodeError;
#define PyExc_UnicodeEncodeError PyPyExc_UnicodeEncodeError
PyAPI_DATA(PyObject*) PyExc_UnicodeEncodeError;
#define PyExc_UnicodeError PyPyExc_UnicodeError
PyAPI_DATA(PyObject*) PyExc_UnicodeError;
#define PyExc_UnicodeTranslateError PyPyExc_UnicodeTranslateError
PyAPI_DATA(PyObject*) PyExc_UnicodeTranslateError;
#define PyExc_UnicodeWarning PyPyExc_UnicodeWarning
PyAPI_DATA(PyObject*) PyExc_UnicodeWarning;
#define PyExc_UserWarning PyPyExc_UserWarning
PyAPI_DATA(PyObject*) PyExc_UserWarning;
#define PyExc_ValueError PyPyExc_ValueError
PyAPI_DATA(PyObject*) PyExc_ValueError;
#define PyExc_Warning PyPyExc_Warning
PyAPI_DATA(PyObject*) PyExc_Warning;
#define PyExc_ZeroDivisionError PyPyExc_ZeroDivisionError
PyAPI_DATA(PyObject*) PyExc_ZeroDivisionError;
#define PyType_Type PyPyType_Type
PyAPI_DATA(PyTypeObject) PyType_Type;
#define PyBytes_Type PyPyBytes_Type
PyAPI_DATA(PyTypeObject) PyBytes_Type;
#define PyUnicode_Type PyPyUnicode_Type
PyAPI_DATA(PyTypeObject) PyUnicode_Type;
#define PyDict_Type PyPyDict_Type
PyAPI_DATA(PyTypeObject) PyDict_Type;
#define PyDictProxy_Type PyPyDictProxy_Type
PyAPI_DATA(PyTypeObject) PyDictProxy_Type;
#define PyTuple_Type PyPyTuple_Type
PyAPI_DATA(PyTypeObject) PyTuple_Type;
#define PyList_Type PyPyList_Type
PyAPI_DATA(PyTypeObject) PyList_Type;
#define PySet_Type PyPySet_Type
PyAPI_DATA(PyTypeObject) PySet_Type;
#define PyFrozenSet_Type PyPyFrozenSet_Type
PyAPI_DATA(PyTypeObject) PyFrozenSet_Type;
#define PyBool_Type PyPyBool_Type
PyAPI_DATA(PyTypeObject) PyBool_Type;
#define PyFloat_Type PyPyFloat_Type
PyAPI_DATA(PyTypeObject) PyFloat_Type;
#define PyLong_Type PyPyLong_Type
PyAPI_DATA(PyTypeObject) PyLong_Type;
#define PyComplex_Type PyPyComplex_Type
PyAPI_DATA(PyTypeObject) PyComplex_Type;
#define PyByteArray_Type PyPyByteArray_Type
PyAPI_DATA(PyTypeObject) PyByteArray_Type;
#define PyMemoryView_Type PyPyMemoryView_Type
PyAPI_DATA(PyTypeObject) PyMemoryView_Type;
#define PyBaseObject_Type PyPyBaseObject_Type
PyAPI_DATA(PyTypeObject) PyBaseObject_Type;
#define _PyNone_Type _PyPyNone_Type
PyAPI_DATA(PyTypeObject) _PyNone_Type;
#define _PyNotImplemented_Type _PyPyNotImplemented_Type
PyAPI_DATA(PyTypeObject) _PyNotImplemented_Type;
#define PyCell_Type PyPyCell_Type
PyAPI_DATA(PyTypeObject) PyCell_Type;
#define PyModule_Type PyPyModule_Type
PyAPI_DATA(PyTypeObject) PyModule_Type;
#define PyProperty_Type PyPyProperty_Type
PyAPI_DATA(PyTypeObject) PyProperty_Type;
#define PySlice_Type PyPySlice_Type
PyAPI_DATA(PyTypeObject) PySlice_Type;
#define PyStaticMethod_Type PyPyStaticMethod_Type
PyAPI_DATA(PyTypeObject) PyStaticMethod_Type;
#define PyCFunction_Type PyPyCFunction_Type
PyAPI_DATA(PyTypeObject) PyCFunction_Type;
#define PyClassMethodDescr_Type PyPyClassMethodDescr_Type
PyAPI_DATA(PyTypeObject) PyClassMethodDescr_Type;
#define PyGetSetDescr_Type PyPyGetSetDescr_Type
PyAPI_DATA(PyTypeObject) PyGetSetDescr_Type;
#define PyMemberDescr_Type PyPyMemberDescr_Type
PyAPI_DATA(PyTypeObject) PyMemberDescr_Type;
#define PyMethodDescr_Type PyPyMethodDescr_Type
PyAPI_DATA(PyTypeObject) PyMethodDescr_Type;
#define PyWrapperDescr_Type PyPyWrapperDescr_Type
PyAPI_DATA(PyTypeObject) PyWrapperDescr_Type;
#define PyInstanceMethod_Type PyPyInstanceMethod_Type
PyAPI_DATA(PyTypeObject) PyInstanceMethod_Type;
#define PyBufferable_Type PyPyBufferable_Type
PyAPI_DATA(PyTypeObject) PyBufferable_Type;

#undef Signed    /* xxx temporary fix */
#undef Unsigned  /* xxx temporary fix */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pypy_macros.h
================================================
#define Py_FatalError PyPy_FatalError
#define PyOS_snprintf PyPyOS_snprintf
#define PyOS_vsnprintf PyPyOS_vsnprintf
#define PyArg_Parse PyPyArg_Parse
#define PyArg_ParseTuple PyPyArg_ParseTuple
#define PyArg_UnpackTuple PyPyArg_UnpackTuple
#define PyArg_ParseTupleAndKeywords PyPyArg_ParseTupleAndKeywords
#define PyArg_VaParse PyPyArg_VaParse
#define PyArg_VaParseTupleAndKeywords PyPyArg_VaParseTupleAndKeywords
#define _PyArg_NoKeywords _PyPyArg_NoKeywords
#define PyUnicode_FromFormat PyPyUnicode_FromFormat
#define PyUnicode_FromFormatV PyPyUnicode_FromFormatV
#define PyUnicode_AsWideCharString PyPyUnicode_AsWideCharString
#define PyUnicode_GetSize PyPyUnicode_GetSize
#define PyUnicode_GetLength PyPyUnicode_GetLength
#define PyUnicode_FromWideChar PyPyUnicode_FromWideChar
#define PyModule_AddObject PyPyModule_AddObject
#define PyModule_AddIntConstant PyPyModule_AddIntConstant
#define PyModule_AddStringConstant PyPyModule_AddStringConstant
#define PyModule_GetDef PyPyModule_GetDef
#define PyModuleDef_Init PyPyModuleDef_Init
#define PyModule_GetState PyPyModule_GetState
#define Py_BuildValue PyPy_BuildValue
#define Py_VaBuildValue PyPy_VaBuildValue
#define PyTuple_Pack PyPyTuple_Pack
#define _PyArg_Parse_SizeT _PyPyArg_Parse_SizeT
#define _PyArg_ParseTuple_SizeT _PyPyArg_ParseTuple_SizeT
#define _PyArg_ParseTupleAndKeywords_SizeT _PyPyArg_ParseTupleAndKeywords_SizeT
#define _PyArg_VaParse_SizeT _PyPyArg_VaParse_SizeT
#define _PyArg_VaParseTupleAndKeywords_SizeT _PyPyArg_VaParseTupleAndKeywords_SizeT
#define _Py_BuildValue_SizeT _PyPy_BuildValue_SizeT
#define _Py_VaBuildValue_SizeT _PyPy_VaBuildValue_SizeT
#define PyErr_Format PyPyErr_Format
#define PyErr_NewException PyPyErr_NewException
#define PyErr_NewExceptionWithDoc PyPyErr_NewExceptionWithDoc
#define PyErr_WarnFormat PyPyErr_WarnFormat
#define _PyErr_FormatFromCause _PyPyErr_FormatFromCause
#define PySys_WriteStdout PyPySys_WriteStdout
#define PySys_WriteStderr PyPySys_WriteStderr
#define PyEval_CallFunction PyPyEval_CallFunction
#define PyEval_CallMethod PyPyEval_CallMethod
#define PyObject_CallFunction PyPyObject_CallFunction
#define PyObject_CallMethod PyPyObject_CallMethod
#define PyObject_CallFunctionObjArgs PyPyObject_CallFunctionObjArgs
#define PyObject_CallMethodObjArgs PyPyObject_CallMethodObjArgs
#define _PyObject_CallFunction_SizeT _PyPyObject_CallFunction_SizeT
#define _PyObject_CallMethod_SizeT _PyPyObject_CallMethod_SizeT
#define PyObject_DelItemString PyPyObject_DelItemString
#define PyObject_GetBuffer PyPyObject_GetBuffer
#define PyBuffer_Release PyPyBuffer_Release
#define _Py_setfilesystemdefaultencoding _PyPy_setfilesystemdefaultencoding
#define PyCapsule_New PyPyCapsule_New
#define PyCapsule_IsValid PyPyCapsule_IsValid
#define PyCapsule_GetPointer PyPyCapsule_GetPointer
#define PyCapsule_GetName PyPyCapsule_GetName
#define PyCapsule_GetDestructor PyPyCapsule_GetDestructor
#define PyCapsule_GetContext PyPyCapsule_GetContext
#define PyCapsule_SetPointer PyPyCapsule_SetPointer
#define PyCapsule_SetName PyPyCapsule_SetName
#define PyCapsule_SetDestructor PyPyCapsule_SetDestructor
#define PyCapsule_SetContext PyPyCapsule_SetContext
#define PyCapsule_Import PyPyCapsule_Import
#define PyCapsule_Type PyPyCapsule_Type
#define _Py_get_capsule_type _PyPy_get_capsule_type
#define PyComplex_AsCComplex PyPyComplex_AsCComplex
#define PyComplex_FromCComplex PyPyComplex_FromCComplex
#define PyObject_AsReadBuffer PyPyObject_AsReadBuffer
#define PyObject_AsWriteBuffer PyPyObject_AsWriteBuffer
#define PyObject_CheckReadBuffer PyPyObject_CheckReadBuffer
#define PyBuffer_GetPointer PyPyBuffer_GetPointer
#define PyBuffer_ToContiguous PyPyBuffer_ToContiguous
#define PyBuffer_FromContiguous PyPyBuffer_FromContiguous
#define PyImport_ImportModuleLevel PyPyImport_ImportModuleLevel
#define PyOS_getsig PyPyOS_getsig
#define PyOS_setsig PyPyOS_setsig
#define _Py_RestoreSignals _PyPy_RestoreSignals
#define PyThread_get_thread_ident PyPyThread_get_thread_ident
#define PyThread_allocate_lock PyPyThread_allocate_lock
#define PyThread_free_lock PyPyThread_free_lock
#define PyThread_acquire_lock PyPyThread_acquire_lock
#define PyThread_release_lock PyPyThread_release_lock
#define PyThread_create_key PyPyThread_create_key
#define PyThread_delete_key PyPyThread_delete_key
#define PyThread_set_key_value PyPyThread_set_key_value
#define PyThread_get_key_value PyPyThread_get_key_value
#define PyThread_delete_key_value PyPyThread_delete_key_value
#define PyThread_ReInitTLS PyPyThread_ReInitTLS
#define PyThread_init_thread PyPyThread_init_thread
#define PyThread_start_new_thread PyPyThread_start_new_thread
#define PyStructSequence_InitType PyPyStructSequence_InitType
#define PyStructSequence_InitType2 PyPyStructSequence_InitType2
#define PyStructSequence_New PyPyStructSequence_New
#define PyStructSequence_UnnamedField PyPyStructSequence_UnnamedField
#define PyStructSequence_NewType PyPyStructSequence_NewType
#define PyFunction_Type PyPyFunction_Type
#define PyMethod_Type PyPyMethod_Type
#define PyRange_Type PyPyRange_Type
#define PyTraceBack_Type PyPyTraceBack_Type
#define Py_FrozenFlag PyPy_FrozenFlag
#define Py_UnbufferedStdioFlag PyPy_UnbufferedStdioFlag
#define _Py_PackageContext _PyPy_PackageContext
#define PyOS_InputHook PyPyOS_InputHook
#define _Py_PackageContext _PyPy_PackageContext
#define PyMem_RawMalloc PyPyMem_RawMalloc
#define PyMem_RawCalloc PyPyMem_RawCalloc
#define PyMem_RawRealloc PyPyMem_RawRealloc
#define PyMem_RawFree PyPyMem_RawFree
#define PyMem_Malloc PyPyMem_Malloc
#define PyMem_Calloc PyPyMem_Calloc
#define PyMem_Realloc PyPyMem_Realloc
#define PyMem_Free PyPyMem_Free
#define PyObject_CallFinalizerFromDealloc PyPyObject_CallFinalizerFromDealloc
#define PyTraceMalloc_Track PyPyTraceMalloc_Track
#define PyTraceMalloc_Untrack PyPyTraceMalloc_Untrack
#define PyBytes_FromFormat PyPyBytes_FromFormat
#define PyBytes_FromFormatV PyPyBytes_FromFormatV
#define PyType_FromSpec PyPyType_FromSpec
#define Py_IncRef PyPy_IncRef
#define Py_DecRef PyPy_DecRef
#define PyObject_Free PyPyObject_Free
#define PyObject_GC_Del PyPyObject_GC_Del
#define PyType_GenericAlloc PyPyType_GenericAlloc
#define _PyObject_New _PyPyObject_New
#define _PyObject_NewVar _PyPyObject_NewVar
#define _PyObject_GC_Malloc _PyPyObject_GC_Malloc
#define _PyObject_GC_New _PyPyObject_GC_New
#define _PyObject_GC_NewVar _PyPyObject_GC_NewVar
#define PyObject_Init PyPyObject_Init
#define PyObject_InitVar PyPyObject_InitVar
#define PyTuple_New PyPyTuple_New
#define _Py_Dealloc _PyPy_Dealloc
#define PyVectorcall_Call PyPyVectorcall_Call
#define Py_DebugFlag PyPy_DebugFlag
#define Py_InspectFlag PyPy_InspectFlag
#define Py_InteractiveFlag PyPy_InteractiveFlag
#define Py_OptimizeFlag PyPy_OptimizeFlag
#define Py_DontWriteBytecodeFlag PyPy_DontWriteBytecodeFlag
#define Py_NoUserSiteDirectory PyPy_NoUserSiteDirectory
#define Py_NoSiteFlag PyPy_NoSiteFlag
#define Py_IgnoreEnvironmentFlag PyPy_IgnoreEnvironmentFlag
#define Py_VerboseFlag PyPy_VerboseFlag
#define Py_BytesWarningFlag PyPy_BytesWarningFlag
#define Py_QuietFlag PyPy_QuietFlag
#define Py_HashRandomizationFlag PyPy_HashRandomizationFlag
#define Py_IsolatedFlag PyPy_IsolatedFlag
#define SIZEOF_LONG_LONG 8
#define SIZEOF_VOID_P 8
#define SIZEOF_SIZE_T 8
#define SIZEOF_TIME_T 8
#define SIZEOF_LONG 8
#define SIZEOF_SHORT 2
#define SIZEOF_INT 4
#define SIZEOF_FLOAT 4
#define SIZEOF_DOUBLE 8


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pypy_marshal_decl.h
================================================
#include "cpyext_object.h"

#ifdef _WIN64
#define Signed   Py_ssize_t          /* xxx temporary fix */
#define Unsigned unsigned long long  /* xxx temporary fix */
#else
#define Signed   Py_ssize_t     /* xxx temporary fix */
#define Unsigned unsigned long  /* xxx temporary fix */
#endif
        
#define PyMarshal_ReadObjectFromString PyPyMarshal_ReadObjectFromString
PyAPI_FUNC(struct _object *) PyMarshal_ReadObjectFromString(char *arg0, Signed arg1);
#define PyMarshal_WriteObjectToString PyPyMarshal_WriteObjectToString
PyAPI_FUNC(struct _object *) PyMarshal_WriteObjectToString(struct _object *arg0, int arg1);

#undef Signed    /* xxx temporary fix */
#undef Unsigned  /* xxx temporary fix */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pypy_structmember_decl.h
================================================
#include "cpyext_object.h"

#ifdef _WIN64
#define Signed   Py_ssize_t          /* xxx temporary fix */
#define Unsigned unsigned long long  /* xxx temporary fix */
#else
#define Signed   Py_ssize_t     /* xxx temporary fix */
#define Unsigned unsigned long  /* xxx temporary fix */
#endif
        
#define PyMember_GetOne PyPyMember_GetOne
PyAPI_FUNC(struct _object *) PyMember_GetOne(const char *arg0, struct PyMemberDef *arg1);
#define PyMember_SetOne PyPyMember_SetOne
PyAPI_FUNC(int) PyMember_SetOne(char *arg0, struct PyMemberDef *arg1, struct _object *arg2);

#undef Signed    /* xxx temporary fix */
#undef Unsigned  /* xxx temporary fix */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pysignals.h
================================================

/* signal interface */

#ifndef Py_PYSIGNALS_H
#define Py_PYSIGNALS_H
#ifdef __cplusplus
extern "C" {
#endif

typedef void (*PyOS_sighandler_t)(int);

PyAPI_FUNC(PyOS_sighandler_t) PyOS_setsig(int sig, PyOS_sighandler_t handler);
PyAPI_FUNC(PyOS_sighandler_t) PyOS_getsig(int sig);


#ifdef __cplusplus
}
#endif
#endif /* !Py_PYSIGNALS_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pystate.h
================================================
#ifndef Py_PYSTATE_H
#define Py_PYSTATE_H

struct _ts; /* Forward */
struct _is; /* Forward */

typedef struct _is {
    struct _is *next;
} PyInterpreterState;

typedef struct _ts {
    PyInterpreterState *interp;
    PyObject *dict;  /* Stores per-thread state */
} PyThreadState;

#define Py_BEGIN_ALLOW_THREADS { \
			PyThreadState *_save; \
			_save = PyEval_SaveThread();
#define Py_BLOCK_THREADS	PyEval_RestoreThread(_save);
#define Py_UNBLOCK_THREADS	_save = PyEval_SaveThread();
#define Py_END_ALLOW_THREADS	PyEval_RestoreThread(_save); \
		 }

enum {PyGILState_LOCKED, PyGILState_UNLOCKED};
typedef int PyGILState_STATE;

#define PyThreadState_GET() PyThreadState_Get()

#endif /* !Py_PYSTATE_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pystrhex.h
================================================
/* empty */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pystrtod.h
================================================
#ifndef Py_STRTOD_H
#define Py_STRTOD_H

#ifdef __cplusplus
extern "C" {
#endif

/* PyOS_double_to_string's "flags" parameter can be set to 0 or more of: */
#define Py_DTSF_SIGN      0x01 /* always add the sign */
#define Py_DTSF_ADD_DOT_0 0x02 /* if the result is an integer add ".0" */
#define Py_DTSF_ALT       0x04 /* "alternate" formatting. it's format_code
                                  specific */

/* PyOS_double_to_string's "type", if non-NULL, will be set to one of: */
#define Py_DTST_FINITE 0
#define Py_DTST_INFINITE 1
#define Py_DTST_NAN 2

#ifdef __cplusplus
}
#endif

#endif /* !Py_STRTOD_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pythonrun.h
================================================
/* Interfaces to parse and execute pieces of python code */

#ifndef Py_PYTHONRUN_H
#define Py_PYTHONRUN_H
#ifdef __cplusplus
extern "C" {
#endif

PyAPI_FUNC(void) Py_FatalError(const char *msg);

/* taken from Python-3.2.3/Include/pydebug.h */
/* Note: they are always 0 for now, expect Py_DebugFlag which is always 1 */
PyAPI_DATA(int) Py_DebugFlag;
PyAPI_DATA(int) Py_VerboseFlag;
PyAPI_DATA(int) Py_QuietFlag;
PyAPI_DATA(int) Py_InteractiveFlag;
PyAPI_DATA(int) Py_InspectFlag;
PyAPI_DATA(int) Py_OptimizeFlag;
PyAPI_DATA(int) Py_NoSiteFlag;
PyAPI_DATA(int) Py_BytesWarningFlag;
PyAPI_DATA(int) Py_FrozenFlag; /* set when the python is "frozen" */
PyAPI_DATA(int) Py_IgnoreEnvironmentFlag;
PyAPI_DATA(int) Py_DontWriteBytecodeFlag;
PyAPI_DATA(int) Py_NoUserSiteDirectory;
PyAPI_DATA(int) Py_UnbufferedStdioFlag;
PyAPI_DATA(int) Py_HashRandomizationFlag;
PyAPI_DATA(int) Py_IsolatedFlag;

#ifdef _WIN32
PyAPI_DATA(int) Py_LegacyWindowsStdioFlag;
#endif

#define Py_GETENV(s) (Py_IgnoreEnvironmentFlag ? NULL : getenv(s))


typedef struct {
    int cf_flags;  /* bitmask of CO_xxx flags relevant to future */
    int cf_feature_version;  /* minor Python version (PyCF_ONLY_AST) */
} PyCompilerFlags;

#define _PyCompilerFlags_INIT \
    (PyCompilerFlags){.cf_flags = 0, .cf_feature_version = PY_MINOR_VERSION}

#define PyCF_MASK (CO_FUTURE_DIVISION | CO_FUTURE_ABSOLUTE_IMPORT | \
                   CO_FUTURE_WITH_STATEMENT | CO_FUTURE_PRINT_FUNCTION | \
                   CO_FUTURE_UNICODE_LITERALS)
#define PyCF_MASK_OBSOLETE (CO_NESTED)
#define PyCF_SOURCE_IS_UTF8  0x0100
#define PyCF_DONT_IMPLY_DEDENT 0x0200
#define PyCF_ONLY_AST 0x0400

#define Py_CompileString(str, filename, start) Py_CompileStringFlags(str, filename, start, NULL)

/* Stuff with no proper home (yet) */
PyAPI_DATA(int) (*PyOS_InputHook)(void);
typedef int (*_pypy_pyos_inputhook)(void);
PyAPI_FUNC(_pypy_pyos_inputhook) _PyPy_get_PyOS_InputHook(void);

#ifdef __cplusplus
}
#endif
#endif /* !Py_PYTHONRUN_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pythread.h
================================================
#ifndef Py_PYTHREAD_H
#define Py_PYTHREAD_H

#define WITH_THREAD

typedef void *PyThread_type_lock;

#ifdef __cplusplus
extern "C" {
#endif

PyAPI_FUNC(long) PyThread_get_thread_ident(void);

PyAPI_FUNC(PyThread_type_lock) PyThread_allocate_lock(void);
PyAPI_FUNC(void) PyThread_free_lock(PyThread_type_lock);
PyAPI_FUNC(int) PyThread_acquire_lock(PyThread_type_lock, int);
#define WAIT_LOCK	1
#define NOWAIT_LOCK	0
PyAPI_FUNC(void) PyThread_release_lock(PyThread_type_lock);

PyAPI_FUNC(void) PyThread_init_thread(void);
PyAPI_FUNC(long) PyThread_start_new_thread(void (*func)(void *), void *arg);

/* Thread Local Storage (TLS) API */
PyAPI_FUNC(int) PyThread_create_key(void);
PyAPI_FUNC(void) PyThread_delete_key(int);
PyAPI_FUNC(int) PyThread_set_key_value(int, void *);
PyAPI_FUNC(void *) PyThread_get_key_value(int);
PyAPI_FUNC(void) PyThread_delete_key_value(int key);

/* Cleanup after a fork */
PyAPI_FUNC(void) PyThread_ReInitTLS(void);

#if !defined(Py_LIMITED_API) || Py_LIMITED_API+0 >= 0x03070000
/* New in 3.7 */
/* Thread Specific Storage (TSS) API */

typedef struct _Py_tss_t Py_tss_t;  /* opaque */

#ifndef Py_LIMITED_API
#if defined(_POSIX_THREADS)
    /* Darwin needs pthread.h to know type name the pthread_key_t. */
#   include <pthread.h>
#   define NATIVE_TSS_KEY_T     pthread_key_t
#elif defined(_WIN32)
    /* In Windows, native TSS key type is DWORD,
       but hardcode the unsigned long to avoid errors for include directive.
    */
#   define NATIVE_TSS_KEY_T     unsigned long
#else
#   error "Require native threads. See https://bugs.python.org/issue31370"
#endif

/* When Py_LIMITED_API is not defined, the type layout of Py_tss_t is
   exposed to allow static allocation in the API clients.  Even in this case,
   you must handle TSS keys through API functions due to compatibility.
*/
struct _Py_tss_t {
    int _is_initialized;
    NATIVE_TSS_KEY_T _key;
};

#undef NATIVE_TSS_KEY_T

/* When static allocation, you must initialize with Py_tss_NEEDS_INIT. */
#define Py_tss_NEEDS_INIT   {0}
#endif  /* !Py_LIMITED_API */

PyAPI_FUNC(Py_tss_t *) PyThread_tss_alloc(void);
PyAPI_FUNC(void) PyThread_tss_free(Py_tss_t *key);

/* The parameter key must not be NULL. */
PyAPI_FUNC(int) PyThread_tss_is_created(Py_tss_t *key);
PyAPI_FUNC(int) PyThread_tss_create(Py_tss_t *key);
PyAPI_FUNC(void) PyThread_tss_delete(Py_tss_t *key);
PyAPI_FUNC(int) PyThread_tss_set(Py_tss_t *key, void *value);
PyAPI_FUNC(void *) PyThread_tss_get(Py_tss_t *key);
#endif  /* New in 3.7 */

#ifdef __cplusplus
}
#endif

#endif


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/pytime.h
================================================
#ifndef Py_LIMITED_API
#ifndef Py_PYTIME_H
#define Py_PYTIME_H

#include <pyconfig.h> /* include for defines */
#include "object.h"

/**************************************************************************
Symbols and macros to supply platform-independent interfaces to time related
functions and constants
**************************************************************************/
#ifdef __cplusplus
extern "C" {
#endif

/* _PyTime_t: Python timestamp with subsecond precision. It can be used to
   store a duration, and so indirectly a date (related to another date, like
   UNIX epoch). */
typedef int64_t _PyTime_t;
#define _PyTime_MIN PY_LLONG_MIN
#define _PyTime_MAX PY_LLONG_MAX

typedef enum {
    /* Round towards minus infinity (-inf).
       For example, used to read a clock. */
    _PyTime_ROUND_FLOOR=0,
    /* Round towards infinity (+inf).
       For example, used for timeout to wait "at least" N seconds. */
    _PyTime_ROUND_CEILING=1,
    /* Round to nearest with ties going to nearest even integer.
       For example, used to round from a Python float. */
    _PyTime_ROUND_HALF_EVEN=2,
    /* Round away from zero
       For example, used for timeout. _PyTime_ROUND_CEILING rounds
       -1e-9 to 0 milliseconds which causes bpo-31786 issue.
       _PyTime_ROUND_UP rounds -1e-9 to -1 millisecond which keeps
       the timeout sign as expected. select.poll(timeout) must block
       for negative values." */
    _PyTime_ROUND_UP=3,
    /* _PyTime_ROUND_TIMEOUT (an alias for _PyTime_ROUND_UP) should be
       used for timeouts. */
    _PyTime_ROUND_TIMEOUT = _PyTime_ROUND_UP
} _PyTime_round_t;


/* Convert a time_t to a PyLong. */
PyAPI_FUNC(PyObject *) _PyLong_FromTime_t(
    time_t sec);

/* Convert a PyLong to a time_t. */
PyAPI_FUNC(time_t) _PyLong_AsTime_t(
    PyObject *obj);

/* Convert a number of seconds, int or float, to time_t. */
PyAPI_FUNC(int) _PyTime_ObjectToTime_t(
    PyObject *obj,
    time_t *sec,
    _PyTime_round_t);

/* Convert a number of seconds, int or float, to a timeval structure.
   usec is in the range [0; 999999] and rounded towards zero.
   For example, -1.2 is converted to (-2, 800000). */
PyAPI_FUNC(int) _PyTime_ObjectToTimeval(
    PyObject *obj,
    time_t *sec,
    long *usec,
    _PyTime_round_t);

/* Convert a number of seconds, int or float, to a timespec structure.
   nsec is in the range [0; 999999999] and rounded towards zero.
   For example, -1.2 is converted to (-2, 800000000). */
PyAPI_FUNC(int) _PyTime_ObjectToTimespec(
    PyObject *obj,
    time_t *sec,
    long *nsec,
    _PyTime_round_t);


/* Create a timestamp from a number of seconds. */
PyAPI_FUNC(_PyTime_t) _PyTime_FromSeconds(int seconds);

/* Macro to create a timestamp from a number of seconds, no integer overflow.
   Only use the macro for small values, prefer _PyTime_FromSeconds(). */
#define _PYTIME_FROMSECONDS(seconds) \
            ((_PyTime_t)(seconds) * (1000 * 1000 * 1000))

/* Create a timestamp from a number of nanoseconds. */
PyAPI_FUNC(_PyTime_t) _PyTime_FromNanoseconds(_PyTime_t ns);

/* Create a timestamp from nanoseconds (Python int). */
PyAPI_FUNC(int) _PyTime_FromNanosecondsObject(_PyTime_t *t,
    PyObject *obj);

/* Convert a number of seconds (Python float or int) to a timetamp.
   Raise an exception and return -1 on error, return 0 on success. */
PyAPI_FUNC(int) _PyTime_FromSecondsObject(_PyTime_t *t,
    PyObject *obj,
    _PyTime_round_t round);

/* Convert a number of milliseconds (Python float or int, 10^-3) to a timetamp.
   Raise an exception and return -1 on error, return 0 on success. */
PyAPI_FUNC(int) _PyTime_FromMillisecondsObject(_PyTime_t *t,
    PyObject *obj,
    _PyTime_round_t round);

/* Convert a timestamp to a number of seconds as a C double. */
PyAPI_FUNC(double) _PyTime_AsSecondsDouble(_PyTime_t t);

/* Convert timestamp to a number of milliseconds (10^-3 seconds). */
PyAPI_FUNC(_PyTime_t) _PyTime_AsMilliseconds(_PyTime_t t,
    _PyTime_round_t round);

/* Convert timestamp to a number of microseconds (10^-6 seconds). */
PyAPI_FUNC(_PyTime_t) _PyTime_AsMicroseconds(_PyTime_t t,
    _PyTime_round_t round);

/* Convert timestamp to a number of nanoseconds (10^-9 seconds) as a Python int
   object. */
PyAPI_FUNC(PyObject *) _PyTime_AsNanosecondsObject(_PyTime_t t);

/* Convert a timestamp to a timeval structure (microsecond resolution).
   tv_usec is always positive.
   Raise an exception and return -1 if the conversion overflowed,
   return 0 on success. */
PyAPI_FUNC(int) _PyTime_AsTimeval(_PyTime_t t,
    struct timeval *tv,
    _PyTime_round_t round);

/* Similar to _PyTime_AsTimeval(), but don't raise an exception on error. */
PyAPI_FUNC(int) _PyTime_AsTimeval_noraise(_PyTime_t t,
    struct timeval *tv,
    _PyTime_round_t round);

/* Convert a timestamp to a number of seconds (secs) and microseconds (us).
   us is always positive. This function is similar to _PyTime_AsTimeval()
   except that secs is always a time_t type, whereas the timeval structure
   uses a C long for tv_sec on Windows.
   Raise an exception and return -1 if the conversion overflowed,
   return 0 on success. */
PyAPI_FUNC(int) _PyTime_AsTimevalTime_t(
    _PyTime_t t,
    time_t *secs,
    int *us,
    _PyTime_round_t round);

#if defined(HAVE_CLOCK_GETTIME) || defined(HAVE_KQUEUE)
/* Convert a timestamp to a timespec structure (nanosecond resolution).
   tv_nsec is always positive.
   Raise an exception and return -1 on error, return 0 on success. */
PyAPI_FUNC(int) _PyTime_AsTimespec(_PyTime_t t, struct timespec *ts);
#endif

/* Get the current time from the system clock.

   The function cannot fail. _PyTime_Init() ensures that the system clock
   works. */
PyAPI_FUNC(_PyTime_t) _PyTime_GetSystemClock(void);

/* Get the time of a monotonic clock, i.e. a clock that cannot go backwards.
   The clock is not affected by system clock updates. The reference point of
   the returned value is undefined, so that only the difference between the
   results of consecutive calls is valid.

   The function cannot fail. _PyTime_Init() ensures that a monotonic clock
   is available and works. */
PyAPI_FUNC(_PyTime_t) _PyTime_GetMonotonicClock(void);


/* Structure used by time.get_clock_info() */
typedef struct {
    const char *implementation;
    int monotonic;
    int adjustable;
    double resolution;
} _Py_clock_info_t;

/* Get the current time from the system clock.
 * Fill clock information if info is not NULL.
 * Raise an exception and return -1 on error, return 0 on success.
 */
PyAPI_FUNC(int) _PyTime_GetSystemClockWithInfo(
    _PyTime_t *t,
    _Py_clock_info_t *info);

/* Get the time of a monotonic clock, i.e. a clock that cannot go backwards.
   The clock is not affected by system clock updates. The reference point of
   the returned value is undefined, so that only the difference between the
   results of consecutive calls is valid.

   Fill info (if set) with information of the function used to get the time.

   Return 0 on success, raise an exception and return -1 on error. */
PyAPI_FUNC(int) _PyTime_GetMonotonicClockWithInfo(
    _PyTime_t *t,
    _Py_clock_info_t *info);


/* Initialize time.
   Return 0 on success, raise an exception and return -1 on error. */
PyAPI_FUNC(int) _PyTime_Init(void);

/* Converts a timestamp to the Gregorian time, using the local time zone.
   Return 0 on success, raise an exception and return -1 on error. */
PyAPI_FUNC(int) _PyTime_localtime(time_t t, struct tm *tm);

/* Converts a timestamp to the Gregorian time, assuming UTC.
   Return 0 on success, raise an exception and return -1 on error. */
PyAPI_FUNC(int) _PyTime_gmtime(time_t t, struct tm *tm);

#ifdef __cplusplus
}
#endif

#endif /* Py_PYTIME_H */
#endif /* Py_LIMITED_API */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/setobject.h
================================================

/* set object interface */

#ifndef Py_SETOBJECT_H
#define Py_SETOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

typedef struct {
    PyObject_HEAD
    PyObject *_tmplist; /* a private place to put values during _PySet_Next */
} PySetObject;

#ifdef __cplusplus
}
#endif
#endif /* !Py_SETOBJECT_H */



================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/sliceobject.h
================================================
#ifndef Py_SLICEOBJECT_H
#define Py_SLICEOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

/* The unique ellipsis object "..." */

PyAPI_DATA(PyObject) _Py_EllipsisObject; /* Don't use this directly */

#define Py_Ellipsis (&_Py_EllipsisObject)

typedef struct {
    PyObject_HEAD
    PyObject *start;
    PyObject *stop;
    PyObject *step;
} PySliceObject;

#define PySlice_Check(op) (Py_TYPE(op) == &PySlice_Type)

PyAPI_FUNC(Py_ssize_t) PySlice_AdjustIndices(Py_ssize_t length,
                         Py_ssize_t *start, Py_ssize_t *stop, Py_ssize_t step);

#ifdef __cplusplus
}
#endif
#endif /* !Py_SLICEOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/structmember.h
================================================
#ifndef Py_STRUCTMEMBER_H
#define Py_STRUCTMEMBER_H
#ifdef __cplusplus
extern "C" {
#endif


/* Interface to map C struct members to Python object attributes */

#include <stddef.h> /* For offsetof */

/* The offsetof() macro calculates the offset of a structure member
   in its structure.  Unfortunately this cannot be written down
   portably, hence it is provided by a Standard C header file.
   For pre-Standard C compilers, here is a version that usually works
   (but watch out!): */

#ifndef offsetof
#define offsetof(type, member) ( (int) & ((type*)0) -> member )
#endif


/* Types */
#define T_SHORT         0
#define T_INT           1
#define T_LONG          2
#define T_FLOAT         3
#define T_DOUBLE        4
#define T_STRING        5
#define T_OBJECT        6
/* XXX the ordering here is weird for binary compatibility */
#define T_CHAR          7       /* 1-character string */
#define T_BYTE          8       /* 8-bit signed int */
/* unsigned variants: */
#define T_UBYTE         9
#define T_USHORT        10
#define T_UINT          11
#define T_ULONG         12

/* Added by Jack: strings contained in the structure */
#define T_STRING_INPLACE        13

/* Added by Lillo: bools contained in the structure (assumed char) */
#define T_BOOL          14

#define T_OBJECT_EX     16      /* Like T_OBJECT, but raises AttributeError
                   when the value is NULL, instead of
                   converting to None. */
#ifdef HAVE_LONG_LONG
#define T_LONGLONG      17
#define T_ULONGLONG      18
#endif /* HAVE_LONG_LONG */

#define T_PYSSIZET       19 /* Py_ssize_t */

/* Flags. These constants are also in structmemberdefs.py. */
#define READONLY        1
#define RO              READONLY                /* Shorthand */
#define READ_RESTRICTED 2
#define PY_WRITE_RESTRICTED 4
#define RESTRICTED      (READ_RESTRICTED | PY_WRITE_RESTRICTED)


/* API functions. */
/* Don't include them while building PyPy, RPython also generated signatures
 * which are similar but not identical. */
#ifndef PYPY_STANDALONE
#include "pypy_structmember_decl.h"
#endif


#ifdef __cplusplus
}
#endif
#endif /* !Py_STRUCTMEMBER_H */



================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/structseq.h
================================================

/* Tuple object interface */

#ifndef Py_STRUCTSEQ_H
#define Py_STRUCTSEQ_H
#ifdef __cplusplus
extern "C" {
#endif

typedef struct PyStructSequence_Field {
    const char *name;
    const char *doc;
} PyStructSequence_Field;

typedef struct PyStructSequence_Desc {
    const char *name;
    const char *doc;
    struct PyStructSequence_Field *fields;
    int n_in_sequence;
} PyStructSequence_Desc;

PyAPI_DATA(char *) PyStructSequence_UnnamedField;

PyAPI_FUNC(void) PyStructSequence_InitType(PyTypeObject *type,
                                           PyStructSequence_Desc *desc);

PyAPI_FUNC(int) PyStructSequence_InitType2(PyTypeObject *type,
                                           PyStructSequence_Desc *desc);

PyAPI_FUNC(PyTypeObject*) PyStructSequence_NewType(PyStructSequence_Desc *desc);

PyAPI_FUNC(PyObject *) PyStructSequence_New(PyTypeObject* type);

typedef struct {
	PyObject_VAR_HEAD
	PyObject *ob_item[1];
} PyStructSequence;

/* Macro, *only* to be used to fill in brand new objects */
#define PyStructSequence_SET_ITEM(op, i, v) PyTuple_SET_ITEM(op, i, v)

#define PyStructSequence_GET_ITEM(op, i) PyTuple_GET_ITEM(op, i)

#ifdef __cplusplus
}
#endif
#endif /* !Py_STRUCTSEQ_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/sysmodule.h
================================================
#ifndef Py_SYSMODULE_H
#define Py_SYSMODULE_H
#ifdef __cplusplus
extern "C" {
#endif

PyAPI_FUNC(void) PySys_WriteStdout(const char *format, ...);
PyAPI_FUNC(void) PySys_WriteStderr(const char *format, ...);

#ifdef __cplusplus
}
#endif
#endif /* !Py_SYSMODULE_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/traceback.h
================================================
#ifndef Py_TRACEBACK_H
#define Py_TRACEBACK_H
#ifdef __cplusplus
extern "C" {
#endif

struct _frame;

typedef struct _traceback {
        PyObject_HEAD
        struct _traceback *tb_next;
        struct _frame *tb_frame;
        int tb_lasti;
        int tb_lineno;
} PyTracebackObject;

#ifdef __cplusplus
}
#endif
#endif /* !Py_TRACEBACK_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/tupleobject.h
================================================

/* Tuple object interface */

#ifndef Py_TUPLEOBJECT_H
#define Py_TUPLEOBJECT_H
#ifdef __cplusplus
extern "C" {
#endif

typedef struct {
    PyObject_VAR_HEAD
    PyObject *ob_item[1];
    /* ob_item contains space for 'ob_size' elements.
     * Items must normally not be NULL, except during construction when
     * the tuple is not yet visible outside the function that builds it.
     */
} PyTupleObject;

PyAPI_FUNC(PyObject *) PyTuple_New(Py_ssize_t size);
PyAPI_FUNC(void) _PyPy_tuple_dealloc(PyObject *);
PyAPI_FUNC(PyObject *) _PyPy_tuple_new(PyTypeObject *type, PyObject *args, PyObject *kwds);

/* defined in varargswrapper.c */
PyAPI_FUNC(PyObject *) PyTuple_Pack(Py_ssize_t, ...);


/* Macro, trading safety for speed */
#define PyTuple_GET_ITEM(op, i) (((PyTupleObject *)(op))->ob_item[i])
#define PyTuple_GET_SIZE(op)    Py_SIZE(op)

/* Macro, *only* to be used to fill in brand new tuples */
#define PyTuple_SET_ITEM(op, i, v) (((PyTupleObject *)(op))->ob_item[i] = v)

#define PyTuple_Check(op) \
		 PyType_FastSubclass(Py_TYPE(op), Py_TPFLAGS_TUPLE_SUBCLASS)
#define PyTuple_CheckExact(op) (Py_TYPE(op) == &PyTuple_Type)

#define _PyTuple_CAST(op) (assert(PyTuple_Check(op)), (PyTupleObject *)(op))
#define _PyTuple_ITEMS(op) (_PyTuple_CAST(op)->ob_item)


#ifdef __cplusplus
}
#endif
#endif /* !Py_TUPLEOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/typeslots.h
================================================
/* Do not renumber the file; these numbers are part of the stable ABI. */
#define Py_mp_ass_subscript 3
#define Py_mp_length 4
#define Py_mp_subscript 5
#define Py_nb_absolute 6
#define Py_nb_add 7
#define Py_nb_and 8
#define Py_nb_bool 9
#define Py_nb_divmod 10
#define Py_nb_float 11
#define Py_nb_floor_divide 12
#define Py_nb_index 13
#define Py_nb_inplace_add 14
#define Py_nb_inplace_and 15
#define Py_nb_inplace_floor_divide 16
#define Py_nb_inplace_lshift 17
#define Py_nb_inplace_multiply 18
#define Py_nb_inplace_or 19
#define Py_nb_inplace_power 20
#define Py_nb_inplace_remainder 21
#define Py_nb_inplace_rshift 22
#define Py_nb_inplace_subtract 23
#define Py_nb_inplace_true_divide 24
#define Py_nb_inplace_xor 25
#define Py_nb_int 26
#define Py_nb_invert 27
#define Py_nb_lshift 28
#define Py_nb_multiply 29
#define Py_nb_negative 30
#define Py_nb_or 31
#define Py_nb_positive 32
#define Py_nb_power 33
#define Py_nb_remainder 34
#define Py_nb_rshift 35
#define Py_nb_subtract 36
#define Py_nb_true_divide 37
#define Py_nb_xor 38
#define Py_sq_ass_item 39
#define Py_sq_concat 40
#define Py_sq_contains 41
#define Py_sq_inplace_concat 42
#define Py_sq_inplace_repeat 43
#define Py_sq_item 44
#define Py_sq_length 45
#define Py_sq_repeat 46
#define Py_tp_alloc 47
#define Py_tp_base 48
#define Py_tp_bases 49
#define Py_tp_call 50
#define Py_tp_clear 51
#define Py_tp_dealloc 52
#define Py_tp_del 53
#define Py_tp_descr_get 54
#define Py_tp_descr_set 55
#define Py_tp_doc 56
#define Py_tp_getattr 57
#define Py_tp_getattro 58
#define Py_tp_hash 59
#define Py_tp_init 60
#define Py_tp_is_gc 61
#define Py_tp_iter 62
#define Py_tp_iternext 63
#define Py_tp_methods 64
#define Py_tp_new 65
#define Py_tp_repr 66
#define Py_tp_richcompare 67
#define Py_tp_setattr 68
#define Py_tp_setattro 69
#define Py_tp_str 70
#define Py_tp_traverse 71
#define Py_tp_members 72
#define Py_tp_getset 73
#define Py_tp_free 74
#define Py_nb_matrix_multiply 75
#define Py_nb_inplace_matrix_multiply 76
#define Py_am_await 77
#define Py_am_aiter 78
#define Py_am_anext 79
/* New in 3.5 */
#define Py_tp_finalize 80


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/unicodeobject.h
================================================
#ifndef Py_UNICODEOBJECT_H
#define Py_UNICODEOBJECT_H

#ifndef SIZEOF_WCHAR_T
#error Must define SIZEOF_WCHAR_T
#endif

#define Py_UNICODE_SIZE SIZEOF_WCHAR_T

/* If wchar_t can be used for UCS-4 storage, set Py_UNICODE_WIDE.
   Otherwise, Unicode strings are stored as UCS-2 (with limited support
   for UTF-16) */

#if Py_UNICODE_SIZE >= 4
#define Py_UNICODE_WIDE
#endif

/* Set these flags if the platform has "wchar.h" and the
   wchar_t type is a 16-bit unsigned type */
/* #define HAVE_WCHAR_H */
/* #define HAVE_USABLE_WCHAR_T */

#ifdef HAVE_WCHAR_H
/* Work around a cosmetic bug in BSDI 4.x wchar.h; thanks to Thomas Wouters */
# ifdef _HAVE_BSDI
#  include <time.h>
# endif
#  include <wchar.h>
#endif

#ifdef __cplusplus
extern "C" {
#endif

#include "cpyext_unicodeobject.h"

#define PyUnicode_Check(op) \
    PyType_FastSubclass(Py_TYPE(op), Py_TPFLAGS_UNICODE_SUBCLASS)
#define PyUnicode_CheckExact(op) (Py_TYPE(op) == &PyUnicode_Type)




/* Fast access macros */
#ifndef Py_LIMITED_API

#define PyUnicode_WSTR_LENGTH(op) \
    (PyUnicode_IS_COMPACT_ASCII(op) ?                  \
     ((PyASCIIObject*)op)->length :                    \
     ((PyCompactUnicodeObject*)op)->wstr_length)

/* Returns the deprecated Py_UNICODE representation's size in code units
   (this includes surrogate pairs as 2 units).
   If the Py_UNICODE representation is not available, it will be computed
   on request.  Use PyUnicode_GET_LENGTH() for the length in code points. */

#define PyUnicode_GET_SIZE(op)                       \
    (assert(PyUnicode_Check(op)),                    \
     (((PyASCIIObject *)(op))->wstr) ?               \
      PyUnicode_WSTR_LENGTH(op) :                    \
      ((void)PyUnicode_AsUnicode((PyObject *)(op)),  \
       assert(((PyASCIIObject *)(op))->wstr),        \
       PyUnicode_WSTR_LENGTH(op)))

#define PyUnicode_GET_DATA_SIZE(op) \
    (PyUnicode_GET_SIZE(op) * Py_UNICODE_SIZE)

/* Alias for PyUnicode_AsUnicode().  This will create a wchar_t/Py_UNICODE
   representation on demand.  Using this macro is very inefficient now,
   try to port your code to use the new PyUnicode_*BYTE_DATA() macros or
   use PyUnicode_WRITE() and PyUnicode_READ(). */

#define PyUnicode_AS_UNICODE(op) \
     ((((PyASCIIObject *)(op))->wstr) ? (((PyASCIIObject *)(op))->wstr) : \
      PyUnicode_AsUnicode((PyObject *)(op)))

#define PyUnicode_AS_DATA(op) \
    ((const char *)(PyUnicode_AS_UNICODE(op)))


/* --- Flexible String Representation Helper Macros (PEP 393) -------------- */

/* Values for PyASCIIObject.state: */

/* Interning state. */
#define SSTATE_NOT_INTERNED 0
#define SSTATE_INTERNED_MORTAL 1
#define SSTATE_INTERNED_IMMORTAL 2

/* Return true if the string contains only ASCII characters, or 0 if not. The
   string may be compact (PyUnicode_IS_COMPACT_ASCII) or not, but must be
   ready. */
#define PyUnicode_IS_ASCII(op)                   \
    (assert(PyUnicode_Check(op)),                \
     assert(PyUnicode_IS_READY(op)),             \
     ((PyASCIIObject*)op)->state.ascii)

/* Return true if the string is compact or 0 if not.
   No type checks or Ready calls are performed. */
#define PyUnicode_IS_COMPACT(op) \
    (((PyASCIIObject*)(op))->state.compact)

/* Return true if the string is a compact ASCII string (use PyASCIIObject
   structure), or 0 if not.  No type checks or Ready calls are performed. */
#define PyUnicode_IS_COMPACT_ASCII(op)                 \
    (((PyASCIIObject*)op)->state.ascii && PyUnicode_IS_COMPACT(op))

enum PyUnicode_Kind {
/* String contains only wstr byte characters.  This is only possible
   when the string was created with a legacy API and _PyUnicode_Ready()
   has not been called yet.  */
    PyUnicode_WCHAR_KIND = 0,
/* Return values of the PyUnicode_KIND() macro: */
    PyUnicode_1BYTE_KIND = 1,
    PyUnicode_2BYTE_KIND = 2,
    PyUnicode_4BYTE_KIND = 4
};

/* Return pointers to the canonical representation cast to unsigned char,
   Py_UCS2, or Py_UCS4 for direct character access.
   No checks are performed, use PyUnicode_KIND() before to ensure
   these will work correctly. */

#define PyUnicode_1BYTE_DATA(op) ((Py_UCS1*)PyUnicode_DATA(op))
#define PyUnicode_2BYTE_DATA(op) ((Py_UCS2*)PyUnicode_DATA(op))
#define PyUnicode_4BYTE_DATA(op) ((Py_UCS4*)PyUnicode_DATA(op))

/* Return one of the PyUnicode_*_KIND values defined above. */
#define PyUnicode_KIND(op) \
    (assert(PyUnicode_Check(op)), \
     assert(PyUnicode_IS_READY(op)),            \
     ((PyASCIIObject *)(op))->state.kind)

/* Return a void pointer to the raw unicode buffer. */
#define _PyUnicode_COMPACT_DATA(op)                     \
    (PyUnicode_IS_ASCII(op) ?                   \
     ((void*)((PyASCIIObject*)(op) + 1)) :              \
     ((void*)((PyCompactUnicodeObject*)(op) + 1)))

#define _PyUnicode_NONCOMPACT_DATA(op)                  \
    (assert(((PyUnicodeObject*)(op))->data),        \
     ((((PyUnicodeObject *)(op))->data)))

#define PyUnicode_DATA(op) \
    (assert(PyUnicode_Check(op)), \
     PyUnicode_IS_COMPACT(op) ? _PyUnicode_COMPACT_DATA(op) :   \
     _PyUnicode_NONCOMPACT_DATA(op))

/* In the access macros below, "kind" may be evaluated more than once.
   All other macro parameters are evaluated exactly once, so it is safe
   to put side effects into them (such as increasing the index). */

/* Write into the canonical representation, this macro does not do any sanity
   checks and is intended for usage in loops.  The caller should cache the
   kind and data pointers obtained from other macro calls.
   index is the index in the string (starts at 0) and value is the new
   code point value which should be written to that location. */
#define PyUnicode_WRITE(kind, data, index, value) \
    do { \
        switch ((kind)) { \
        case PyUnicode_1BYTE_KIND: { \
            ((Py_UCS1 *)(data))[(index)] = (Py_UCS1)(value); \
            break; \
        } \
        case PyUnicode_2BYTE_KIND: { \
            ((Py_UCS2 *)(data))[(index)] = (Py_UCS2)(value); \
            break; \
        } \
        default: { \
            assert((kind) == PyUnicode_4BYTE_KIND); \
            ((Py_UCS4 *)(data))[(index)] = (Py_UCS4)(value); \
        } \
        } \
    } while (0)

/* Read a code point from the string's canonical representation.  No checks
   or ready calls are performed. */
#define PyUnicode_READ(kind, data, index) \
    ((Py_UCS4) \
    ((kind) == PyUnicode_1BYTE_KIND ? \
        ((const Py_UCS1 *)(data))[(index)] : \
        ((kind) == PyUnicode_2BYTE_KIND ? \
            ((const Py_UCS2 *)(data))[(index)] : \
            ((const Py_UCS4 *)(data))[(index)] \
        ) \
    ))

/* PyUnicode_READ_CHAR() is less efficient than PyUnicode_READ() because it
   calls PyUnicode_KIND() and might call it twice.  For single reads, use
   PyUnicode_READ_CHAR, for multiple consecutive reads callers should
   cache kind and use PyUnicode_READ instead. */
#define PyUnicode_READ_CHAR(unicode, index) \
    (assert(PyUnicode_Check(unicode)),          \
     assert(PyUnicode_IS_READY(unicode)),       \
     (Py_UCS4)                                  \
        (PyUnicode_KIND((unicode)) == PyUnicode_1BYTE_KIND ? \
            ((const Py_UCS1 *)(PyUnicode_DATA((unicode))))[(index)] : \
            (PyUnicode_KIND((unicode)) == PyUnicode_2BYTE_KIND ? \
                ((const Py_UCS2 *)(PyUnicode_DATA((unicode))))[(index)] : \
                ((const Py_UCS4 *)(PyUnicode_DATA((unicode))))[(index)] \
            ) \
        ))

/* Returns the length of the unicode string. The caller has to make sure that
   the string has it's canonical representation set before calling
   this macro.  Call PyUnicode_(FAST_)Ready to ensure that. */
#define PyUnicode_GET_LENGTH(op)                \
    (assert(PyUnicode_Check(op)),               \
     assert(PyUnicode_IS_READY(op)),            \
     ((PyASCIIObject *)(op))->length)


/* Fast check to determine whether an object is ready. Equivalent to
   PyUnicode_IS_COMPACT(op) || ((PyUnicodeObject*)(op))->data.any) */

#define PyUnicode_IS_READY(op) (((PyASCIIObject*)op)->state.ready)

/* PyUnicode_READY() does less work than _PyUnicode_Ready() in the best
   case.  If the canonical representation is not yet set, it will still call
   _PyUnicode_Ready().
   Returns 0 on success and -1 on errors. */
#define PyUnicode_READY(op)                        \
    (assert(PyUnicode_Check(op)),                       \
     (PyUnicode_IS_READY(op) ?                          \
      0 : _PyUnicode_Ready((PyObject *)(op))))

/* Return a maximum character value which is suitable for creating another
   string based on op.  This is always an approximation but more efficient
   than iterating over the string. */
#define PyUnicode_MAX_CHAR_VALUE(op) \
    (assert(PyUnicode_IS_READY(op)),                                    \
     (PyUnicode_IS_ASCII(op) ?                                          \
      (0x7f) :                                                          \
      (PyUnicode_KIND(op) == PyUnicode_1BYTE_KIND ?                     \
       (0xffU) :                                                        \
       (PyUnicode_KIND(op) == PyUnicode_2BYTE_KIND ?                    \
        (0xffffU) :                                                     \
        (0x10ffffU)))))

#endif

/* --- Constants ---------------------------------------------------------- */

/* This Unicode character will be used as replacement character during
   decoding if the errors argument is set to "replace". Note: the
   Unicode character U+FFFD is the official REPLACEMENT CHARACTER in
   Unicode 3.0. */

#define Py_UNICODE_REPLACEMENT_CHARACTER ((Py_UCS4) 0xFFFD)

/* === Public API ========================================================= */

/* Get the length of the Unicode object. */

PyAPI_FUNC(Py_ssize_t) PyUnicode_GetLength(
    PyObject *unicode
);

/* Get the number of Py_UNICODE units in the
   string representation. */

PyAPI_FUNC(Py_ssize_t) PyUnicode_GetSize(
    PyObject *unicode           /* Unicode object */
    );

PyAPI_FUNC(PyObject *) PyUnicode_FromFormatV(
    const char *format,   /* ASCII-encoded string  */
    va_list vargs
    );
PyAPI_FUNC(PyObject *) PyUnicode_FromFormat(
    const char *format,   /* ASCII-encoded string  */
    ...
    );

/* Use only if you know it's a string */
#define PyUnicode_CHECK_INTERNED(op) \
    (((PyASCIIObject *)(op))->state.interned)

/* --- wchar_t support for platforms which support it --------------------- */

#ifdef HAVE_WCHAR_H

/* Create a Unicode Object from the wchar_t buffer w of the given
   size.

   The buffer is copied into the new object. */

PyAPI_FUNC(PyObject*) PyUnicode_FromWideChar(
    const wchar_t *w,           /* wchar_t buffer */
    Py_ssize_t size             /* size of buffer */
    );

/* Convert the Unicode object to a wide character string. The output string
   always ends with a nul character. If size is not NULL, write the number of
   wide characters (excluding the null character) into *size.

   Returns a buffer allocated by PyMem_Malloc() (use PyMem_Free() to free it)
   on success. On error, returns NULL, *size is undefined and raises a
   MemoryError. */

PyAPI_FUNC(wchar_t*) PyUnicode_AsWideCharString(
    PyObject *unicode,          /* Unicode object */
    Py_ssize_t *size            /* number of characters of the result */
    );

#endif

/* === Builtin Codecs =====================================================

   Many of these APIs take two arguments encoding and errors. These
   parameters encoding and errors have the same semantics as the ones
   of the builtin str() API.

   Setting encoding to NULL causes the default encoding (UTF-8) to be used.

   Error handling is set by errors which may also be set to NULL
   meaning to use the default handling defined for the codec. Default
   error handling for all builtin codecs is "strict" (ValueErrors are
   raised).

   The codecs all use a similar interface. Only deviation from the
   generic ones are documented.

*/

/* --- Manage the default encoding ---------------------------------------- */

/* Returns a pointer to the default encoding (UTF-8) of the
   Unicode object unicode and the size of the encoded representation
   in bytes stored in *size.

   In case of an error, no *size is set.

   This function caches the UTF-8 encoded string in the unicodeobject
   and subsequent calls will return the same string.  The memory is released
   when the unicodeobject is deallocated.

   _PyUnicode_AsStringAndSize is a #define for PyUnicode_AsUTF8AndSize to
   support the previous internal function with the same behaviour.

   *** This API is for interpreter INTERNAL USE ONLY and will likely
   *** be removed or changed in the future.

   *** If you need to access the Unicode object as UTF-8 bytes string,
   *** please use PyUnicode_AsUTF8String() instead.
*/

#ifndef Py_LIMITED_API
PyAPI_FUNC(char *) PyUnicode_AsUTF8AndSize(
    PyObject *unicode,
    Py_ssize_t *size);
#define _PyUnicode_AsStringAndSize PyUnicode_AsUTF8AndSize
#endif

/* Returns a pointer to the default encoding (UTF-8) of the
   Unicode object unicode.

   Like PyUnicode_AsUTF8AndSize(), this also caches the UTF-8 representation
   in the unicodeobject.

   _PyUnicode_AsString is a #define for PyUnicode_AsUTF8 to
   support the previous internal function with the same behaviour.

   Use of this API is DEPRECATED since no size information can be
   extracted from the returned data.

   *** This API is for interpreter INTERNAL USE ONLY and will likely
   *** be removed or changed for Python 3.1.

   *** If you need to access the Unicode object as UTF-8 bytes string,
   *** please use PyUnicode_AsUTF8String() instead.

*/

#ifndef Py_LIMITED_API
#define _PyUnicode_AsString PyUnicode_AsUTF8
#endif

Py_LOCAL_INLINE(size_t) Py_UNICODE_strlen(const Py_UNICODE *u)
{
    size_t res = 0;
    while(*u++)
        res++;
    return res;
}

Py_LOCAL_INLINE(int)
Py_UNICODE_strcmp(const Py_UNICODE *s1, const Py_UNICODE *s2)
{
    while (*s1 && *s2 && *s1 == *s2)
        s1++, s2++;
    if (*s1 && *s2)
        return (*s1 < *s2) ? -1 : +1;
    if (*s1)
        return 1;
    if (*s2)
        return -1;
    return 0;
}

#ifdef __cplusplus
}
#endif
#endif /* !Py_UNICODEOBJECT_H */


================================================
File: resource/pypy3.8-v7.3.9-osx64/include/pypy3.8/warnings.h
================================================
#ifndef Py_WARNINGS_H
#define Py_WARNINGS_H
#ifdef __cplusplus
extern "C" {
#endif

#define PyErr_WarnPy3k(msg, stacklevel) 0

PyAPI_FUNC(int) PyErr_WarnFormat(PyObject *category, Py_ssize_t stack_level,
                                 const char *format, ...);

#ifdef __cplusplus
}
#endif
#endif /* !Py_WARNINGS_H */


================================================
File: scripts/build_and_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 




================================================
File: scripts/build_and_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
pypy -m unittest discover

================================================
File: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test


================================================
File: scripts/run_pipeline.sh
================================================
python3 /code/src/component.py | python3 /code/src/process_to_csv.py -c /code/src/output_config.json


================================================
File: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi

================================================
File: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi

================================================
File: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"

================================================
File: src/component.py
================================================
#!/usr/bin/env python
"""Component main class for data extraction.

Executes component endpoint executions based on dependent api_mappings.json file in same path. This file should have
a set structure, see example below.

Essentially at the table table and column level, add "replication-method" of: FULL_TABLE, INCREMENTAL, or LOG_BASED.
If INCREMENTAL, you need to specify a "replication-key".

# Primary To-Do Items
TODO: Schema changes handling in extractor
TODO: Handle schema changes in KBC Storage
TODO: Switch to specifying schema based on array options?

# Secondary To-do Items
TODO: Table Mappings - Handle prior user inputs
TODO: Option to do "one-time" table resync, where just one table resyncs once
TODO: Add testing framework
TODO: Support Ticket for UI for this component (maybe they handle SSL?)
"""
import ast
import base64
import binascii
import copy
import csv
import itertools
import json
import logging
import os
import shutil
import sys
import tempfile
import warnings
from collections import namedtuple
from contextlib import nullcontext
from io import StringIO
from typing import List

from cryptography.utils import CryptographyDeprecationWarning

import table_metadata

with warnings.catch_warnings():
    warnings.filterwarnings('ignore', category=CryptographyDeprecationWarning)
    import paramiko
import pendulum
import pymysql
import yaml
from sshtunnel import SSHTunnelForwarder

from mysql.replication.stream_reader import TableColumnSchemaCache

KEY_SHOW_BIN_LOG_CFG = 'show_binary_log_config'

try:
    import core as core
    import core.metrics as metrics
    import core.datatypes as datatypes

    from core import metadata
    from core.catalog import Catalog, CatalogEntry
    from core.env_handler import KBCEnvHandler
    from core.schema import Schema
    from core.yaml_mappings import convert_yaml_to_json_mapping, make_yaml_mapping_file

    # import mysql.result as result_writer
    import mysql.replication.binlog as binlog
    import mysql.replication.common as common
    import mysql.replication.full_table as full_table
    import mysql.replication.incremental as incremental

    from mysql.client import connect_with_backoff, MySQLConnection
except ImportError:
    import src.core as core
    import src.core.metrics as metrics
    import src.core.datatypes as datatypes

    from src.core import metadata
    from src.core.catalog import Catalog, CatalogEntry
    from src.core.env_handler import KBCEnvHandler
    from src.core.schema import Schema
    from src.core.yaml_mappings import convert_yaml_to_json_mapping, make_yaml_mapping_file

    # import src.mysql.result as result_writer
    import src.mysql.replication.binlog as binlog
    import src.mysql.replication.common as common
    import src.mysql.replication.full_table as full_table
    import src.mysql.replication.incremental as incremental

    from src.mysql.client import connect_with_backoff, MySQLConnection

current_path = os.path.dirname(__file__)
module_path = os.path.dirname(current_path)

# Define mandatory parameter constants, matching Config Schema.
KEY_OBJECTS_ONLY = 'fetchObjectsOnly'
KEY_TABLE_MAPPINGS_JSON = 'inputMappingsJson'
KEY_DATABASES = 'databases'
KEY_MYSQL_HOST = 'host'
KEY_MYSQL_PORT = 'port'
KEY_MYSQL_USER = 'username'
KEY_MYSQL_PWD = '#password'
KEY_INCREMENTAL_SYNC = 'runIncrementalSync'
KEY_OUTPUT_BUCKET = 'outputBucket'
KEY_USE_SSH_TUNNEL = 'sshTunnel'
KEY_USE_SSL = 'ssl'
KEY_MAPPINGS_FILE = 'storageMappingsFile'
KEY_INPUT_MAPPINGS_YAML = 'inputMappingsYaml'
KEY_STATE_JSON = 'base64StateJson'
KEY_APPEND_MODE = 'appendMode'
KEY_HANDLE_BINARY = 'handle_binary'

# Define optional parameters as constants for later use.
KEY_SSH_HOST = 'sshHost'
KEY_SSH_PORT = 'sshPort'
KEY_SSH_PUBLIC_KEY = 'sshPublicKey'
KEY_SSH_PRIVATE_KEY = '#sshBase64PrivateKey'
KEY_SSH_USERNAME = 'sshUser'
KEY_SSL_CA = 'sslCa'
KEY_VERIFY_CERT = 'verifyCert'
KEY_MAX_EXECUTION_TIME = 'maxExecutionTime'

ENV_COMPONENT_ID = 'KBC_COMPONENTID'
ENV_CONFIGURATION_ID = 'KBC_CONFIGID'

MAPPINGS_FILE = 'table_mappings.json'
LOCAL_ADDRESS = '127.0.0.1'
SSH_BIND_PORT = 3307
CONNECT_TIMEOUT = 30
FLUSH_STORE_THRESHOLD = 1000000

# Keep for debugging
KEY_DEBUG = 'debug'
MANDATORY_PARS = (KEY_OBJECTS_ONLY, KEY_MYSQL_HOST, KEY_MYSQL_PORT, KEY_MYSQL_USER, KEY_MYSQL_PWD,
                  KEY_USE_SSH_TUNNEL, KEY_USE_SSL)
MANDATORY_IMAGE_PARS = ()

APP_VERSION = '0.9.4'

pymysql.converters.conversions[pendulum.Pendulum] = pymysql.converters.escape_datetime

# Bin database sub-types by type.
SUPPORTED_STRING_TYPES = {'char', 'enum', 'longtext', 'mediumtext', 'text', 'varchar'}
SUPPORTED_BLOB_TYPES = {"tinyblob", "blob", "mediumblob"}
SUPPORTED_FLOAT_TYPES = {'double', 'float'}
SUPPORTED_DATETIME_TYPES = {'date', 'datetime', 'time', 'timestamp'}
SET_TYPE = 'set'
BYTES_FOR_INTEGER_TYPE = {
    'tinyint': 1,
    'smallint': 2,
    'mediumint': 3,
    'int': 4,
    'bigint': 8
}
BINARY_TYPES = {'binary', 'varbinary'}

TYPES_SUPPORTING_LENGTH = ['CHAR', 'VARCHAR', 'TEXT', 'FLOAT', 'DECIMAL', 'DEC', 'DOUBLE', 'DOUBLE PRECISION']

Column = namedtuple('Column', [
    "table_schema",
    "table_name",
    "column_name",
    "data_type",
    "character_maximum_length",
    "numeric_precision",
    "numeric_scale",
    "column_type",
    "column_key",
    "character_set_name",
    "ordinal_position"
])


def schema_for_column(c):
    """Returns the Schema object for the given Column."""
    data_type = c.data_type.lower()
    column_type = c.column_type.lower()

    inclusion = 'available'
    # We want to automatically include all primary key columns
    if c.column_key.lower() == 'pri':
        inclusion = 'automatic'

    result = Schema(inclusion=inclusion)

    if data_type == 'bit' or column_type.startswith('tinyint(1)'):
        result.type = ['null', 'boolean']

    elif data_type in BYTES_FOR_INTEGER_TYPE:
        result.type = ['null', 'integer']
        bits = BYTES_FOR_INTEGER_TYPE[data_type] * 8
        if 'unsigned' in c.column_type:
            result.minimum = 0
            result.maximum = 2 ** bits - 1
        else:
            result.minimum = 0 - 2 ** (bits - 1)
            result.maximum = 2 ** (bits - 1) - 1

    elif data_type in SUPPORTED_FLOAT_TYPES:
        result.type = ['null', 'number']

    elif data_type == 'json':
        result.type = ['null', 'string']

    elif data_type == 'decimal':
        result.type = ['null', 'number']
        result.multipleOf = 10 ** (0 - c.numeric_scale)
        return result

    elif data_type in SUPPORTED_STRING_TYPES:
        result.type = ['null', 'string']
        result.maxLength = c.character_maximum_length
        result.characterSet = c.character_set_name

    elif data_type in SUPPORTED_BLOB_TYPES:
        result.type = ['null', 'string']
        result.maxLength = c.character_maximum_length
        result.characterSet = c.character_set_name

    elif data_type in SUPPORTED_DATETIME_TYPES:
        result.type = ['null', 'string']
        result.format = 'date-time'

    elif data_type.startswith(SET_TYPE):
        result.type = ['null', 'string']

    elif data_type in BINARY_TYPES:
        result.type = ['null', 'binary']

    else:
        result = Schema(None, inclusion='unsupported', description='Unsupported column type {}'.format(column_type))
    return result


def create_column_metadata(cols):
    """Write metadata to catalog entry for given columns."""
    mdata = {}
    mdata = metadata.write(mdata, (), 'selected-by-default', False)
    for c in cols:
        schema = schema_for_column(c)
        mdata = metadata.write(mdata,
                               ('properties', c.column_name),
                               'selected-by-default',
                               schema.inclusion != 'unsupported')
        mdata = metadata.write(mdata,
                               ('properties', c.column_name),
                               'sql-datatype',
                               c.column_type.lower())
        mdata = metadata.write(mdata,
                               ('properties', c.column_name),
                               'ordinal-position',
                               c.ordinal_position)

    return metadata.to_list(mdata)


def discover_catalog(mysql_conn, config, append_mode):
    """Returns a Catalog describing the structure of the database."""
    filter_dbs_config = config.get(KEY_DATABASES)
    logging.debug('Filtering databases via config to: {}'.format(filter_dbs_config))

    if filter_dbs_config:
        filter_dbs_clause = ",".join(["'{}'".format(db) for db in filter_dbs_config])
        table_schema_clause = "WHERE t.table_schema IN ({})".format(filter_dbs_clause)
    else:
        table_schema_clause = """
        WHERE t.table_schema NOT IN (
        'information_schema',
        'performance_schema',
        'mysql',
        'sys'
        )"""

    with connect_with_backoff(mysql_conn) as open_conn:
        with open_conn.cursor() as cur:
            # TODO: allow views as well, or to choose
            cur.execute("""
            SELECT table_schema,
                   table_name,
                   table_type,
                   table_rows
            FROM information_schema.tables t
                {}
            AND table_type != 'VIEW'
            """.format(table_schema_clause))

            table_info = {}

            for (db, table, table_type, rows) in cur.fetchall():
                if db not in table_info:
                    table_info[db] = {}

                table_info[db][table] = {
                    'row_count': rows,
                    'is_view': table_type == 'VIEW'
                }

                # Get primary keys
                if append_mode is True:
                    table_info[db][table]['primary_keys'] = []
                else:
                    pk_sql = """
                    SELECT
                        k.column_name
                    FROM
                        information_schema.table_constraints t
                        INNER JOIN information_schema.key_column_usage k
                            USING(constraint_name, table_schema, table_name)
                    WHERE
                        t.constraint_type='PRIMARY KEY'
                        AND t.table_schema = '{}'
                        AND t.table_name='{}';""".format(db, table)
                    cur.execute(pk_sql)

                    rec = cur.fetchone()
                    table_primary_keys = []
                    while rec is not None:
                        table_primary_keys.append(rec[0])
                        rec = cur.fetchone()

                    table_info[db][table]['primary_keys'] = table_primary_keys

            cur.execute("""
                SELECT c.table_schema,
                       c.table_name,
                       c.column_name,
                       c.data_type,
                       c.character_maximum_length,
                       c.numeric_precision,
                       c.numeric_scale,
                       c.column_type,
                       c.column_key,
                       c.character_set_name,
                       c.ordinal_position
                    FROM information_schema.columns c JOIN
                    information_schema.tables t ON c.table_schema = t.table_schema and c.table_name = t.table_name
                    {}
                    AND t.table_type != 'VIEW'
                    ORDER BY table_schema, table_name
            """.format(table_schema_clause))

            columns = []
            rec = cur.fetchone()
            while rec is not None:
                columns.append(Column(*rec))
                rec = cur.fetchone()

            entries = []
            logging.debug(f"Table info: {table_info}")
            for (k, cols) in itertools.groupby(columns, lambda c: (c.table_schema, c.table_name)):
                cols = list(cols)
                (table_schema, table_name) = k
                schema = Schema(type='object', properties={c.column_name: schema_for_column(c) for c in cols})
                md = create_column_metadata(cols)
                md_map = metadata.to_map(md)

                md_map = metadata.write(md_map, (), 'database-name', table_schema)

                is_view = table_info[table_schema][table_name]['is_view']
                primary_keys = table_info[table_schema][table_name].get('primary_keys')

                if table_schema in table_info and table_name in table_info[table_schema]:
                    row_count = table_info[table_schema][table_name].get('row_count')

                    if row_count is not None:
                        md_map = metadata.write(md_map, (), 'row-count', row_count)

                    md_map = metadata.write(md_map, (), 'is-view', is_view)

                def column_is_key_prop(c, s):
                    return c.column_key == 'PRI' and s.properties[c.column_name].inclusion != 'unsupported'

                key_properties = [c.column_name for c in cols if column_is_key_prop(c, schema)]

                if not is_view:
                    md_map = metadata.write(md_map, (), 'table-key-properties', key_properties)

                binary_columns = []
                for column in cols:
                    if column.data_type.lower() == 'binary':
                        binary_columns += [column.column_name]
                entry = CatalogEntry(table=table_name, stream=table_name, metadata=metadata.to_list(md_map),
                                     tap_stream_id=common.generate_tap_stream_id(table_schema, table_name),
                                     schema=schema, primary_keys=primary_keys, database=table_schema,
                                     binary_columns=binary_columns)

                entries.append(entry)

    return Catalog(entries)


def do_discover(mysql_conn, config, append_mode):
    return discover_catalog(mysql_conn, config, append_mode=append_mode).dumps()


def desired_columns(selected, table_schema, table_name: str = ''):
    """Return the set of column names we need to include in the SELECT.
    selected - set of column names marked as selected in the input catalog
    table_schema - the most recently discovered Schema for the table
    """
    all_columns = set()
    available = set()
    automatic = set()
    unsupported = dict()

    for column, column_schema in table_schema.properties.items():
        all_columns.add(column)
        inclusion = column_schema.inclusion
        if inclusion == 'automatic':
            automatic.add(column)
        elif inclusion == 'available':
            available.add(column)
        elif inclusion == 'unsupported':
            unsupported[column] = column_schema
        else:
            raise Exception('Unknown inclusion ' + inclusion)

    selected_but_unsupported = selected.intersection(list(unsupported.keys()))
    if selected_but_unsupported:
        logging.warning(f'Columns in table {table_name} were selected but are not supported, skipping. '
                        f'Invalid columns:  {[f"{c}:{unsupported[c]}" for c in selected_but_unsupported]}')

    selected_but_nonexistent = selected.difference(all_columns)
    if selected_but_nonexistent:
        logging.warning(
            'Columns %s were selected but do not exist.',
            selected_but_nonexistent)

    not_selected_but_automatic = automatic.difference(selected)
    if not_selected_but_automatic:
        logging.warning(
            'Columns %s are primary keys but were not selected. Adding them.',
            not_selected_but_automatic)

    return selected.intersection(available).union(automatic), all_columns


def log_engine(mysql_conn, catalog_entry):
    is_view = common.get_is_view(catalog_entry)
    database_name = common.get_database_name(catalog_entry)

    if is_view:
        logging.info("Beginning sync for view %s.%s", database_name, catalog_entry.table)
    else:
        with connect_with_backoff(mysql_conn) as open_conn:
            with open_conn.cursor() as cur:
                cur.execute("""
                    SELECT engine
                      FROM information_schema.tables
                     WHERE table_schema = %s
                       AND table_name   = %s
                """, (database_name, catalog_entry.table))

                row = cur.fetchone()

                if row:
                    logging.info("Beginning sync for %s table %s.%s", row[0], database_name, catalog_entry.table)


def is_valid_currently_syncing_stream(selected_stream, state):
    stream_metadata = metadata.to_map(selected_stream.metadata)
    replication_method = stream_metadata.get((), {}).get('replication-method')

    if replication_method.upper() != 'LOG_BASED':
        return True

    if replication_method.upper() == 'LOG_BASED' and binlog_stream_requires_historical(selected_stream, state):
        return True

    return False


def binlog_stream_requires_historical(catalog_entry, state):
    log_file = core.get_bookmark(state, catalog_entry.tap_stream_id, 'log_file')
    log_pos = core.get_bookmark(state, catalog_entry.tap_stream_id, 'log_pos')
    max_pk_values = core.get_bookmark(state, catalog_entry.tap_stream_id, 'max_pk_values')
    last_pk_fetched = core.get_bookmark(state, catalog_entry.tap_stream_id, 'last_pk_fetched')

    if (log_file and log_pos) and (not max_pk_values and not last_pk_fetched):
        return False

    return True


def resolve_catalog(discovered_catalog, streams_to_sync) -> Catalog:
    result = Catalog(streams=[])

    # Iterate over the streams in the input catalog and match each one up
    # with the same stream in the discovered catalog.
    for catalog_entry in streams_to_sync:
        catalog_metadata = metadata.to_map(catalog_entry.metadata)
        replication_key = catalog_metadata.get((), {}).get('replication-key')

        discovered_table = discovered_catalog.get_stream(catalog_entry.tap_stream_id)
        database_name = common.get_database_name(catalog_entry)

        if not discovered_table:
            logging.warning('Database %s table %s was selected but does not exist',
                            database_name, catalog_entry.table)
            continue

        selected = {k for k, v in catalog_entry.schema.properties.items()
                    if common.property_is_selected(catalog_entry, k) or k == replication_key}

        # These are the columns we need to select
        columns, all_columns = desired_columns(selected, discovered_table.schema, discovered_table.table)
        binary_columns = []

        for column, column_vals in discovered_table.schema.properties.items():
            if column_vals.type and 'binary' in column_vals.type:
                binary_columns += [column]

        result.streams.append(CatalogEntry(
            tap_stream_id=catalog_entry.tap_stream_id,
            database=database_name,
            metadata=catalog_entry.metadata,
            stream=catalog_entry.stream,
            table=catalog_entry.table,
            schema=Schema(
                type='object',
                properties={col: discovered_table.schema.properties[col]
                            for col in columns}
            ),
            full_schema=Schema(
                type='object',
                properties={col: discovered_table.schema.properties[col]
                            for col in all_columns}
            ),
            binary_columns=binary_columns
        ))

    return result


# TODO: Add check for change in schema for new column, if so full sync that table.
def get_non_binlog_streams(mysql_conn, catalog, config, state, append_mode):
    """Returns the Catalog of data we're going to sync for all SELECT-based
    streams (i.e. INCREMENTAL, FULL_TABLE, and LOG_BASED that require a historical
    sync). LOG_BASED streams that require a historical sync are inferred from lack
    of any state.
    Using the Catalog provided from the input file, this function will return a
    Catalog representing exactly which tables and columns that will be emitted
    by SELECT-based syncs. This is achieved by comparing the input Catalog to a
    freshly discovered Catalog to determine the resulting Catalog.
    The resulting Catalog will include the following any streams marked as
    "selected" that currently exist in the database. Columns marked as "selected"
    and those labeled "automatic" (e.g. primary keys and replication keys) will be
    included. Streams will be prioritized in the following order:
      1. currently_syncing if it is SELECT-based
      2. any streams that do not have state
      3. any streams that do not have a replication method of LOG_BASED
    """
    discovered = discover_catalog(mysql_conn, config, append_mode)

    # Filter catalog to include only selected streams
    selected_streams = list(filter(lambda s: common.stream_is_selected(s), catalog.streams))
    streams_with_state = []
    streams_without_state = []

    for stream in selected_streams:
        stream_metadata = metadata.to_map(stream.metadata)
        replication_method = stream_metadata.get((), {}).get('replication-method').upper()
        stream_state = state.get('bookmarks', {}).get(stream.tap_stream_id)

        if not stream_state:
            if replication_method.upper() == 'LOG_BASED':
                logging.info("LOG_BASED stream %s requires full historical sync", stream.tap_stream_id)

            streams_without_state.append(stream)
        elif stream_state and replication_method.upper() == 'LOG_BASED' \
                and binlog_stream_requires_historical(stream, state):
            is_view = common.get_is_view(stream)

            if is_view:
                raise Exception("Unable to replicate stream({}) with binlog because it's a view.".format(stream.stream))

            logging.info("LOG_BASED stream %s will resume its historical sync", stream.tap_stream_id)

            streams_with_state.append(stream)
        elif stream_state and replication_method.upper() != 'LOG_BASED':
            streams_with_state.append(stream)

    # If the state says we were in the middle of processing a stream, skip
    # to that stream. Then process streams without prior state and finally
    # move onto streams with state (i.e. have been synced in the past)
    currently_syncing = core.get_currently_syncing(state)

    # prioritize streams that have not been processed
    ordered_streams = streams_without_state + streams_with_state

    if currently_syncing:
        currently_syncing_stream = list(filter(
            lambda s: s.tap_stream_id == currently_syncing and is_valid_currently_syncing_stream(s, state),
            streams_with_state))

        non_currently_syncing_streams = list(filter(lambda s: s.tap_stream_id != currently_syncing, ordered_streams))

        streams_to_sync = currently_syncing_stream + non_currently_syncing_streams
    else:
        # prioritize streams that have not been processed
        streams_to_sync = ordered_streams

    return resolve_catalog(discovered, streams_to_sync)


def get_binlog_streams(mysql_conn, catalog, config, state, append_mode):
    discovered = discover_catalog(mysql_conn, config, append_mode)

    selected_streams = list(filter(lambda s: common.stream_is_selected(s), catalog.streams))
    binlog_streams = []

    for stream in selected_streams:
        stream_metadata = metadata.to_map(stream.metadata)
        replication_method = stream_metadata.get((), {}).get('replication-method').upper()
        stream_state = state.get('bookmarks', {}).get(stream.tap_stream_id)
        logging.debug(stream_state)

        if replication_method.upper() == 'LOG_BASED' and not binlog_stream_requires_historical(stream, state):
            binlog_streams.append(stream)

    return resolve_catalog(discovered, binlog_streams)


def write_schema_message(catalog_entry, message_store=None, bookmark_properties=[]):
    key_properties = common.get_key_properties(catalog_entry)
    core.write_message(core.SchemaMessage(stream=catalog_entry.stream, schema=catalog_entry.schema.to_dict(),
                                          key_properties=key_properties, bookmark_properties=bookmark_properties),
                       message_store=message_store, database_schema=catalog_entry.database)


class Component(KBCEnvHandler):
    """Keboola extractor component."""

    def __init__(self, debug: bool = False, data_path: str = None):
        KBCEnvHandler.__init__(self, MANDATORY_PARS, data_path=data_path,
                               log_level=logging.DEBUG if debug else logging.INFO)

        if self.cfg_params.get(KEY_DEBUG, False) is True:
            logger = logging.getLogger()
            logger.setLevel(logging.DEBUG)
            # sys.tracebacklimit = 10

            for h in logger.handlers:
                h.setFormatter(logging.Formatter('%(levelname)10s - %(filename)s - %(lineno)4d: %(message)s'))

        self.files_out_path = os.path.join(self.data_path, 'out', 'files')
        self.files_in_path = os.path.join(self.data_path, 'in', 'files')
        self.state_out_file_path = os.path.join(self.data_path, 'out', 'state.json')
        self.params = self.cfg_params
        logging.info('Running version %s', APP_VERSION)
        logging.info('Loading configuration...')

        try:
            self.validate_config()
            self.validate_image_parameters(MANDATORY_IMAGE_PARS)
        except ValueError as err:
            logging.exception(err)
            exit(1)

        max_execution_time = self.params.get(KEY_MAX_EXECUTION_TIME)
        if max_execution_time:
            max_execution_time = self.params.get(KEY_MAX_EXECUTION_TIME)
            if len(str(max_execution_time)) > 0:
                try:
                    max_execution_time = int(max_execution_time)
                    logging.info(f"Using parameter max_execution time from config: {max_execution_time}")
                except ValueError as e:
                    raise Exception(f"Cannot cast parameter {max_execution_time} to integer.") from e

        self.mysql_config_params = {
            "host": self.params[KEY_MYSQL_HOST],
            "port": self.params[KEY_MYSQL_PORT],
            "user": self.params[KEY_MYSQL_USER],
            "password": self.params[KEY_MYSQL_PWD],
            "ssl": self.params.get(KEY_USE_SSL),
            "ssl_ca": self.params.get(KEY_SSL_CA),
            "verify_mode": self.params.get(KEY_VERIFY_CERT) or False,
            "connect_timeout": CONNECT_TIMEOUT,
            "show_binary_log_config": self.params.get(KEY_SHOW_BIN_LOG_CFG, {}),
            "max_execution_time": max_execution_time
        }

        # TODO: Update to more clear environment variable; used must set local time to UTC.
        os.environ['TZ'] = 'UTC'

    def _check_file_inputs(self) -> str:
        """Return path name of file inputs if any."""
        file_input = self.files_in_path
        has_file_inputs = any(os.path.isfile(os.path.join(file_input, file)) for file in os.listdir(file_input))

        if has_file_inputs:
            return file_input

    # Sync Methods
    @staticmethod
    def do_sync_incremental(mysql_conn, catalog_entry, state, columns, optional_limit=None,
                            message_store: core.MessageStore = None):
        logging.info("Stream %s is using incremental replication", catalog_entry.stream)

        md_map = metadata.to_map(catalog_entry.metadata)
        replication_key = md_map.get((), {}).get('replication-key')

        if not replication_key:
            raise Exception("Cannot use INCREMENTAL replication for table ({}) without a replication key.".format(
                catalog_entry.stream))

        write_schema_message(catalog_entry=catalog_entry, bookmark_properties=[replication_key],
                             message_store=message_store)

        if optional_limit:
            logging.info("Incremental Stream %s is using an optional limit clause of %d", catalog_entry.stream,
                         int(optional_limit))
            incremental.sync_table(mysql_conn, catalog_entry, state, columns, int(optional_limit),
                                   message_store=message_store)
        else:
            incremental.sync_table(mysql_conn, catalog_entry, state, columns, message_store=message_store)

        core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)

    @staticmethod
    def do_sync_historical_binlog(mysql_conn, config, catalog_entry, state, columns, tables_destination: str = None,
                                  message_store: core.MessageStore = None):
        binlog.verify_binlog_config(mysql_conn)

        is_view = common.get_is_view(catalog_entry)
        key_properties = common.get_key_properties(catalog_entry)  # noqa

        if is_view:
            raise Exception(
                "Unable to replicate stream({}) with binlog because it is a view.".format(catalog_entry.stream))

        log_file = core.get_bookmark(state, catalog_entry.tap_stream_id, 'log_file')

        log_pos = core.get_bookmark(state, catalog_entry.tap_stream_id, 'log_pos')

        max_pk_values = core.get_bookmark(state, catalog_entry.tap_stream_id, 'max_pk_values')

        last_pk_fetched = core.get_bookmark(state, catalog_entry.tap_stream_id, 'last_pk_fetched')  # noqa

        write_schema_message(catalog_entry, message_store=message_store)

        stream_version = common.get_stream_version(catalog_entry.tap_stream_id, state)

        # Update state last_table_schema with current schema, and store KBC cols
        table_schema = Component._build_schema_cache_from_catalog_entry(catalog_entry, full=True)
        state = core.update_schema_in_state(state, {catalog_entry.tap_stream_id: table_schema})

        if log_file and log_pos and max_pk_values:
            logging.info("Resuming initial full table sync for LOG_BASED stream %s", catalog_entry.tap_stream_id)
            full_table.sync_table_chunks(mysql_conn, catalog_entry, state, columns, stream_version,
                                         tables_destination=tables_destination, message_store=message_store)

        else:
            logging.info("Performing initial full table sync for LOG_BASED table {}".format(
                catalog_entry.tap_stream_id))

            state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'initial_binlog_complete', False)

            current_log_file, current_log_pos = binlog.fetch_current_log_file_and_pos(mysql_conn, config)
            state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'version', stream_version)

            if full_table.sync_is_resumable(mysql_conn, catalog_entry):
                # We must save log_file and log_pos across FULL_TABLE syncs when performing
                # a resumable full table sync
                state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'log_file', current_log_file)
                state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'log_pos', current_log_pos)

                full_table.sync_table_chunks(mysql_conn, catalog_entry, state, columns, stream_version,
                                             tables_destination=tables_destination, message_store=message_store)
            else:
                full_table.sync_table_chunks(mysql_conn, catalog_entry, state, columns, stream_version,
                                             tables_destination=tables_destination, message_store=message_store)
                state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'log_file', current_log_file)
                state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'log_pos', current_log_pos)

    @staticmethod
    def _build_schema_cache_from_catalog_entry(catalog_entry, full=False):

        table_schema = []
        primary_keys = common.get_key_properties(catalog_entry)
        if full:
            column_properties = catalog_entry.full_schema.properties
        else:
            column_properties = catalog_entry.schema.properties

        for idx, column_metadata in enumerate(catalog_entry.metadata[1:], start=1):
            col_name = column_metadata['breadcrumb'][1]

            if col_name not in column_properties:
                logging.debug(f"Skipping columns: {col_name}")
                continue
            col_type = column_metadata['metadata']['sql-datatype']
            ordinal_position = column_metadata['metadata']['ordinal-position']
            is_pkey = col_name in primary_keys
            character_set = column_properties[col_name].characterSet
            schema = TableColumnSchemaCache.build_column_schema(col_name.upper(), ordinal_position, col_type, is_pkey,
                                                                character_set_name=character_set)
            table_schema.append(schema)

        # ensure sort by ordinal position to avoid later shift
        table_schema = sorted(table_schema, key=lambda sch: sch['ORDINAL_POSITION'])

        return table_schema

    @staticmethod
    def do_sync_full_table(mysql_conn, config, catalog_entry, state, columns, tables_destination: str = None,
                           message_store: core.MessageStore = None):
        logging.info("Stream %s is using full table replication", catalog_entry.stream)
        key_properties = common.get_key_properties(catalog_entry)  # noqa

        write_schema_message(catalog_entry, message_store=message_store)

        stream_version = common.get_stream_version(catalog_entry.tap_stream_id, state)

        full_table.sync_table_chunks(mysql_conn, catalog_entry, state, columns, stream_version,
                                     tables_destination=tables_destination, message_store=message_store)

        # Prefer initial_full_table_complete going forward
        core.clear_bookmark(state, catalog_entry.tap_stream_id, 'version')

        state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'initial_full_table_complete', True)

        core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)

    def sync_non_binlog_streams(self, mysql_conn, non_binlog_catalog, config, state, tables_destination: str = None,
                                message_store: core.MessageStore = None):
        if tables_destination is None:
            logging.info('No table destination specified, so will not work on new CSV write implementation')

        for catalog_entry in non_binlog_catalog.streams:
            # only selected
            columns = [k for k, v in catalog_entry.schema.properties.items()]

            if not columns:
                logging.warning('There are no columns selected for stream %s, skipping it.', catalog_entry.stream)
                continue

            state = core.set_currently_syncing(state, catalog_entry.tap_stream_id)

            # Emit a state message to indicate that we've started this stream
            core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)

            md_map = metadata.to_map(catalog_entry.metadata)

            replication_method = md_map.get((), {}).get('replication-method')

            database_name = common.get_database_name(catalog_entry)

            with metrics.job_timer('sync_table') as timer:
                timer.tags['database'] = database_name
                timer.tags['table'] = catalog_entry.table

                log_engine(mysql_conn, catalog_entry)

                if replication_method.upper() == 'INCREMENTAL':
                    optional_limit = config.get('incremental_limit')
                    self.do_sync_incremental(mysql_conn, catalog_entry, state, columns, optional_limit,
                                             message_store=message_store)
                elif replication_method.upper() == 'LOG_BASED':
                    self.do_sync_historical_binlog(mysql_conn, config, catalog_entry, state, columns,
                                                   tables_destination=tables_destination, message_store=message_store)
                elif replication_method.upper() == 'FULL_TABLE':
                    self.do_sync_full_table(mysql_conn, config, catalog_entry, state, columns,
                                            tables_destination=tables_destination, message_store=message_store)
                else:
                    raise Exception("only INCREMENTAL, LOG_BASED, and FULL TABLE replication methods are supported")

        state = core.set_currently_syncing(state, None)
        core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)

    @staticmethod
    def sync_binlog_streams(mysql_conn, binlog_catalog, mysql_config, state,
                            message_store: core.MessageStore = None, schemas=[], tables=[], columns={}):
        if binlog_catalog.streams:
            for stream in binlog_catalog.streams:
                write_schema_message(stream, message_store=message_store)

            with metrics.job_timer('sync_binlog'):
                binlog.sync_binlog_stream(mysql_conn, mysql_config, binlog_catalog.streams, state,
                                          message_store=message_store, schemas=schemas, tables=tables, columns=columns)

    def do_sync(self, mysql_conn, config, mysql_config, catalog, state,
                message_store: core.MessageStore = None, schemas=[], tables=[], columns={}):
        non_binlog_catalog = get_non_binlog_streams(mysql_conn, catalog, config, state,
                                                    self.params.get(KEY_APPEND_MODE))
        logging.info('Number of non-binlog tables to process: {}'.format(len(non_binlog_catalog)))
        binlog_catalog = get_binlog_streams(mysql_conn, catalog, config, state, self.params.get(KEY_APPEND_MODE))
        logging.info('Number of binlog catalog tables to process: {}'.format(len(binlog_catalog)))

        self.sync_non_binlog_streams(mysql_conn, non_binlog_catalog, config, state,
                                     tables_destination=self.tables_out_path, message_store=message_store)
        self.sync_binlog_streams(mysql_conn, binlog_catalog, mysql_config, state,
                                 message_store=message_store, schemas=schemas, tables=tables, columns=columns)

    @staticmethod
    def log_server_params(mysql_conn):
        with connect_with_backoff(mysql_conn) as open_conn:
            try:
                with open_conn.cursor() as cur:
                    cur.execute('''
                    SELECT VERSION() as version,
                           @@session.wait_timeout as wait_timeout,
                           @@session.innodb_lock_wait_timeout as innodb_lock_wait_timeout,
                           @@session.max_allowed_packet as max_allowed_packet,
                           @@session.interactive_timeout as interactive_timeout,
                           @@session.max_execution_time as max_execution_time''')
                    row = cur.fetchone()
                    logging.info('Server Parameters: ' +
                                 'version: %s, ' +
                                 'wait_timeout: %s, ' +
                                 'innodb_lock_wait_timeout: %s, ' +
                                 'max_allowed_packet: %s, ' +
                                 'interactive_timeout: %s ' +
                                 'max_execution_time: %s',
                                 *row)
                with open_conn.cursor() as cur:
                    cur.execute('''
                    show session status where Variable_name IN ('Ssl_version', 'Ssl_cipher')''')
                    rows = cur.fetchall()
                    mapped_row = dict(rows)
                    logging.info('Server SSL Parameters (blank means SSL is not active): ' +
                                 '[ssl_version: %s], ' +
                                 '[ssl_cipher: %s]',
                                 mapped_row['Ssl_version'],
                                 mapped_row['Ssl_cipher'])

            except pymysql.err.InternalError as ie:
                logging.warning("Encountered error checking server params. Error: (%s) %s", *ie.args)

    # End of sync methods

    def write_table_mappings_file(self, table_mapping: Catalog, file_name: str = 'table_mappings.json'):
        """Write table mappings to output file destination."""
        write_destination = os.path.join(self.files_out_path, file_name)
        with open(write_destination, 'w') as mapping_file:
            mapping_file.write(table_mapping.dumps())

    @staticmethod
    def write_result(result: list, output_file: str = 'results.json'):
        """Write table mappings to output file destination."""
        with open(output_file, 'w') as mapping_file:
            mapping_file.write(json.dumps(result))

    def get_table_column_metadata(self, columns_metadata: dict):
        """Return metadata for all columns for given table stream ID."""
        table_columns_metadata = {}

        # First, determine if column is selected. Selected if "selected" is true, or "selected-by-default" is true and
        # "selected" is not specifically set to false
        for column_metadata in columns_metadata:
            column_detail = column_metadata['metadata']
            is_selected_in_detail = True if 'selected' in column_detail else False
            is_column_set_as_selected = column_detail.get('selected')
            is_selected_by_default = column_detail.get('selected-by-default')
            if is_column_set_as_selected or (is_selected_by_default and not is_selected_in_detail):
                column_name = column_metadata['breadcrumb'][1].upper()
                data_type = column_detail.get('sql-datatype')

                table_columns_metadata[column_name] = self.generate_column_metadata(data_type=data_type, nullable=True)

        # Append KBC metadata column types, hard coded for now
        table_columns_metadata[common.KBC_SYNCED] = self.generate_column_metadata(data_type='timestamp', nullable=True)
        table_columns_metadata[common.KBC_DELETED] = self.generate_column_metadata(
            data_type='timestamp', nullable=True)
        table_columns_metadata[common.BINLOG_CHANGE_AT] = self.generate_column_metadata(data_type='integer',
                                                                                        nullable=True)
        table_columns_metadata[common.BINLOG_READ_AT] = self.generate_column_metadata(data_type='integer',
                                                                                      nullable=True)

        return table_columns_metadata

    def _get_size_and_precision(self, datatype: str):
        length = None
        precision = None
        size = ()
        if len(split_parts := datatype.split('(')) > 1:
            # remove anything after ) e.g. int(12) unsigned)
            size_str = split_parts[1].split(')')[0]
            size = ast.literal_eval(f'({size_str})')

        if size and isinstance(size, tuple):
            length = size[0]
            precision = size[1]
        elif size:
            length = size
        return length, precision

    def generate_column_metadata(self, data_type: str = None, nullable: bool = None):
        """Return metadata for given column"""
        column_metadata = []
        base_data_type = self._convert_mysql_data_types_to_kbc_types(data_type)

        # Append metadata per input parameter, if present
        if data_type:
            length, precision = self._get_size_and_precision(data_type)
            type_metadata = {}
            type_key, type_value = 'KBC.datatype.type', data_type
            type_metadata['key'] = type_key
            type_metadata['value'] = type_value
            column_metadata.append(type_metadata)

            base_type_metadata = {}
            base_type_key, base_type_value = 'KBC.datatype.basetype', base_data_type
            base_type_metadata['key'] = base_type_key
            base_type_metadata['value'] = base_type_value
            column_metadata.append(base_type_metadata)

            # Add length data type if String, just using max for now
            if table_metadata.is_type_with_length(data_type, TYPES_SUPPORTING_LENGTH):
                length_type_key = 'KBC.datatype.length'
                if length:
                    if precision:
                        length = f'{length},{precision}'
                if base_data_type in ['NUMERIC', 'FLOAT']:
                    string_length_metadata = {}

                    if length:
                        string_length_metadata['key'] = length_type_key
                        string_length_metadata['value'] = length
                        column_metadata.append(string_length_metadata)
                elif base_data_type in ['STRING']:
                    string_length_metadata = {}
                    if 'binary' in data_type.lower():
                        # store binary as TEXT size
                        length = 16777216
                    if length:
                        string_length_metadata['key'] = length_type_key
                        string_length_metadata['value'] = length
                        column_metadata.append(string_length_metadata)

        if nullable:
            nullable_metadata = {}
            nullable_key, nullable_value = 'KBC.datatype.nullable', nullable
            nullable_metadata['key'] = nullable_key
            nullable_metadata['value'] = nullable_value
            column_metadata.append(nullable_metadata)

        return column_metadata

    @staticmethod
    def _convert_mysql_data_types_to_kbc_types(column_type: str) -> str:
        """Convert given column data type from MySQL data type to Keboola base type."""
        column_type = column_type.lower()
        for db_data_type in datatypes.BASE_STRING:
            if column_type.startswith(db_data_type):
                return 'STRING'
        for db_data_type in datatypes.BASE_INTEGER:
            if column_type.startswith(db_data_type):
                return 'INTEGER'
        for db_data_type in datatypes.BASE_TIMESTAMP:
            if column_type.startswith(db_data_type):
                return 'TIMESTAMP'
        for db_data_type in datatypes.BASE_FLOAT:
            if column_type.startswith(db_data_type):
                return 'FLOAT'
        for db_data_type in datatypes.BASE_BOOLEAN:
            if column_type.startswith(db_data_type):
                return 'BOOLEAN'
        for db_data_type in datatypes.BASE_DATE:
            if column_type.startswith(db_data_type):
                return 'DATE'
        for db_data_type in datatypes.BASE_NUMERIC:
            if column_type.startswith(db_data_type):
                return 'NUMERIC'

        logging.warning('Processed data type {} does not match any KBC base types'.format(column_type))

    def create_manifests(self, entry: dict, data_path: str, columns: list = None, column_metadata: dict = None,
                         set_incremental: bool = True, output_bucket: str = None):
        """Write manifest files for the results produced by the results writer.

        :param entry: Dict entry from catalog
        :param data_path: Path to the result output files
        :param columns: List of strings representing columns for the output table, necessary for sliced tables.
        the `.column` attribute should be
        used in manifest file.
        :param column_metadata: Dict column metadata as keys and values.
        :param set_incremental: Incremental choice true or false for whether to write incrementally to manifest file.
        :param output_bucket: The name of the output bucket to be written to Storage
        :return:
        """
        if bool(self.cfg_params.get(KEY_APPEND_MODE, False)) is True:
            primary_keys = None
        elif entry.get('primary_keys'):
            primary_keys = [key.upper() for key in entry.get('primary_keys')]
        else:
            primary_keys = None
        table_name = entry.get('table_name').upper()
        result_full_path = os.path.join(data_path, table_name + '.csv')

        # for r in results:
        if not columns:
            self.write_table_manifest(result_full_path, destination=table_name,
                                      primary_key=primary_keys, column_metadata=column_metadata,
                                      is_incremental=set_incremental, output_bucket=output_bucket)
        else:
            self.write_table_manifest(result_full_path, destination=table_name,
                                      primary_key=primary_keys, columns=columns,
                                      column_metadata=column_metadata, is_incremental=set_incremental,
                                      output_bucket=output_bucket)

    @staticmethod
    def write_table_manifest(file_name: str, destination: str = '', primary_key: list = None, columns: list = None,
                             column_metadata: dict = None, is_incremental: bool = None, output_bucket: str = None):
        """Write manifest for output table Manifest is used for the table to be stored in KBC Storage.

        Args:
            file_name: Local file name of the CSV with table data.
            destination: String name of the table in Storage.
            primary_key: List with names of columns used for primary key.
            columns: List of columns as strings that are written to table, necessary to specify for sliced tables.
            column_metadata: Metadata keys and values about columns in table
            is_incremental: Set to true to enable incremental loading
            output_bucket: The output bucket in storage
        """
        manifest = {}
        if output_bucket:
            manifest['destination'] = output_bucket + '.' + destination
        else:
            pass
        if primary_key:
            if isinstance(primary_key, list):
                manifest['primary_key'] = primary_key
            else:
                raise TypeError("Primary key must be a list")
        if columns:
            if isinstance(columns, list):
                manifest['columns'] = columns
            else:
                raise TypeError("Columns must by a list")
        if column_metadata:
            if isinstance(column_metadata, dict):
                manifest['column_metadata'] = column_metadata
            else:
                raise TypeError("Columns must by a list")
        if is_incremental:
            manifest['incremental'] = True

        with open(file_name + '.manifest', 'w') as manifest_file:
            json.dump(manifest, manifest_file)
            logging.info('Wrote manifest table {}'.format(file_name + '.manifest'))

    @staticmethod
    def deduplicate_binlog_result(table_path: str, primary_keys: List[str], buffer_size=8192):
        """
        Reads table backwards and deduplicates based on  primary key.
        Args:
            table_path:
            primary_keys:
            buffer_size

        Returns:

        """

        with open(table_path, 'r') as inp:
            header = csv.DictReader(inp).fieldnames
        pkey_hashes = set()

        def create_pkey_hash(row_record: dict):
            try:
                pkey_hash_str = '|'.join(row_record[idx] for idx in primary_keys)
                return pkey_hash_str
            except IndexError:
                # TODO: remove temp debug statement
                for idx in primary_keys:
                    try:
                        row_record[idx]
                    except IndexError:
                        logging.error(f"Pkey index {idx} not found in row: {row_record}")
                        raise Exception(f"Pkey index {idx} not found in row: {row_record} "
                                        f"for primary key: {primary_keys}")

        fd, temp_result = tempfile.mkstemp()
        # FIX line 1: field larger than field limit error
        # as proposed here https://stackoverflow.com/questions/15063936/csv-error-field-larger-than-field-limit-131072
        csv.field_size_limit(sys.maxsize)
        with open(temp_result, 'w+', newline='', encoding='utf-8') as out_file, open(table_path, 'rb') as inp:
            writer = csv.DictWriter(out_file, fieldnames=header, lineterminator='\n')
            reader = csv.DictReader(core.utils.reverse_readline(inp, buf_size=buffer_size), fieldnames=header)
            writer.writeheader()
            for row in reader:
                if not row:
                    logging.warning("Empty row in result")
                    continue
                pkey_hash = create_pkey_hash(row)
                if pkey_hash in pkey_hashes:
                    continue

                pkey_hashes.add(pkey_hash)

                if list(row.values()) != header:
                    writer.writerow(row)

        os.remove(table_path)
        shutil.move(temp_result, table_path)

    def write_only_latest_result_binlogs(self, csv_table_path: str, primary_keys: list = None,
                                         append_mode: bool = False) -> None:
        """For given result CSV file path, remove non-latest binlog event by primary key.

        A primary key can only have a single Write Event and a max of one Delete Event. It can have infinite Update
        Events. Each event returns the current state of the row, so we just want the latest row per extraction.
        """

        with metrics.job_timer('latest_binlog_results') as timer:
            timer.tags['csv_table'] = csv_table_path
            timer.tags['primary_key'] = primary_keys

            # Read DF as Strings to avoid incorrect rounding issues with conversions of ints/numerics to floats

            if primary_keys and append_mode is not True:
                logging.info('Keeping only latest per primary key from binary row event results for {} '
                             'based on table primary keys: {}'.format(csv_table_path, primary_keys))

                # TODO: remove
                shutil.copy(csv_table_path, self.files_out_path + '/test.csv')

                self.deduplicate_binlog_result(csv_table_path, [pk.upper() for pk in primary_keys])

            else:

                if append_mode is True:
                    logging.info("Append mode active, no deduplication of rows occuring.")

                else:
                    logging.warning('Table at path {} does not have primary key, '
                                    'so no binlog de-duplication will occur, '
                                    'records must be processed downstream'.format(csv_table_path))

    def get_conn_context_manager(self):
        if self.cfg_params[KEY_USE_SSH_TUNNEL]:
            b64_input_key = self.cfg_params.get(KEY_SSH_PRIVATE_KEY)
            input_key = None
            try:
                input_key = base64.b64decode(b64_input_key, validate=True).decode('utf-8')
            except binascii.Error as bin_err:
                logging.error('Failed to base64-decode the private key, confirm you have base64-encoded your private '
                              'key input variable. Detail: {}'.format(bin_err))
                exit(1)

            pkey_from_input = paramiko.RSAKey.from_private_key(StringIO(input_key))
            context_manager = SSHTunnelForwarder(
                (self.cfg_params[KEY_SSH_HOST], self.cfg_params[KEY_SSH_PORT]),
                ssh_username=self.cfg_params[KEY_SSH_USERNAME],
                ssh_pkey=pkey_from_input,
                remote_bind_address=(self.cfg_params[KEY_MYSQL_HOST], self.cfg_params[KEY_MYSQL_PORT]),
                local_bind_address=(LOCAL_ADDRESS, SSH_BIND_PORT),
                ssh_config_file=None,
                allow_agent=False
            )
        else:
            context_manager = nullcontext(None)

        return context_manager

    def walk_path(self, path: str = None, is_pre_manifest: bool = False):
        """Walk through specified path to QA files/directories/tables (an manifests, if generated)."""
        logging.info('Walking path {} to QA directories and files'.format(path))
        if not path:
            path = self.tables_out_path
        directories = []
        files = []
        for (_, dirs, file_names) in os.walk(path):
            directories.extend(dirs)
            files.extend(file_names)
        if is_pre_manifest:
            logging.info('All pre-manifest directories at walked path: {}'.format(directories))
            logging.info('All pre-manifest files at walked path: {}'.format(files))
        else:
            logging.info('All directories at walked path: {}'.format(directories))
            logging.info('All files sent at walked path: {}'.format(files))

    def parse_input_mapping(self, input_mapping, input_mapping_type='json'):
        """Parses provided input mappings and returns a list of selected tables and schemas."""
        schemas = []
        tables = []
        output_mapping = []
        columns = {}

        if input_mapping_type == 'json':

            for schema in input_mapping:
                # Because yaml and json mapping have different specification. YAML is an array, JSON is an object.
                # Converting this to be same as YAML mapping.
                output_mapping += [{schema: input_mapping[schema]}]
                _tables = input_mapping[schema].get('tables', [])

                if _tables == []:
                    logging.warn(f"No tables specified for schema {schema}. Skipping.")
                elif isinstance(_tables, list) is False:
                    logging.error(f"Tables for schema {schema} are not an array.")
                    sys.exit(1)
                else:
                    schemas += [schema]
                    tables += [list(table.keys())[0] for table in _tables]

                for table in _tables:
                    table_name = list(table.keys())[0]

                    tap_stream_id = '-'.join([schema, table_name])
                    desired_columns = [k for k, v in table[table_name].get('columns', {}).items() if v is True]
                    columns_to_watch = table[table_name].get('columns_to_watch')
                    columns_to_ignore = table[table_name].get('columns_to_ignore')
                    columns[tap_stream_id] = {
                        'desired': None if desired_columns == [] else desired_columns + list(common.KBC_METADATA_COLS),
                        'watch': columns_to_watch,
                        'ignore': columns_to_ignore
                    }

        elif input_mapping_type == 'yaml':
            # Because yaml and json mapping have different specification. YAML is an array, JSON is an object.
            output_mapping = input_mapping

            for schema_spec in input_mapping:
                schema = list(schema_spec.keys())[0]
                _tables = schema_spec[schema]['tables']

                if _tables == []:
                    logging.warn(f"No tables specified for schema {schema}. Skipping.")
                elif isinstance(_tables, list) is False:
                    logging.error(f"Tables for schema {schema} are not an array.")
                    sys.exit(1)
                else:
                    schemas += [schema]
                    tables += [list(table.keys())[0] for table in _tables]

                for table in _tables:
                    table_name = list(table.keys())[0]

                    tap_stream_id = '-'.join([schema, table_name])
                    desired_columns = [k for k, v in table[table_name].get('columns', {}).items() if v is True]
                    columns_to_watch = table[table_name].get('columns_to_watch', [])
                    columns_to_ignore = table[table_name].get('columns_to_ignore', [])
                    columns[tap_stream_id] = {
                        'desired': None if desired_columns == [] else desired_columns + list(common.KBC_METADATA_COLS),
                        'watch': columns_to_watch,
                        'ignore': columns_to_ignore
                    }

        else:
            logging.error(f"Incorrect mapping file specification provided: {input_mapping_type}.")
            sys.exit(1)

        logging.debug(f"Parsed following schemas: {schemas}.")
        logging.debug(f"Parsed following tables: {tables}.")

        return output_mapping, schemas, tables, columns

    @staticmethod
    def create_output_bucket(bucket_name: str = None):

        if bucket_name is not None and bucket_name.strip() != '':
            return f'in.c-{bucket_name.strip()}'

        else:
            _component_id = os.environ.get(ENV_COMPONENT_ID)
            _configuration_id = os.environ.get(ENV_CONFIGURATION_ID)

            if _component_id is not None and _component_id is not None:
                return f"in.c-{_component_id.replace('.', '-')}-{_configuration_id}"

            else:
                return None

    def run(self):
        """Execute main component extraction process."""
        table_mappings = {}
        file_input_path = self._check_file_inputs()  # noqa

        # QA Input data
        self.walk_path(self.files_in_path)
        self.walk_path(self.tables_in_path)

        connection_context = self.get_conn_context_manager()

        with connection_context as server:
            if server:  # True if set an SSH tunnel returns false if using the null context.
                logging.info('Connecting via SSH tunnel over bind port {}'.format(SSH_BIND_PORT))
                self.mysql_config_params['host'] = server.local_bind_host
                self.mysql_config_params['port'] = server.local_bind_port
            else:
                logging.info('Connecting directly to database via port {}'.format(self.cfg_params[KEY_MYSQL_PORT]))

            mysql_client = MySQLConnection(self.mysql_config_params)
            self.log_server_params(mysql_client)

            # elif file_input_path:
            #     manual_table_mappings_file = os.path.join(file_input_path, MAPPINGS_FILE)
            #     logging.info('Fetching table mappings from file input mapping configuration: {}.'.format(
            #         manual_table_mappings_file))
            #     with open(manual_table_mappings_file, 'r') as mappings_file:
            #         table_mappings = json.load(mappings_file)

            # TESTING: Fetch current state of database: schemas, tables, columns, datatypes, etc.
            catalog_mapping = discover_catalog(mysql_client, self.params, append_mode=self.params.get(KEY_APPEND_MODE))

            # Make Raw Mapping file to allow edits
            raw_yaml_mapping = make_yaml_mapping_file(catalog_mapping.to_dict())

            if (_json := self.cfg_params.get(KEY_TABLE_MAPPINGS_JSON)) and _json != '{}' and _json != '':  # noqa
                input_method = 'json'
                logging.info('Using table mappings based on input JSON mappings.')

                try:
                    input_mapping = json.loads(self.cfg_params.get(KEY_TABLE_MAPPINGS_JSON))
                    logging.debug(f"Received input schema: {input_mapping}")
                except ValueError:
                    logging.error("Invalid JSON mappins provided. Could not parse JSON.")
                    sys.exit(1)

                input_mapping, schemas_to_sync, tables_to_sync, columns_to_sync = self.parse_input_mapping(
                    input_mapping, input_method)
                table_mappings = json.loads(convert_yaml_to_json_mapping(input_mapping, catalog_mapping.to_dict()))

            elif self.params.get(KEY_INPUT_MAPPINGS_YAML) and self.params.get(KEY_MAPPINGS_FILE):
                input_method = 'yaml'
                logging.info('Using table mappings based on input YAML mappings.')
                input_mapping = yaml.safe_load(self.params[KEY_INPUT_MAPPINGS_YAML])
                table_mappings = json.loads(convert_yaml_to_json_mapping(input_mapping, catalog_mapping.to_dict()))

                logging.debug(f"Received input schema: {input_mapping}")

                _, schemas_to_sync, tables_to_sync, columns_to_sync = self.parse_input_mapping(input_mapping,
                                                                                               input_method)

            else:
                raise AttributeError('You are missing either a YAML input mapping, or the '
                                     'JSON input mapping. Please specify either to appropriately execute the extractor')

            if self.params[KEY_OBJECTS_ONLY]:
                # Run only schema discovery process.
                logging.info('Fetching only object and field names, not running full extraction.')

                # TODO: Retain prior selected choices by user despite refresh.
                input_file_name = self.params.get(KEY_MAPPINGS_FILE) or 'mappings'
                if input_method == 'json':
                    logging.info('Outputting JSON to file {}.json in KBC storage'.format(input_file_name))
                    out_path = os.path.join(self.files_out_path, input_file_name + '_raw.json')
                    with open(out_path, 'w') as json_out:
                        json.dump(raw_yaml_mapping, json_out)

                elif input_method == 'yaml':
                    logging.info('Outputting YAML to file {}.yaml in KBC storage'.format(input_file_name))
                    out_path = os.path.join(self.files_out_path, input_file_name + '_raw.yaml')
                    with open(out_path, 'w') as yaml_out:
                        yaml_out.write(yaml.dump(raw_yaml_mapping))

            elif table_mappings:
                # Run extractor data sync.
                if self.cfg_params[KEY_INCREMENTAL_SYNC]:
                    prior_state = self.get_state_file() or {}
                else:
                    prior_state = {}

                if prior_state:
                    logging.info('Using prior state file to execute sync')
                elif prior_state == {}:
                    logging.info('No prior state was found, will execute full data sync')
                else:
                    logging.info('Incremental sync set to false, ignoring prior state and running full data sync')
                output_bucket = self.create_output_bucket(self.cfg_params.get(KEY_OUTPUT_BUCKET))
                with core.MessageStore(state=prior_state, flush_row_threshold=FLUSH_STORE_THRESHOLD,
                                       output_table_path=self.tables_out_path,
                                       binary_handler=self.cfg_params.get(KEY_HANDLE_BINARY, 'plain'),
                                       output_bucket=output_bucket) as message_store:
                    catalog = Catalog.from_dict(table_mappings)
                    self.do_sync(mysql_client, self.params, self.mysql_config_params, catalog, prior_state,
                                 message_store=message_store, schemas=schemas_to_sync, tables=tables_to_sync,
                                 columns=columns_to_sync)

                    logging.info('Data extraction completed')

                # QA: Walk through output destination pre-manifest
                self.walk_path(path=self.tables_out_path, is_pre_manifest=True)

                # Determine Manifest file outputs
                tables_and_columns = dict()
                if os.path.exists(os.path.join(current_path, 'table_headers.csv')):
                    with open(os.path.join(current_path, 'table_headers.csv')) as headers_file:
                        tables_and_columns = {row.split('\t')[0]: row.split('\t')[1] for row in headers_file}
                        for item, value in tables_and_columns.items():
                            tables_and_columns[item] = [column.strip().upper() for column in ast.literal_eval(value)]
                        logging.debug('Tables and columns mappings for manifests set to: {}'.format(tables_and_columns))

                for entry in catalog.to_dict()['streams']:
                    entry_table_name = entry.get('table_name')
                    table_metadata = entry['metadata'][0]['metadata']
                    column_metadata = entry['metadata'][1:]

                    output_bucket = self.create_output_bucket(self.cfg_params.get(KEY_OUTPUT_BUCKET))

                    if table_metadata.get('selected'):
                        table_replication_method = table_metadata.get('replication-method').upper()

                        # Confirm corresponding table or folder exists
                        table_specific_sliced_path = os.path.join(self.tables_out_path,
                                                                  entry_table_name.upper() + '.csv')
                        if os.path.isdir(table_specific_sliced_path):
                            logging.info('Table {} at location {} is a directory'.format(entry_table_name,
                                                                                         table_specific_sliced_path))
                            output_is_sliced = True
                        elif os.path.isfile(table_specific_sliced_path):
                            logging.info('Table {} at location {} is a file'.format(entry_table_name,
                                                                                    table_specific_sliced_path))
                            output_is_sliced = False
                        else:
                            output_is_sliced = False
                            logging.info('NO DATA found for table {} in either a file or sliced table directory, this '
                                         'table is not being synced'.format(entry_table_name))

                        # TODO: Consider other options for writing to storage based on user choices
                        logging.info('Table has rep method {} and user incremental param is {}'.format(
                            table_replication_method, self.cfg_params[KEY_INCREMENTAL_SYNC]
                        ))
                        if table_replication_method.upper() == 'FULL_TABLE' or \
                                not self.cfg_params[KEY_INCREMENTAL_SYNC]:
                            logging.info('Manifest file will have incremental false for Full Table syncs')
                            manifest_incremental = False
                        else:
                            logging.info('Manifest file will have incremental True for {} sync'.format(
                                table_replication_method
                            ))
                            manifest_incremental = True

                        _table_column_metadata = self.get_table_column_metadata(column_metadata)

                        try:
                            if not output_is_sliced:
                                with open(table_specific_sliced_path) as io:
                                    rdr = csv.DictReader(io)
                                    fields = rdr.fieldnames
                            else:
                                fields = None
                        except FileNotFoundError:
                            fields = []
                        except IsADirectoryError:
                            fields = None

                        logging.info('Table specific path {} for table {}'.format(table_specific_sliced_path,
                                                                                  entry_table_name))

                        if fields is not None:
                            table_column_metadata = dict()
                            for key, val in _table_column_metadata.items():
                                if key in fields:
                                    table_column_metadata[key] = val
                        else:
                            table_column_metadata = _table_column_metadata

                        # Write manifest files
                        if output_is_sliced:
                            if core.find_files(table_specific_sliced_path, '*.csv'):
                                logging.info('Writing manifest for {} to "{}" with columns for sliced table'.format(
                                    entry_table_name, self.tables_out_path))
                                self.create_manifests(entry, self.tables_out_path,
                                                      columns=list(tables_and_columns.get(entry_table_name)),
                                                      column_metadata=table_column_metadata,
                                                      set_incremental=manifest_incremental, output_bucket=output_bucket)
                        elif os.path.isfile(table_specific_sliced_path):
                            logging.info('Writing manifest for {} to path "{}" for non-sliced table'.format(
                                entry_table_name, self.tables_out_path))
                            self.create_manifests(entry, self.tables_out_path, column_metadata=table_column_metadata,
                                                  set_incremental=manifest_incremental, output_bucket=output_bucket)

                            # For binlogs (only binlogs are written non-sliced) rewrite CSVs de-duped to latest per PK
                            self.write_only_latest_result_binlogs(table_specific_sliced_path, entry.get('primary_keys'),
                                                                  self.cfg_params.get(KEY_APPEND_MODE, False))
                        else:
                            logging.info('No manifest file found for selected table {}, because no data was synced '
                                         'from the database for this table. This may be expected behavior if the table '
                                         'is empty or no new rows were added (if incremental)'.format(entry_table_name))

                        # Write output state file
                        logging.debug('Got final state {}'.format(message_store.get_state()))
                        self.write_state_file(message_store.get_state())
                        file_state_destination = os.path.join(self.files_out_path, 'state.json')
                        self.write_state_file(message_store.get_state(), output_path=file_state_destination)

                # QA: Walk through output destination
                self.walk_path(path=self.tables_out_path, is_pre_manifest=False)

            else:
                logging.error('You have either specified incorrect input parameters, or have not chosen to either '
                              'specify a table mappings file manually or via the File Input Mappings configuration.')
                exit(1)

        # all_tables = glob.glob(os.path.join(self.tables_in_path, '*.csv'))
        # for table in all_tables:
        #     if os.path.isdir(table) is True:
        #         pass
        #     else:
        #         self._uppercase_table(table)

        logging.info('Process execution completed')


if __name__ == "__main__":
    component_start = core.utils.now()
    if len(sys.argv) > 1:
        set_debug = sys.argv[1]
    else:
        set_debug = False

    try:
        if os.path.dirname(current_path) == '/code':
            # Running in docker, assumes volume ./code
            comp = Component(debug=set_debug)
            comp.run()
        else:
            # Running locally, not in Docker
            debug_data_path = os.path.join(module_path, 'data')
            comp = Component(debug=set_debug, data_path=debug_data_path)
            comp.run()

        component_end = core.utils.now()
        component_duration = (component_end - component_start).total_seconds()
        logging.info('Extraction completed successfully in {} seconds'.format(component_duration))

    except Exception as generic_err:
        logging.exception(generic_err)
        exit(1)


================================================
File: src/table_metadata.py
================================================
import ast
from dataclasses import dataclass
from typing import Optional, List


@dataclass
class ColumnSchema:
    """
    Defines the name and type specifications of a single field in a table
    """
    name: str
    source_type: Optional[str] = None
    base_type: str = 'STRING'
    source_type_signature: Optional[str] = None
    description: Optional[str] = ""
    nullable: bool = False
    length: Optional[str] = None
    precision: Optional[str] = None
    default: Optional[str] = None


def is_type_with_length(source_type: str, types_with_length: list[str]):
    for t in types_with_length:
        if source_type.upper() in t.upper():
            return True
    return False


def column_metadata_to_schema(col_name: str, column_metadata: List[dict], types_with_length: list[str]):
    schema = ColumnSchema(col_name)
    for md in column_metadata:
        try:
            if md['key'] == 'KBC.datatype.type':
                schema.source_type = md['value']

                if is_type_with_length(md['value'], types_with_length):
                    size = ()
                    if len(split_parts := md['value'].split('(')) > 1:
                        # remove anything after ) e.g. int(12) unsigned)
                        size_str = split_parts[1].split(')')[0]
                        size = ast.literal_eval(f'({size_str})')

                    if size and isinstance(size, tuple):
                        schema.length = size[0]
                        schema.precision = size[1]
                    elif size:
                        schema.length = size

            if md['key'] == 'KBC.datatype.basetype':
                schema.base_type = md['value']
        except Exception as e:
            raise Exception(f'Failed to convert column datatype - {e}. Column: {col_name}, Metadata: {md}') from e

    return schema


================================================
File: src/core/__init__.py
================================================
from . import utils
from .utils import (
    chunk,
    load_json,
    parse_args,
    ratelimit,
    strftime,
    strptime_to_utc,
    update_state,
    should_sync_field,
    find_files
)

from .metrics import (
    Counter,
    Timer,
    job_timer,
    record_counter
)

from .messages import (
    ActivateVersionMessage,
    Message,
    MessageStore,
    RecordMessage,
    SchemaMessage,
    StateMessage,
    format_message,
    parse_message,
    write_message
)

from .catalog import (
    Catalog,
    CatalogEntry
)
from .schema import Schema

from .bookmarks import (
    write_bookmark,
    get_bookmark,
    clear_bookmark,
    reset_stream,
    set_offset,
    clear_offset,
    get_offset,
    set_currently_syncing,
    get_currently_syncing,
    update_schema_in_state
)

from .env_handler import (
    KBCEnvHandler
)

from .datatypes import (
    BASE_BOOLEAN,
    BASE_DATE,
    BASE_FLOAT,
    BASE_INTEGER,
    BASE_NUMERIC,
    BASE_STRING,
    BASE_TIMESTAMP
)

if __name__ == "__main__":
    import doctest
    doctest.testmod()


================================================
File: src/core/bookmarks.py
================================================
import logging

from mysql.replication import common

KEY_STORAGE_COLUMNS = 'storage_columns'
KEY_LAST_TABLE_SCHEMAS = 'last_table_schemas'


def ensure_bookmark_path(state, path):
    submap = state
    for path_component in path:
        if submap.get(path_component) is None:
            submap[path_component] = {}

        submap = submap[path_component]
    return state


def write_bookmark(state, tap_stream_id, key, val):
    state = ensure_bookmark_path(state, ['bookmarks', tap_stream_id])
    state['bookmarks'][tap_stream_id][key] = val
    return state


def clear_bookmark(state, tap_stream_id, key):
    state = ensure_bookmark_path(state, ['bookmarks', tap_stream_id])
    state['bookmarks'][tap_stream_id].pop(key, None)
    return state


def reset_stream(state, tap_stream_id):
    state = ensure_bookmark_path(state, ['bookmarks', tap_stream_id])
    state['bookmarks'][tap_stream_id] = {}
    return state


def get_bookmark(state, tap_stream_id, key, default=None):
    return state.get('bookmarks', {}).get(tap_stream_id, {}).get(key, default)


def set_offset(state, tap_stream_id, offset_key, offset_value):
    state = ensure_bookmark_path(state, ['bookmarks', tap_stream_id, "offset", offset_key])
    state['bookmarks'][tap_stream_id]["offset"][offset_key] = offset_value
    return state


def clear_offset(state, tap_stream_id):
    state = ensure_bookmark_path(state, ['bookmarks', tap_stream_id, "offset"])
    state['bookmarks'][tap_stream_id]["offset"] = {}
    return state


def get_offset(state, tap_stream_id, default=None):
    return state.get('bookmarks', {}).get(tap_stream_id, {}).get("offset", default)


def set_currently_syncing(state, tap_stream_id):
    state['currently_syncing'] = tap_stream_id
    return state


def get_currently_syncing(state, default=None):
    return state.get('currently_syncing', default)


# ################ state parameters

def update_schema_in_state(state: dict, table_schema_cache: dict):
    last_table_schemas = state.get(KEY_LAST_TABLE_SCHEMAS, {})  # SAPI converts empty dicts to lists
    if last_table_schemas == []:
        last_table_schemas = {}

    if table_schema_cache == []:
        table_schema_cache = {}

    state[KEY_LAST_TABLE_SCHEMAS] = {**last_table_schemas, **table_schema_cache}
    # store columns
    if not state.get(KEY_STORAGE_COLUMNS):
        state[KEY_STORAGE_COLUMNS] = {}

    for table in table_schema_cache:

        last_storage_columns = state.get(KEY_STORAGE_COLUMNS, {})
        if last_storage_columns == []:
            last_storage_columns = {}

        columns = last_storage_columns.get(table, [])
        # append non-existing
        for schema in table_schema_cache[table]:
            if schema['COLUMN_NAME'].upper() not in columns:
                logging.debug(f"Adding new column {schema['COLUMN_NAME']} to the column state "
                              f"for table {schema}.{table}. "
                              f"Current state: {columns}. Current table_schema_cache: {table_schema_cache} ")
                columns.append(schema['COLUMN_NAME'].upper())

        # append system
        if common.KBC_SYNCED not in columns:
            columns.append(common.KBC_SYNCED)
        if common.KBC_DELETED not in columns:
            columns.append(common.KBC_DELETED)
        if common.BINLOG_CHANGE_AT not in columns:
            columns.append(common.BINLOG_CHANGE_AT)
        if common.BINLOG_READ_AT not in columns:
            columns.append(common.BINLOG_READ_AT)

        state[KEY_STORAGE_COLUMNS][table] = columns
    return state


================================================
File: src/core/catalog.py
================================================
"""
Provides an object model for a table mapping.
"""
import json
import logging
import sys

from . import metadata as metadata_module
from .bookmarks import get_currently_syncing
from .schema import Schema


def write_catalog(catalog):
    # If the catalog has no streams, log a warning
    if not catalog.streams:
        logging.warning("Catalog being written with no streams.")

    json.dump(catalog.to_dict(), sys.stdout, indent=2)


# pylint: disable=too-many-instance-attributes
class CatalogEntry:
    """Table mapping catalog."""

    def __init__(self, tap_stream_id=None, stream=None, primary_keys: list = None, key_properties=None,
                 schema=None, replication_key=None, is_view=None, database=None, table=None, row_count=None,
                 stream_alias=None, metadata=None, replication_method=None, ordered_output_columns: list = None,
                 binary_columns: list = None, full_schema=None, ):

        self.tap_stream_id = tap_stream_id
        self.stream = stream
        self.key_properties = key_properties
        self.schema = schema
        self.full_schema = full_schema
        self.ordered_output_columns = ordered_output_columns
        self.replication_key = replication_key
        self.replication_method = replication_method
        self.is_view = is_view
        self.database = database
        self.table = table
        self.row_count = row_count
        self.stream_alias = stream_alias
        self.metadata = metadata
        self.binary_columns = binary_columns

        self.primary_keys = primary_keys

        self.current_column_cache = {}

    def __str__(self):
        return str(self.__dict__)

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def is_selected(self):
        mdata = metadata_module.to_map(self.metadata)
        # pylint: disable=no-member
        return self.schema.selected or metadata_module.get(mdata, (), 'selected')

    def to_dict(self):
        result = {}
        if self.tap_stream_id:
            result['tap_stream_id'] = self.tap_stream_id
        if self.database:
            result['database_name'] = self.database
        if self.table:
            result['table_name'] = self.table
        if self.primary_keys:
            result['primary_keys'] = self.primary_keys
        if self.replication_key is not None:
            result['replication_key'] = self.replication_key
        if self.replication_method is not None:
            result['replication_method'] = self.replication_method
        if self.key_properties is not None:
            result['key_properties'] = self.key_properties
        if self.schema is not None:
            schema = self.schema.to_dict()  # pylint: disable=no-member
            result['schema'] = schema
        if self.is_view is not None:
            result['is_view'] = self.is_view
        if self.stream is not None:
            result['stream'] = self.stream
        if self.row_count is not None:
            result['row_count'] = self.row_count
        if self.stream_alias is not None:
            result['stream_alias'] = self.stream_alias
        if self.metadata is not None:
            result['metadata'] = self.metadata
        if self.ordered_output_columns is not None:
            result['schema_ordered_list'] = self.ordered_output_columns
        if self.binary_columns is not None:
            result['binary_columns'] = self.binary_columns
        else:
            result['binary_columns'] = []

        if self.current_column_cache:
            result['current_column_cache'] = self.current_column_cache
        return result


class Catalog:
    def __init__(self, streams):
        self.streams = streams

    def __str__(self):
        return str(self.__dict__)

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __len__(self):
        return len(self.to_dict()['streams'])

    @classmethod
    def load(cls, filename):
        with open(filename) as fp:  # pylint: disable=invalid-name
            return Catalog.from_dict(json.load(fp))

    @classmethod
    def from_dict(cls, data):
        # TODO: We may want to store streams as a dict where the key is a
        # tap_stream_id and the value is a CatalogEntry. This will allow
        # faster lookup based on tap_stream_id. This would be a breaking
        # change, since callers typically access the streams property
        # directly.
        streams = []
        for stream in data['streams']:
            entry = CatalogEntry()
            entry.tap_stream_id = stream.get('tap_stream_id')
            entry.stream = stream.get('stream')
            entry.replication_key = stream.get('replication_key')
            entry.key_properties = stream.get('key_properties')
            entry.database = stream.get('database_name')
            entry.table = stream.get('table_name')
            entry.schema = Schema.from_dict(stream.get('schema'))
            entry.is_view = stream.get('is_view')
            entry.stream_alias = stream.get('stream_alias')
            entry.metadata = stream.get('metadata')
            entry.primary_keys = stream.get('primary_keys')
            entry.replication_method = stream.get('replication_method')
            entry.binary_columns = stream.get('binary_columns')
            streams.append(entry)
        return Catalog(streams)

    def to_dict(self):
        return {'streams': [stream.to_dict() for stream in self.streams]}

    def dump(self):
        json.dump(self.to_dict(), sys.stdout, indent=2)

    def dumps(self):
        return json.dumps(self.to_dict(), indent=2)

    def get_stream(self, tap_stream_id):
        for stream in self.streams:
            if stream.tap_stream_id == tap_stream_id:
                return stream
        return None

    def _shuffle_streams(self, state):
        currently_syncing = get_currently_syncing(state)

        if currently_syncing is None:
            return self.streams

        matching_index = 0
        for i, catalog_entry in enumerate(self.streams):
            if catalog_entry.tap_stream_id == currently_syncing:
                matching_index = i
                break
        top_half = self.streams[matching_index:]
        bottom_half = self.streams[:matching_index]
        return top_half + bottom_half

    def get_selected_streams(self, state):
        for stream in self._shuffle_streams(state):
            if not stream.is_selected():
                logging.info('Skipping stream: %s', stream.tap_stream_id)
                continue

            yield stream


================================================
File: src/core/datatypes.py
================================================
"""
Keboola base data types for manifest files.
"""
BASE_STRING = {'string', 'char', 'enum', 'longtext', 'mediumtext', 'text', 'year',
               'varchar', 'set', 'json', 'binary', 'varbinary', 'blob', "tinyblob", "blob", "mediumblob", "longblob"}
BASE_INTEGER = {'integer', 'tinyint', 'smallint', 'mediumint', 'int', 'bigint'}
BASE_NUMERIC = {'numeric', 'decimal'}
BASE_FLOAT = {'float', 'double'}
BASE_BOOLEAN = {'boolean', 'bit', 'bool'}
BASE_DATE = {'date'}
BASE_TIMESTAMP = {'timestamp', 'datetime', 'time'}


================================================
File: src/core/env_handler.py
================================================
"""
KBC Environment Handler.
"""
import csv
import json
import logging
import math
import os
import sys
from datetime import timedelta
from collections import Counter
from datetime import datetime

import dateparser
import pytz
from dateutil.relativedelta import relativedelta
from keboola import docker
from pygelf import GelfUdpHandler, GelfTcpHandler

DEFAULT_DEL = ','
DEFAULT_ENCLOSURE = '"'


class KBCEnvHandler:
    """Class handling standard tasks for KBC component manipulation i.e. config load, validation

    It contains some useful methods helping with boilerplate tasks.
    """
    LOGGING_TYPE_STD = 'std'
    LOGGING_TYPE_GELF = 'gelf'

    def __init__(self, mandatory_params, data_path=None, log_level='INFO', logging_type=None):
        """

        Args:
            mandatory_params (array(str)): Array of parameter names that needs to be present in the config.json.
            May be nested, see :func:`KBCEnvHandler.validateConfig()` docs for more details.

            data_path (str): optional path to data folder - if not specified data folder fetched
                             from KBC_DATADIR if present, otherwise '/data' as default
            env variable by default.
        """
        # setup GELF if available
        # backward compatibility
        logging_type_inf = KBCEnvHandler.LOGGING_TYPE_GELF if os.getenv('KBC_LOGGER_ADDR',
                                                                        None) else KBCEnvHandler.LOGGING_TYPE_STD
        if not logging_type:
            logging_type = logging_type_inf

        if logging_type == KBCEnvHandler.LOGGING_TYPE_STD:
            self.set_default_logger(log_level)
        elif logging_type == KBCEnvHandler.LOGGING_TYPE_GELF:
            self.set_gelf_logger(log_level)

        if not data_path and os.environ.get('KBC_DATADIR'):
            data_path = os.environ.get('KBC_DATADIR')
        elif not data_path:
            data_path = '/data'

        self.kbc_config_id = os.environ.get('KBC_CONFIGID')

        self.data_path = data_path
        self.configuration = docker.Config(data_path)
        self.cfg_params = self.configuration.get_parameters()
        self.image_params = self.configuration.config_data["image_parameters"]
        self.tables_out_path = os.path.join(data_path, 'out', 'tables')
        self.tables_in_path = os.path.join(data_path, 'in', 'tables')

        self._mandatory_params = mandatory_params

    def validate_config(self, mandatory_params=[]):
        """
                Validates config parameters based on provided mandatory parameters.
                All provided parameters must be present in config to pass.
                ex1.:
                par1 = 'par1'
                par2 = 'par2'
                mandatory_params = [par1, par2]
                Validation will fail when one of the above parameters is not found

                Two levels of nesting:
                Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
                => at least one of the pars has to be present
                ex2.
                par1 = 'par1'
                par2 = 'par2'
                par3 = 'par3'
                groupPar1 = 'groupPar1'
                groupPar2 = 'groupPar2'
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group1]

                Folowing logical expression is evaluated:
                Par1 AND Par2 AND (groupPar1 OR groupPar2)

                ex3
                par1 = 'par1'
                par2 = 'par2'
                par3 = 'par3'
                groupPar1 = 'groupPar1'
                groupPar2 = 'groupPar2'
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group3]

                Following logical expression is evaluated:
                par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
                """
        return self.validate_parameters(self.cfg_params, mandatory_params, 'config parameters')

    def validate_image_parameters(self, mandatory_params):
        """
                Validates image parameters based on provided mandatory parameters.
                All provided parameters must be present in config to pass.
                ex1.:
                par1 = 'par1'
                par2 = 'par2'
                mandatory_params = [par1, par2]
                Validation will fail when one of the above parameters is not found

                Two levels of nesting:
                Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
                => at least one of the pars has to be present
                ex2.
                par1 = 'par1'
                par2 = 'par2'
                par3 = 'par3'
                groupPar1 = 'groupPar1'
                groupPar2 = 'groupPar2'
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group1]

                Folowing logical expression is evaluated:
                Par1 AND Par2 AND (groupPar1 OR groupPar2)

                ex3
                par1 = 'par1'
                par2 = 'par2'
                par3 = 'par3'
                groupPar1 = 'groupPar1'
                groupPar2 = 'groupPar2'
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group3]

                Following logical expression is evaluated:
                par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
                """
        return self.validate_parameters(self.image_params, mandatory_params, 'image/stack parameters')

    def validate_parameters(self, parameters, mandatory_params, _type):
        """
        Validates provided parameters based on provided mandatory parameters.
        All provided parameters must be present in config to pass.
        ex1.:
        par1 = 'par1'
        par2 = 'par2'
        mandatory_params = [par1, par2]
        Validation will fail when one of the above parameters is not found

        Two levels of nesting:
        Parameters can be grouped as arrays par3 = [groupPar1, groupPar2] => at least one of the pars has to be present
        ex2.
        par1 = 'par1'
        par2 = 'par2'
        par3 = 'par3'
        groupPar1 = 'groupPar1'
        groupPar2 = 'groupPar2'
        group1 = [groupPar1, groupPar2]
        group3 = [par3, group1]
        mandatory_params = [par1, par2, group1]

        Folowing logical expression is evaluated:
        Par1 AND Par2 AND (groupPar1 OR groupPar2)

        ex3
        par1 = 'par1'
        par2 = 'par2'
        par3 = 'par3'
        groupPar1 = 'groupPar1'
        groupPar2 = 'groupPar2'
        group1 = [groupPar1, groupPar2]
        group3 = [par3, group1]
        mandatory_params = [par1, par2, group3]

        Following logical expression is evaluated:
        par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
        """
        missing_fields = []
        for field in mandatory_params:
            if isinstance(field, list):
                missing_fields.extend(self._validate_par_group(field, parameters))
            elif not parameters.get(field):
                missing_fields.append(field)

        if missing_fields:
            raise ValueError(
                'Missing mandatory {} fields: [{}] '.format(_type, ', '.join(missing_fields)))

    # ================================= Logging ==============================
    @staticmethod
    def set_default_logger(log_level=logging.INFO):  # noqa: E301
        """
        Sets default console logger.

        Args:
            log_level: logging level, default: 'INFO'

        Returns: logging object

        """

        class InfoFilter(logging.Filter):
            def filter(self, rec):
                return rec.levelno in (logging.DEBUG, logging.INFO)

        hd1 = logging.StreamHandler(sys.stdout)
        hd1.addFilter(InfoFilter())
        hd2 = logging.StreamHandler(sys.stderr)
        hd2.setLevel(logging.WARNING)

        logging.getLogger().setLevel(log_level)
        # remove default handler
        for h in logging.getLogger().handlers:
            logging.getLogger().removeHandler(h)
        logging.getLogger().addHandler(hd1)
        logging.getLogger().addHandler(hd2)

        logger = logging.getLogger()
        return logger

    def set_gelf_logger(self, log_level=logging.INFO, transport_layer='TCP', stdout=False):  # noqa: E301
        """
        Sets gelf console logger. Handler for console output is not included by default,
        for testing in non-gelf environments use stdout=True.

        Args:
            log_level: logging level, default: 'INFO'
            transport_layer: 'TCP' or 'UDP', default:'UDP
            stdout:

        Returns: logging object

        """
        # remove existing handlers
        for h in logging.getLogger().handlers:
            logging.getLogger().removeHandler(h)
        if stdout:
            self.set_default_logger(log_level)

        # gelf handler setup
        host = os.getenv('KBC_LOGGER_ADDR', 'localhost')
        port = os.getenv('KBC_LOGGER_PORT', 12201)
        if transport_layer == 'TCP':
            gelf = GelfTcpHandler(host=host, port=port)
        elif transport_layer == 'UDP':
            gelf = GelfUdpHandler(host=host, port=port)
        else:
            raise ValueError(F'Unsupported gelf transport layer: {transport_layer}. Choose TCP or UDP')

        logging.getLogger().setLevel(log_level)
        logging.getLogger().addHandler(gelf)

        logger = logging.getLogger()
        return logger

    def _validate_par_group(self, par_group, parameters):
        missing_fields = []
        is_present = False
        for par in par_group:
            if isinstance(par, list):
                missing_subset = self._get_par_missing_fields(par, parameters)
                missing_fields.extend(missing_subset)
                if not missing_subset:
                    is_present = True

            elif parameters.get(par):
                is_present = True
            else:
                missing_fields.append(par)
        if not is_present:
            return missing_fields
        else:
            return []

    @staticmethod
    def _get_par_missing_fields(mand_params, parameters):
        missing_fields = []
        for field in mand_params:
            if not parameters.get(field):
                missing_fields.append(field)
        return missing_fields

    @staticmethod
    def get_storage_token():
        try:
            return os.environ["KBC_TOKEN"]
        except Exception:
            logging.error("Storage token is missing in KBC_TOKEN env variable.")
            exit(2)

    def get_authorization(self):
        """
        Returns a dictionary containing the authentication part of the configuration file. If not present,
        an exception is raised.
        The dictionary returned has the following form:
        {
            "id": "main",
            "authorizedFor": "Myself",
            "creator": {
              "id": "1234",
              "description": "me@keboola.com"
            },
            "created": "2016-01-31 00:13:30",
            "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*$\"}",
            "oauthVersion": "2.0",
            "appKey": "000003423433C184A49",
            "#appSecret": "sdsadasdas-CiN"
        }
        """

        try:
            return self.configuration.config_data["authorization"]["oauth_api"]["credentials"]
        except KeyError:
            logging.error("Authorization is missing in configuration file.")
            exit(2)

    def get_input_table_by_name(self, table_name):
        tables = self.configuration.get_input_tables()
        table = [t for t in tables if t.get('destination') == table_name]
        if not table:
            raise ValueError(
                'Specified input mapping [{}] does not exist'.format(table_name))
        return table[0]

    def get_image_parameters(self):
        return self.configuration.config_data["image_parameters"]

    def get_state_file(self):
        """

        Return dict representation of state file or nothing if not present

        Returns:
            dict:

        """
        logging.info('Loading state file...')
        state_file_path = os.path.join(self.data_path, 'in', 'state.json')

        if not os.path.isfile(state_file_path):
            logging.info('State file not found. First run?')
            return
        try:
            with open(state_file_path, 'r') \
                    as state_file:
                state = json.load(state_file)
                # TMP convert small keys to uppercase TODO: remove
                cols = state.get('storage_columns', {})
                for c in cols:
                    cols[c] = [c.upper() for c in cols[c]]
                return state

        except (OSError, IOError):
            raise ValueError(
                "Unable to read state file state.json"
            )

    def write_state_file(self, state_dict: dict, output_path: str = None):
        """
        Stores state file.
        Args:
            state_dict: State output
            output_path: Full output path file including name, optional
        """
        if output_path is None:
            output_path = os.path.join(self.configuration.data_dir, 'out', 'state.json')

        if not isinstance(state_dict, dict):
            raise TypeError('Dictionary expected as the state file data type!')

        with open(output_path, 'w+') as state_file:
            json.dump(state_dict, state_file)

    def get_and_remove_headers_in_all(self, files, delimiter, enclosure):
        """
        Removes header from all specified files and return it as a list of strings

        Throws error if there is some file with different header.

        """
        first_run = True
        for file in files:
            curr_header = self._get_and_remove_headers(
                file, delimiter, enclosure)
            if first_run:
                header = curr_header
                first_file = file
                first_run = False
            # check whether header matches
            if Counter(header) != Counter(curr_header):
                raise Exception('Header in file {}:[{}] is different than header in file {}:[{}]'.format(
                    first_file, header, file, curr_header))
        return header

    @staticmethod
    def _get_and_remove_headers(file, delimiter, enclosure):
        """
        Removes header from specified file and return it as a list of strings.
        Creates new updated file 'upd_'+origFileName and deletes the original
        """
        head, tail = os.path.split(file)
        with open(file, "r") as input_file:
            with open(os.path.join(head, 'upd_' + tail), 'w+', newline='') as updated:
                reader = csv.DictReader(
                    input_file, delimiter=delimiter, quotechar=enclosure)
                header = reader.fieldnames
                writer = csv.DictWriter(
                    updated, fieldnames=header, delimiter=DEFAULT_DEL, quotechar=DEFAULT_ENCLOSURE)
                for row in reader:
                    # write row
                    writer.writerow(row)
        os.remove(file)
        return header

    # UTIL functions
    @staticmethod
    def get_date_period_converted(period_from, period_to):
        """
        Returns given period parameters in datetime format, or next step in back-fill mode
        along with generated last state for next iteration.

        :param period_from: str YYYY-MM-DD or relative string supported by date parser e.g. 5 days ago
        :param period_to: str YYYY-MM-DD or relative string supported by date parser e.g. 5 days ago

        :return: start_date: datetime, end_date: datetime
        """

        start_date_form = dateparser.parse(period_from)
        end_date_form = dateparser.parse(period_to)
        day_diff = (end_date_form - start_date_form).days
        if day_diff < 0:
            raise ValueError("start_date cannot exceed end_date.")

        return start_date_form, end_date_form

    @staticmethod
    def get_backfill_period(period_from, period_to, last_state):
        """
        Get backfill period, either specified period in datetime type or period based on a previous run (last_state)
        Continues iterating date periods based on the initial period size defined by from and to parameters.
        ex.:
        Run 1:
        _get_backfill_period("2018-01-01", "2018-01-03", None ) -> datetime("2018-01-01"),datetime("2018-01-03"),state)

        Run 2:
        _get_backfill_period("2018-01-01", "2018-01-03", last_state(from previous) )
                -> datetime("2018-01-03"), datetime("2018-01-05"), state)

        etc...

        :type last_state: dict
        - None or state file produced by backfill mode
        e.g. {"last_period" : {
                                "start_date": "2018-01-01",
                                "end_date": "2018-01-02"
                                }
            }

        :type period_to: str YYYY-MM-DD format or relative string supported by date parser e.g. 5 days ago
        :type period_from: str YYYY-MM-DD format or relative string supported by date parser e.g. 5 days ago
        :rtype: start_date: datetime, end_date: datetime, state_file: dict
        """
        if last_state and last_state.get('last_period'):
            last_start_date = datetime.strptime(
                last_state['last_period']['start_date'], '%Y-%m-%d')
            last_end_date = datetime.strptime(
                last_state['last_period']['end_date'], '%Y-%m-%d')

            diff = last_end_date - last_start_date
            # if period is a single day
            if diff.days == 0:
                diff = timedelta(days=1)

            start_date = last_end_date
            if (last_end_date.date() + diff) >= datetime.now(pytz.utc).date() + timedelta(days=1):
                end_date = datetime.now(pytz.utc)
            else:
                end_date = last_end_date + diff
        else:
            start_date = dateparser.parse(period_from)
            end_date = dateparser.parse(period_to)
        return start_date, end_date

    @staticmethod
    def get_past_date(str_days_ago: str, to_date: datetime = None,
                      tz: pytz.tzinfo.BaseTzInfo = pytz.utc) -> object:
        """
        Returns date in specified timezone relative to today.

        e.g.
        '5 hours ago',
        'yesterday',
        '3 days ago',
        '4 months ago',
        '2 years ago',
        'today'

        :param str_days_ago: (str)
        :param to_date: (datetime)
        :param tz: (pytz.tzinfo.BaseTzInfo)
        :return:
        """
        if to_date:
            today = to_date
        else:
            today = datetime.now(tz)

        try:
            today_diff = (datetime.now(tz) - today).days
            past_date = dateparser.parse(str_days_ago)
            past_date.replace(tzinfo=tz)
            date = past_date - relativedelta(days=today_diff)
            return date
        except TypeError:
            raise ValueError(
                "Please enter valid date parameters. Some of the values (%s, %s)are not in supported format",
                str_days_ago)

    def split_dates_to_chunks(self, start_date, end_date, intv, strformat="%m%d%Y"):
        """
        Splits dates in given period into chunks of specified max size.

        Params:
        start_date -- start_period [datetime]
        end_date -- end_period [datetime]
        intv -- max chunk size
        strformat -- dateformat of result periods

        Usage example:
        list(split_dates_to_chunks("2018-01-01", "2018-01-04", 2, "%Y-%m-%d"))

            returns [{start_date: "2018-01-01", "end_date":"2018-01-02"}
                     {start_date: "2018-01-02", "end_date":"2018-01-04"}]
        """
        return list(self._split_dates_to_chunks_gen(start_date, end_date, intv, strformat))

    @staticmethod
    def _split_dates_to_chunks_gen(start_date, end_date, intv, strformat="%m%d%Y"):
        """
        Splits dates in given period into chunks of specified max size.

        Params:
        start_date -- start_period [datetime]
        end_date -- end_period [datetime]
        intv -- max chunk size
        strformat -- dateformat of result periods

        Usage example:
        list(split_dates_to_chunks("2018-01-01", "2018-01-04", 2, "%Y-%m-%d"))

            returns [{start_date: "2018-01-01", "end_date":"2018-01-02"}
                     {start_date: "2018-01-02", "end_date":"2018-01-04"}]
        """

        nr_days = (end_date - start_date).days

        if nr_days <= intv:
            yield {'start_date': start_date.strftime(strformat),
                   'end_date': end_date.strftime(strformat)}
        elif intv == 0:
            diff = timedelta(days=1)
            for i in range(nr_days):
                yield {'start_date': (start_date + diff * i).strftime(strformat),
                       'end_date': (start_date + diff * i).strftime(strformat)}
        else:
            nr_parts = math.ceil(nr_days / intv)
            diff = (end_date - start_date) / nr_parts
            for i in range(nr_parts):
                yield {'start_date': (start_date + diff * i).strftime(strformat),
                       'end_date': (start_date + diff * (i + 1)).strftime(strformat)}


================================================
File: src/core/messages.py
================================================
import base64
import csv
import logging
import os
from dataclasses import dataclass
from typing import Dict, List, TextIO

import ciso8601
import pytz
import simplejson as json

from core.bookmarks import KEY_STORAGE_COLUMNS
from mysql.replication import common

SCHEMA_CHANGE_COLS = ['schema', 'table', 'change_type', 'column_name', 'query', 'timestamp']

try:
    import core.utils as u
except ImportError:
    import src.core.utils as u


@dataclass
class WriterCacheRecord:
    writer: csv.DictWriter
    out_stream: TextIO


class Message:
    """Base class for messages."""

    def asdict(self):  # pylint: disable=no-self-use
        raise Exception('Not implemented')

    def __eq__(self, other):
        return isinstance(other, Message) and self.asdict() == other.asdict()

    def __repr__(self):
        pairs = ["{}={}".format(k, v) for k, v in self.asdict().items()]
        attrstr = ", ".join(pairs)
        return "{}({})".format(self.__class__.__name__, attrstr)

    def __str__(self):
        return str(self.asdict())


class RecordMessage(Message):
    """RECORD message.
    The RECORD message has these fields:
      * stream (string) - The name of the stream the record belongs to.
      * record (dict) - The raw data for the record
      * version (optional, int) - For versioned streams, the version
        number. Note that this feature is experimental and most Taps and
        Targets should not need to use versioned streams.
    msg = core.RecordMessage(
        stream='users',
        record={'id': 1, 'name': 'Mary'})
    """

    def __init__(self, stream, record, column_map: dict, version=None, time_extracted=None):
        self.stream = stream
        self.record = record
        self.version = version
        self.time_extracted = time_extracted
        self.column_map = column_map
        if time_extracted and not time_extracted.tzinfo:
            raise ValueError("'time_extracted' must be either None " +
                             "or an aware datetime (with a time zone)")

    def asdict(self):
        result = {
            'type': 'RECORD',
            'stream': self.stream,
            'record': self.record,
            'column_map': self.column_map
        }
        if self.version is not None:
            result['version'] = self.version
        if self.time_extracted:
            as_utc = self.time_extracted.astimezone(pytz.utc)
            result['time_extracted'] = u.strftime(as_utc)
        return result

    def __str__(self):
        return str(self.asdict())


class SchemaMessage(Message):
    """SCHEMA message.
    The SCHEMA message has these fields:
      * stream (string) - The name of the stream this schema describes.
      * schema (dict) - The JSON schema.
      * key_properties (list of strings) - List of primary key properties.
    msg = core.SchemaMessage(
        stream='users',
        schema={'type': 'object',
                'properties': {
                    'id': {'type': 'integer'},
                    'name': {'type': 'string'}
                }
               },
        key_properties=['id'])
    """

    def __init__(self, stream, schema, key_properties, bookmark_properties=None):
        self.stream = stream
        self.schema = schema
        self.key_properties = key_properties

        if isinstance(bookmark_properties, (str, bytes)):
            bookmark_properties = [bookmark_properties]
        if bookmark_properties and not isinstance(bookmark_properties, list):
            raise Exception("bookmark_properties must be a string or list of strings")

        self.bookmark_properties = bookmark_properties

    def asdict(self):
        result = {
            'type': 'SCHEMA',
            'stream': self.stream,
            'schema': self.schema,
            'key_properties': self.key_properties
        }
        if self.bookmark_properties:
            result['bookmark_properties'] = self.bookmark_properties
        return result


class StateMessage(Message):
    """STATE message.
    The STATE message has one field:
      * value (dict) - The value of the state.
    msg = core.StateMessage(
        value={'users': '2017-06-19T00:00:00'})
    """

    def __init__(self, value):
        self.value = value

    def asdict(self):
        return {
            'type': 'STATE',
            'value': self.value
        }


class ActivateVersionMessage(Message):
    """ACTIVATE_VERSION message (EXPERIMENTAL).
    The ACTIVATE_VERSION messages has these fields:
      * stream - The name of the stream.
      * version - The version number to activate.
    This is a signal to the Target that it should delete all previously
    seen data and replace it with all the RECORDs it has seen where the
    record's version matches this version number.
    Note that this feature is experimental. Most Taps and Targets should
    not need to use the "version" field of "RECORD" messages or the
    "ACTIVATE_VERSION" message at all.
    msg = core.ActivateVersionMessage(
        stream='users',
        version=2)
    """

    def __init__(self, stream, version):
        self.stream = stream
        self.version = version

    def asdict(self):
        return {
            'type': 'ACTIVATE_VERSION',
            'stream': self.stream,
            'version': self.version
        }


def _required_key(msg, k):
    if k not in msg:
        raise Exception("Message is missing required key '{}': {}".format(k, msg))

    return msg[k]


def get_message_type(input_message: dict):
    return _required_key(input_message, 'type')


def get_stream_name(input_message: dict):
    return _required_key(input_message, 'stream')


def parse_message(msg):
    """Parse a message string into a Message object."""
    # TODO: May use Decimal for parsing data here for perfect precision
    obj = json.loads(msg, use_decimal=True)
    msg_type = _required_key(obj, 'type')

    if msg_type == 'RECORD':
        time_extracted = obj.get('time_extracted')
        if time_extracted:
            try:
                time_extracted = ciso8601.parse_datetime(time_extracted)
            except Exception:
                logging.warning("unable to parse time_extracted with ciso8601 library")
                time_extracted = None

            # time_extracted = dateutil.parser.parse(time_extracted)
        return RecordMessage(stream=_required_key(obj, 'stream'), record=_required_key(obj, 'record'),
                             version=obj.get('version'), time_extracted=time_extracted)

    elif msg_type == 'SCHEMA':
        return SchemaMessage(stream=_required_key(obj, 'stream'), schema=_required_key(obj, 'schema'),
                             key_properties=_required_key(obj, 'key_properties'),
                             bookmark_properties=obj.get('bookmark_properties'))

    elif msg_type == 'STATE':
        return StateMessage(value=_required_key(obj, 'value'))

    elif msg_type == 'ACTIVATE_VERSION':
        return ActivateVersionMessage(stream=_required_key(obj, 'stream'), version=_required_key(obj, 'version'))
    else:
        return None


def handle_binary_data(row: dict, binary_columns: list, binary_data_handler, replace_nulls: bool = False):
    for column in binary_columns:
        if column not in row:
            pass

        else:
            try:
                value_to_convert = row[column]
                if value_to_convert is None:
                    value_to_convert = b'\x00'
                if replace_nulls is True:
                    value_to_convert = value_to_convert.strip(b'\x00')

                if binary_data_handler == 'plain':
                    row[column] = value_to_convert.decode()
                elif binary_data_handler == 'hex':
                    row[column] = value_to_convert.hex().upper()
                elif binary_data_handler == 'base64':
                    row[column] = base64.b64encode(value_to_convert).decode()
                else:
                    logging.error(f"Unknown binary data handler format: {binary_data_handler}.")
                    exit(1)
            except Exception as e:
                logging.error(f'Failed to process binary column {column}, failed to convert value "{row[column]}". '
                              'Try to choose another binary handler strategy.')
                raise e

    return row


class MessageStore(dict):
    """Storage for log-based messages"""

    def __init__(self, state: dict = None, flush_row_threshold: int = 5000,
                 output_table_path: str = '/data/out/tables', binary_handler: str = 'plain', output_bucket=''):
        super().__init__()
        self.state = state
        self.flush_row_threshold = flush_row_threshold
        self.output_table_path = output_table_path
        self.binary_data_handler = binary_handler

        # cache of dict writers
        self._writer_cache: Dict[str, WriterCacheRecord] = {}
        self._schema_change_writer = None

        self._data_store = {}
        self._found_schemas = []
        self._found_tables = []
        self._found_headers = {}
        self._processed_records = 0
        self._flush_count = 0
        self.output_bucket = output_bucket

        # self.io = {}
        # self.io_csv = {}

    def __str__(self):
        return str(self._data_store)
        # return json.dumps(self._data_store)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, exc_traceback):
        self.flush_records()
        self._close_writer_cache()

    @property
    def found_schemas(self):
        return self._found_schemas

    @property
    def found_tables(self):
        return self._found_tables

    @property
    def found_headers(self):
        return self._found_headers

    @property
    def flush_count(self):
        return self._flush_count

    @property
    def total_records_flushed(self):
        return self._flush_count * self.flush_row_threshold

    def get_state(self):
        return self.state

    def add_schema(self, schema: str):
        logging.debug('Adding schema {} to message store'.format(schema))
        self._data_store[schema] = {}

    def add_table(self, schema: str, table: str):
        logging.debug('Adding table {} to message store in schema {}'.format(table, schema))
        store_schema = self._data_store.get(schema)
        if not store_schema:
            self.add_schema(schema)

        self._data_store[schema][table] = {'records': [], 'schemas': []}

    def _get_all_table_columns(self, schema: str, table: str):
        columns = list(self._data_store[schema][table]['column_schema'].keys())
        index = common.generate_tap_stream_id(schema, table)
        # merge with storage columns from state
        for c in self.state.get(KEY_STORAGE_COLUMNS, {}).get(index, []):
            if c not in columns:
                columns.append(c)
        return columns

    def write_schema_change_message(self, message: dict):
        if self._schema_change_writer is None:
            path = os.path.join(self.output_table_path, 'SCHEMA_CHANGES.csv')
            writer = csv.DictWriter(open(path, 'w+', newline=''), fieldnames=SCHEMA_CHANGE_COLS)
            self._schema_change_writer = writer
            self._schema_change_writer.writeheader()
            # create manifest
            _schema_changes_destination = f'{self.output_bucket}_metadata.SCHEMA_CHANGES'
            manifest = {'primary_key': ['column_name', 'query', 'timestamp'],
                        'incremental': True,
                        'destination': _schema_changes_destination}
            with open(path + '.manifest', 'w') as manifest_file:
                json.dump(manifest, manifest_file)

        self._schema_change_writer.writerow(message)

    def flush_records(self):
        logging.debug('Flushing records for each of found tables: {}'.format(self._found_tables))
        for schema in self._found_schemas:
            for table in self.found_tables:
                if self._data_store[schema][table].get('records'):
                    # logging.debug('got records for {} {}'.format(schema, table))

                    binary_columns = self._data_store[schema][table]['schemas'][0]['binary']
                    if binary_columns == []:
                        binary_columns = None
                    file_output = table.upper() + '.csv'

                    table_schema = self._get_all_table_columns(schema, table)

                    self.write_to_csv(self._data_store[schema][table].get('records'), file_output,
                                      table_schema, binary_columns)

                    self._clear_records(schema, table)

        self._processed_records = 0
        self._flush_count += 1

    def write_to_csv(self, data_records: List[dict], file_name: str, schema: list,
                     binary_columns: list = None):
        full_path = os.path.expanduser(os.path.join(self.output_table_path, file_name))
        if not data_records:
            logging.warning("No records to write!")
            return

        if not self._writer_cache.get(full_path):
            # init writer
            # get columns of all collected columns so far -> the rest will handle the writer
            columns = schema
            out_stream = open(full_path, 'w+')
            writer = csv.DictWriter(out_stream, columns, quoting=csv.QUOTE_ALL)
            writer.writeheader()
            self._writer_cache[full_path] = WriterCacheRecord(writer, out_stream)
        else:
            writer = self._writer_cache.get(full_path).writer

        for record in data_records:
            if binary_columns is not None:
                record = handle_binary_data(record, binary_columns, self.binary_data_handler)
            writer.writerow(record)

    def add_message(self, schema: str, input_message: dict):
        msg_type = get_message_type(input_message)

        if schema and schema not in self._found_schemas:
            self.add_schema(schema)
            self._found_schemas.append(schema)

        if msg_type == 'RECORD':
            table_name = get_stream_name(input_message)
            if table_name not in self._found_tables:
                self.add_table(schema, table_name)
                self._found_tables.append(table_name)
            self._expand_column_schema(schema, table_name, input_message['column_map'])
            self._add_record_message(schema, table_name, _required_key(input_message, 'record'))

        elif msg_type == 'SCHEMA':
            table_name = get_stream_name(input_message)
            if table_name not in self._found_tables:
                self.add_table(schema, table_name)
                self._found_tables.append(table_name)

            self._add_schema_message(schema, table_name, _required_key(input_message, 'schema'))

        elif msg_type == 'STATE':
            self._set_state(_required_key(input_message, 'value'))

        else:
            logging.info('Message type not found: {}'.format(msg_type))

    def _clear_records(self, schema: str, table: str):
        self._data_store[schema][table]['records'] = []

    def _expand_column_schema(self, schema, table, column_schema):
        cache_record = self._data_store[schema][table]

        if cache_record.get('column_schema') is None:
            cache_record['column_schema'] = {}

        for col in column_schema:
            col_name = col['COLUMN_NAME']
            if col_name not in cache_record['column_schema']:
                cache_record['column_schema'][col_name] = col

            # originaly updated from initial schema message
            if 'binary' in col['COLUMN_TYPE'].lower() and col_name not in cache_record['schemas'][0]['binary']:
                cache_record['schemas'][0]['binary'].append(col_name)

    def _add_record_message(self, schema: str, table: str, record_message: dict):
        if self._processed_records > self.flush_row_threshold:
            self.flush_records()

        self._data_store[schema][table]['records'].append(record_message)
        self._processed_records += 1

    def _add_schema_message(self, schema: str, table: str, schema_message: dict):
        binary_columns = []

        for column_name, column_schema in schema_message['properties'].items():
            if 'binary' in column_schema['type']:
                binary_columns += [column_name]

        schema_message['binary'] = binary_columns
        self._data_store[schema][table]['schemas'].append(schema_message)
        if not self.found_headers.get(schema):
            self._found_headers[schema] = []
        self._found_headers[schema].append({table: list(schema_message.get('properties').keys())})

    def _set_state(self, state_message):
        self.state = state_message

    def _close_writer_cache(self):
        for table, wr in self._writer_cache.items():
            logging.info(f"Closing out stream for {table}")
            wr.out_stream.close()


def format_message(message):
    try:
        return json.dumps(message.asdict(), use_decimal=True)
    except AttributeError:  # Message may be already dict if converted to handle for non-JSON-serializable columns
        return json.dumps(message, use_decimal=True)


def write_message(message, database_schema: str = None, message_store: MessageStore = None):
    if message_store is None:  # Specifically none, as default message store is empty dict
        logging.warning('NOTE: Write message declared without message store: {}'.format(message.asdict()))
    else:
        message_store.add_message(database_schema, message.asdict())


================================================
File: src/core/metadata.py
================================================
def new():
    return {}


def to_map(raw_metadata):
    return {tuple(md['breadcrumb']): md['metadata'] for md in raw_metadata}


def to_list(compiled_metadata):
    return [{'breadcrumb': k, 'metadata': v} for k, v in compiled_metadata.items()]


def delete(compiled_metadata, breadcrumb, k):
    del compiled_metadata[breadcrumb][k]


def write(compiled_metadata, breadcrumb, k, val):
    if val is None:
        raise Exception()
    if breadcrumb in compiled_metadata:
        compiled_metadata.get(breadcrumb).update({k: val})
    else:
        compiled_metadata[breadcrumb] = {k: val}
    return compiled_metadata


def get(compiled_metadata, breadcrumb, k):
    return compiled_metadata.get(breadcrumb, {}).get(k)


def get_standard_metadata(schema=None, schema_name=None, key_properties=None,
                          valid_replication_keys=None, replication_method=None):
    mdata = {}

    if key_properties is not None:
        mdata = write(mdata, (), 'table-key-properties', key_properties)
    if replication_method:
        mdata = write(mdata, (), 'forced-replication-method', replication_method)
    if valid_replication_keys is not None:
        mdata = write(mdata, (), 'valid-replication-keys', valid_replication_keys)
    if schema:
        mdata = write(mdata, (), 'inclusion', 'available')

        if schema_name:
            mdata = write(mdata, (), 'schema-name', schema_name)
        for field_name in schema['properties'].keys():
            if key_properties and field_name in key_properties:
                mdata = write(mdata, ('properties', field_name), 'inclusion', 'automatic')
            else:
                mdata = write(mdata, ('properties', field_name), 'inclusion', 'available')

    return to_list(mdata)


================================================
File: src/core/metrics.py
================================================
"""
Utilities for logging and parsing metrics.
An extractor should use this library to log structured messages about the read
operations it makes.
Counter is a general-purpose class that allows you to increment a
"counter"-type metric. You initialize a Counter as a context manager, with
a metric name and a dictionary of tags. The Counter will periodically emit
a metric that indicates the amount by which the counter was incremented
since the last time it reported. For example, to increment a record count
for records from a "users" endpoint, you could do:
    with Counter('record_count', {'endpoint': 'users'}) as counter:
        for record in my_records:
            # Do stuff...
            counter.increment()
Timer is class that allows you to track the timing of operations. Like
Counter, you initialize it as a context manager, with a metric name and a
dictionary of tags. When the context exits, the timer will emit a single
metric that indicates how long it took in seconds. The metric will
automatically include a tag called "status" that is set to "failed" if an
Exception was raised, or "succeeded" otherwise.
    with Timer('http_request_duration', {'endpoint': 'users'}):
        # Make a request, do some things
In order to encourage consistent metric and tag names, this module
provides several functions for creating Counters and Timers for very
commonly used metrics.
  * record_counter - Increments a 'record_count' metric to track number of
    records fetched from a source. Provides an "endpoint" tag.
  * http_request_timer - Emits an 'http_request_duration' metric to time
    HTTP requests. Provides "endpoint" tag.
  * job_timer - Emits a 'job_duration' metric to track time of
    asynchronous jobs. Provides "job_type" tag.
"""
import json
import logging
import re
import time
import timeit
from collections import namedtuple

DEFAULT_LOG_INTERVAL = 60


class Status:
    """Constants for status codes"""
    succeeded = 'succeeded'
    failed = 'failed'


class Metric:
    """Constants for metric names"""
    record_count = 'record_count'
    job_duration = 'job_duration'
    http_request_duration = 'http_request_duration'


class Tag:
    """Constants for commonly used tags"""

    endpoint = 'endpoint'
    job_type = 'job_type'
    http_status_code = 'http_status_code'
    status = 'status'


Point = namedtuple('Point', ['metric_type', 'metric', 'value', 'tags'])


def log(logger, point):
    """Log a single data point."""
    result = {
        'type': point.metric_type,
        'metric': point.metric,
        'value': point.value,
        'tags': point.tags
    }
    logger.info('METRIC: %s', json.dumps(result))


class Counter:
    """Increments a counter metric.
    When you use Counter as a context manager, it will automatically emit
    points for a "counter" metric periodically and also when the context
    exits. The only thing you need to do is initialize the Counter and
    then call increment().
    with core.metrics.Counter('record_count', {'endpoint': 'users'}) as counter:
       for user in get_users(...):
           # Print out the user
           counter.increment()
    This would print a metric like this:
    {
      "type":   "counter",
      "metric": "record_count",
      "value":   12345,
      "tags": {
        "endpoint": "users",
      }
    }
    """

    def __init__(self, metric, tags=None, log_interval=DEFAULT_LOG_INTERVAL):
        self.metric = metric
        self.value = 0
        self.tags = tags if tags else {}
        self.log_interval = log_interval
        self.logger = logging.getLogger(__name__)
        self.last_log_time = time.time()

    def __enter__(self):
        self.last_log_time = time.time()
        return self

    def increment(self, amount=1):
        """Increments value by the specified amount."""
        self.value += amount
        if self._ready_to_log():
            self._pop()

    def _pop(self):
        log(self.logger, Point('counter', self.metric, self.value, self.tags))
        self.value = 0
        self.last_log_time = time.time()

    def __exit__(self, exc_type, exc_value, traceback):
        self._pop()

    def _ready_to_log(self):
        return time.time() - self.last_log_time > self.log_interval


def timer(function):
    def new_function():
        start_time = timeit.default_timer()
        function()
        elapsed = timeit.default_timer() - start_time
        print('Function "{name}" took {time} seconds to complete.'.format(name=function.__name__, time=elapsed))
    return new_function()


class Timer:  # pylint: disable=too-few-public-methods
    """Produces metrics about the duration of operations.
    You use a Timer as a context manager wrapping around some operation.
    When the context exits, the Timer emits a metric that indicates how
    long (in seconds) the operation took.
    It will automatically include a "status" tag that is "failed" if the
    context exits with an Exception or "success" if it exits cleanly. You
    can override this by setting timer.status within the context.
    with core.metrics.Timer('request_duration', {'endpoint': 'users'}):
       # Do some stuff
    This produces a metric like this:
    {
      "type": "timer",
      "metric": "request_duration",
      "value": 1.23,
      "tags": {
        "endpoint": "users",
        "status": "success"
      }
    },
    """
    def __init__(self, metric, tags):
        self.metric = metric
        self.tags = tags if tags else {}
        self.logger = logging.getLogger(__name__)
        self.start_time = None

    def __enter__(self):
        self.start_time = time.time()
        return self

    def elapsed(self):
        """Return elapsed time."""
        return time.time() - self.start_time

    def __exit__(self, exc_type, exc_value, traceback):
        if Tag.status not in self.tags:
            if exc_type is None:
                self.tags[Tag.status] = Status.succeeded
            else:
                self.tags[Tag.status] = Status.failed
        log(self.logger, Point('timer', self.metric, self.elapsed(), self.tags))


def record_counter(endpoint=None, log_interval=DEFAULT_LOG_INTERVAL):
    """Use for counting records retrieved from the source.
    with core.metrics.record_counter(endpoint="users") as counter:
         for record in my_records:
             # Do something with the record
             counter.increment()
    """
    tags = {}
    if endpoint:
        tags[Tag.endpoint] = endpoint
    return Counter(Metric.record_count, tags, log_interval=log_interval)


def job_timer(job_type=None):
    """Use for timing asynchronous jobs
    with core.metrics.job_timer(job_type="users") as timer:
         # Make a request
    """
    tags = {}
    if job_type:
        tags[Tag.job_type] = job_type
    return Timer(Metric.job_duration, tags)


def parse(line):
    """Parse a Point from a log line and return it, or None if no data point."""
    match = re.match(r'^INFO METRIC: (.*)$', line)
    if match:
        json_str = match.group(1)
        try:
            raw = json.loads(json_str)
            return Point(
                metric_type=raw.get('type'),
                metric=raw.get('metric'),
                value=raw.get('value'),
                tags=raw.get('tags'))
        except Exception as exc:  # pylint: disable=broad-except
            logging.warning('Error parsing metric: %s', exc)
    return None


================================================
File: src/core/schema.py
================================================
# pylint: disable=redefined-builtin, too-many-arguments, invalid-name
"""
Provides an object model for JSON Schema.
"""
import json

# These are standard keys defined in the JSON Schema spec
STANDARD_KEYS = [
    'selected',
    'inclusion',
    'description',
    'minimum',
    'maximum',
    'exclusiveMinimum',
    'exclusiveMaximum',
    'multipleOf',
    'maxLength',
    'minLength',
    'format',
    'type',
    'additionalProperties',
    'anyOf',
    'patternProperties',
]


class Schema:  # pylint: disable=too-many-instance-attributes
    """Object model for JSON Schema.
    Tap and Target authors may find this to be more convenient than
    working directly with JSON Schema data structures.
    """

    # pylint: disable=too-many-locals
    def __init__(self, type=None, format=None, properties=None, items=None,
                 selected=None, inclusion=None, description=None, minimum=None,
                 maximum=None, exclusiveMinimum=None, exclusiveMaximum=None,
                 multipleOf=None, maxLength=None, minLength=None, additionalProperties=None,
                 anyOf=None, patternProperties=None, characterSet=None):

        self.type = type
        self.properties = properties
        self.items = items
        self.selected = selected
        self.inclusion = inclusion
        self.description = description
        self.minimum = minimum
        self.maximum = maximum
        self.exclusiveMinimum = exclusiveMinimum
        self.exclusiveMaximum = exclusiveMaximum
        self.multipleOf = multipleOf
        self.maxLength = maxLength
        self.minLength = minLength
        self.anyOf = anyOf
        self.format = format
        self.additionalProperties = additionalProperties
        self.patternProperties = patternProperties
        self.characterSet = characterSet

    def __str__(self):
        return json.dumps(self.to_dict())

    def __repr__(self):
        pairs = [k + '=' + repr(v) for k, v in self.__dict__.items()]
        args = ', '.join(pairs)
        return 'Schema(' + args + ')'

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def to_dict(self):
        """Return the raw JSON Schema as a (possibly nested) dict."""
        result = {}

        if self.properties is not None:
            result['properties'] = {
                k: v.to_dict()
                for k, v
                in self.properties.items()  # pylint: disable=no-member
            }

        if self.items is not None:
            result['items'] = self.items.to_dict()  # pylint: disable=no-member

        for key in STANDARD_KEYS:
            if self.__dict__.get(key) is not None:
                result[key] = self.__dict__[key]

        return result

    @classmethod
    def from_dict(cls, data, **schema_defaults):
        """Initialize a Schema object based on the JSON Schema structure.
        :param schema_defaults: The default values to the Schema
        constructor."""
        kwargs = schema_defaults.copy()
        properties = data.get('properties')
        items = data.get('items')

        if properties is not None:
            kwargs['properties'] = {
                k: Schema.from_dict(v, **schema_defaults)
                for k, v in properties.items()
            }
        if items is not None:
            kwargs['items'] = Schema.from_dict(items, **schema_defaults)
        for key in STANDARD_KEYS:
            if key in data:
                kwargs[key] = data[key]
        return Schema(**kwargs)


================================================
File: src/core/statediff.py
================================================
import collections

# Named tuples for holding add, change, and remove operations
Add = collections.namedtuple('Add', ['path', 'newval'])
Change = collections.namedtuple('Change', ['path', 'oldval', 'newval'])
Remove = collections.namedtuple('Remove', ['path', 'oldval'])


def paths(data, base=None):
    """Walk a data structure and return a list of (path, value) tuples, where
    each path is the path to a leaf node in the data structure and the
    value is the value it points to. Each path will be a tuple.
    """
    if base is None:
        base = ()

    result = []
    if isinstance(data, dict):
        for key, val in sorted(data.items()):
            result.extend(paths(val, base + (key,)))

    elif isinstance(data, list):
        for i, val in enumerate(data):
            result.extend(paths(val, base + (i,)))

    elif base:
        result.append((base, data))

    return result


def diff(oldstate, newstate):
    """Compare two states, returning a list of Add, Change, and Remove
    objects.
    Add(path, newval) means path exists in newstate but not oldstate and
    its value in newstate is newval.
    Change(path, oldval, newval) means that path exists in both oldstate
    and newstate but has different values. oldval is the val in oldstate
    and newval is the value in newstate.
    Remove(path, oldval) means the path exists in oldstate but not in
    newstate, and the value in oldstate is oldval.
    """

    # Convert oldstate and newstate from a deeply nested dict into a
    # single-level dict, mapping a path to a value.
    olddict = dict(paths(oldstate))
    newdict = dict(paths(newstate))

    # Build the list of all paths in both oldstate and newstate to iterate
    # over.
    all_paths = set()
    all_paths.update(set(olddict.keys()))
    all_paths.update(set(newdict.keys()))

    result = []
    for path in sorted(all_paths):
        if path in olddict:
            if path in newdict:
                if olddict[path] == newdict[path]:
                    pass  # Don't emit anything if values are the same
                else:
                    result.append(Change(path, olddict[path], newdict[path]))
            else:
                result.append(Remove(path, olddict[path]))
        else:
            result.append(Add(path, newdict[path]))
    return result


================================================
File: src/core/utils.py
================================================
"""
Utility functions to assist in data replication processes.
"""
import argparse
import collections
import datetime
import fnmatch
import functools
import json
import os
import time
from warnings import warn

import backoff as backoff_module
import dateutil.parser
import pytz
from typing.io import IO

from .catalog import Catalog

DATETIME_PARSE = "%Y-%m-%dT%H:%M:%SZ"
DATETIME_FMT = "%04Y-%m-%d %H:%M:%S.%f"
DATETIME_FMT_SAFE = "%Y-%m-%d %H:%M:%S.%f"


def now(format='dt'):
    if format == 'dt':
        return datetime.datetime.utcnow().replace(tzinfo=pytz.UTC)
    elif format == 'ts_1e6':
        return int(time.time() * 1e6)
    else:
        raise ValueError(f"Invalid format {format}.")


def strptime_with_tz(dtime):
    d_object = dateutil.parser.parse(dtime)
    if d_object.tzinfo is None:
        return d_object.replace(tzinfo=pytz.UTC)

    return d_object


def strptime(dtime):
    """DEPRECATED Use strptime_to_utc instead.
    Parse DTIME according to DATETIME_PARSE without TZ safety.
    >>> strptime("2018-01-01T00:00:00Z")
    datetime.datetime(2018, 1, 1, 0, 0)
    Requires the Z TZ signifier
    >>> strptime("2018-01-01T00:00:00")
    Traceback (most recent call last):
      ...
    ValueError: time data '2018-01-01T00:00:00' does not match format '%Y-%m-%dT%H:%M:%SZ'
    Can't parse non-UTC DTs
    >>> strptime("2018-01-01T00:00:00-04:00")
    Traceback (most recent call last):
      ...
    ValueError: time data '2018-01-01T00:00:00-04:00' does not match format '%Y-%m-%dT%H:%M:%SZ'
    Does not support fractional seconds
    >>> strptime("2018-01-01T00:00:00.000000Z")
    Traceback (most recent call last):
      ...
    ValueError: time data '2018-01-01T00:00:00.000000Z' does not match format '%Y-%m-%dT%H:%M:%SZ'
    """

    warn("Use strptime_to_utc instead", DeprecationWarning, stacklevel=2)

    return datetime.datetime.strptime(dtime, DATETIME_PARSE)


def strptime_to_utc(dtimestr):
    d_object = dateutil.parser.parse(dtimestr)
    if d_object.tzinfo is None:
        return d_object.replace(tzinfo=pytz.UTC)
    else:
        return d_object.astimezone(tz=pytz.UTC)


def strftime(dtime, format_str=DATETIME_FMT):
    if dtime.utcoffset() != datetime.timedelta(0):
        raise Exception("datetime must be pegged at UTC tzoneinfo")

    dt_str = None
    try:
        dt_str = dtime.strftime(format_str)
        if dt_str.startswith('4Y'):
            dt_str = dtime.strftime(DATETIME_FMT_SAFE)
    except ValueError:
        dt_str = dtime.strftime(DATETIME_FMT_SAFE)

    return dt_str


def ratelimit(limit, every):
    def limitdecorator(func):
        times = collections.deque()

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            if len(times) >= limit:
                tim0 = times.pop()
                tim = time.time()
                sleep_time = every - (tim - tim0)
                if sleep_time > 0:
                    time.sleep(sleep_time)

            times.appendleft(time.time())
            return func(*args, **kwargs)

        return wrapper

    return limitdecorator


def chunk(array, num):
    for i in range(0, len(array), num):
        yield array[i:i + num]


def load_json(path):
    with open(path) as fil:
        return json.load(fil)


def update_state(state, entity, dtime):
    if dtime is None:
        return

    if isinstance(dtime, datetime.datetime):
        dtime = strftime(dtime)

    if entity not in state:
        state[entity] = dtime

    if dtime >= state[entity]:
        state[entity] = dtime


def parse_args(required_config_keys):
    """Parse standard command-line args.
    Parses the command-line arguments mentioned in the SPEC and the
    BEST_PRACTICES documents:
    -c,--config     Config file
    -s,--state      State file
    -d,--discover   Run in discover mode
    -p,--properties Properties file: DEPRECATED, please use --catalog instead
    --catalog       Catalog file
    Returns the parsed args object from argparse. For each argument that
    point to JSON files (config, state, properties), we will automatically
    load and parse the JSON file.
    """
    parser = argparse.ArgumentParser()

    parser.add_argument(
        '-c', '--config',
        help='Config file',
        required=True)

    parser.add_argument(
        '-s', '--state',
        help='State file')

    parser.add_argument(
        '-p', '--properties',
        help='Property selections: DEPRECATED, Please use --catalog instead')

    parser.add_argument(
        '--catalog',
        help='Catalog file')

    parser.add_argument(
        '-d', '--discover',
        action='store_true',
        help='Do schema discovery')

    args = parser.parse_args()
    if args.config:
        setattr(args, 'config_path', args.config)
        args.config = load_json(args.config)
    if args.state:
        setattr(args, 'state_path', args.state)
        args.state = load_json(args.state)
    else:
        args.state = {}
    if args.properties:
        setattr(args, 'properties_path', args.properties)
        args.properties = load_json(args.properties)
    if args.catalog:
        setattr(args, 'catalog_path', args.catalog)
        args.catalog = Catalog.load(args.catalog)

    check_config(args.config, required_config_keys)

    return args


def check_config(config, required_keys):
    missing_keys = [key for key in required_keys if key not in config]
    if missing_keys:
        raise Exception("Config is missing required keys: {}".format(missing_keys))


def backoff(exceptions, giveup):
    """Decorates a function to retry up to 5 times using an exponential backoff
    function.
    exceptions is a tuple of exception classes that are retried
    giveup is a function that accepts the exception and returns True to retry
    """
    return backoff_module.on_exception(
        backoff_module.expo,
        exceptions,
        max_tries=5,
        giveup=giveup,
        factor=2)


def exception_is_4xx(exception):
    """Returns True if exception is in the 4xx range."""
    if not hasattr(exception, "response"):
        return False

    if exception.response is None:
        return False

    if not hasattr(exception.response, "status_code"):
        return False

    return 400 <= exception.response.status_code < 500


def handle_top_exception(logger):
    """A decorator that will catch exceptions and log the exception's message
    as a CRITICAL log."""

    def decorator(fnc):
        @functools.wraps(fnc)
        def wrapped(*args, **kwargs):
            try:
                return fnc(*args, **kwargs)
            except Exception as exc:
                logger.critical(exc)
                raise

        return wrapped

    return decorator


def find_files(base, pattern):
    """Return list of files matching pattern in base folder."""
    if not os.path.exists(base):
        return None
    else:
        return [n for n in fnmatch.filter(os.listdir(base), pattern) if
                os.path.isfile(os.path.join(base, n))]


def should_sync_field(inclusion, selected, default=False):
    """
    Returns True if a field should be synced.
    inclusion: automatic|available|unsupported
    selected: True|False|None
    default: (default: False) True|False
    "automatic" inclusion always returns True:
    >>> should_sync_field("automatic", None, False)
    True
    >>> should_sync_field("automatic", True, False)
    True
    >>> should_sync_field("automatic", False, False)
    True
    >>> should_sync_field("automatic", None, True)
    True
    >>> should_sync_field("automatic", True, True)
    True
    >>> should_sync_field("automatic", False, True)
    True
    "unsupported" inclusion always returns False
    >>> should_sync_field("unsupported", None, False)
    False
    >>> should_sync_field("unsupported", True, False)
    False
    >>> should_sync_field("unsupported", False, False)
    False
    >>> should_sync_field("unsupported", None, True)
    False
    >>> should_sync_field("unsupported", True, True)
    False
    >>> should_sync_field("unsupported", False, True)
    False
    "available" inclusion uses the selected value when set
    >>> should_sync_field("available", True, False)
    True
    >>> should_sync_field("available", False, False)
    False
    >>> should_sync_field("available", True, True)
    True
    >>> should_sync_field("available", False, True)
    False
    "available" inclusion uses the default value when selected is None
    >>> should_sync_field("available", None, False)
    False
    >>> should_sync_field("available", None, True)
    True
    """
    # always select automatic fields
    if inclusion == "automatic":
        return True

    # never select unsupported fields
    if inclusion == "unsupported":
        return False

    # at this point inclusion == "available"
    # selected could be None, otherwise use the value of selected
    if selected is not None:
        return selected

    # if there was no selected value, use the default
    return default


def _read_next_block_of_lines(file: IO, position: int, block_size: int, max_position: int, encoding='UTF-8'):
    """
    Read the smallest next block of bytes representing characters that ends with new line.
    Retry if incomplete byte sequence is hit and adjust the position.
    Args:
        file:
        position:
        block_size:
        max_position: latest position in file
        encoding:

    Returns: block:str, position:int - new adjusted position

    """

    new_block_size = block_size
    is_end = False
    while not is_end:
        file.seek(position, os.SEEK_END)
        block = file.read(new_block_size)
        try:
            decoded = block.decode(encoding)
            if not decoded.startswith('\n'):
                new_block_size += 1
                position -= 1

                if is_end := (abs(position) > max_position):
                    return decoded, position
                else:
                    continue
            return decoded, position
        except UnicodeDecodeError as e:
            # in case we hit partial character representation (not full byte sequence)
            new_block_size += 1
            position -= 1


def _reversed_blocks(file, blocksize=4096):
    """
    Generate blocks of file's contents in reverse order.
    Args:
        file:
        blocksize:

    Returns:

    """
    file.seek(0, os.SEEK_END)
    max_pos = file.tell()
    position = 0
    delta = blocksize
    while abs(position) <= max_pos:
        positions_remaining = max_pos + position
        delta = blocksize if delta <= positions_remaining else positions_remaining
        position = (-delta) + position

        decoded, position = _read_next_block_of_lines(file, position, delta, max_pos)
        yield decoded


def reverse_readline(file, buf_size=8192):
    """
    Generate the lines of file in reverse order.
    Args:
        file:
        buf_size:

    Returns:

    """

    part = ''
    quoting = False
    for block in _reversed_blocks(file, buf_size):

        for c in reversed(block):
            if c == '"':
                quoting = not quoting
            elif c == '\n' and part and not quoting:
                # NULLBYTE removal to prevent line contains NULL byte issues
                yield part[::-1].replace('\0', '')
                part = ''
            part += c
    if part:
        yield part[::-1].replace('\0', '')


================================================
File: src/core/yaml_mappings.py
================================================
import json
# import yaml


def make_yaml_mapping_file(json_mappings: dict):
    """Creates the input YAML mapping file for choosing desired tables and columns based on mappings JSON."""
    yaml_data = []
    stream_data = json_mappings['streams']
    databases_and_tables = {}

    for stream in stream_data:
        # Iterate through mappings per database
        table_name = stream['stream']
        table_metadata = stream['metadata'][0]['metadata']
        columns_metadata = stream['metadata'][1:]

        # Mandatory metadata: Get tables and table choices
        stream_database = table_metadata['database-name']
        is_selected = table_metadata.get('selected') or False
        replication_method_raw = table_metadata.get('replication-method') or 'log_based'
        replication_method = str(replication_method_raw).lower()
        replication_key = table_metadata.get('replication-key')

        # Mandatory metadata: Get columns and column choices
        column_meta_mappings = {}
        for column_metadata in columns_metadata:
            column_details = column_metadata['metadata']
            column = column_metadata['breadcrumb'][1]
            column_is_selected = column_details.get('selected') or column_details.get('selected-by-default')

            column_meta_mappings[column] = column_is_selected

        # Put the table and column mappings together
        table_meta_mappings = {"selected": is_selected, "replication-method": replication_method}
        if replication_key or replication_method.upper() == 'INCREMENTAL':
            table_meta_mappings["replication-key"] = replication_key
        table_meta_mappings["columns"] = column_meta_mappings

        table_mapping = {table_name: table_meta_mappings}
        if databases_and_tables.get(stream_database):
            databases_and_tables[stream_database]['tables'].append(table_mapping)
        else:
            databases_and_tables[stream_database] = {"tables": [table_mapping]}

    yaml_data.append(databases_and_tables)
    return yaml_data


# Convert YAML to JSON table mappings choices
def convert_yaml_to_json_mapping(yaml_mappings, raw_json_mapping):
    """Convert YAML table and column choices to JSON mapping.

    Args:
        yaml_mappings: The YAML mappings to be converted
        raw_json_mapping: The Raw JSON table mappings from the database
    """
    syncing_tables = []

    # First, determine which items to pull from YAML mappings
    for database_mapping in yaml_mappings:
        for database, tables_info in database_mapping.items():

            for table in tables_info['tables']:
                for table_name, table_metadata in table.items():
                    stream_id = database + '-' + table_name
                    table_columns = table_metadata.get('columns', {})
                    selected_table = table_metadata.get('selected', False)
                    replication_method = table_metadata.get('replication-method')
                    replication_key = table_metadata.get('replication-key')
                    output_table = table_metadata.get('output-table')

                    if selected_table:
                        syncing_table_data = {
                            "stream-id": stream_id,
                            "selected": True,
                            "replication-method": replication_method.upper(),
                            "columns": table_columns
                        }
                        if replication_key:
                            syncing_table_data['replication-key'] = replication_key
                        if output_table:
                            syncing_table_data['output-table'] = output_table

                        syncing_tables.append(syncing_table_data)

    synced_stream_ids = [stream['stream-id'] for stream in syncing_tables]
    json_mapping_streams = raw_json_mapping['streams']
    for json_mapping in json_mapping_streams:
        if json_mapping['tap_stream_id'] not in synced_stream_ids:
            continue

        matched_sync_table_index = -1
        for index, synced_table in enumerate(syncing_tables):
            if synced_table['stream-id'] == json_mapping['tap_stream_id']:
                matched_sync_table_index = index

        json_table_metadata = json_mapping['metadata'][0]['metadata']
        json_table_metadata['selected'] = syncing_tables[matched_sync_table_index]['selected']
        json_table_metadata['replication-method'] = syncing_tables[matched_sync_table_index]['replication-method']

        if syncing_tables[matched_sync_table_index].get('replication-key'):
            json_table_metadata['replication-key'] = syncing_tables[matched_sync_table_index]['replication-key']
        if syncing_tables[matched_sync_table_index].get('output-table'):
            json_table_metadata['output-table'] = syncing_tables[matched_sync_table_index]['output-table']

        json_columns_metadata = json_mapping['metadata'][1:]
        syncing_table_columns = syncing_tables[matched_sync_table_index]['columns']

        for json_column_metadata in json_columns_metadata:
            json_column_name = json_column_metadata['breadcrumb'][1]
            json_column_metadata_metadata_path = json_column_metadata['metadata']

            for syncing_column, column_selected in syncing_table_columns.items():
                if json_column_name == syncing_column:
                    if not column_selected:
                        json_column_metadata_metadata_path['selected'] = False

    output_stream_mappings = json.dumps({'streams': json_mapping_streams})
    return output_stream_mappings


# Keeping the below for test purposes - can be used to test YAML mappings by running this file directly and plugging in
# your testing files. If you do, uncomment both the code below (and substitute what you need to) as well as the Yaml
# if __name__ == '__main__':
#     table_mappings_file = '{json_file}'
#     new_yaml_file = '/Users/johnathanbrooks/PycharmProjects/keboola_ex_mysql_nextv2/data/in/files/mappings.yaml'
#
#     new_mappings = '/Users/johnathanbrooks/PycharmProjects/keboola_ex_mysql_nextv2/data/in/tables/xxl_tables.json'
#
    # with open(table_mappings_file, encoding='utf-8') as json_input_mapping:
    #     json_mappings = json.load(json_input_mapping)
    #
    # raw_yaml_mapping = make_yaml_mapping_file(json_mappings)
    #
    # with open(new_yaml_file, 'w') as yaml_out:
    #     yaml_out.write(yaml.dump(raw_yaml_mapping))
    #
    # with open(new_yaml_file, encoding='utf-8') as yaml_input_mapping:
    #     yaml_mappings = yaml.safe_load(yaml_input_mapping)
    #
    # with open(new_mappings, encoding='utf-8') as new_raw_mapping_file:
    #     new_raw_mappings = json.load(new_raw_mapping_file)
    #     # json_mapping = convert_yaml_to_json_mapping(yaml_mappings, new_raw_mappings)
    #
    # with open(new_yaml_file, encoding='utf-8') as yaml_input_mapping:
    #     yaml_mappings = yaml.safe_load(yaml_input_mapping)

    # print('got yaml mappings:')
    # print(yaml_mappings)
    # table_mappings = json.loads(convert_yaml_to_json_mapping(yaml_mappings, dict(new_raw_mappings)))


================================================
File: src/mysql/client.py
================================================
"""
Define MySQL connection, session parameters and backoff configuration.

TODO: Confirm correct behavior and security of SSL setup.
"""
import logging
import ssl

import backoff
import pymysql
from pymysql.constants import CLIENT

from typing import Union

MAX_CONNECT_RETRIES = 5
BACKOFF_FACTOR = 2
CONNECTION_TIMEOUT_SECONDS = 30
READ_TIMEOUT_SECONDS = 3000


@backoff.on_exception(backoff.expo, pymysql.err.OperationalError, max_tries=MAX_CONNECT_RETRIES, factor=BACKOFF_FACTOR)
def connect_with_backoff(connection):
    logging.debug('Connecting to MySQL server.')
    connection.connect()
    with connection.cursor() as cursor:
        set_session_parameters(cursor, net_read_timeout=READ_TIMEOUT_SECONDS,
                               max_execution_time=connection.connection_parameters["max_execution_time"])
    return connection


def set_session_parameters(cursor: pymysql.connections.Connection.cursor, max_execution_time: Union[int, bool],
                           wait_timeout: int = 300,
                           net_read_timeout: int = 60, innodb_lock_wait_timeout: int = 300, time_zone: str = '+0:00'
                           ):
    """Set MySQL session parameters to handle for data extraction appropriately.

    Args:
        cursor: MySQLDB connection cursor object for the active connection.
        wait_timeout: (Optional) Seconds server waits for activity on inactive session before closing, default 300.
        net_read_timeout: (Optional) Seconds to wait for more data from connection before aborting the read, default 60.
        innodb_lock_wait_timeout: (Optional) Seconds a transaction waits for a row lock before giving up, default 300.
        time_zone: (Optional) String representing the session time zone, default is UTC: '+0:00').
        max_execution_time: This is here as a workaround for server-caused timeouts
    Returns:
        None.
    """
    logged_warnings = []
    try:
        cursor.execute('SET @@session.wait_timeout={}'.format(wait_timeout))
        logging.info(f"Setting session parameter wait_timeout to {wait_timeout}")
    except pymysql.err.InternalError as internal_err:
        logged_warnings.append('Could not set session.wait_timeout. Error: ({}) {}'.format(*internal_err.args))
    try:
        cursor.execute("SET @@session.net_read_timeout={}".format(net_read_timeout))
        logging.info(f"Setting session parameter net_read_timeout to {net_read_timeout}")
    except pymysql.err.InternalError as internal_err:
        logged_warnings.append('Could not set session.net_read_timeout. Error: ({}) {}'.format(*internal_err.args))
    try:
        cursor.execute('SET @@session.time_zone="{}"'.format(time_zone))
        logging.info(f"Setting session parameter time_zone to {time_zone}")
    except pymysql.err.InternalError as internal_err:
        logged_warnings.append('Could not set session.time_zone. Error: ({}) {}'.format(*internal_err.args))
    try:
        cursor.execute('SET @@session.innodb_lock_wait_timeout={}'.format(innodb_lock_wait_timeout))
        logging.info(f"Setting session parameter innodb_lock_wait_timeout to {innodb_lock_wait_timeout}")
    except pymysql.err.InternalError as e:
        logged_warnings.append('Could not set session.innodb_lock_wait_timeout. Error: ({}) {}'.format(*e.args))

    if max_execution_time:
        try:
            cursor.execute('SET @@session.max_execution_time={}'.format(max_execution_time))
            logging.info(f"Setting session parameter max_execution_time to {max_execution_time}")
        except pymysql.err.InternalError as e:
            logged_warnings.append('Could not set session.max_execution_time. Error: ({}) {}'.format(*e.args))

    if logged_warnings:
        logging.info('Setting session parameters failed for at least one process, which may impact execution speed.')
        for warn_message in logged_warnings:
            logging.warning(warn_message)


class MySQLConnection(pymysql.connections.Connection):
    def __init__(self, config):
        args = {
            "user": config.get('user') or config.get('username'),
            "password": config.get('password') or config.get('#password'),
            "host": config['host'],
            "port": int(config['port']),
            "cursorclass": config.get('cursorclass') or pymysql.cursors.SSCursor,
            "connect_timeout": CONNECTION_TIMEOUT_SECONDS,
            "read_timeout": READ_TIMEOUT_SECONDS,
            "charset": 'utf8',
        }

        if config.get("database"):
            args["database"] = config["database"]

        ssl_arg = None
        use_ssl = config.get('ssl') == 'true'

        # Attempt self-signed SSL, if config vars are present
        use_self_signed_ssl = config.get("ssl_ca")
        self.connection_parameters = config
        super().__init__(defer_connect=True, ssl=ssl_arg, **args)

        # Configure SSL w/o custom CA -- Manually create context, override default behavior of CERT_NONE w/o CA supplied
        if use_ssl and not use_self_signed_ssl:
            logging.info("Attempting SSL connection")
            # For compatibility with previous version, verify mode is off by default
            verify_mode = config.get("verify_mode", "false") == 'true'
            if not verify_mode:
                logging.warning('Not verifying server certificate. The connection is encrypted, but the server '
                                'hasn''t been verified. Please provide a root CA certificate to enable verification.')
            self.ssl = True
            self.ctx = ssl.create_default_context()
            check_hostname = config.get("check_hostname", "false") == 'true'
            self.ctx.check_hostname = check_hostname
            self.ctx.verify_mode = ssl.CERT_REQUIRED if verify_mode else ssl.CERT_NONE
            self.client_flag |= CLIENT.SSL

    def __enter__(self):
        return self

    def __exit__(self, *exc_info):
        del exc_info
        self.close()


def make_connection_wrapper(config):
    class ConnectionWrapper(MySQLConnection):
        def __init__(self, *args, **kwargs):
            config["cursorclass"] = kwargs.get('cursorclass')
            super().__init__(config)
            connect_with_backoff(self)

    return ConnectionWrapper


================================================
File: src/mysql/replication/binlog.py
================================================
"""
Binary log row-based replication.
"""

import copy
import datetime
import json
import logging
import uuid
from typing import Tuple, List

import pymysql.connections
import pymysql.err
import pytz
import requests
from pymysqlreplication.constants import FIELD_TYPE
from pymysqlreplication.event import RotateEvent
from pymysqlreplication.row_event import (DeleteRowsEvent, UpdateRowsEvent, WriteRowsEvent)
from requests.adapters import HTTPAdapter
from urllib3 import Retry

from core import bookmarks
from mysql.replication.stream_reader import BinLogStreamReaderAlterTracking, SchemaOffsyncError, \
    QueryEventWithSchemaChanges

try:
    import core as core
    from core import utils
    from core.schema import Schema

    import mysql.replication.common as common
    from mysql.client import connect_with_backoff, make_connection_wrapper
except ImportError:
    import src.core as core
    from src.core import utils
    from src.core.schema import Schema

    import src.mysql.replication.common as common
    from src.mysql.client import connect_with_backoff, make_connection_wrapper

BOOKMARK_KEYS = {'log_file', 'log_pos', 'version'}
UPDATE_BOOKMARK_PERIOD = 10000

mysql_timestamp_types = {FIELD_TYPE.TIMESTAMP, FIELD_TYPE.TIMESTAMP2}


def add_automatic_properties(catalog_entry, columns):
    catalog_entry.schema.properties[common.KBC_SYNCED] = Schema(type=["null", "string"], format="date-time")
    catalog_entry.schema.properties[common.KBC_DELETED] = Schema(type=["null", "string"], format="date-time")
    catalog_entry.schema.properties[common.BINLOG_CHANGE_AT] = Schema(type=["null", "integer"])
    catalog_entry.schema.properties[common.BINLOG_READ_AT] = Schema(type=["null", "integer"])
    columns.append(common.KBC_SYNCED)
    columns.append(common.KBC_DELETED)
    columns.append(common.BINLOG_CHANGE_AT)
    columns.append(common.BINLOG_READ_AT)

    return columns


def verify_binlog_config(mysql_conn):
    with connect_with_backoff(mysql_conn) as open_conn:
        with open_conn.cursor() as cur:
            cur.execute("SELECT  @@binlog_format")
            binlog_format = cur.fetchone()[0]

            if binlog_format != 'ROW':
                raise Exception("Unable to replicate binlog stream because binlog_format is not set to 'ROW': {}."
                                .format(binlog_format))

            try:
                cur.execute("SELECT  @@binlog_row_image")
                binlog_row_image = cur.fetchone()[0]
            except pymysql.err.InternalError as ex:
                if ex.args[0] == 1193:
                    raise Exception("Unable to replicate binlog stream because binlog_row_image system variable does"
                                    "not exist. MySQL version must be at least 5.6.2 to use binlog replication.")
                raise ex

            if binlog_row_image != 'FULL':
                raise Exception("Unable to replicate binlog stream because binlog_row_image is not set to "
                                "'FULL': {}.".format(binlog_row_image))


def verify_log_file_exists(binary_logs, log_file, log_pos):
    existing_log_file = list(filter(lambda log: log[0] == log_file, binary_logs))

    if not existing_log_file:
        raise Exception("Unable to replicate binlog stream because log file {} does not exist."
                        .format(log_file))

    current_log_pos = existing_log_file[0][1]

    if log_pos > current_log_pos:
        raise Exception("Unable to replicate binlog stream because requested position ({}) for log file {} is "
                        "greater than current position ({}).".format(log_pos, log_file, current_log_pos))


def fetch_current_log_file_and_pos(mysql_conn, config):
    show_binlog_method_factory = ShowBinlogMethodFactory(mysql_conn, config.get('show_binary_log_config', {}))
    show_binlog_method = show_binlog_method_factory.get_show_binlog_method()
    binlogs = show_binlog_method()
    current_log_file, current_log_pos = binlogs[-1]  # last binlog record corresponds to show master status
    return current_log_file, current_log_pos


def fetch_server_id(mysql_conn):
    with connect_with_backoff(mysql_conn) as open_conn:
        with open_conn.cursor() as cur:
            cur.execute("SELECT @@server_id")
            server_id = cur.fetchone()[0]

            return server_id


def json_bytes_to_string(data):
    if isinstance(data, bytes):
        return data.decode()
    if isinstance(data, dict):
        return dict(map(json_bytes_to_string, data.items()))
    if isinstance(data, tuple):
        return tuple(map(json_bytes_to_string, data))
    if isinstance(data, list):
        return list(map(json_bytes_to_string, data))
    return data


def row_to_data_record(catalog_entry, version, db_column_map, row, time_extracted):
    row_to_persist = {}
    # This is to get columns that do not contain skipped ones (e.g. if unselected or unsupported type)
    # In such case the value will be empty
    supported_cols = [c.upper() for c in list(catalog_entry.schema.properties.keys())]

    for column_name, val in row.items():
        db_column_type = None
        is_boolean_type = False
        try:
            # TODO: WTF is this?? aparently coming from the actual datatype name
            # property_type = catalog_entry.schema.properties[column_name].type replaced with type got from the
            # BinlogReader
            is_boolean_type = db_column_map[column_name].get('is_boolean')
            db_column_type = db_column_map[column_name].get('type')
        except KeyError:
            # skip system columns
            if column_name.startswith('_KBC') or column_name.startswith('_BINLOG'):
                pass
            else:
                raise SchemaOffsyncError(f'Schema for {column_name} is not available!')

        if isinstance(val, (datetime.datetime, datetime.date)):
            the_utc_date = common.to_utc_datetime_str(val)
            row_to_persist[column_name] = the_utc_date

        # row_event.__read_time() returns timedelta in case it is a time
        if isinstance(val, datetime.timedelta):
            row_to_persist[column_name] = str(val)

        elif db_column_type == FIELD_TYPE.JSON:
            row_to_persist[column_name] = json.dumps(json_bytes_to_string(val))
        # TODO: WTF is this??
        # elif 'boolean' in property_type or property_type == 'boolean':
        elif is_boolean_type:
            if val is None:
                boolean_representation = None
            elif val == 0:
                # boolean_representation = False
                boolean_representation = 0
            elif db_column_type == FIELD_TYPE.BIT:
                boolean_representation = 1 if int(val) != 0 else 0
                # boolean_representation = int(val) != 0
            else:
                boolean_representation = 1
                # boolean_representation = True
            row_to_persist[column_name] = boolean_representation
        # This is to get columns that do not contain skipped ones (e.g. if unselected or unsupported type)
        # In such case the value will be empty
        elif column_name not in supported_cols:
            row_to_persist[column_name] = ''
        else:
            row_to_persist[column_name] = val

    return core.RecordMessage(stream=catalog_entry.stream, record=row_to_persist,
                              column_map=catalog_entry.current_column_cache,
                              version=version,
                              time_extracted=time_extracted)


def get_min_log_pos_per_log_file(binlog_streams_map, state):
    min_log_pos_per_file = {}

    for tap_stream_id, bookmark in state.get('bookmarks', {}).items():
        stream = binlog_streams_map.get(tap_stream_id)

        if not stream:
            continue

        log_file = bookmark.get('log_file')
        log_pos = bookmark.get('log_pos')

        if not min_log_pos_per_file.get(log_file):
            min_log_pos_per_file[log_file] = {
                'log_pos': log_pos,
                'streams': [tap_stream_id]
            }

        elif min_log_pos_per_file[log_file]['log_pos'] > log_pos:
            min_log_pos_per_file[log_file]['log_pos'] = log_pos
            min_log_pos_per_file[log_file]['streams'].append(tap_stream_id)

        else:
            min_log_pos_per_file[log_file]['streams'].append(tap_stream_id)

    return min_log_pos_per_file


def calculate_bookmark(show_binlog_method, binlog_streams_map, state):
    min_log_pos_per_file = get_min_log_pos_per_log_file(binlog_streams_map, state)

    binary_logs = show_binlog_method()

    if binary_logs:
        server_logs_set = {log[0] for log in binary_logs}
        state_logs_set = set(min_log_pos_per_file.keys())
        expired_logs = state_logs_set.difference(server_logs_set)

        if expired_logs:
            raise Exception("Unable to replicate binlog stream because the following binary log(s) no longer "
                            "exist: {}".format(", ".join(expired_logs)))

        for log_file in sorted(server_logs_set):
            if min_log_pos_per_file.get(log_file):
                return log_file, min_log_pos_per_file[log_file]['log_pos'], binary_logs

    raise Exception("Unable to replicate binlog stream because no binary logs exist on the server.")


def update_bookmarks(state, binlog_streams_map, log_file, log_pos):
    for tap_stream_id in binlog_streams_map.keys():
        state = core.write_bookmark(state, tap_stream_id, 'log_file', log_file)
        state = core.write_bookmark(state, tap_stream_id, 'log_pos', log_pos)

    return state


def get_db_column_types(event):
    return {c.name: {"type": c.type, "is_boolean": c.type_is_bool} for c in event.columns}


def handle_write_rows_event(event, catalog_entry, state, columns, rows_saved, time_extracted,
                            message_store: core.MessageStore = None):
    stream_version = common.get_stream_version(catalog_entry.tap_stream_id, state)
    db_column_types = get_db_column_types(event)

    for row in event.rows:
        vals = row['values']
        vals[common.KBC_DELETED] = None
        vals[common.KBC_SYNCED] = common.SYNC_STARTED_AT
        vals[common.BINLOG_CHANGE_AT] = event.timestamp
        vals[common.BINLOG_READ_AT] = utils.now(format='ts_1e6')

        if columns == [] or columns is None:
            filtered_vals = vals
        else:
            filtered_vals = {k: v for k, v in vals.items() if k in columns}

        record_message = row_to_data_record(catalog_entry, stream_version, db_column_types, filtered_vals,
                                            time_extracted)
        core.write_message(record_message, message_store=message_store, database_schema=catalog_entry.database)
        rows_saved = rows_saved + 1

    return rows_saved


def handle_update_rows_event(event, catalog_entry, state, columns, rows_saved, time_extracted, watch_columns=None,
                             ignore_columns=None, message_store: core.MessageStore = None):
    stream_version = common.get_stream_version(catalog_entry.tap_stream_id, state)
    db_column_types = get_db_column_types(event)

    for row in event.rows:

        changed = False
        if ignore_columns is not None and ignore_columns != []:
            all_cols = list(row['after_values'].keys())

            for wc in all_cols:
                if wc in ignore_columns:
                    continue
                else:
                    before_value = row['before_values'].get(wc)
                    after_value = row['after_values'].get(wc)

                    if before_value != after_value:
                        changed = True
                        break

        elif watch_columns is not None and watch_columns != []:
            for wc in watch_columns:
                before_value = row['before_values'].get(wc)
                after_value = row['after_values'].get(wc)

                if before_value != after_value:
                    changed = True
                    break

        else:
            changed = True

        if changed is False:
            pass
        else:
            vals = row['after_values']

            vals[common.KBC_DELETED] = None
            vals[common.KBC_SYNCED] = common.SYNC_STARTED_AT
            vals[common.BINLOG_CHANGE_AT] = event.timestamp
            vals[common.BINLOG_READ_AT] = utils.now(format='ts_1e6')

            if columns == [] or columns is None:
                filtered_vals = vals
            else:
                filtered_vals = {k: v for k, v in vals.items() if k in columns}

            record_message = row_to_data_record(catalog_entry, stream_version, db_column_types, filtered_vals,
                                                time_extracted)

            core.write_message(record_message, message_store=message_store, database_schema=catalog_entry.database)

            rows_saved += 1

    return rows_saved


def handle_delete_rows_event(event, catalog_entry, state, columns, rows_saved, time_extracted,
                             message_store: core.MessageStore = None):
    stream_version = common.get_stream_version(catalog_entry.tap_stream_id, state)
    db_column_types = get_db_column_types(event)

    for row in event.rows:
        event_ts = datetime.datetime.utcfromtimestamp(event.timestamp).replace(tzinfo=pytz.UTC)
        vals = row['values']

        vals[common.KBC_DELETED] = event_ts
        vals[common.KBC_SYNCED] = common.SYNC_STARTED_AT
        vals[common.BINLOG_CHANGE_AT] = event.timestamp
        vals[common.BINLOG_READ_AT] = utils.now(format='ts_1e6')

        if columns == [] or columns is None:
            filtered_vals = vals
        else:
            filtered_vals = {k: v for k, v in vals.items() if k in columns}

        record_message = row_to_data_record(catalog_entry, stream_version, db_column_types, filtered_vals,
                                            time_extracted)

        core.write_message(record_message, message_store=message_store, database_schema=catalog_entry.database)

        rows_saved = rows_saved + 1

    return rows_saved


def generate_streams_map(binlog_streams):
    stream_map = {}

    for catalog_entry in binlog_streams:
        columns = add_automatic_properties(catalog_entry,
                                           list(catalog_entry.schema.properties.keys()))

        stream_map[catalog_entry.tap_stream_id] = {
            'catalog_entry': catalog_entry,
            'desired_columns': columns
        }

    return stream_map


def handle_schema_change_event(binlog_event: QueryEventWithSchemaChanges, message_store):
    if binlog_event.event_type != QueryEventWithSchemaChanges.QueryType.ALTER_QUERY:
        return

    for change in binlog_event.schema_changes:
        row = {'schema': change.schema,
               'table': change.table_name,
               'change_type': change.type.value,
               'column_name': change.column_name,
               'query': change.query,
               'timestamp': str(binlog_event.timestamp)}
        message_store.write_schema_change_message(row)


def _run_binlog_sync(mysql_conn, config, reader: BinLogStreamReaderAlterTracking, binlog_streams_map, state, columns={},
                     message_store: core.MessageStore = None):
    time_extracted = utils.now()

    rows_saved = 0
    events_skipped = 0

    current_log_file, current_log_pos = fetch_current_log_file_and_pos(mysql_conn, config)
    parsing_log_file = ''
    parsing_log_pos = current_log_pos

    for binlog_event in reader:
        try:

            if parsing_log_file != reader.log_file:
                parsing_log_file = reader.log_file
                parsing_log_pos = reader.log_pos
                logging.info("Parsing first 50M records in binary logs file.")

            if reader.log_pos - parsing_log_pos > 50000000:
                parsing_log_pos = reader.log_pos
                logging.info(f"Parsing binary another 50M records in logs file {parsing_log_file}, "
                             f"starting position {parsing_log_pos}.")

            if isinstance(binlog_event, RotateEvent):
                state = update_bookmarks(state, binlog_streams_map, binlog_event.next_binlog, binlog_event.position)

            elif isinstance(binlog_event, QueryEventWithSchemaChanges):
                handle_schema_change_event(binlog_event, message_store)

            else:
                tap_stream_id = common.generate_tap_stream_id(binlog_event.schema, binlog_event.table)
                streams_map_entry = binlog_streams_map.get(tap_stream_id, {})
                catalog_entry = streams_map_entry.get('catalog_entry')

                desired_columns = columns[tap_stream_id].get('desired', [])
                ignored_columns = columns[tap_stream_id].get('ignore', [])
                watched_columns = columns[tap_stream_id].get('watch', [])

                if not catalog_entry:
                    logging.debug('No catalog entry, skip events number: {}'.format(events_skipped))
                    events_skipped = events_skipped + 1

                    if events_skipped % UPDATE_BOOKMARK_PERIOD == 0:
                        logging.info("Skipped %s events so far as they were not for selected tables; %s rows extracted",
                                     events_skipped, rows_saved)

                elif catalog_entry:
                    # ugly injection of current schema
                    current_column_schema = reader.schema_cache.get_column_schema(binlog_event.schema,
                                                                                  binlog_event.table)
                    catalog_entry.current_column_cache = current_column_schema

                    if isinstance(binlog_event, WriteRowsEvent):
                        rows_saved = handle_write_rows_event(binlog_event, catalog_entry, state, desired_columns,
                                                             rows_saved, time_extracted, message_store=message_store)

                    elif isinstance(binlog_event, UpdateRowsEvent):
                        rows_saved = handle_update_rows_event(binlog_event, catalog_entry, state, desired_columns,
                                                              rows_saved, time_extracted,
                                                              watch_columns=watched_columns,
                                                              ignore_columns=ignored_columns,
                                                              message_store=message_store)

                    elif isinstance(binlog_event, DeleteRowsEvent):
                        rows_saved = handle_delete_rows_event(binlog_event, catalog_entry, state, desired_columns,
                                                              rows_saved, time_extracted, message_store=message_store)
                    else:
                        logging.info("Skipping event for table %s.%s as it is not an INSERT, UPDATE, or DELETE",
                                     binlog_event.schema,
                                     binlog_event.table)

            state = update_bookmarks(state, binlog_streams_map, reader.log_file, reader.log_pos)
            # Store last schema cache
            state = bookmarks.update_schema_in_state(state, reader.schema_cache.table_schema_cache)

            # The iterator across python-mysql-replication's fetchone method should ultimately terminate
            # upon receiving an EOF packet. There seem to be some cases when a MySQL server will not send
            # one causing binlog replication to hang.

            # TODO: Consider moving log pos back slightly to avoid hanging process (maybe 200 or so)
            if current_log_file == reader.log_file and reader.log_pos >= current_log_pos:
                break

            if ((rows_saved and rows_saved % UPDATE_BOOKMARK_PERIOD == 0) or
                    (events_skipped and events_skipped % UPDATE_BOOKMARK_PERIOD == 0)):
                core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)

        except Exception as e:
            raise Exception(f'Failed to process event. Table: {binlog_event.table}. '
                            f'Columns: {[c.data for c in binlog_event.columns]}') from e


class ShowBinlogMethodFactory:
    """
    SHOW BINLOG may be slow, this supports other methods of binlog retrieval
    """

    def __init__(self, mysql_connection, method_configuration: dict):
        self._mysql_conn = mysql_connection
        self._configuration = method_configuration

    def get_show_binlog_method(self):
        method_config = self._configuration
        if method_config.get('method', 'direct') == 'direct':
            show_binlog_method = self._show_binlog_from_db
        elif method_config.get('method') == 'endpoint':
            show_binlog_method = self._show_binlog_from_endpoint
        else:
            raise ValueError(f'Provided show_binlog_method: {method_config} is invalid')
        return show_binlog_method

    def _show_binlog_from_db(self) -> dict:
        with connect_with_backoff(self._mysql_conn) as open_conn:
            with open_conn.cursor() as cur:
                logging.debug('Executing SHOW BINARY LOGS')
                cur.execute("SHOW BINARY LOGS")

                binary_logs = cur.fetchall()
                return binary_logs

    def _show_binlog_from_endpoint(self) -> List[Tuple]:
        """
        Expects endpoint returning SHOW BINLOGS array in
        {"logs":[{'log_name': 'mysql-bin-changelog.189135', 'file_size': '134221723'}]} response

        Returns:

        """

        def get_session(max_retries: int = 3, backoff_factor: float = 0.3,
                        status_forcelist: Tuple[int, ...] = (500, 502, 503, 504)) -> requests.Session:
            session = requests.Session()
            retry = Retry(
                total=max_retries,
                read=max_retries,
                connect=max_retries,
                backoff_factor=backoff_factor,
                status_forcelist=status_forcelist,
                allowed_methods='GET'
            )
            adapter = HTTPAdapter(max_retries=retry)
            session.mount('http://', adapter)
            session.mount('https://', adapter)
            return session

        endpoint_url = self._configuration.get('endpoint_url')
        auth = None
        if self._configuration.get('authentication'):
            auth = (self._configuration['user'], self._configuration['#password'])

        if not endpoint_url:
            raise ValueError(f'Show binlog method from endpoint requires "endpoint_url" parameters defined! '
                             f'Provided configuration is invalid: {self._configuration}.')
        logging.info(f"Getting SHOW Binary logs from {endpoint_url} endpoint")
        response = get_session().get(endpoint_url, auth=auth, timeout=300)
        response.raise_for_status()
        log_array = response.json()['logs']
        binlogs = [(lg['log_name'], int(lg['file_size'])) for lg in log_array]
        return binlogs


def sync_binlog_stream(mysql_conn, config, binlog_streams, state, message_store: core.MessageStore = None,
                       schemas=[], tables=[], columns={}):
    last_table_schema_cache = state.get(bookmarks.KEY_LAST_TABLE_SCHEMAS, {})
    binlog_streams_map = generate_streams_map(binlog_streams)

    for tap_stream_id in binlog_streams_map.keys():
        common.whitelist_bookmark_keys(BOOKMARK_KEYS, tap_stream_id, state)

    # build show binary log method
    shbn_factory = ShowBinlogMethodFactory(mysql_conn, config.get('show_binary_log_config', {}))
    show_binlog_method = shbn_factory.get_show_binlog_method()

    log_file, log_pos, binary_logs = calculate_bookmark(show_binlog_method, binlog_streams_map, state)

    verify_log_file_exists(binary_logs, log_file, log_pos)

    if config.get('server_id'):
        server_id = int(config.get('server_id'))
        logging.info("Using provided server_id=%s", server_id)
    else:
        server_id = fetch_server_id(mysql_conn)
        logging.info("No server_id provided, will use global server_id=%s", server_id)

    connection_wrapper = make_connection_wrapper(config)

    slave_uuid = 'kbc-slave-{}-{}'.format(str(uuid.uuid4()), server_id)
    logging.info('Connecting with Stream Reader to Slave UUID {}'.format(slave_uuid))
    try:
        reader = BinLogStreamReaderAlterTracking(
            connection_settings={},
            server_id=server_id,
            slave_uuid=slave_uuid,
            log_file=log_file,
            log_pos=log_pos,
            resume_stream=True,
            only_events=[RotateEvent, WriteRowsEvent, UpdateRowsEvent, DeleteRowsEvent, QueryEventWithSchemaChanges],
            freeze_schema=True,
            pymysql_wrapper=connection_wrapper,
            only_schemas=schemas,
            only_tables=tables,
            table_schema_cache=last_table_schema_cache
        )

        logging.info("Starting binlog replication with log_file=%s, log_pos=%s", log_file, log_pos)
        _run_binlog_sync(mysql_conn, config, reader, binlog_streams_map, state, columns, message_store=message_store)
    except Exception as e:
        logging.exception(e)
        raise e
    finally:
        # BinLogStreamReader doesn't implement the `with` methods
        # So, try/finally will close the chain from the top
        reader.close()

    core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)


================================================
File: src/mysql/replication/common.py
================================================
"""
Common patterns for data replication.
"""
import copy
import csv
import datetime
import logging
import os
import time

import pymysql
import pytz

import core as core
import core.metrics as metrics
from core import metadata
from core import utils
from core.messages import handle_binary_data

CURRENT_PATH = os.path.dirname(__file__)

# NB: Upgrading pymysql from 0.7.11 --> 0.9.3 had the undocumented change
# to how `0000-00-00 00:00:00` date/time types are returned. In 0.7.11,
# they are returned as NULL, and in 0.9.3, they are returned as the string
# `0000-00-00 00:00:00`. To maintain backwards-compatability, we are
# monkey patching the functions so they continue returning None
original_convert_datetime = pymysql.converters.convert_datetime
original_convert_date = pymysql.converters.convert_date

CSV_CHUNK_SIZE = 100000
SYNC_STARTED_AT = datetime.datetime.utcnow().strftime(utils.DATETIME_FMT_SAFE)
KBC_SYNCED = '_KBC_SYNCED_AT'
KBC_DELETED = '_KBC_DELETED_AT'
BINLOG_CHANGE_AT = '_BINLOG_CHANGE_AT'
BINLOG_READ_AT = '_BINLOG_READ_AT'
KBC_METADATA_COLS = (KBC_SYNCED, KBC_DELETED, BINLOG_CHANGE_AT, BINLOG_READ_AT)
KBC_METADATA = (SYNC_STARTED_AT, None, 0, 0)


def now():
    return time.time()


def patch_datetime(datetime_str):
    value = original_convert_datetime(datetime_str)
    if datetime_str == value:
        return None
    return value


def patch_date(date_str):
    value = original_convert_date(date_str)
    if date_str == value:
        return None
    return value


# Patch converters to properly handle date representations as strings (i.e. 0/0/0000)
pymysql.converters.convert_datetime = patch_datetime
pymysql.converters.convert_date = patch_date

pymysql.converters.conversions[pymysql.constants.FIELD_TYPE.DATETIME] = patch_datetime
pymysql.converters.conversions[pymysql.constants.FIELD_TYPE.DATE] = patch_date


def escape(string):
    if '`' in string:
        raise Exception("Can't escape identifier {} because it contains a backtick"
                        .format(string))
    return '`' + string + '`'


def generate_tap_stream_id(table_schema, table_name):
    return table_schema + '-' + table_name


def get_stream_version(tap_stream_id, state):
    stream_version = core.get_bookmark(state, tap_stream_id, 'version')

    if stream_version is None:
        stream_version = int(time.time() * 1000)

    return stream_version


def stream_is_selected(stream):
    md_map = metadata.to_map(stream.metadata)
    selected_md = metadata.get(md_map, (), 'selected')

    return selected_md


def property_is_selected(stream, property_name):
    md_map = metadata.to_map(stream.metadata)
    return core.should_sync_field(
        metadata.get(md_map, ('properties', property_name), 'inclusion'),
        metadata.get(md_map, ('properties', property_name), 'selected'),
        True)


def get_is_view(catalog_entry):
    md_map = metadata.to_map(catalog_entry.metadata)

    return md_map.get((), {}).get('is-view')


def get_database_name(catalog_entry):
    md_map = metadata.to_map(catalog_entry.metadata)

    return md_map.get((), {}).get('database-name')


def get_key_properties(catalog_entry):
    catalog_metadata = metadata.to_map(catalog_entry.metadata)
    stream_metadata = catalog_metadata.get((), {})

    is_view = get_is_view(catalog_entry)

    if is_view:
        key_properties = stream_metadata.get('view-key-properties', [])
    else:
        key_properties = stream_metadata.get('table-key-properties', [])

    return key_properties


def generate_select_sql(catalog_entry, columns):
    database_name = get_database_name(catalog_entry)
    escaped_db = escape(database_name)
    escaped_table = escape(catalog_entry.table)
    escaped_columns = [escape(c) for c in columns]

    select_sql = 'SELECT {} FROM {}.{}'.format(
        ','.join(escaped_columns),
        escaped_db,
        escaped_table)

    # escape percent signs
    select_sql = select_sql.replace('%', '%%')
    return select_sql


def to_utc_datetime_str(val):
    if isinstance(val, datetime.datetime):
        the_datetime = val
    elif isinstance(val, datetime.date):
        # the_datetime = datetime.datetime.combine(val, datetime.datetime.min.time())
        return val.strftime('%Y-%m-%d')

    else:
        raise ValueError("{!r} is not a valid date or time type".format(val))

    if the_datetime.tzinfo is None:
        # The mysql-replication library creates naive date and datetime objects
        # which will use the local timezone thus we must set tzinfo accordingly
        # See: https://github.com/noplay/python-mysql-replication/blob/master/pymysqlreplication/row_event.py#L143-L145

        # NB> this code will only work correctly when the local time is set to UTC because of the method timestamp()
        the_datetime = datetime.datetime.fromtimestamp(the_datetime.timestamp(), pytz.timezone('UTC'))

    return utils.strftime(the_datetime.astimezone(tz=pytz.UTC))


def row_to_data_record(catalog_entry, version, row, columns, time_extracted):
    # Adding metadata for Keboola. Can presume not there since should only be called for full sync

    kbc_metadata = (SYNC_STARTED_AT, None, 0, 0)
    kbc_metadata_cols = (KBC_SYNCED, KBC_DELETED, BINLOG_CHANGE_AT, BINLOG_READ_AT)
    row_with_metadata = row + kbc_metadata
    columns.extend(kbc_metadata_cols)
    catalog_entry.schema.properties[KBC_SYNCED] = core.schema.Schema(type=["null", "string"], format="date-time")
    catalog_entry.schema.properties[KBC_DELETED] = core.schema.Schema(type=["null", "string"], format="date-time")
    catalog_entry.schema.properties[BINLOG_CHANGE_AT] = core.schema.Schema(type=["null", "integer"])
    catalog_entry.schema.properties[BINLOG_READ_AT] = core.schema.Schema(type=["null", "integer"])

    row_to_persist = ()
    for idx, elem in enumerate(row_with_metadata):
        property_type = catalog_entry.schema.properties[columns[idx]].type

        if isinstance(elem, (datetime.datetime, datetime.date, datetime.timedelta)):
            the_utc_date = to_utc_datetime_str(elem)
            row_to_persist += (the_utc_date,)

        elif isinstance(elem, bytes):
            # for BIT value, treat 0 as False and anything else as True
            boolean_representation = elem != b'\x00'
            row_to_persist += (boolean_representation,)

        elif 'boolean' in property_type or property_type == 'boolean':
            if elem is None:
                boolean_representation = None
            elif elem == 0:
                boolean_representation = False
            else:
                boolean_representation = True
            row_to_persist += (boolean_representation,)

        else:
            row_to_persist += (elem,)
    rec = dict(zip(columns, row_to_persist))

    # rec[KBC_DELETED] = None
    # rec[KBC_SYNCED] = utils.now()

    return core.RecordMessage(stream=catalog_entry.stream, record=rec, version=version, time_extracted=time_extracted)


def whitelist_bookmark_keys(bookmark_key_set, tap_stream_id, state):
    for bk in [non_whitelisted_bookmark_key
               for non_whitelisted_bookmark_key
               in state.get('bookmarks', {}).get(tap_stream_id, {}).keys()
               if non_whitelisted_bookmark_key not in bookmark_key_set]:
        core.clear_bookmark(state, tap_stream_id, bk)


def sync_query(cursor, catalog_entry, state, select_sql, columns, stream_version, params,
               message_store: core.MessageStore = None):
    replication_key = core.get_bookmark(state, catalog_entry.tap_stream_id, 'replication_key')

    query_string = cursor.mogrify(select_sql, params)
    time_extracted = utils.now()

    logging.info('Running %s', query_string)
    cursor.execute(select_sql, params)

    row = cursor.fetchone()
    rows_saved = 0

    database_name = get_database_name(catalog_entry)

    with metrics.record_counter(None) as counter:
        counter.tags['database'] = database_name
        counter.tags['table'] = catalog_entry.table

        while row:
            counter.increment()
            rows_saved += 1

            record_message = row_to_data_record(catalog_entry, stream_version, row, columns, time_extracted)
            core.write_message(record_message)

            md_map = metadata.to_map(catalog_entry.metadata)
            stream_metadata = md_map.get((), {})
            replication_method = stream_metadata.get('replication-method')

            if replication_method.upper() in {'FULL_TABLE', 'LOG_BASED'}:
                key_properties = get_key_properties(catalog_entry)

                max_pk_values = core.get_bookmark(state, catalog_entry.tap_stream_id, 'max_pk_values')

                if max_pk_values:
                    last_pk_fetched = {k: v for k, v in record_message.record.items()
                                       if k in key_properties}

                    state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'last_pk_fetched', last_pk_fetched)

            elif replication_method.upper() == 'INCREMENTAL':
                if replication_key is not None:
                    state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'replication_key', replication_key)

                    state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'replication_key_value',
                                                record_message.record[replication_key])

            if rows_saved % 1000 == 0:
                core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)
                if rows_saved % 1000000 == 0:
                    logging.info('Ingested row count has hit {} for table {}'.format(rows_saved,
                                                                                     catalog_entry.tap_stream_id))

            row = cursor.fetchone()

    core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)
    return rows_saved


def _add_kbc_metadata_to_rows(rows, catalog_entry):
    """Add metadata for Keboola. Can presume not there since should only be called for full sync."""
    # Add headers to schema definition (data is added in write step)
    catalog_entry.schema.properties[KBC_SYNCED] = core.schema.Schema(type=["null", "string"], format="date-time")
    catalog_entry.schema.properties[KBC_DELETED] = core.schema.Schema(type=["null", "string"], format="date-time")
    catalog_entry.schema.properties[BINLOG_CHANGE_AT] = core.schema.Schema(type=["null", "integer"])
    catalog_entry.schema.properties[BINLOG_READ_AT] = core.schema.Schema(type=["null", "integer"])

    return rows


def sync_query_bulk(conn, cursor: pymysql.cursors.Cursor, catalog_entry, state, select_sql, columns, stream_version,
                    params, tables_destination: str = None, message_store: core.MessageStore = None):
    replication_key = core.get_bookmark(state, catalog_entry.tap_stream_id, 'replication_key')  # noqa

    query_string = cursor.mogrify(select_sql, params)
    logging.info('Running query {}'.format(query_string))

    # Chunk Processing
    has_more_data = True
    current_chunk = 0

    logging.info('Starting chunk processing for stream {}'.format(catalog_entry.tap_stream_id))
    all_chunks_start_time = utils.now()

    try:
        cursor.execute(select_sql)
        while has_more_data:
            chunk_start = utils.now()
            current_chunk += 1

            query_output_rows = cursor.fetchmany(CSV_CHUNK_SIZE)

            if query_output_rows:
                number_of_rows = len(query_output_rows)

                # Add Keboola metadata columns
                rows = _add_kbc_metadata_to_rows(query_output_rows, catalog_entry)
                logging.debug('Fetched {} rows from query result'.format(number_of_rows))

                if current_chunk == 1:
                    table_and_headers = {}

                    # Fetch column names from first item in cursor description tuple
                    headers = list()
                    for i in cursor.description:
                        headers.append(i[0])
                    for column in KBC_METADATA_COLS:
                        headers.append(column)

                    table_and_headers[catalog_entry.table] = headers

                    # Write to CSV of specific structure: table, headers (no header is written to this CSV)
                    tables_headers_path = os.path.join(CURRENT_PATH, '..', '..', '')
                    with open(tables_headers_path + 'table_headers.csv', 'a+', newline='') as headers_csv:
                        writer = csv.writer(headers_csv, delimiter='\t')
                        writer.writerow([catalog_entry.table, headers])
                        logging.info('Setting table {} metadata for columns to {}, staged for manifest'.format(
                            catalog_entry.table, headers
                        ))

                destination_output_path = os.path.join(tables_destination, catalog_entry.table.upper() + '.csv', '')

                if not os.path.exists(destination_output_path):
                    os.mkdir(destination_output_path)

                csv_path = os.path.join(destination_output_path, catalog_entry.table.upper() + '-' +
                                        str(current_chunk) + '.csv')

                with open(csv_path, 'w', encoding='utf-8', newline='') as output_data_file:
                    writer = csv.DictWriter(output_data_file, fieldnames=headers, quoting=csv.QUOTE_MINIMAL)
                    for row in rows:
                        rows_with_metadata = row + KBC_METADATA
                        rows_dict = dict(zip(headers, rows_with_metadata))
                        rows_to_write = handle_binary_data(rows_dict, catalog_entry.binary_columns,
                                                           message_store.binary_data_handler, True)
                        writer.writerow(rows_to_write)

                chunk_end = utils.now()
                chunk_processing_duration = (chunk_end - chunk_start).total_seconds()
                logging.info(
                    'Chunk {} had processing time: {} seconds'.format(current_chunk, chunk_processing_duration))

            else:
                has_more_data = False

    except Exception:
        logging.error('Failed to execute query {}'.format(query_string))
        raise

    logging.info('Finished chunk processing for stream {}'.format(catalog_entry.tap_stream_id))

    all_chunks_end_time = utils.now()
    full_chunk_processing_duration = (all_chunks_end_time - all_chunks_start_time).total_seconds()
    logging.info('Total processing time: {} seconds'.format(full_chunk_processing_duration))

    core.write_message(core.StateMessage(value=copy.deepcopy(state)), message_store=message_store)


================================================
File: src/mysql/replication/ddl_parser.py
================================================
import re
from dataclasses import dataclass
from enum import Enum
from typing import List, Optional, Tuple

import sqlparse
from sqlparse.sql import Identifier, Statement, Token, IdentifierList, TokenList, Parenthesis
from sqlparse.tokens import Whitespace

TABLE_NAME_INDEX = 4

FIRST_KEYWORD_INDEX = 8


class TableChangeType(Enum):
    DROP_COLUMN = 'DROP_COLUMN'
    ADD_COLUMN = 'ADD_COLUMN'


@dataclass
class TableSchemaChange:
    type: TableChangeType
    table_name: str
    schema: Optional[str]
    column_name: str
    after_column: str = None
    first_position: bool = False
    data_type: str = None
    collation: str = None
    column_key: str = None
    charset_name: str = None
    query: str = ''


class AlterStatementParser:
    """
    Parse ALTER statements.

    - Case sensitive.
    - Expects valid queries.
    - Multi statements are not supported. Apart from multi statement including USE {schema}; at the beginning.
    """
    # Supported statements - match patterns based on position
    SUPPORTED_ALTER_TABLE_STATEMENTS = ['ALTER TABLE {table_name} DROP COLUMN {col_name}',
                                        'ALTER TABLE {table_name} DROP {col_name}',
                                        'ALTER TABLE {table_name} ADD COLUMN {col_name}',
                                        'ALTER TABLE {table_name} ADD {col_name}']

    # minimal size of a query (ALTER TABLE xx XXX SOMETHING)
    MINIMAL_TOKEN_COUNT = 9

    def _is_column_name(self, statement: Statement, idx: int):
        next_idx, next_token = self._get_element_next_to_position(statement, idx)
        if statement.tokens[idx].normalized.upper() == 'FOREIGN' and next_token.upper() == 'KEY':
            return False
        if statement.tokens[idx].normalized.upper() == 'CONSTRAINT':
            return False
        if statement.tokens[idx].normalized.upper() == 'UNIQUE':
            return False
        if statement.tokens[idx].normalized == 'INDEX':
            return False
            return False
        if statement.tokens[idx].normalized == 'KEY':
            return False

        return True

    def _is_matching_pattern(self, statement: Statement, pattern: str):
        match = True
        for idx, value in enumerate(re.split(r'(\s+)', pattern)):
            if value.startswith('{'):
                if value == '{col_name}' and not self._is_column_name(statement, idx):
                    match = False
                continue
            if statement.tokens[idx].normalized != value:
                match = False
                break
        return match

    def _is_supported_alter_table_statement(self, normalized_statement):
        token_count = len(normalized_statement.tokens)
        if not (normalized_statement.get_type() == 'ALTER' and
                token_count >= self.MINIMAL_TOKEN_COUNT and
                normalized_statement.tokens[2].value.upper() == 'TABLE'):
            return False
        else:
            for pattern in self.SUPPORTED_ALTER_TABLE_STATEMENTS:
                match = self._is_matching_pattern(statement=normalized_statement, pattern=pattern)
                if match:
                    return True

            return False

    @staticmethod
    def __is_column_identifier(token: Token) -> bool:
        parent_is_name = (token.ttype == sqlparse.tokens.Name and isinstance(token.parent, Identifier))
        return isinstance(token, Identifier) or parent_is_name

    @staticmethod
    def _split_parenthesis(statement, token, position):
        ddl_type = statement.token_prev(position, skip_cm=True)
        new_tokens = []
        token_flat = []
        for t in list(token)[1:-1]:
            token_flat.extend(t.flatten()) if isinstance(t, IdentifierList) else token_flat.append(t)
        for t in token_flat:

            if t.ttype == sqlparse.tokens.Punctuation and t.normalized == ',':
                new_tokens.append(t)
                new_tokens.append(Token(Whitespace, ' '))
                new_tokens.append(ddl_type[1])
                new_tokens.append(Token(Whitespace, ' '))
            else:
                new_tokens.append(t)
        return new_tokens

    def __ungroup_identifier_lists(self, statement: Statement):
        """
        Dirty fix of a sqlparser bug that falsely groups statements like (FIRST, ADD) in
        ADD COLUMN email VARCHAR(100) NOT NULL FIRST, ADD
        """
        tokens = []
        for idx, t in enumerate(statement):

            if isinstance(t, IdentifierList):
                tokens.extend(t.flatten())
            elif isinstance(t, Parenthesis):
                tokens.extend(self._split_parenthesis(statement, t, idx))
            else:
                tokens.append(t)
        return TokenList(tokens)

    @staticmethod
    def _is_column_keyword(statement: Token):
        return statement.ttype == sqlparse.tokens.Keyword and statement.normalized == 'COLUMN'

    def _normalize_identifier(self, identifier: str):
        """
        Remove quotes
        """
        return identifier.replace('`', '')

    def _get_table_name(self, statement: Statement):
        schema = ''
        table_name = statement.tokens[TABLE_NAME_INDEX].value
        split = table_name.split('.')
        if len(split) > 1:
            table_name = split[1]
            schema = split[0]
        return self._normalize_identifier(schema), self._normalize_identifier(table_name)

    def _get_element_next_to_position(self, statement: TokenList, position):
        index, value = statement.token_next(position, skip_cm=True)
        next_value = value.normalized if value else ''
        return index, next_value

    def _process_drop_event(self, table_name, schema,
                            statement: Statement, original_statement_sql) -> List[TableSchemaChange]:
        schema_changes = []

        token_count = len(statement.tokens)
        first_keyword_index = FIRST_KEYWORD_INDEX
        for idx, value in enumerate(statement.tokens[FIRST_KEYWORD_INDEX:], start=FIRST_KEYWORD_INDEX):
            if (idx != token_count - 1) and value.ttype in [sqlparse.tokens.Whitespace, sqlparse.tokens.Newline]:
                # skip empty chars if not end
                continue
                # capture new col name
            elif idx == first_keyword_index:
                next_index = idx
                if self._is_column_keyword(value):
                    next_index, column_name = self._get_element_next_to_position(statement, idx)
                else:
                    column_name = value.normalized
                schema_changes.append(TableSchemaChange(TableChangeType.DROP_COLUMN, table_name, schema,
                                                        self._normalize_identifier(column_name),
                                                        query=original_statement_sql))

            elif value.ttype == sqlparse.tokens.Punctuation and value.normalized == ',':
                # next is always another DROP, if not it may algorithm, lock, etc, so quit
                add_keyword_index, element = self._get_element_next_to_position(statement, idx)
                if element.upper() != 'DROP':
                    continue
                first_keyword_index, element = self._get_element_next_to_position(statement, add_keyword_index)

        return schema_changes

    def _process_add_event(self, table_name, schema,
                           statement: Statement, original_statement_sql) -> List[TableSchemaChange]:
        schema_changes = []
        schema_change = None
        token_count = len(statement.tokens)

        # index of first keyword after ADD statement
        first_keyword_index = FIRST_KEYWORD_INDEX
        for idx, value in enumerate(statement.tokens[FIRST_KEYWORD_INDEX:], start=FIRST_KEYWORD_INDEX):

            if (idx != token_count - 1) and value.ttype in [sqlparse.tokens.Whitespace, sqlparse.tokens.Newline]:
                # skip empty chars if not end
                continue
            # capture new col name
            elif idx == first_keyword_index:
                next_index = idx
                if self._is_column_keyword(value):
                    next_index, column_name = self._get_element_next_to_position(statement, idx)
                else:
                    column_name = value.value
                # next one is always datatype
                next_index, data_type = self._get_element_next_to_position(statement, next_index)
                schema_change = TableSchemaChange(TableChangeType.ADD_COLUMN, table_name, schema,
                                                  self._normalize_identifier(column_name),
                                                  data_type=data_type.upper(),
                                                  query=original_statement_sql)

            # AFTER statement
            elif value.ttype == sqlparse.tokens.Keyword and value.normalized == 'AFTER':
                # next one is always column name
                next_index, schema_change.after_column = self._get_element_next_to_position(statement, idx)

            # CHARACTER SET statement
            elif value.ttype == sqlparse.tokens.Keyword and value.normalized == 'CHARACTER':
                # next should be SET statement
                next_index, next_element = self._get_element_next_to_position(statement, idx)
                if 'SET' == next_element:
                    schema_change.charset_name = self._get_element_next_to_position(statement, next_index)[1]

            # COLLATE  statement
            elif value.ttype == sqlparse.tokens.Keyword and value.normalized == 'COLLATE':
                # next one is always column name
                next_index, schema_change.collation = self._get_element_next_to_position(statement, idx)

            # PRIMARY KEY  statement
            elif value.ttype == sqlparse.tokens.Keyword and value.normalized == 'PRIMARY' \
                    and self._get_element_next_to_position(statement, idx)[1] == 'KEY':
                # next one is always column name
                schema_change.column_key = 'PRI'

            # process FIRST statement
            elif value.ttype == sqlparse.tokens.Keyword and value.normalized == 'FIRST':
                schema_change.first_position = True

            # is at the end of multiline statement or end of the query
            elif value.ttype == sqlparse.tokens.Punctuation and value.normalized == ',':
                # next is always another ADD, if not it may algorithm, lock, etc, so quit
                add_keyword_index, element = self._get_element_next_to_position(statement, idx)
                if element.upper() != 'ADD':
                    continue
                first_keyword_index, element = self._get_element_next_to_position(statement, add_keyword_index)
                if schema_change is None:
                    raise RuntimeError(f"Invalid ALTER statement query: {statement.normalized}")
                schema_changes.append(schema_change)

            # save schema change on end, should never be empty
            if idx == token_count - 1:
                if schema_change is None:
                    raise RuntimeError(f"Invalid ALTER statement query: {statement.normalized}")
                schema_changes.append(schema_change)

        return schema_changes

    def _get_schema_from_use_statement(self, statement: Statement):
        schema = self._get_element_next_to_position(statement, 0)[1]
        return self._normalize_identifier(schema)

    def _extract_alter_statement_and_schema(self, normalized_statements: Statement):
        use_schema = ''
        normalized_statement = ''
        for statement in normalized_statements:
            first_token = statement.token_first(skip_cm=True)
            if first_token.normalized == 'ALTER':
                normalized_statement = statement
            elif first_token.normalized == 'USE':
                use_schema = self._get_schema_from_use_statement(statement)
        return use_schema, normalized_statement

    def _split_drop_add(self, flattened_tokens, change_type) -> Tuple[Statement, Statement]:
        change_tokens = {'ADD': [], 'DROP': []}
        alter_tokens = flattened_tokens.tokens[:6]
        last_index = len(flattened_tokens.tokens) - 1
        skip_indexes = [-1]
        type_change = False
        for idx, value in enumerate(flattened_tokens.tokens[6:], start=6):
            if idx in skip_indexes:
                continue

            if value.ttype == sqlparse.tokens.Punctuation and value.normalized == ',':
                next_index, element = self._get_element_next_to_position(flattened_tokens, idx)
                if element.upper() in ['DROP', 'ADD']:
                    type_change = change_type != element.upper()
                    change_type = element.upper()
                    skip_indexes = list(range(idx, next_index))

                if type_change or idx == last_index:
                    # reset
                    type_change = False
                    continue

            change_tokens[change_type].append(value)

        add_statement = Statement(alter_tokens + change_tokens['ADD']) if change_tokens['ADD'] else Statement([])
        drop_statement = Statement(alter_tokens + change_tokens['DROP']) if change_tokens['DROP'] else Statement([])
        return add_statement, drop_statement

    def _normalize_whitespaces(self, statement: TokenList):
        """
        There can never be two empty characters in a row, remove the additional ones
        Args:
            statement:

        Returns:

        """
        tokens = []
        for idx, t in enumerate(statement.tokens):
            if idx == 0:
                tokens.append(t)
            elif t.ttype in [sqlparse.tokens.Whitespace]:
                if statement.tokens[idx - 1].ttype in [sqlparse.tokens.Whitespace]:
                    continue
                tokens.append(t)
            else:
                tokens.append(t)
        return TokenList(tokens)

    def _process_add_drop_changes(self, table_name, schema_name, normalized_statement):
        first_type = normalized_statement.tokens[6].normalized
        table_changes = []
        # because some statements including FIRST were invalidly parsed as identifier groups
        # happens when tokens of type Keyword are separated by comma
        flattened_tokens = self.__ungroup_identifier_lists(normalized_statement)
        normalized = self._normalize_whitespaces(flattened_tokens)
        add_statement, drop_statement = self._split_drop_add(normalized,
                                                             change_type=first_type)

        table_changes.extend(self._process_drop_event(table_name,
                                                      schema_name,
                                                      drop_statement,
                                                      normalized_statement.normalized))

        table_changes.extend(self._process_add_event(table_name,
                                                     schema_name,
                                                     add_statement,
                                                     normalized_statement.normalized))
        self._validate_table_changes(table_changes)
        return table_changes

    def _validate_table_changes(self, table_changes: List[TableSchemaChange]):
        errors = []
        for change in table_changes:
            if not change.table_name:
                errors.append(f"Failed to parse table name from {change.query}")
        if errors:
            raise RuntimeError(f"Failed to parse ALTER statements: {', '.join(errors)}")

    def get_table_changes(self, sql: str, schema: str) -> List[TableSchemaChange]:
        normalized_statements = sqlparse.parse(sqlparse.format(sql, strip_comments=True, reindent_aligned=True,
                                                               strip_whitespace=True).replace('\n', ' '))
        use_schema, normalized_statement = self._extract_alter_statement_and_schema(normalized_statements)

        # normalized / formatted by now, should be safe to use fixed index
        if not normalized_statement or not self._is_supported_alter_table_statement(normalized_statement):
            return []

        query_schema, table_name = self._get_table_name(normalized_statement)

        schema_name = schema or query_schema or use_schema

        return self._process_add_drop_changes(table_name, schema_name, normalized_statement)


================================================
File: src/mysql/replication/full_table.py
================================================
"""
Full table replication sync.
"""
import datetime
import logging

try:
    import core as core
    from core import metadata
    from mysql.client import connect_with_backoff
    import mysql.replication.binlog as binlog
    import mysql.replication.common as common
except ImportError:
    import src.core as core
    from src.core import metadata
    from src.mysql.client import connect_with_backoff
    import src.mysql.replication.binlog as binlog
    import src.mysql.replication.common as common

# Date-type fields included as may be potentially part of composite key.
RESUMABLE_PK_TYPES = {'tinyint', 'smallint', 'mediumint', 'int', 'bigint', 'char', 'varchar',
                      'datetime', 'timestamp', 'date', 'time'}


def generate_bookmark_keys(catalog_entry):
    md_map = metadata.to_map(catalog_entry.metadata)
    stream_metadata = md_map.get((), {})
    replication_method = stream_metadata.get('replication-method')

    base_bookmark_keys = {'last_pk_fetched', 'max_pk_values', 'version', 'initial_full_table_complete'}

    if replication_method.upper() == 'FULL_TABLE':
        bookmark_keys = base_bookmark_keys
    else:
        bookmark_keys = base_bookmark_keys.union(binlog.BOOKMARK_KEYS)

    return bookmark_keys


def sync_is_resumable(mysql_conn, catalog_entry):
    """In order to resume a full table sync, a table requires."""
    database_name = common.get_database_name(catalog_entry)
    key_properties = common.get_key_properties(catalog_entry)

    if not key_properties:
        return False

    sql = """SELECT data_type
               FROM information_schema.columns
              WHERE table_schema = '{}'
                AND table_name = '{}'
                AND column_name = '{}'
    """

    with connect_with_backoff(mysql_conn) as open_conn:
        with open_conn.cursor() as cur:
            for pk in key_properties:
                cur.execute(sql.format(database_name, catalog_entry.table, pk))

                result = cur.fetchone()

                if not result:
                    raise Exception("Primary key column {} does not exist.".format(pk))

                if result[0] not in RESUMABLE_PK_TYPES:
                    logging.warning("Found primary key column %s with type %s. Will not be able " +
                                    "to resume interrupted FULL_TABLE sync using this key.", pk, result[0])
                    return False

    return True


def get_max_pk_values(cursor, catalog_entry):
    database_name = common.get_database_name(catalog_entry)
    escaped_db = common.escape(database_name)
    escaped_table = common.escape(catalog_entry.table)

    key_properties = common.get_key_properties(catalog_entry)
    escaped_columns = [common.escape(c) for c in key_properties]

    sql = """SELECT {}
               FROM {}.{}
    """

    select_column_clause = ", ".join(["max(" + pk + ")" for pk in escaped_columns])
    cursor.execute(sql.format(select_column_clause, escaped_db, escaped_table))

    result = cursor.fetchone()
    processed_results = []
    for bm in result:
        if isinstance(bm, (datetime.date, datetime.datetime, datetime.timedelta)):
            processed_results += [common.to_utc_datetime_str(bm)]
        elif bm is not None:
            processed_results += [bm]

    max_pk_values = {}
    if processed_results:
        max_pk_values = dict(zip(key_properties, processed_results))

    return max_pk_values


def quote_where_clause_value(value, column_type):
    if 'string' in column_type:
        return "'" + str(value) + "'"

    return str(value)


def generate_pk_bookmark_clause(key_properties, last_pk_fetched, catalog_entry):
    """
    Generates a bookmark clause based on `key_properties`, and
    `last_pk_fetched` bookmark. This ensures that the stream is resumed at
    the location in the data set per primary key component. Inclusivity is
    not maintained, since these are primary keys.
    Example:
    key_properties = ['name','birthday']
    last_pk_fetched = {'name': "Phil Collins", 'birthday': "1951-01-30"}
    Returns:
    "(`name` > 'Phil Collins') OR (`name` = 'Phil Collins' AND `birthday` > '1951-01-30')
    """
    assert last_pk_fetched is not None, \
        "Must call generate_pk_bookmark with a non-null 'last_pk_fetched' dict"

    clause_terms = []
    inclusive_pk_values = []
    for pk in key_properties:
        term = []
        for prev_pk, prev_pk_val, prev_col_type in inclusive_pk_values:
            term.append(common.escape(prev_pk) + ' = ' + quote_where_clause_value(prev_pk_val, prev_col_type))

        column_type = catalog_entry.schema.properties.get(pk).type
        term.append(common.escape(pk) + ' > ' + quote_where_clause_value(last_pk_fetched[pk], column_type))
        inclusive_pk_values.append((pk, last_pk_fetched[pk], column_type))

        clause_terms.append(' AND '.join(term))
    return '({})'.format(') OR ('.join(clause_terms)) if clause_terms else ''


def generate_pk_clause(catalog_entry, state):
    key_properties = common.get_key_properties(catalog_entry)

    max_pk_values = core.get_bookmark(state, catalog_entry.tap_stream_id, 'max_pk_values')

    last_pk_fetched = core.get_bookmark(state, catalog_entry.tap_stream_id, 'last_pk_fetched')

    last_pk_clause = ''
    max_pk_comparisons = []

    if not max_pk_values:
        return ""

    if last_pk_fetched:
        for pk in key_properties:
            column_type = catalog_entry.schema.properties.get(pk).type

            # Add AND to interpolate along with max_pk_values clauses
            last_pk_clause = '({}) AND '.format(generate_pk_bookmark_clause(key_properties,
                                                                            last_pk_fetched,
                                                                            catalog_entry))
            max_pk_comparisons.append("{} <= {}".format(common.escape(pk),
                                                        quote_where_clause_value(max_pk_values[pk],
                                                                                 column_type)))
    else:
        for pk in key_properties:
            column_schema = catalog_entry.schema.properties.get(pk)
            column_type = column_schema.type

            pk_val = quote_where_clause_value(max_pk_values[pk],
                                              column_type)

            max_pk_comparisons.append("{} <= {}".format(common.escape(pk), pk_val))

    order_by_columns = [common.escape(c) for c in key_properties]
    sql = " WHERE {}{} ORDER BY {} ASC".format(last_pk_clause,
                                               " AND ".join(max_pk_comparisons),
                                               ", ".join(order_by_columns))

    return sql


def update_incremental_full_table_state(catalog_entry, state, cursor):
    max_pk_values = core.get_bookmark(state, catalog_entry.tap_stream_id,
                                      'max_pk_values') or get_max_pk_values(cursor, catalog_entry)

    if not max_pk_values:
        logging.info("No max value for PK found for table {}".format(catalog_entry.table))
    else:
        state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'max_pk_values', max_pk_values)

    return state


def sync_table(mysql_conn, catalog_entry, state, columns, stream_version):
    common.whitelist_bookmark_keys(generate_bookmark_keys(catalog_entry), catalog_entry.tap_stream_id, state)

    # bookmark = state.get('bookmarks', {}).get(catalog_entry.tap_stream_id, {})
    # version_exists = True if 'version' in bookmark else False

    # initial_full_table_complete = core.get_bookmark(state, catalog_entry.tap_stream_id, 'initial_full_table_complete')

    # state_version = core.get_bookmark(state, catalog_entry.tap_stream_id, 'version')

    # activate_version_message = core.ActivateVersionMessage(
    #     stream=catalog_entry.stream,
    #     version=stream_version
    # )
    #
    # # For the initial replication, emit an ACTIVATE_VERSION message
    # # at the beginning so the records show up right away.
    # if not initial_full_table_complete and not (version_exists and state_version is None):
    #     core.write_message(activate_version_message)

    perform_resumable_sync = sync_is_resumable(mysql_conn, catalog_entry)

    pk_clause = ""

    with connect_with_backoff(mysql_conn) as open_conn:
        with open_conn.cursor() as cur:
            select_sql = common.generate_select_sql(catalog_entry, columns)

            if perform_resumable_sync:
                logging.info("Full table sync is "
                             "resumable based on primary key definition, replicating incrementally")

                state = update_incremental_full_table_state(catalog_entry, state, cur)
                pk_clause = generate_pk_clause(catalog_entry, state)

            select_sql += pk_clause
            params = {}

            common.sync_query(cur, catalog_entry, state, select_sql, columns, stream_version, params)

    # clear max pk value and last pk fetched upon successful sync
    core.clear_bookmark(state, catalog_entry.tap_stream_id, 'max_pk_values')
    core.clear_bookmark(state, catalog_entry.tap_stream_id, 'last_pk_fetched')

    # core.write_message(activate_version_message)


def sync_table_chunks(mysql_conn, catalog_entry, state, columns, stream_version, tables_destination: str = None,
                      message_store: core.MessageStore = None):
    common.whitelist_bookmark_keys(generate_bookmark_keys(catalog_entry), catalog_entry.tap_stream_id, state)

    # bookmark = state.get('bookmarks', {}).get(catalog_entry.tap_stream_id, {})
    # version_exists = True if 'version' in bookmark else False

    # initial_full_table_complete = core.get_bookmark(state, catalog_entry.tap_stream_id, 'initial_full_table_complete')

    # state_version = core.get_bookmark(state, catalog_entry.tap_stream_id, 'version')

    # activate_version_message = core.ActivateVersionMessage(
    #     stream=catalog_entry.stream,
    #     version=stream_version
    # )
    #
    # # For the initial replication, emit an ACTIVATE_VERSION message
    # # at the beginning so the records show up right away.
    # if not initial_full_table_complete and not (version_exists and state_version is None):
    #     core.write_message(activate_version_message)

    perform_resumable_sync = sync_is_resumable(mysql_conn, catalog_entry)

    pk_clause = ""

    with connect_with_backoff(mysql_conn) as open_conn:
        with open_conn.cursor() as cur:
            select_sql = common.generate_select_sql(catalog_entry, columns)

            if perform_resumable_sync:
                logging.info("Full table sync is resumable based on primary key definition, replicating incrementally")

                state = update_incremental_full_table_state(catalog_entry, state, cur)
                pk_clause = generate_pk_clause(catalog_entry, state)

            select_sql += pk_clause
            params = {}

            common.sync_query_bulk(open_conn, cur, catalog_entry, state, select_sql, columns, stream_version, params,
                                   tables_destination, message_store=message_store)

    # clear max pk value and last pk fetched upon successful sync
    core.clear_bookmark(state, catalog_entry.tap_stream_id, 'max_pk_values')
    core.clear_bookmark(state, catalog_entry.tap_stream_id, 'last_pk_fetched')

    # core.write_message(activate_version_message)


================================================
File: src/mysql/replication/incremental.py
================================================
"""
Incremental replication via key-based replication.
"""
import logging
import pendulum

try:
    import core as core
    from core import metadata
    from mysql.client import connect_with_backoff
    import mysql.replication.common as common
except ImportError:
    import src.core as core
    from src.core import metadata
    from src.mysql.client import connect_with_backoff
    import src.mysql.replication.common as common

BOOKMARK_KEYS = {'replication_key', 'replication_key_value', 'version'}


def sync_table(mysql_conn, catalog_entry, state, columns, limit=None, message_store: core.MessageStore = None):
    logging.warning('Note: Syncing incrementally with key is not yet fully supported, results may be incomplete')
    common.whitelist_bookmark_keys(BOOKMARK_KEYS, catalog_entry.tap_stream_id, state)

    catalog_metadata = metadata.to_map(catalog_entry.metadata)
    stream_metadata = catalog_metadata.get((), {})

    iterate_limit = True
    while iterate_limit:
        replication_key_metadata = stream_metadata.get('replication-key')
        replication_key_state = core.get_bookmark(state, catalog_entry.tap_stream_id, 'replication_key')
        replication_key_value = None

        if replication_key_metadata == replication_key_state:
            replication_key_value = core.get_bookmark(state, catalog_entry.tap_stream_id, 'replication_key_value')
        else:
            state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'replication_key',
                                        replication_key_metadata)
            state = core.clear_bookmark(state, catalog_entry.tap_stream_id, 'replication_key_value')

        stream_version = common.get_stream_version(catalog_entry.tap_stream_id, state)
        state = core.write_bookmark(state, catalog_entry.tap_stream_id, 'version', stream_version)

        # activate_version_message = core.ActivateVersionMessage(
        #     stream=catalog_entry.stream,
        #     version=stream_version
        # )
        #
        # core.write_message(activate_version_message)

        with connect_with_backoff(mysql_conn) as open_conn:
            with open_conn.cursor() as cursor:
                select_sql = common.generate_select_sql(catalog_entry, columns)
                params = {}

                if replication_key_value is not None:
                    if catalog_entry.schema.properties[replication_key_metadata].format == 'date-time':
                        replication_key_value = pendulum.parse(replication_key_value)

                    select_sql += ' WHERE `{}` >= %(replication_key_value)s ORDER BY `{}` ASC'.format(
                        replication_key_metadata,
                        replication_key_metadata)

                    params['replication_key_value'] = replication_key_value
                elif replication_key_metadata is not None:
                    select_sql += ' ORDER BY `{}` ASC'.format(replication_key_metadata)

                if limit:
                    select_sql += ' LIMIT {}'.format(limit)

                num_rows = common.sync_query(cursor, catalog_entry, state, select_sql, columns, stream_version, params,
                                             message_store=message_store)
                if limit is None or num_rows < limit:
                    iterate_limit = False


================================================
File: src/mysql/replication/stream_reader.py
================================================
import functools
import logging
import struct
from distutils.version import LooseVersion
from enum import Enum
from typing import List

import backoff
import pymysql
from pymysqlreplication import BinLogStreamReader
from pymysqlreplication import constants, row_event, event
from pymysqlreplication.column import Column
from pymysqlreplication.constants.BINLOG import TABLE_MAP_EVENT, ROTATE_EVENT, QUERY_EVENT
from pymysqlreplication.event import (QueryEvent, RotateEvent, BinLogEvent)
from pymysqlreplication.packet import BinLogPacketWrapper
from pymysqlreplication.table import Table

from mysql.replication import common
from mysql.replication.ddl_parser import AlterStatementParser, TableChangeType, TableSchemaChange

try:
    from pymysql.constants.COMMAND import COM_BINLOG_DUMP_GTID
except ImportError:
    # Handle old pymysql versions
    # See: https://github.com/PyMySQL/PyMySQL/pull/261
    COM_BINLOG_DUMP_GTID = 0x1e

# 2013 Connection Lost
# 2006 MySQL server has gone away
MYSQL_EXPECTED_ERROR_CODES = [2013, 2006]


class SchemaOffsyncError(Exception):
    pass


class TableColumnSchemaCache:
    """
    Container object for table Schema cache. Caches table column schema based on table name.

    The column schema object is a dictionary:

    ```
                   stream-id: {
                        'COLUMN_NAME': 'name',
                        'ORDINAL_POSITION': 1,
                        'COLLATION_NAME': None,
                        'CHARACTER_SET_NAME': None,
                        'COLUMN_COMMENT': None,
                        'COLUMN_TYPE': 'BLOB',
                        'COLUMN_KEY': ''
                    }
    ```

    """

    def __init__(self, table_schema_cache: dict, table_schema_current: dict = None):
        """

        Args:
            table_schema_cache: Column schemas cached from the last time. It needs to be updated with each ALTER event.
            table_schema_current:
                internal cache of column schema that is actual at the time of execution
                (possibly newer than table_schema_cache)
        """
        self.table_schema_cache = table_schema_cache or {}
        self.__table_indexes = {}

        # internal cache of column schema that is actual at the time of execution
        #         (possibly newer than table_schema_cache)
        self._table_schema_current = table_schema_current or {}

    def _get_db_default_schema(self):
        table_schema = {}
        if self._table_schema_current:
            key = next(iter(self._table_schema_current))
            current_schema = self._table_schema_current[key]
            if current_schema:
                table_schema = current_schema[0]

        if not table_schema.get('DEFAULT_CHARSET'):
            logging.warning('No default charset found, using utf8',
                            extra={"full_message": self._table_schema_current})
        return table_schema.get('DEFAULT_CHARSET', 'utf8')

    @staticmethod
    def build_column_schema(column_name: str, ordinal_position: int, column_type: str, is_primary_key: bool,
                            collation=None,
                            character_set_name=None, column_comment=None) -> dict:
        if is_primary_key:
            key = 'PRI'
        else:
            key = ''
        return {
            'COLUMN_NAME': column_name,
            'ORDINAL_POSITION': ordinal_position,
            'COLLATION_NAME': collation,
            'CHARACTER_SET_NAME': character_set_name,
            'COLUMN_COMMENT': column_comment,
            'COLUMN_TYPE': column_type,
            'COLUMN_KEY': key
        }

    def is_current_schema_cached(self, schema: str, table: str):
        if self._table_schema_current.get(self.get_table_cache_index(schema, table)):
            return True
        else:
            return False

    def update_current_schema_cache(self, schema: str, table: str, column_schema: []):
        """
        Update internal cache of column schema that is actual at the time of execution
        (possibly newer than table_schema_cache)
        Args:
            column_schema:

        Returns:

        """
        index = self.get_table_cache_index(schema, table)
        self._table_schema_current[index] = column_schema

    def get_column_schema(self, schema: str, table: str) -> List[dict]:
        index = self.get_table_cache_index(schema, table)
        return self.table_schema_cache.get(index)

    def update_table_ids_cache(self, schema: str, table: str, mysql_table_id: int):
        """
        Keeps MySQL internal table_ids cached

        Args:
            schema:
            table:
            mysql_table_id: Internal Mysql table id

        Returns:

        """
        index = self.get_table_cache_index(schema, table)
        if not self.__table_indexes.get(index):
            self.__table_indexes[index] = set()

        self.__table_indexes[index].add(mysql_table_id)

    def invalidate_table_ids_cache(self):
        self.__table_indexes = {}

    def get_table_ids(self, schema: str, table: str):
        """
        Returns internal table ID from cache.
        Args:
            schema:
            table:

        Returns:

        """
        index = self.get_table_cache_index(schema, table)
        return self.__table_indexes.get(index, [])

    def set_column_schema(self, schema: str, table: str, column_schema: List[dict]):
        index = self.get_table_cache_index(schema, table)
        self.table_schema_cache[index] = column_schema

    def get_table_cache_index(self, schema: str, table: str):
        # index as not case sensitive, use the same method as the tap_stream_id
        return common.generate_tap_stream_id(schema, table)

    def update_cache(self, table_change: TableSchemaChange):
        """
        Updates schema cache based on table changes.

        Args:
            table_changes:

        Returns:

        """

        if table_change.type == TableChangeType.DROP_COLUMN:
            self.update_cache_drop_column(table_change)
        elif table_change.type == TableChangeType.ADD_COLUMN:
            self.update_cache_add_column(table_change)

    def update_cache_drop_column(self, drop_change: TableSchemaChange):
        index = self.get_table_cache_index(drop_change.schema, drop_change.table_name)
        column_schema = self.table_schema_cache.get(index, [])

        # drop column if exists
        drop_at_position = None
        update_ordinal_position = False
        new_schema = []
        # 1 based
        for idx, col in enumerate(column_schema, start=1):
            if col['COLUMN_NAME'].upper() == drop_change.column_name.upper():
                # mark and skip
                update_ordinal_position = True
                continue

            if update_ordinal_position:
                # shift ordinal position
                col['ORDINAL_POSITION'] = idx - 1
            new_schema.append(col)

        if not update_ordinal_position:
            raise SchemaOffsyncError(f'Dropped column: "{drop_change.column_name}" '
                                     f'is already missing in the provided starting '
                                     f'schema of table {drop_change.table_name} => may lead to value shift!'
                                     f' The affected query is: {drop_change.query}')

        if not column_schema:
            # should not happen
            raise SchemaOffsyncError(f'Table {index} not found in the provided table schema cache!')

        self.table_schema_cache[index] = new_schema

    def __add_column_at_position(self, original_schema, added_column, after_col, table_name):
        new_schema = []
        # add column if not exists
        update_ordinal_position = False
        # 1 based
        for idx, col in enumerate(original_schema, start=1):

            # on first position
            if idx == 1 and after_col == '':
                added_column['ORDINAL_POSITION'] = 1
                update_ordinal_position = True
                new_schema.append(col)

            # after specific
            elif after_col and col['COLUMN_NAME'].upper() == after_col.upper():
                added_column['ORDINAL_POSITION'] = idx + 1
                # mark and add both
                new_schema.append(col)
                new_schema.append(added_column)
                update_ordinal_position = True

            elif update_ordinal_position:
                # shift ordinal position of others
                col['ORDINAL_POSITION'] = idx + 1
                new_schema.append(col)
            else:
                # otherwise append unchanged
                new_schema.append(col)

        if not update_ordinal_position:
            raise SchemaOffsyncError(f'Dropped column: "{added_column["COLUMN_NAME"]}" in table {table_name}'
                                     f'is already missing in the provided starting schema => may lead to value shift!')
        return new_schema

    def update_cache_add_column(self, add_change: TableSchemaChange):
        index = self.get_table_cache_index(add_change.schema, add_change.table_name)
        column_schema = self.table_schema_cache.get(index, [])

        logging.debug(f'Current column schema cache: {self.table_schema_cache}, index: {index}')
        column_names = [c['COLUMN_NAME'].upper() for c in column_schema]
        if not column_names:
            raise RuntimeError(f'The schema cache for table {index} is not initialized!')

        if add_change.column_name.upper() in column_names:
            logging.warning(f'The added column "{add_change.column_name.upper()}" is already present '
                            f'in the schema "{index}", skipping.')
            return
        logging.debug(f"New schema ADD change received {add_change}")
        added_column = self._build_new_column_schema(add_change)
        logging.debug(f"New column schema built {added_column}")
        new_schema = []
        # get after_column
        if add_change.first_position:
            after_col = ''
        elif add_change.after_column:
            after_col = add_change.after_column
        else:
            # add after last
            added_column['ORDINAL_POSITION'] = len(column_schema) + 1
            column_schema.append(added_column)
            new_schema = column_schema

        # exit
        if not new_schema:
            new_schema = self.__add_column_at_position(column_schema, added_column, after_col, add_change.table_name)

        if not column_schema:
            # should not happen
            raise SchemaOffsyncError(f'Table {index} not found in the provided table schema cache!')

        self.table_schema_cache[index] = new_schema

    def _build_new_column_schema(self, table_change: TableSchemaChange) -> dict:
        index = self.get_table_cache_index(table_change.schema, table_change.table_name)

        current_schema = self._table_schema_current.get(index, [])
        if not current_schema:
            logging.warning(f'Table {table_change.table_name} not found in current schema cache.',
                            extra={'full_message': self._table_schema_current})
        existing_col = None

        # check if column exists in current schema
        # this allows to get all column metadata properly in case
        # we missed some ALTER COLUMN statement, e.g. for changing datatypes
        for c in current_schema:
            logging.debug(
                f"Added column '{table_change.column_name.upper()}' "
                f"exists in the current schema: {current_schema}")
            if c['COLUMN_NAME'].upper() == table_change.column_name.upper():
                # convert name to upper_case just in case
                # TODO: consider moving this to current_schema build-up
                c['COLUMN_NAME'] = c['COLUMN_NAME'].upper()
                existing_col = c

        if existing_col:
            new_column = existing_col
        else:
            # add metadata from the ALTER event
            is_pkey = table_change.column_key == 'PRI'

            charset_name = table_change.charset_name or self._get_db_default_schema()
            new_column = self.build_column_schema(column_name=table_change.column_name.upper(),
                                                  # to be updated later
                                                  ordinal_position=0,
                                                  column_type=table_change.data_type,
                                                  is_primary_key=is_pkey,
                                                  collation=table_change.collation,
                                                  character_set_name=charset_name
                                                  )

        return new_column


class BinLogStreamReaderAlterTracking(BinLogStreamReader):
    """Connect to replication stream and read event.
    Modification of default BinLogStreamReader, that is capable of handling schema changes (e.g. column drops)
    properly by processing ALTER query events.

    The client application needs to keep and pass the current schema of the replicated table to the Reader
    in order to track changes properly. The client application needs to collect the latest schema cache from the
    `BinLogStreamReaderAlterTracking.table_schema_cache` attribute after each sync.

    """
    report_slave = None

    def __init__(self, connection_settings, server_id,
                 ctl_connection_settings=None, resume_stream=False,
                 blocking=False, only_events=None, log_file=None,
                 log_pos=None, filter_non_implemented_events=True,
                 ignored_events=None, auto_position=None,
                 only_tables=None, ignored_tables=None,
                 only_schemas=None, ignored_schemas=None,
                 freeze_schema=False, skip_to_timestamp=None,
                 report_slave=None, slave_uuid=None,
                 pymysql_wrapper=None,
                 fail_on_table_metadata_unavailable=False,
                 slave_heartbeat=None, table_schema_cache: dict = None,
                 ignore_decode_errors=False):
        """
        Attributes:
            ctl_connection_settings: Connection settings for cluster holding
                                     schema information
            resume_stream: Start for event from position or the latest event of
                           binlog or from older available event
            blocking: When master has finished reading/sending binlog it will
                      send EOF instead of blocking connection.
            only_events: Array of allowed events
            ignored_events: Array of ignored events
            log_file: Set replication start log file
            log_pos: Set replication start log pos (resume_stream should be
                     true)
            auto_position: Use master_auto_position gtid to set position
            only_tables: An array with the tables you want to watch (only works
                         in binlog_format ROW)
            ignored_tables: An array with the tables you want to skip
            only_schemas: An array with the schemas you want to watch
            ignored_schemas: An array with the schemas you want to skip
            freeze_schema: If true do not support ALTER TABLE. It's faster.
            skip_to_timestamp: Ignore all events until reaching specified
                               timestamp.
            report_slave: Report slave in SHOW SLAVE HOSTS.
            slave_uuid: Report slave_uuid in SHOW SLAVE HOSTS.
            fail_on_table_metadata_unavailable: Should raise exception if we
                                                can't get table information on
                                                row_events
            slave_heartbeat: (seconds) Should master actively send heartbeat on
                             connection. This also reduces traffic in GTID
                             replication on replication resumption (in case
                             many event to skip in binlog). See
                             MASTER_HEARTBEAT_PERIOD in mysql documentation
                             for semantics
            table_schema_cache: Latest schemas of the synced tables (from previous execution). Indexed by table name.

        """
        super().__init__(connection_settings, server_id,
                         ctl_connection_settings=ctl_connection_settings,
                         resume_stream=resume_stream,
                         blocking=blocking,
                         only_events=only_events,
                         log_file=log_file,
                         log_pos=log_pos,
                         filter_non_implemented_events=filter_non_implemented_events,
                         ignored_events=ignored_events,
                         auto_position=auto_position,
                         only_tables=only_tables,
                         ignored_tables=ignored_tables,
                         only_schemas=only_schemas,
                         ignored_schemas=ignored_schemas,
                         freeze_schema=freeze_schema,
                         skip_to_timestamp=skip_to_timestamp,
                         report_slave=report_slave,
                         slave_uuid=slave_uuid,
                         pymysql_wrapper=pymysql_wrapper,
                         fail_on_table_metadata_unavailable=fail_on_table_metadata_unavailable,
                         slave_heartbeat=slave_heartbeat,
                         ignore_decode_errors=ignore_decode_errors)

        # We can't filter on packet level TABLE_MAP, Query and rotate event because
        # we need them for handling other operations. Add custom events
        self._BinLogStreamReader__allowed_events_in_packet = frozenset(
            [TableMapEventAlterTracking, RotateEvent, QueryEventWithSchemaChanges]).union(
            self._BinLogStreamReader__allowed_events)
        # expose
        self.__allowed_events = self._BinLogStreamReader__allowed_events

        # Store table meta information cached from the last time
        self.schema_cache = TableColumnSchemaCache(table_schema_cache)

        self.alter_parser = AlterStatementParser()

        # init just in case of redeploying new version
        self._init_column_schema_cache(only_schemas[0], only_tables)

    def _init_column_schema_cache(self, schema: str, tables: List[str]):
        """
        Helper method to initi schema cache in case the extraction started with empty state
        Returns:

        """
        for table in tables:
            # hacky way to call the parent secret method
            if self.schema_cache.get_column_schema(schema, table):
                continue
            logging.warning(f"Schema for table {schema}-{table} is not initialized, using current schema")
            current_column_schema = self._get_table_information_from_db(schema, table)
            # update cache with current schema
            self.schema_cache.set_column_schema(schema, table, current_column_schema)

            self.schema_cache.update_current_schema_cache(schema, table, current_column_schema)

    def fetchone(self):
        while True:
            if not self._BinLogStreamReader__connected_stream:
                self._BinLogStreamReader__connect_to_stream()

            if not self._BinLogStreamReader__connected_ctl:
                self._BinLogStreamReader__connect_to_ctl()

            try:
                if pymysql.__version__ < LooseVersion("0.6"):
                    pkt = self._stream_connection.read_packet()
                else:
                    pkt = self._stream_connection._read_packet()
            except pymysql.OperationalError as error:
                code, message = error.args
                if code in MYSQL_EXPECTED_ERROR_CODES:
                    self._stream_connection.close()
                    self._BinLogStreamReader__connected_stream = False
                    continue
                raise

            if pkt.is_eof_packet():
                self.close()
                return None

            if not pkt.is_ok_packet():
                continue

            binlog_event = BinLogPacketWrapperModified(pkt, self.table_map,
                                                       self._ctl_connection,
                                                       self.mysql_version,
                                                       self._BinLogStreamReader__use_checksum,
                                                       self._BinLogStreamReader__allowed_events_in_packet,
                                                       self._BinLogStreamReader__only_tables,
                                                       self._BinLogStreamReader__ignored_tables,
                                                       self._BinLogStreamReader__only_schemas,
                                                       self._BinLogStreamReader__ignored_schemas,
                                                       self._BinLogStreamReader__freeze_schema,
                                                       self._BinLogStreamReader__fail_on_table_metadata_unavailable,
                                                       self._BinLogStreamReader__ignore_decode_errors)

            if binlog_event.event_type == ROTATE_EVENT:
                self.log_pos = binlog_event.event.position
                self.log_file = binlog_event.event.next_binlog
                # Table Id in binlog are NOT persistent in MySQL - they are in-memory identifiers
                # that means that when MySQL master restarts, it will reuse same table id for different tables
                # which will cause errors for us since our in-memory map will try to decode row data with
                # wrong table schema.
                # The fix is to rely on the fact that MySQL will also rotate to a new binlog file every time it
                # restarts. That means every rotation we see *could* be a sign of restart and so potentially
                # invalidates all our cached table id to schema mappings. This means we have to load them all
                # again for each logfile which is potentially wasted effort but we can't really do much better
                # without being broken in restart case
                self.table_map = {}
                self.schema_cache.invalidate_table_ids_cache()
            elif binlog_event.log_pos:
                self.log_pos = binlog_event.log_pos

            # This check must not occur before clearing the ``table_map`` as a
            # result of a RotateEvent.
            #
            # The first RotateEvent in a binlog file has a timestamp of
            # zero.  If the server has moved to a new log and not written a
            # timestamped RotateEvent at the end of the previous log, the
            # RotateEvent at the beginning of the new log will be ignored
            # if the caller provided a positive ``skip_to_timestamp``
            # value.  This will result in the ``table_map`` becoming
            # corrupt.
            #
            # https://dev.mysql.com/doc/internals/en/event-data-for-specific-event-types.html
            # From the MySQL Internals Manual:
            #
            #   ROTATE_EVENT is generated locally and written to the binary
            #   log on the master. It is written to the relay log on the
            #   slave when FLUSH LOGS occurs, and when receiving a
            #   ROTATE_EVENT from the master. In the latter case, there
            #   will be two rotate events in total originating on different
            #   servers.
            #
            #   There are conditions under which the terminating
            #   log-rotation event does not occur. For example, the server
            #   might crash.
            if self.skip_to_timestamp and binlog_event.timestamp < self.skip_to_timestamp:
                continue

            if binlog_event.event_type == TABLE_MAP_EVENT and \
                    binlog_event.event is not None:
                table_obj = binlog_event.event.get_table()
                self.table_map[binlog_event.event.table_id] = table_obj

                # store current schema
                self._update_current_schema(table_obj.schema, table_obj.table)

                # store internal table ids for convenience
                self.schema_cache.update_table_ids_cache(table_obj.schema, table_obj.table,
                                                         table_obj.table_id)

            # Process ALTER events and update schema cache so the mapping works properly
            if binlog_event.event_type == QUERY_EVENT and 'ALTER' in binlog_event.event.query.upper():
                table_changes = self._update_cache_and_map(binlog_event.event)
                binlog_event.event.schema_changes = table_changes

            # event is none if we have filter it on packet level
            # we filter also not allowed events
            if binlog_event.event is None or (binlog_event.event.__class__ not in self.__allowed_events):
                continue

            return binlog_event.event

    def _update_current_schema(self, schema, table):
        """
        Keeps current schema (at the point of execution) for later reference
        """
        if not self.schema_cache.is_current_schema_cached(schema, table):
            column_schema = self._get_table_information_from_db(schema, table)
            self.schema_cache.update_current_schema_cache(schema, table, column_schema)

    def _update_cache_and_map(self, binlog_event: QueryEvent):
        """
        Updates schema cache based on given ALTER events. Refreshes the table_map
        """
        table_changes = self.alter_parser.get_table_changes(binlog_event.query, binlog_event.schema.decode())
        monitored_changes = []
        for table_change in table_changes:
            # normalize table_name. We expect table names in lower case here.
            table_change.table_name = table_change.table_name.lower()
            table_change.schema = table_change.schema.lower()
            # only monitored tables
            logging.debug(
                f'Table change detected: {table_change}, monitored tables: {self._BinLogStreamReader__only_tables}, '
                f'monitored schemas: {self._BinLogStreamReader__only_schemas}')
            if table_change.table_name not in self._BinLogStreamReader__only_tables:
                continue
            # only monitored schemas
            if table_change.schema not in self._BinLogStreamReader__only_schemas:
                continue
            monitored_changes.append(table_change)
            self.schema_cache.update_cache(table_change)

            # invalidate table_map cache so it is rebuilt from schema cache next TABLE_MAP_EVENT
            self._invalidate_table_map(table_change.schema, table_change.table_name)
        if not monitored_changes:
            logging.debug(f"ALTER statements detected, but no table change recognised. {binlog_event.query}")
        return monitored_changes

    def _invalidate_table_map(self, schema: str, table_name: str):
        indexes = self.schema_cache.get_table_ids(schema, table_name)
        for i in indexes:
            self.table_map.pop(i, None)

    def _BinLogStreamReader__get_table_information(self, schema, table):
        """ Uglily overridden BinLogStreamReader private method via the "name mangling" black magic.
            Get table information from Cache or fetch the current state.

            This is being used in the TableMapEvent to get the schema if not present in the cache.
            It allows to retrieve schema from the cache if present.

        """
        if self.schema_cache.get_column_schema(schema, table):
            return self._get_column_schema_from_cache(schema, table)
        else:
            # hacky way to call the parent secret method
            current_column_schema = self._get_table_information_from_db(schema, table)
            # update cache with current schema
            self.schema_cache.set_column_schema(schema, table, current_column_schema)

            # TODO: consider moving this to the binlog init so it's in sync and done only once
            self.schema_cache.update_current_schema_cache(schema, table, current_column_schema)
            return current_column_schema

    def _table_info_backoff(func):
        @functools.wraps(func)
        @backoff.on_exception(backoff.expo,
                              pymysql.OperationalError,
                              max_tries=3)
        def wrapper(self, *args, **kwargs):
            if not self._BinLogStreamReader__connected_ctl:
                logging.warning('Mysql unreachable, trying to reconnect')
                self._BinLogStreamReader__connect_to_ctl()
            return func(self, *args, **kwargs)

        return wrapper

    @_table_info_backoff
    def _get_table_information_from_db(self, schema, table):
        for i in range(1, 3):
            try:
                if not self._BinLogStreamReader__connected_ctl:
                    self._BinLogStreamReader__connect_to_ctl()

                cur = self._ctl_connection.cursor()

                query = cur.mogrify("""SELECT
                        COLUMN_NAME, COLLATION_NAME, CHARACTER_SET_NAME,
                        COLUMN_COMMENT, COLUMN_TYPE, COLUMN_KEY, ORDINAL_POSITION, DATA_TYPE, CHARACTER_OCTET_LENGTH,
                        defaults.DEFAULT_COLLATION_NAME,
                        defaults.DEFAULT_CHARSET
                    FROM
                        information_schema.columns col
                    JOIN (SELECT
                              default_character_set_name AS DEFAULT_CHARSET
                            , DEFAULT_COLLATION_NAME     AS DEFAULT_COLLATION_NAME
                        FROM information_schema.SCHEMATA
                        WHERE
                            SCHEMA_NAME = %s) as defaults ON 1=1
                    WHERE
                        table_schema = %s AND table_name = %s
                    ORDER BY ORDINAL_POSITION;
                    """, (schema, schema, table))
                logging.debug(query)
                cur.execute(query)
                return cur.fetchall()
            except pymysql.OperationalError as error:
                code, message = error.args
                if code in MYSQL_EXPECTED_ERROR_CODES:
                    self._BinLogStreamReader__connected_ctl = False
                    raise pymysql.OperationalError("Getting the initial schema failed, server unreachable!") from error
                else:
                    raise error

    def _get_column_schema_from_cache(self, schema: str, table: str):
        column_schema = self.schema_cache.get_column_schema(schema, table)
        # convert to upper case
        for c in column_schema:
            c['COLUMN_NAME'] = c['COLUMN_NAME'].upper()
            # backward compatibility after update to mysql-replication==0.43.0
            if not c.get('DATA_TYPE'):
                c['DATA_TYPE'] = c['COLUMN_TYPE']

        return column_schema


class TableMapEventAlterTracking(BinLogEvent):
    """This evenement describe the structure of a table.
    It's sent before a change happens on a table.
    An end user of the lib should have no usage of this.

    Modified to work with table schema cache, regularly updated from ALTER events.

    Converts column names to upper case.

    """

    def __init__(self, from_packet, event_size, table_map, ctl_connection, **kwargs):
        super(TableMapEventAlterTracking, self).__init__(from_packet, event_size,
                                                         table_map, ctl_connection, **kwargs)

        self.__only_tables = kwargs["only_tables"]
        self.__ignored_tables = kwargs["ignored_tables"]
        self.__only_schemas = kwargs["only_schemas"]
        self.__ignored_schemas = kwargs["ignored_schemas"]
        self.__freeze_schema = kwargs["freeze_schema"]

        # Post-Header
        self.table_id = self._read_table_id()

        if self.table_id in table_map and self.__freeze_schema:
            self._processed = False
            return

        self.flags = struct.unpack('<H', self.packet.read(2))[0]

        # Payload
        self.schema_length = struct.unpack("!B", self.packet.read(1))[0]
        self.schema = self.packet.read(self.schema_length).decode()
        self.packet.advance(1)
        self.table_length = struct.unpack("!B", self.packet.read(1))[0]
        self.table = self.packet.read(self.table_length).decode()

        if self.__only_tables is not None and self.table not in self.__only_tables:
            self._processed = False
            return
        elif self.__ignored_tables is not None and self.table in self.__ignored_tables:
            self._processed = False
            return

        if self.__only_schemas is not None and self.schema not in self.__only_schemas:
            self._processed = False
            return
        elif self.__ignored_schemas is not None and self.schema in self.__ignored_schemas:
            self._processed = False
            return

        self.packet.advance(1)
        self.column_count = self.packet.read_length_coded_binary()

        self.columns = []

        if self.table_id in table_map:
            self.column_schemas = table_map[self.table_id].column_schemas
        else:
            self.column_schemas = self._ctl_connection._get_table_information(self.schema, self.table)

        ordinal_pos_loc = 0

        if len(self.column_schemas) != 0:
            # Read columns meta data
            column_types = list(self.packet.read(self.column_count))
            self.packet.read_length_coded_binary()
            # TODO: fail on different schema size against cache
            for i in range(0, len(column_types)):
                column_type = column_types[i]
                try:
                    column_schema = self.column_schemas[ordinal_pos_loc]

                    # normalize header
                    column_schema['COLUMN_NAME'] = column_schema['COLUMN_NAME'].upper()

                    # only acknowledge the column definition if the iteration matches with ordinal position of
                    # the column. this helps in maintaining support for restricted columnar access
                    if i != (column_schema['ORDINAL_POSITION'] - 1):
                        # raise IndexError to follow the workflow of dropping columns which are not matching the
                        # underlying table schema
                        raise IndexError

                    ordinal_pos_loc += 1
                except IndexError:
                    # this should not happen as the Schema should be reconstructed from the Cache
                    raise SchemaOffsyncError(f"The schema of the table {self.schema}.{self.table} "
                                             f"is off-sync (e.g. column missing). "
                                             f"It is not possible to safely replicate the values. "
                                             f"Please run the full sync-first or restore the state "
                                             f"to include schema matching the last execution time and provide it "
                                             f"within the table_schema_cache parameter. ")

                col = Column(column_type, column_schema, from_packet)
                self.columns.append(col)

        self.table_obj = Table(self.column_schemas, self.table_id, self.schema,
                               self.table, self.columns)

    def get_table(self):
        return self.table_obj

    def _dump(self):
        super(TableMapEventAlterTracking, self)._dump()
        print("Table id: %d" % (self.table_id))
        print("Schema: %s" % (self.schema))
        print("Table: %s" % (self.table))
        print("Columns: %s" % (self.column_count))


class QueryEventWithSchemaChanges(QueryEvent):
    """
    Query event wrapper that contains list of Alter Table Changes.


    """

    class QueryType(Enum):
        QUERY = 'query'
        ALTER_QUERY = 'alter_query'

    def __init__(self, from_packet, event_size, table_map,
                 ctl_connection, **kwargs):
        super().__init__(from_packet, event_size, table_map,
                         ctl_connection, **kwargs)

        self._schema_changes = []
        # default event type
        self.event_type = QueryEventWithSchemaChanges.QueryType.QUERY

    @property
    def schema_changes(self) -> List[TableSchemaChange]:
        return self._schema_changes

    @schema_changes.setter
    def schema_changes(self, schema_changes: List[TableSchemaChange]):
        self.event_type = QueryEventWithSchemaChanges.QueryType.ALTER_QUERY
        self._schema_changes = schema_changes


class BinLogPacketWrapperModified(BinLogPacketWrapper):
    """
    Modified version of BinLogPacketWrapper to include custom TABLE_MAP event class
    Bin Log Packet Wrapper. It uses an existing packet object, and wraps
    around it, exposing useful variables while still providing access
    to the original packet objects variables and methods.
    """

    _BinLogPacketWrapper__event_map = {
        # event
        constants.QUERY_EVENT: QueryEventWithSchemaChanges,
        constants.ROTATE_EVENT: event.RotateEvent,
        constants.FORMAT_DESCRIPTION_EVENT: event.FormatDescriptionEvent,
        constants.XID_EVENT: event.XidEvent,
        constants.INTVAR_EVENT: event.IntvarEvent,
        constants.GTID_LOG_EVENT: event.GtidEvent,
        constants.STOP_EVENT: event.StopEvent,
        constants.BEGIN_LOAD_QUERY_EVENT: event.BeginLoadQueryEvent,
        constants.EXECUTE_LOAD_QUERY_EVENT: event.ExecuteLoadQueryEvent,
        constants.HEARTBEAT_LOG_EVENT: event.HeartbeatLogEvent,
        # row_event
        constants.UPDATE_ROWS_EVENT_V1: row_event.UpdateRowsEvent,
        constants.WRITE_ROWS_EVENT_V1: row_event.WriteRowsEvent,
        constants.DELETE_ROWS_EVENT_V1: row_event.DeleteRowsEvent,
        constants.UPDATE_ROWS_EVENT_V2: row_event.UpdateRowsEvent,
        constants.WRITE_ROWS_EVENT_V2: row_event.WriteRowsEvent,
        constants.DELETE_ROWS_EVENT_V2: row_event.DeleteRowsEvent,

        # MODIFIED TABLE MAP
        constants.TABLE_MAP_EVENT: TableMapEventAlterTracking,
        # 5.6 GTID enabled replication events
        constants.ANONYMOUS_GTID_LOG_EVENT: event.NotImplementedEvent,
        constants.PREVIOUS_GTIDS_LOG_EVENT: event.NotImplementedEvent

    }


================================================
File: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
File: tests/test_binlogstreamreader.py
================================================
import unittest

from mock import patch

from mysql.replication.stream_reader import BinLogStreamReaderAlterTracking


class TestComponent(unittest.TestCase):

    def setUp(self) -> None:
        self.cached_table = 'test1'
        self.cached_schema = 'schema1'
        self.cached_table_index = f"{self.cached_schema}-{self.cached_table}"
        self.mock_stream_reader: BinLogStreamReaderAlterTracking = BinLogStreamReaderAlterTracking({}, '', only_tables=[
            self.cached_table],
                                                                                                   only_schemas=[
                                                                                                       self.cached_schema],
                                                                                                   table_schema_cache={
                                                                                                       f"{self.cached_schema}-{self.cached_table}": [
                                                                                                           {
                                                                                                               'COLUMN_NAME': 'SOMECOL',
                                                                                                               'ORDINAL_POSITION': 1,
                                                                                                               'COLLATION_NAME': None,
                                                                                                               'CHARACTER_SET_NAME': None,
                                                                                                               'COLUMN_COMMENT': None,
                                                                                                               'COLUMN_TYPE': 'int(11)',
                                                                                                               'COLUMN_KEY': 'PRI'
                                                                                                           }]})

    @patch("pymysqlreplication.event.QueryEvent")
    def test_update_cache_different_schema_passes(self, event):
        event.schema = 'someotherschema'.encode('utf-8')
        event.query = 'ALTER TABLE someotherschema.test DROP COLUMN SOMECOL'
        self.mock_stream_reader._update_cache_and_map(event)

    @patch("pymysqlreplication.event.QueryEvent")
    def test_update_cache_different_table_same_schema_passes(self, event):
        event.schema = 'someotherschema'.encode('utf-8')
        event.query = 'ALTER TABLE schema1.test2 DROP COLUMN SOMECOL'
        self.mock_stream_reader._update_cache_and_map(event)

    @patch("pymysqlreplication.event.QueryEvent")
    def test_update_cache_results_in_cache_updated(self, event):
        event.schema = self.cached_schema.encode('utf-8')
        event.query = f'ALTER TABLE {self.cached_schema}.{self.cached_table} DROP COLUMN SOMECOL'
        self.mock_stream_reader._update_cache_and_map(event)
        self.assertEqual(self.mock_stream_reader.schema_cache.table_schema_cache,
                         {f"{self.cached_schema}-{self.cached_table}": []})

    @patch("pymysqlreplication.event.QueryEvent")
    def test_update_cache_add_existing_column_in_schema_column_name_uppercase(self, event):
        event.schema = self.cached_schema.encode('utf-8')
        event.query = f'ALTER TABLE {self.cached_schema}.{self.cached_table} ADD COLUMN newcol TINYINT(1)'
        self.mock_stream_reader.schema_cache.update_current_schema_cache(self.cached_schema, self.cached_table, [{
            'COLUMN_NAME': 'newcol',
            'COLLATION_NAME': None,
            'CHARACTER_SET_NAME': None,
            'COLUMN_COMMENT': '',
            'COLUMN_TYPE': 'int(11)',
            'COLUMN_KEY': '',
            'ORDINAL_POSITION': 50,
            'DEFAULT_COLLATION_NAME': 'latin1_swedish_ci',
            'DEFAULT_CHARSET': 'latin1'
        }])

        expected_schema = [
            {'COLUMN_NAME': 'SOMECOL', 'ORDINAL_POSITION': 1, 'COLLATION_NAME': None, 'CHARACTER_SET_NAME': None,
             'COLUMN_COMMENT': None, 'COLUMN_TYPE': 'int(11)', 'COLUMN_KEY': 'PRI'},
            {'COLUMN_NAME': 'NEWCOL', 'COLLATION_NAME': None, 'CHARACTER_SET_NAME': None, 'COLUMN_COMMENT': '',
             'COLUMN_TYPE': 'int(11)', 'COLUMN_KEY': '', 'ORDINAL_POSITION': 2,
             'DEFAULT_COLLATION_NAME': 'latin1_swedish_ci', 'DEFAULT_CHARSET': 'latin1'}]

        self.mock_stream_reader._update_cache_and_map(event)
        self.assertEqual(self.mock_stream_reader.schema_cache.table_schema_cache[self.cached_table_index],
                         expected_schema)


================================================
File: tests/test_component.py
================================================
"""
Created on 12. 11. 2018

@author: esner
"""
import filecmp
import os
import shutil
import tempfile
import unittest

import mock
from freezegun import freeze_time

try:
    from component import Component
except ImportError:
    from src.component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()

    def test_dedupe_binlog_result(self):
        temp_directory = tempfile.TemporaryDirectory().name
        os.makedirs(temp_directory, exist_ok=True)
        temp_file = os.path.join(temp_directory, 'test.csv')

        source_test = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                                   'resources', 'test.csv')
        expected_path = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                                     'resources', 'deduped.csv')
        shutil.copy(source_test, os.path.join(temp_directory, 'test.csv'))

        Component.deduplicate_binlog_result(temp_file, ['Start_Date', 'End_Date', 'Campaign_Name'], buffer_size=10)

        outcome = filecmp.cmp(temp_file, expected_path, shallow=False)
        self.assertTrue(outcome)


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: tests/test_ddl_parser.py
================================================
"""
Created on 12. 11. 2018

@author: esner
"""
import os
import unittest

import sqlparse

from mysql.replication.ddl_parser import AlterStatementParser, TableSchemaChange, TableChangeType


class TestComponent(unittest.TestCase):

    def normalize_sql(self, sql):
        normalized = sqlparse.parse(sqlparse.format(sql, strip_comments=True, reindent_aligned=True).replace('\n', ' '))
        use_schema, normalized_statement = self.parser._extract_alter_statement_and_schema(normalized)
        return normalized_statement.normalized

    def setUp(self) -> None:
        self.parser = AlterStatementParser()

    def test_multi_add_statement_w_comments(self):
        add_multi = """ /* some commmennts
          aaa */ ALTER   TABLE      TableName
            ADD COLUMN email VARCHAR(100) CHARACTER SET utf8 NOT NULL FIRST,
        ADD COLUMN hourly_rate char NOT NULL AFTER some_col;"""

        normalized = self.normalize_sql(add_multi)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='email',
                                    first_position=True,
                                    data_type='VARCHAR(100)',
                                    charset_name='utf8',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='hourly_rate',
                                    after_column='some_col',
                                    data_type='CHAR',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_multi, 'cdc')

        self.assertEqual([change1, change2], table_changes)

    def test_multi_add_statement_w_comments_lowercase(self):
        add_multi = """ /* some commmennts
          aaa */ ALTER   TaBLe      TableName
            AdD COLuMN email VarChar(100) character set utf8 not null first,
        add column hourly_rate char not null after some_col;"""

        normalized = self.normalize_sql(add_multi)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='email',
                                    first_position=True,
                                    data_type='VARCHAR(100)',
                                    charset_name='utf8',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='hourly_rate',
                                    after_column='some_col',
                                    data_type='CHAR',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_multi, 'cdc')

        self.assertEqual([change1, change2], table_changes)

    def test_multi_add_statement_w_additional_params(self):
        add_multi = """ALTER TABLE employee_settings ADD zenefits_id INT DEFAULT NULL, ADD paylocity_id VARCHAR(255) 
        DEFAULT NULL, ALGORITHM=INPLACE, LOCK=NONE"""

        normalized = self.normalize_sql(add_multi)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='employee_settings',
                                    schema='cdc',
                                    column_name='zenefits_id',
                                    data_type='INT',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='employee_settings',
                                    schema='cdc',
                                    column_name='paylocity_id',
                                    data_type='VARCHAR(255)',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_multi, 'cdc')

        self.assertEqual([change1, change2], table_changes)

    def test_multi_add_statement_w_additional_params2(self):
        add_multi = """ALTER TABLE employee_settings ADD zenefits_id INT DEFAULT NULL, ALGORITHM=INPLACE, LOCK=NONE,
         ADD paylocity_id VARCHAR(255) DEFAULT NULL"""

        normalized = self.normalize_sql(add_multi)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='employee_settings',
                                    schema='cdc',
                                    column_name='zenefits_id',
                                    data_type='INT',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='employee_settings',
                                    schema='cdc',
                                    column_name='paylocity_id',
                                    data_type='VARCHAR(255)',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_multi, 'cdc')

        self.assertEqual([change1, change2], table_changes)

    def test_multi_drop_statement_w_additional_params(self):
        drop_multi = """ALTER   TABLE      TableName
            DROP ColuMN Column1,
            DROP COLUMN Column2,
            DROP column_3, ALGORITHM=INPLACE, LOCK=NONE;"""

        normalized = self.normalize_sql(drop_multi)

        change1 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='Column1',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='Column2', query=normalized)
        change3 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='column_3',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(drop_multi, 'cdc')

        self.assertEqual([change1, change2, change3], table_changes)

    def test_multi_drop_statement_w_additional_params2(self):
        drop_multi = """ALTER   TABLE      TableName
            DROP ColuMN Column1, ALGORITHM=INPLACE, LOCK=NONE,
            DROP COLUMN Column2,
            DROP column_3;"""

        normalized = self.normalize_sql(drop_multi)

        change1 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='Column1',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='Column2',
                                    query=normalized)
        change3 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='column_3',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(drop_multi, 'cdc')

        self.assertEqual([change1, change2, change3], table_changes)

    def test_multi_drop_statement_w_comments(self):
        drop_multi = """ /* some commmennts
          aaa */ ALTER   TABLE      TableName
            DROP ColuMN Column1,
            DROP COLUMN Column2,
            DROP column_3;"""
        normalized = self.normalize_sql(drop_multi)

        change1 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='Column1',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='Column2',
                                    query=normalized)
        change3 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='column_3',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(drop_multi, 'cdc')

        self.assertEqual([change1, change2, change3], table_changes)

    def test_single_add_statement_w_comments_use_schema(self):
        add_single = """ use `cdc`; /* ApplicationName=DataGrip 2021.1.3 */ ALTER TABLE customers_binary
            ADD COLUMN tests_col5 VARCHAR(255)"""
        normalized = self.normalize_sql(add_single)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='customers_binary',
                                    schema='cdc',
                                    column_name='tests_col5',
                                    data_type='VARCHAR(255)',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_single, '')

        self.assertEqual([change1], table_changes)

    def test_single_drop_statement_w_comments_use_schema(self):
        add_single = """ use `cdc`; /* ApplicationName=DataGrip 2021.1.3 */ ALTER TABLE customers_binary
            DROP COLUMN tests_col5"""
        normalized = self.normalize_sql(add_single)

        change1 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='customers_binary',
                                    schema='cdc',
                                    column_name='tests_col5',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_single, '')

        self.assertEqual([change1], table_changes)

    def test_single_drop_with_quotes(self):
        query = "alter table `employee_settings` drop `new_lever_id`"
        normalized = self.normalize_sql(query)

        change1 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='employee_settings',
                                    schema='',
                                    column_name='new_lever_id',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(query, '')

        self.assertEqual([change1], table_changes)

    def test_single_add_with_quotes(self):
        query = "alter table `employee_settings` add `new_lever_id` VARCHAR(255)"
        normalized = self.normalize_sql(query)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='employee_settings',
                                    schema='',
                                    data_type='VARCHAR(255)',
                                    column_name='new_lever_id',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(query, '')

        self.assertEqual([change1], table_changes)

    def test_single_add_with_charset(self):
        add_single = """/* ApplicationName=DataGrip 2021.1.3 */ ALTER TABLE cdc.`customers_binary`
    ADD COLUMN charset_col VARCHAR(255) CHARACTER SET utf8 FIRST"""
        normalized = self.normalize_sql(add_single)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='customers_binary',
                                    schema='cdc',
                                    column_name='charset_col',
                                    data_type='VARCHAR(255)',
                                    charset_name='utf8',
                                    first_position=True,
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_single, 'cdc')

        self.assertEqual([change1], table_changes)

    def test_single_add_table_name_keeps_case(self):
        add_single = """ALTER TABLE package ADD external_insurance_id VARCHAR(255) DEFAULT NULL, ALGORITHM=INPLACE, 
        LOCK=NONE"""
        normalized = self.normalize_sql(add_single)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='package',
                                    schema='cdc',
                                    column_name='external_insurance_id',
                                    data_type='VARCHAR(255)',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_single, 'cdc')

        self.assertEqual([change1], table_changes)

    def test_single_add_with_idenitifier_quotes(self):
        add_single = """/* ApplicationName=DataGrip 2021.1.3 */ ALTER TABLE cdc.`customers_binary`
    ADD COLUMN `charset_col` VARCHAR(255) CHARACTER SET utf8 FIRST"""
        normalized = self.normalize_sql(add_single)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='customers_binary',
                                    schema='cdc',
                                    column_name='charset_col',
                                    data_type='VARCHAR(255)',
                                    charset_name='utf8',
                                    first_position=True,
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_single, 'cdc')

        self.assertEqual([change1], table_changes)

    def test_multi_add_with_idenitifier_bracket(self):
        add_single = """ALTER TABLE receive ADD (receive_pallet_id INT DEFAULT NULL, type VARCHAR(255) DEFAULT NULL COMMENT '(DC2Type:receive_type)'), ALGORITHM=INPLACE, LOCK= NONE"""
        normalized = self.normalize_sql(add_single)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='receive',
                                    schema='cdc',
                                    column_name='receive_pallet_id',
                                    data_type='INT',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='receive',
                                    schema='cdc',
                                    column_name='type',
                                    data_type='VARCHAR(255)',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_single, 'cdc')

        self.assertEqual([change1, change2], table_changes)

    def test_multi_add_statement_w_comments_quotes(self):
        add_multi = """ /* some commmennts
          aaa */ ALTER   TABLE      `cdc`.`TableName`
            ADD COLUMN `email` VARCHAR(100) NOT NULL FIRST,
        ADD COLUMN hourly_rate decimal(10,2) NOT NULL AFTER some_col;"""
        normalized = self.normalize_sql(add_multi)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='email',
                                    first_position=True,
                                    data_type='VARCHAR(100)',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='TableName',
                                    schema='cdc',
                                    column_name='hourly_rate',
                                    after_column='some_col',
                                    data_type='DECIMAL(10,2)',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_multi, '')

        self.assertEqual([change1, change2], table_changes)

    def test_multi_add_and_drop_statements(self):
        add_multi = """ALTER TABLE immex_product_information ADD immex_hs_code_id INT DEFAULT NULL, 
                DROP immex_hs_code, ALGORITHM=INPLACE, LOCK=NONE
                """

        normalized = self.normalize_sql(add_multi)

        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='immex_product_information',
                                    schema='',
                                    column_name='immex_hs_code_id',
                                    data_type='INT',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='immex_product_information',
                                    schema='',
                                    column_name='immex_hs_code',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_multi, '')

        self.assertEqual([change2, change1], table_changes)

    def test_real_drop_statements(self):
        file_path = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                                 'resources', 'drop_statements.sql')

        with open(file_path) as in_sql:
            queries = list(sqlparse.parsestream(in_sql))

        table_changes = []
        for q in queries:
            normalized = self.normalize_sql(q.normalized)
            table_changes.extend(self.parser.get_table_changes(q.normalized, ''))

        # TODO: validate parsed changes

    def test_real_drop_statements_new_lines(self):
        add_multi = """ALTER TABLE location DROP machine_number, DROP tray, DROP x, DROP y, ALGORITHM=INPLACE, LOCK=NONE
                       """

        normalized = self.normalize_sql(add_multi)

        change1 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='location',
                                    schema='',
                                    column_name='machine_number',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='location',
                                    schema='',
                                    column_name='tray',
                                    query=normalized)
        change3 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='location',
                                    schema='',
                                    column_name='x',
                                    query=normalized)
        change4 = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                    table_name='location',
                                    schema='',
                                    column_name='y',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add_multi, '')

        self.assertEqual([change1, change2, change3, change4], table_changes)

    def test_drop_foreign_key(self):
        drop = """ALTER TABLE zpl_label DROP FOREIGN KEY FK_569FF7337BE036FC, ALGORITHM=INPLACE, LOCK= NONE"""

        table_changes = self.parser.get_table_changes(drop, '')

        self.assertEqual([], table_changes)

    def test_add_foreign_key(self):
        drop = """ALTER TABLE shipment ADD CONSTRAINT FK_2CB20DC7B216700 FOREIGN KEY (zpl_label_id) 
        REFERENCES zpl_label (id),        ALGORITHM=INPLACE,        LOCK=   NONE"""

        table_changes = self.parser.get_table_changes(drop, '')

        self.assertEqual([], table_changes)

    def test_add_unique_constraint(self):
        drop = """ALTER TABLE shipping_method_warehouse_config 
        ADD UNIQUE UNIQ_SHIPPING_METHOD__WAREHOUSE (shipping_method_id, warehouse_id),        
        ALGORITHM=INPLACE,        LOCK=   NONE"""

        table_changes = self.parser.get_table_changes(drop, '')

        self.assertEqual([], table_changes)

    def test_drop_index(self):
        drop = """ALTER TABLE `ttt`.`delivery_menus` DROP INDEX `dmb_id`"""

        table_changes = self.parser.get_table_changes(drop, '')

        self.assertEqual([], table_changes)

    def test_drop_key(self):
        drop = """ALTER TABLE `ttt`.`delivery_menus` DROP KEY `dmb_id`"""
        table_changes = self.parser.get_table_changes(drop, '')

        self.assertEqual([], table_changes)

    def test_drop_column_key(self):
        drop = """ALTER TABLE `ttt`.`delivery_menus` DROP `KEY`"""

        change = TableSchemaChange(TableChangeType.DROP_COLUMN,
                                   table_name='delivery_menus',
                                   schema='ttt',
                                   column_name='KEY',
                                   query=drop)
        table_changes = self.parser.get_table_changes(drop, '')

        self.assertEqual([change], table_changes)

    def test_add_w_comment(self):
        add = """ALTER TABLE shipping_cogs_rate_response ADD zone VARCHAR(255) DEFAULT NULL,ADD billable_weight DECIMAL(19, 4) 
        DEFAULT NULL COMMENT '(DC2Type:weight_value4)',ADD billable_weight_unit VARCHAR(255) DEFAULT NULL,
        ADD fuel_surcharge_percentage DECIMAL(19, 4) DEFAULT NULL COMMENT '(DC2Type:percentage4)', ALGORITHM=INSTANT"""
        normalized = self.normalize_sql(add)
        change1 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='shipping_cogs_rate_response',
                                    schema='',
                                    data_type='VARCHAR(255)',
                                    column_name='zone',
                                    query=normalized)
        change2 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='shipping_cogs_rate_response',
                                    schema='',
                                    data_type='DECIMAL(19, 4)',
                                    column_name='billable_weight',
                                    query=normalized)
        change3 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='shipping_cogs_rate_response',
                                    schema='',
                                    data_type='VARCHAR(255)',
                                    column_name='billable_weight_unit',
                                    query=normalized)
        change4 = TableSchemaChange(TableChangeType.ADD_COLUMN,
                                    table_name='shipping_cogs_rate_response',
                                    schema='',
                                    data_type='DECIMAL(19, 4)',
                                    column_name='fuel_surcharge_percentage',
                                    query=normalized)
        table_changes = self.parser.get_table_changes(add, '')

        self.assertEqual([change1, change2, change3, change4], table_changes)


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()


================================================
File: tests/resources/deduped.csv
================================================
Type,Campaign_Name,Status,Start_Date,End_Date,Location,Eventbrite_link
Event,Are You Using Data to Understand Your Customers? ,Complete,2018-02-27,2018-02-27,United Kingdom2,https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611
Event,Conversion Rate Optimisation in Travel Industry,Complete,2018-01-30,2018-01-30,United Kingdom,https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719
Event,Becoming data driven in the high street fashion,Complete,2016-10-12,2016-10-12,United Kingdom,https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213
Event,Data Festival London 2016,Complete,2016-06-24,2016-06-26,United Kingdom,https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771
Event,Data Tools for Startups,Complete,2016-03-17,2016-03-17,United Kingdom,https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535
Event,How to become data driven startup,Complete,2015-11-04,2015-11-04,United Kingdom2,https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380
Event,How to become data driven startup,Complete,2015-10-13,2015-10-13,United Kingdom2,https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377
Event,"How to {""asHelloWorld"":""ssbecome
data driven startup""}",Complete,2015-10-13,2015-10-13,United Kingdom1,https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377


================================================
File: tests/resources/drop_statements.sql
================================================

alter table f_transaction drop billing_type, algorithm=inplace, lock=none
;
alter table payment_method drop plaid_id, drop plaid_institution_type, algorithm=inplace, lock=none
;
alter table `employee_settings` drop `new_lever_id`;

alter table employee_settings drop new_lever_id, algorithm=inplace, lock=none
;
alter table store alter column shopify_do_not_change_fulfillment_service drop default, lock=none
;
alter table employee_settings drop lever_id, algorithm=inplace, lock=none
;
alter table return_entity drop returnly_id, drop returnly_status, algorithm=inplace, lock=none
;
alter table product drop lot_control_enabled, algorithm=inplace, lock=none
;
alter table store drop fulfillment_countries, algorithm=inplace, lock=none
;
 alter table warehouse_config              drop pitney_bowes_carrier_facility_id,              drop pitney_bowes_client_facility_id,              drop pitney_bowes_client_id,              drop pitney_bowes_shipper_id,              algorithm=inplace,              lock=none
;

alter table order_fee_breakdown drop index uniq_active__order__category, algorithm=inplace, lock=none
;
alter table product drop barcodes, algorithm=inplace, lock=none
;
alter table sales_prediction_adjusted drop is_main_model, algorithm=inplace, lock=none
;
ALTER TABLE user DROP zenefits_id, DROP paylocity_id, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE tracker DROP date_time_zone_fixed, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE claim DROP createdAt, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE warehouse_config DROP problem_solving_unpick_enabled, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE warehouse_config DROP inventory_adjustment_research_enabled, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE pick DROP picking_session_id, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE pick DROP picking_session_id, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE item DROP product_count, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE mobile_app_log DROP updated_at, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE occurrence DROP checked, ALGORITHM=INPLACE, LOCK=NONE
;
ALTER TABLE picking_tote_assignment DROP problem_code, ALGORITHM=INPLACE, LOCK=NONE;

================================================
File: tests/resources/mock_show_binlog.py
================================================
"""
Mock server to test the SHOW BINLOG method from endpoint
"""
from fastapi import FastAPI

app = FastAPI()

result_data = {
    "logs": [
        {
            "log_name": "binlog.022361",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022362",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022363",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022364",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022365",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022366",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022367",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022368",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022369",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022370",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022371",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022372",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022373",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022374",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022375",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022376",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022377",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022378",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022379",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022380",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022381",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022382",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022383",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022384",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022385",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022386",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022387",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022388",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022389",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022390",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022391",
            "file_size": "241"
        },
        {
            "log_name": "binlog.022392",
            "file_size": "241"
        },
        {
            "log_name": "binlog.000282",
            "file_size": "0"
        }
    ]
}


@app.get("/show_master")
def show_master():
    return result_data

# run using uvicorn main:app --reload


================================================
File: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on:
  push:
    branches:
      - 'feature/*'
      - 'bug/*'
    tags:
      - '*' # Skip the workflow on the main branch without tags

concurrency: ci-${{ github.ref }} # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: "kds-team.ex-mysql-next" # replace with your component id
  KBC_DEVELOPERPORTAL_VENDOR: "kds-team" # replace with your vendor
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: "kds-team+github"

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }} # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (Optional) Test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: "" # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }} # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo ${raw//origin\//} | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh

