Directory structure:
└── keboola-component-ms-sharepoint-lists/
    ├── README.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   ├── sourceCodeUrl.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run.bat
    │   ├── run_kbc_tests.ps1
    │   ├── update_dev_portal_properties.sh
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── result.py
    │   └── ms_graph/
    │       ├── __init__.py
    │       ├── client.py
    │       ├── dataobjects.py
    │       └── exceptions.py
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── .github/
        └── workflows/
            └── push.yml

================================================
FILE: README.md
================================================
# Microsoft SharePoint Lists writer

Write and create SharePoint lists directly from Keboola Connection.

**Table of contents:**  
  
[TOC]

# Functionality notes

This component allows you to create a new SharePoint list directly from Keboola Connection or rewrite content of an existing one.

**Important Note**: The current version supports only `full load`, meaning that the contents of the existing list will always 
be pruned before upload.

## Creating new list

New list may be created directly from configuration. If the list already exists, the configuration section `Create new list`
 is ignored. Once created, changes made in that configuration section will ignored.
 
**NOTE**: When creating a new list the `genericList` template is used and the resulting list always contains a required column named `Title`

### Column definition
 
- Currently only `text`, `number`, `date`, `dateTime` column types are available when creating a new list.
- When no column parameters are specified, all columns will be created as `text` fields and the resulting column display names 
will match the input table.
- Title column name mapping is always required.
- Once created, any changes made to this section will be ignored. It is not possible to update column definition of an exisitng list.

## Writing to an existing list

When the `Create new list` section is empty, a list with the specified name is expected to exist, otherwise the job fails.

**NOTE**: During each execution all existing list items are removed from the destination list prior upload.

# Configuration
 
## Host base name

Your MS SharePoint host, typically something like `{my-tenant}.sharepoint.com`

## List name

Name of the SharePoint list you want to create or update, exactly as it is displayed in the UI (case sensitive). 

![List example](docs/imgs/list.png)

## List description

Optional list description.



## Create new list

### Title column

hen creating a new list the `genericList` template is used and the resulting list always contains a required column named `Title`.
For this reason it is required to define the name of the column in the input table that will be treated as the `Title` column. 
It may be for example you index column.

### List column parameters

Optional parameters of the list columns. If left empty, all fields will be treated as text. 
If specified list already exist, this section is ignored.

- **Source column name** - (REQ) Name of the column in the input table
- **Display name** - (REQ) Display name that will be visible in the SharePoint UI
- **Column data type**

#### Supported data types

- `text`
- `number`
- `dateTime`, `date` - ISO 8601 format is expected, however the default `YYYY-MM-DD` format should work as well. 
The value is then displayed in SharePoint with default formatting. 


# Development
 
This example contains runnable container with simple unittest. For local testing it is useful to include `data` folder in the root
and use docker-compose commands to run the container or execute tests. 

If required, change local data folder (the `CUSTOM_FOLDER` placeholder) path to your custom path:
```yaml
    volumes:
      - ./:/code
      - ./CUSTOM_FOLDER:/data
```

Clone this repository, init the workspace and run the component with following command:

```
git clone https://bitbucket.org:kds_consulting_team/kds-team.ex-ms-sharepoint.git my-new-component
cd my-new-component
docker-compose build
docker-compose run --rm dev
```

Run the test suite and lint check using this command:

```
docker-compose run --rm test
```

# Integration

For information about deployment and integration with KBC, please refer to the [deployment section of developers documentation](https://developers.keboola.com/extend/component/deployment/) 


================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.7.2-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
The MIT License (MIT)

Copyright (c) 2018 Keboola DS, http://keboola.com

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.2.7.zip#egg=kbc
mock
freezegun


================================================
FILE: component_config/component_long_description.md
================================================
Create and upload `Lists` and `List Items` to MS SharePoint. If you're looking for uploading documents and files to the 
Sharepoint platform, use the [One Drive writer](https://components.keboola.com/components/jakub-bartel.wr-onedrive).


================================================
FILE: component_config/component_short_description.md
================================================
SharePoint empowers teamwork with dynamic and productive team sites. A list in SharePoint is a collection of data that gives you and your co-workers a flexible way to organize information.


================================================
FILE: component_config/configSchema.json
================================================
{
  "type": "object",
  "title": "extractor configuration",
  "required": [
    "base_host_name",
    "site_url_rel_path",
    "list_name",
    "create_new"
  ],
  "properties": {
    "base_host_name": {
      "type": "string",
      "title": "Sharepoint host base name",
      "description": "e.g. my-tenant.sharepoint.com",
      "propertyOrder": 100
    },
    "site_url_rel_path": {
      "type": "string",
      "default": "/sites/root",
      "title": "Site relative URL path",
      "description": "Relative path of the Sharepoint site on your Sharepoint server. E.g. /sites/MyTeamSite. If left empty, a root site is used.",
      "propertyOrder": 1000
    },
    "list_name": {
      "type": "string",
      "title": "List name",
      "description": "Name of the new or existing Sharepoint List. To overwrite existing list the name must be specified exactly as displayed in the UI.",
      "propertyOrder": 2000
    },
    "create_new": {
      "type": "array",
      "title": "Create new list",
      "description": "If you wish to create a new list add its properties. If left empty, existing list will be overwritten.",
      "propertyOrder": 4000,
      "maxItems": 1,
      "items": {
        "type": "object",
        "title": "New list parameters",
        "required": [
          "list_description",
          "title_column",
          "column_setup"
        ],
        "properties": {
          "list_description": {
            "type": "string",
            "title": "List description",
            "description": "Result list description",
            "maxItems": 1,
            "propertyOrder": 1
          },
          "title_column": {
            "type": "object",
            "title": "Title column",
            "description": "List will be generated with the genericTemplate that requires one Title column. Specify, which column you want to use as Title. It may be for example you index column.",
            "format": "grid",
            "required": [
              "name"
            ],
            "properties": {
              "name": {
                "type": "string",
                "title": "Source column name",
                "description": "Source table column name",
                "propertyOrder": 10
              }
            }
          },
          "column_setup": {
            "type": "array",
            "title": "List column parameters",
            "description": "Optional parameters of the list columns. If left empty, all fields will be treated as text. If specified list already exist, this section is ignored.",
            "propertyOrder": 4000,
            "items": {
              "type": "object",
              "title": "Column",
              "format": "grid",
              "required": [
                "name",
                "display_name",
                "col_type",
                "description",
                "required"
              ],
              "properties": {
                "name": {
                  "type": "string",
                  "title": "Source column name",
                  "description": "Source table column name",
                  "options": {
                    "grid_columns": 3
                  },
                  "propertyOrder": 10
                },
                "display_name": {
                  "type": "string",
                  "title": "Display name",
                  "options": {
                    "grid_columns": 3
                  },
                  "description": "Display name in Sharepoint",
                  "propertyOrder": 20
                },
                "col_type": {
                  "type": "number",
                  "enum": [
                    "text",
                    "number",
                    "dateTime",
                    "date"
                  ],
                  "options": {
                    "grid_columns": 3
                  },
                  "default": "text",
                  "title": "Column data type",
                  "propertyOrder": 30
                },
                "description": {
                  "type": "string",
                  "format": "textarea",
                  "title": "description",
                  "options": {
                    "input_height": "50px"
                  },
                  "propertyOrder": 4000
                },
                "required": {
                  "type": "boolean",
                  "title": "Required",
                  "format": "checkbox",
                  "propertyOrder": 6000
                }
              }
            }
          }
        }
      }
    }
  }
}






================================================
FILE: component_config/configuration_description.md
================================================
### Warning: 
The writer currently supports only **`full load`**. 
All existing items in the destination list will be removed prior the load. 

### Creating new list

New list may be created directly from configuration. If the list already exists, the configuration section `Create new list`
 is ignored. Once created, changes made in that configuration section will ignored.
 
**NOTE**: When creating a new list the `genericList` template is used and the resulting list always contains a required column named `Title`

### Column definition
 
- Currently only `text`, `number`, `date`, `dateTime` column types are available when creating a new list.
- When no column parameters are specified, all columns will be created as `text` fields and the resulting column display names 
will match the input table.
- Title column name mapping is always required.
- Once created, any changes made to this section will be ignored. It is not possible to update column definition of an exisitng list.

### Writing to an existing list

When the `Create new list` section is empty, a list with the specified name is expected to exist, otherwise the job fails.

**NOTE**: During each execution all existing list items are removed from the destination list prior upload.


Additional documentation is available [here](https://bitbucket.org/kds_consulting_team/kds-team.wr-ms-sharepoint-lists/src/master/README.md).


================================================
FILE: component_config/documentationUrl.md
================================================
[Empty file]


================================================
FILE: component_config/licenseUrl.md
================================================
[Empty file]


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sourceCodeUrl.md
================================================
[Empty file]


================================================
FILE: component_config/stack_parameters.json
================================================
{}


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}



================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
FILE: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi


================================================
FILE: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"


================================================
FILE: src/component.py
================================================
'''
Template Component main class.

'''

import csv
import json
import logging
import sys

from kbc.env_handler import KBCEnvHandler

from ms_graph.client import Client
from ms_graph.dataobjects import get_col_def_name, get_col_definition, TextColumn, SharepointList, ColumnDefinition
from ms_graph.exceptions import BaseError

# global constants'
KEY_LIST_DESC = 'list_description'
BATCH_LIMIT = 20
KEY_COLUMN_SETUP = 'column_setup'
OAUTH_APP_SCOPE = 'offline_access Files.Read Sites.ReadWrite.All'
# configuration variables
KEY_BASE_HOST = 'base_host_name'
KEY_SITE_REL_PATH = 'site_url_rel_path'
KEY_LIST_NAME = 'list_name'
KEY_CREATE_NEW = 'create_new'
KEY_TITLE_COL = 'title_column'
KEY_SRC_NAME = 'name'

# #### Keep for debug
KEY_DEBUG = 'debug'
MANDATORY_PARS = [KEY_BASE_HOST, KEY_LIST_NAME, KEY_SITE_REL_PATH]
MANDATORY_IMAGE_PARS = []


class Component(KBCEnvHandler):

    def __init__(self, debug=False):
        KBCEnvHandler.__init__(self, MANDATORY_PARS, log_level=logging.DEBUG if debug else logging.INFO)
        # override debug from config
        if self.cfg_params.get(KEY_DEBUG):
            debug = True
        if debug:
            logging.getLogger().setLevel(logging.DEBUG)
        logging.info('Loading configuration...')

        try:
            self.validate_config(MANDATORY_PARS)

        except ValueError as ex:
            logging.exception(ex)
            exit(1)

        authorization_data = json.loads(self.get_authorization().get('#data'))
        token = authorization_data.get('refresh_token')
        if not token:
            raise Exception('Missing access token in authorization data!')

        self.client = Client(refresh_token=token, client_id=self.get_authorization()['appKey'],
                             client_secret=self.get_authorization()['#appSecret'], scope=OAUTH_APP_SCOPE)

    def run(self):
        '''
        Main execution code
        '''
        params = self.cfg_params  # noqa

        try:

            in_tables = self.configuration.get_input_tables()

            if len(in_tables) == 0:
                logging.error('There is no table specified on the input mapping! You must provide one input table!')
                exit(1)

            in_table = in_tables[0]

            site = self.client.get_site_by_relative_url(params[KEY_BASE_HOST], params[KEY_SITE_REL_PATH])
            if not site.get('id'):
                raise RuntimeError(
                    f'No site with given url: '
                    f'{"/".join([params[KEY_BASE_HOST], params[KEY_SITE_REL_PATH]])} found.')

            # get existing list
            sh_list = self.client.get_site_list_by_name(site['id'], params[KEY_LIST_NAME])

            table_pars = params.get(KEY_CREATE_NEW, {})
            title_col_mapping = table_pars[0][KEY_TITLE_COL] if table_pars else None

            if table_pars and not sh_list:
                # create new list
                table_pars = table_pars[0]
                list_dsc = table_pars.get(KEY_LIST_DESC, '')
                title_col_mapping = table_pars[KEY_TITLE_COL]
                sh_list = self._create_new_list(site['id'], params[KEY_LIST_NAME], list_dsc, table_pars,
                                                in_table)
            else:
                if not sh_list:
                    raise RuntimeError(
                        f'No list named "{params[KEY_LIST_NAME]}" found on site : '
                        f'{"/".join([params[KEY_BASE_HOST], params[KEY_SITE_REL_PATH]])} .')
                elif params.get(KEY_CREATE_NEW, {}):
                    logging.warning(f'The list "{params[KEY_LIST_NAME]}" already exists. The "new list" '
                                    f'configuration will be ignored and the existing list updated.')

            logging.info('Getting list details...')
            list_columns = self.client.get_site_list_columns(site['id'], sh_list['id'],
                                                             expand_par='columns')

            non_existent_cols = self.validate_table_cols(list_columns, in_table, title_col_mapping)
            if non_existent_cols:
                logging.warning(
                    f'Some columns: {non_existent_cols} were not found in the destination list. They will be ignored!')

            # emtpy the list first
            logging.warning('Removing all existing items..')
            self._empty_list(site['id'], sh_list)

            logging.info('Writing table items.')
            self.write_table(site['id'], sh_list['id'], in_table, non_existent_cols,
                             title_col_mapping)

            logging.info('Export finished!')

        except BaseError as ex:
            logging.exception(ex)
            exit(1)

    def _empty_list(self, site_id, sh_lst):
        for fl in self.client.get_site_list_fields(site_id, sh_lst['id'], expand='fields'):
            f = self.client.delete_list_items(site_id, sh_lst['id'], [f['id'] for f in fl])
            if f:
                raise RuntimeError(f"Some records couldn't be deleted: {f}.")

    def write_table(self, site_id, list_id, in_table, nonexistent_cols, title_col):
        with open(in_table['full_path'], mode='r',
                  encoding='utf-8') as in_file:
            reader = csv.DictReader(in_file, lineterminator='\n')

            batch = []
            failed = []
            batch_index = 0
            for ri, line in enumerate(reader):
                if title_col:
                    # creating new list, have col mapping
                    line['Title'] = line.pop(title_col[KEY_SRC_NAME])
                    if title_col[KEY_SRC_NAME] in nonexistent_cols:
                        nonexistent_cols.remove(title_col[KEY_SRC_NAME])

                self._cleanup_record_fields(line, nonexistent_cols)
                br = self.client.build_create_list_item_batch_request(batch_index, site_id, list_id, line)
                batch.append(br)
                batch_index += 1
                if batch_index >= BATCH_LIMIT:
                    batch_index = 0
                    f = self.client.make_batch_request(batch, 'Create items')
                    f = self._retry_failed_write(site_id, list_id, batch, f)
                    failed.extend(f)
                    batch.clear()
            # last batch
            if batch:
                f = self.client.make_batch_request(batch, 'Create items')
                failed.extend(f)

        if failed:
            raise RuntimeError(f'Write finished with error. Some records failed: {failed}')

    def _retry_failed_write(self, site_id, list_id, batch, failed):
        if failed:
            logging.info(f'Some ({len(failed)}) requests failed, retrying.')
        failed_idx = []
        for fid, f in enumerate(failed):
            self.client.create_list_item(site_id, list_id, batch[int(f['id'])]['body']['fields'])
            failed_idx.append(fid)
        return [f for i, f in enumerate(failed) if i not in failed_idx]

    def validate_table_cols(self, list_columns, in_table, title_col_mapping=None):
        src_cols = list()
        with open(in_table['full_path'], mode='r',
                  encoding='utf-8') as in_file:
            reader = csv.DictReader(in_file, lineterminator='\n')
            src_cols = reader.fieldnames

        dst_cols = [c['name'] for c in list_columns]
        required_dst_cols = [c['name'] for c in list_columns if c['required']]
        nonexisting_cols = [src_col for src_col in src_cols if src_col not in dst_cols]
        if title_col_mapping:
            # just in case the col does not exist
            src_name = title_col_mapping[KEY_SRC_NAME]
            required_dst_cols.append(src_name)
            src_cols.append('Title')
        missing_required = [req_col for req_col in required_dst_cols if req_col not in src_cols]

        if missing_required:
            raise ValueError(f'Some required columns are missing in the source table: {missing_required}')

        return nonexisting_cols

    def _cleanup_record_fields(self, line, nonexistent_cols):
        for c in nonexistent_cols:
            line.pop(c)

    def _create_new_list(self, site_id, list_name, list_desc, table_pars, in_table):
        title_col = table_pars[KEY_TITLE_COL]
        column_pars = table_pars[KEY_COLUMN_SETUP]

        default_cols = self.validate_table_cols(column_pars, in_table, title_col)
        # validate title col
        if title_col[KEY_SRC_NAME] not in default_cols:
            raise ValueError(f'Specified title column "{title_col[KEY_SRC_NAME]}" is missing in the source table.')
        default_cols.remove(title_col[KEY_SRC_NAME])

        lst_def = self._build_table_def(list_name, list_desc, table_pars, default_cols)

        # create list
        res = self.client.create_list(site_id, lst_def)
        logging.debug(f'List created: {res}')
        return res

    def _build_table_def(self, list_name, list_desc, table_pars, default_cols):
        col_def = list()
        for cpar in table_pars[KEY_COLUMN_SETUP]:
            # only text cols for now
            params = {"name": cpar[KEY_SRC_NAME],
                      "displayName": cpar['display_name'],
                      "description": cpar.get('description', ''),
                      get_col_def_name(cpar['col_type']): get_col_definition(cpar['col_type'])}

            cdef = ColumnDefinition(**params)
            col_def.append(cdef)
        # build default text cols
        for c in default_cols:
            cdef = ColumnDefinition(name=c,
                                    displayName=c,
                                    description='',
                                    text=TextColumn())
            col_def.append(cdef)

        return SharepointList(list_name, col_def)


"""
        Main entrypoint
"""
if __name__ == "__main__":
    if len(sys.argv) > 1:
        debug_arg = sys.argv[1]
    else:
        debug_arg = False
    try:
        comp = Component(debug_arg)
        comp.run()
    except Exception as e:
        logging.exception(e)
        exit(1)



================================================
FILE: src/result.py
================================================
from kbc.result import ResultWriter, KBCTableDef

LIST_ID = 'list_id'
SITE_ID = 'site_id'
RES_TABLE_NAME = 'res_table_name'


class ListResultWriter(ResultWriter):
    COLS = ["createdDateTime",
            "description",
            "eTag",
            "id",
            "lastModifiedDateTime",
            "name",
            "webUrl",
            "displayName",
            "createdBy_user",
            "createdBy_email",
            "lastModifiedBy_user",
            "lastModifiedBy_email"
            ]
    TABLE_DEF = KBCTableDef(name='lists_metadata', pk=['id', 'webUrl'], columns=COLS)

    def __init__(self, result_dir_path):
        ResultWriter.__init__(self, result_dir_path, self.TABLE_DEF, fix_headers=True,
                              user_value_cols=[SITE_ID, RES_TABLE_NAME])

    def write(self, data, file_name=None, user_values=None, object_from_arrays=False, write_header=True):
        # flatten obj
        data['createdBy_user'] = data.get('createdBy', {}).get('user', {}).get('displayName')
        data['createdBy_email'] = data.get('createdBy', {}).get('user', {}).get('email')
        data['lastModifiedBy_user'] = data.get('lastModifiedBy', {}).get('user', {}).get('displayName')
        data['lastModifiedBy_email'] = data.get('lastModifiedBy', {}).get('user', {}).get('email')
        super().write(data, file_name, user_values, object_from_arrays, write_header)


class ListDataResultWriter(ResultWriter):

    def __init__(self, result_dir_path, column_mapping, result_name):
        ResultWriter.__init__(self, result_dir_path,
                              KBCTableDef(name=result_name + '_data', pk=['id', 'list_id'], columns=[]),
                              fix_headers=True, flatten_objects=False)
        self.column_mapping = column_mapping
        # override column names with display name
        self.table_def.columns = [c['displayName'] for c in column_mapping]
        # custom user added col
        self.user_value_cols = [LIST_ID]
        self.table_def.columns.append(LIST_ID)

    def write(self, data, file_name=None, user_values=None, object_from_arrays=False, write_header=True):
        # flatten obj
        data = self._change_col_names(data)
        super().write(data, user_values=user_values)

    def _change_col_names(self, data):
        """
        replace name with display names inplace
        :param data:
        :return:
        """

        for key in self.column_mapping:
            if key['name'] == 'ID':
                # because MS bullshit
                key['name'] = 'id'

            if data.get(key['name']):
                data[key['displayName']] = data.pop(key['name'])

        return data



================================================
FILE: src/ms_graph/__init__.py
================================================
[Empty file]


================================================
FILE: src/ms_graph/client.py
================================================
import logging
from dataclasses import asdict
from dataclasses import dataclass
from typing import List

import requests
from kbc.client_base import HttpClientBase
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from ms_graph import exceptions
from ms_graph.dataobjects import SharepointList


@dataclass
class BatchRequest:
    id: str
    url: str
    method: str
    body: dict = None
    headers: dict = None


class Client(HttpClientBase):
    OAUTH_LOGIN_URL = 'https://login.microsoftonline.com/common/oauth2/v2.0/token'
    MAX_RETRIES = 9
    BASE_URL = 'https://graph.microsoft.com/v1.0/'
    SYSTEM_LIST_COLUMNS = ["ComplianceAssetId",
                           "ContentType",
                           # "Modified",
                           # "Created",
                           # "Author",
                           # "Editor",
                           "Attachments",
                           "Edit",
                           "LinkTitleNoMenu",
                           "LinkTitle",
                           "DocIcon",
                           "ItemChildCount",
                           "FolderChildCount",
                           "AppAuthor",
                           "AppEditor"]

    def __init__(self, refresh_token, client_secret, client_id, scope):
        HttpClientBase.__init__(self, base_url=self.BASE_URL, max_retries=self.MAX_RETRIES, backoff_factor=0.3,
                                status_forcelist=(429, 503, 500, 502, 504, 507))
        # refresh always on init
        self.__refresh_token = refresh_token
        self.__clien_secret = client_secret
        self.__client_id = client_id
        self.__scope = scope
        access_token = self.refresh_token()

        # set auth header
        self._auth_header = {"Authorization": 'Bearer ' + access_token,
                             "Content-Type": "application/json"}

    def __response_hook(self, res, *args, **kwargs):
        # refresh token if expired
        if res.status_code == 401:
            token = self.refresh_token()
            # update auth header
            self._auth_header = {"Authorization": 'Bearer ' + token,
                                 "Content-Type": "application/json"}
            # reset header
            res.request.headers['Authorization'] = 'Bearer ' + token
            s = requests.Session()
            # retry request
            return self.requests_retry_session(session=s).send(res.request)

    def refresh_token(self):
        data = {"client_id": self.__client_id,
                "client_secret": self.__clien_secret,
                "refresh_token": self.__refresh_token,
                "grant_type": "refresh_token",
                "scope": self.__scope}
        r = requests.post(url=self.OAUTH_LOGIN_URL, data=data)
        parsed = self._parse_response(r, 'login')
        return parsed['access_token']

    def requests_retry_session(self, session=None):
        session = session or requests.Session()
        retry = Retry(
            total=self.max_retries,
            read=self.max_retries,
            connect=self.max_retries,
            backoff_factor=self.backoff_factor,
            status_forcelist=self.status_forcelist,
            method_whitelist=('GET', 'POST', 'PATCH', 'UPDATE', 'DELETE')
        )
        adapter = HTTPAdapter(max_retries=retry)
        session.mount('http://', adapter)
        session.mount('https://', adapter)
        # append response hook
        session.hooks['response'].append(self.__response_hook)
        return session

    def create_list(self, site_id, lst_object: SharepointList):
        endpoint = f'/sites/{site_id}/lists'
        url = self.base_url + endpoint
        data = asdict(lst_object)
        return self._parse_response(self.post_raw(url=url, json=data), 'create list')

    def _get_paged_result_pages(self, endpoint, parameters):

        has_more = True
        next_url = self.base_url + endpoint
        while has_more:

            resp = self.get_raw(next_url, params=parameters)
            req_response = self._parse_response(resp, endpoint)

            if req_response.get('@odata.nextLink'):
                has_more = True
                next_url = req_response['@odata.nextLink']
            else:
                has_more = False

            yield req_response

    def _delete_raw(self, *args, **kwargs):
        s = requests.Session()
        headers = kwargs.pop('headers', {})
        headers.update(self._auth_header)
        s.headers.update(headers)
        s.auth = self._auth

        # set default params
        params = kwargs.pop('params', {})

        if params is None:
            params = {}

        if self._default_params:

            all_pars = {**params, **self._default_params}
            kwargs.update({'params': all_pars})

        else:

            kwargs.update({'params': params})

        r = self.requests_retry_session(session=s).request('DELETE', *args, **kwargs)
        return r

    def make_batch_request(self, batch_requests: List[dict], r_type=''):
        endpoint = '$batch'
        rq_url = self.base_url + endpoint

        data = {"requests": batch_requests}

        resp = self.post_raw(rq_url, json=data)
        r = self._parse_response(resp, f'batch: {r_type}')
        return self._get_failed_batch_resp(r)

    def get_site_by_relative_url(self, hostname, site_path):
        """

        :param hostname: e.g. mytenant.sharepoint.com
        :param site_path: e.g. /site/MyTeamSite
        :return:
        """
        url = self.base_url + f'sites/{hostname}:/{site_path}'
        resp = self._parse_response(self.get_raw(url), 'sites')
        return resp

    def get_site_lists(self, site_id):
        endpoint = f'/sites/{site_id}/lists'
        lists = []
        for ls in self._get_paged_result_pages(endpoint, {}):
            lists.extend(ls['value'])
        return lists

    def get_site_list_by_name(self, site_id, list_name):
        """

        :param site_id: site id
        :param list_name: unique list name (case sensitive)
        :return: list object
        """
        lists = self.get_site_lists(site_id)
        # ms removes -
        list_name = list_name.replace('-', '')
        res_list = [ls for ls in lists if ls['name'] == list_name]

        return res_list[0] if res_list else None

    def get_site_list_columns(self, site_id, list_id, include_system=False,
                              expand_par='columns(select=name, description, displayName)'):
        """
        Gets array of columns available in the specified list.

        :param site_id:
        :param list_id:
        :param include_system:
        :param expand_par:
        :return:
        """
        endpoint = f'/sites/{site_id}/lists/{list_id}'
        parameters = {'expand': expand_par}

        columns = []
        for ls in self._get_paged_result_pages(endpoint, parameters):
            columns.extend(ls['columns'])

        if not include_system:
            columns = [c for c in columns if
                       c['name'] not in self.SYSTEM_LIST_COLUMNS and not c['name'].startswith('_')]

        self._dedupe_header(columns)
        return columns

    def get_site_list_fields(self, site_id, list_id, expand='fields'):
        endpoint = f'/sites/{site_id}/lists/{list_id}/items'
        params = {'expand': expand}
        for r in self._get_paged_result_pages(endpoint, params):
            yield [f['fields'] for f in r['value']]

    def delete_list_item(self, site_id, list_id, item_id):
        endpoint = f'/sites/{site_id}/lists/{list_id}/items/{item_id}'
        url = self.base_url + endpoint
        r = self._delete_raw(url=url)
        self._parse_response(r, endpoint)

    def delete_list_items(self, site_id, list_id, item_ids, batch_limit=20):

        batch = []
        failed = []
        batch_index = 0
        for ri, item_id in enumerate(item_ids):
            endpoint = f'/sites/{site_id}/lists/{list_id}/items/{item_id}'
            batch.append(asdict(BatchRequest(str(ri), endpoint, 'DELETE')))
            batch_index += 1
            if batch_index >= batch_limit:
                batch_index = 0
                f = self.make_batch_request(batch, 'Delete items')
                failed.extend(f)
                batch.clear()
            # last batch
        if batch:
            f = self.make_batch_request(batch, 'Delete items')
            failed.extend(f)

        # retry failed one by one. Retry strategy applied
        if failed:
            logging.info(f'Some requests failed ({failed}), retrying. ')

        failed_idx = []

        for fid, f in enumerate(failed):
            try:
                self.delete_list_item(site_id, list_id, item_ids[int(f['id'])])
            except exceptions.NotFound:
                logging.warning(f'Item {item_ids[int(f["id"])]} already deleted.')

            failed_idx.append(fid)

        return [f for i, f in enumerate(failed) if i not in failed_idx]

    def create_list_item(self, site_id, list_id, fields):
        """

        :param site_id:
        :param list_id:
        :param fields: Dictionary with fields. {key: value}
        :return:
        """
        endpoint = f'/sites/{site_id}/lists/{list_id}/items'

        data = {'fields': fields}
        url = self.base_url + endpoint
        rs = self.post_raw(url=url, json=data)
        return self._parse_response(rs, 'create list item')

    def build_create_list_item_batch_request(self, rq_id, site_id, list_id, fields):
        """

        :param site_id:
        :param list_id:
        :param fields: Dictionary with fields. {key: value}
        :return:
        """
        endpoint = f'/sites/{site_id}/lists/{list_id}/items'

        data = {'fields': fields}
        headers = {'Content-Type': 'application/json'}

        return asdict(BatchRequest(rq_id, endpoint, 'POST', data, headers))

    def _parse_response(self, response, endpoint):
        status_code = response.status_code
        if 'application/json' in response.headers['Content-Type']:
            r = response.json()
        else:
            r = response.text
        if status_code in (200, 201, 202):
            return r
        elif status_code == 204:
            return None
        elif status_code == 400:
            raise exceptions.BadRequest(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 401:
            raise exceptions.Unauthorized(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 403:
            raise exceptions.Forbidden(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 404:
            raise exceptions.NotFound(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 405:
            raise exceptions.MethodNotAllowed(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 406:
            raise exceptions.NotAcceptable(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 409:
            raise exceptions.Conflict(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 410:
            raise exceptions.Gone(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 411:
            raise exceptions.LengthRequired(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 412:
            raise exceptions.PreconditionFailed(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 413:
            raise exceptions.RequestEntityTooLarge(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 415:
            raise exceptions.UnsupportedMediaType(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 416:
            raise exceptions.RequestedRangeNotSatisfiable(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 422:
            raise exceptions.UnprocessableEntity(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 429:
            raise exceptions.TooManyRequests(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 500:
            raise exceptions.InternalServerError(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 501:
            raise exceptions.NotImplemented(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 503:
            raise exceptions.ServiceUnavailable(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 504:
            raise exceptions.GatewayTimeout(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 507:
            raise exceptions.InsufficientStorage(f'Calling endpoint {endpoint} failed', r)
        elif status_code == 509:
            raise exceptions.BandwidthLimitExceeded(f'Calling endpoint {endpoint} failed', r)
        else:
            raise exceptions.UnknownError(f'Calling endpoint {endpoint} failed', r)

    def _get_failed_batch_resp(self, response):
        failed = []
        for r in response['responses']:
            if r['status'] >= 300:
                failed.append(r)
        return failed

    def _dedupe_header(self, columns):
        col_keys = dict()
        dup_headers = set()
        for col in columns:
            if col['displayName'] in col_keys:
                dup_headers.add(col['displayName'])
                col['displayName'] = col['displayName'] + '_' + col['name']
            else:
                col_keys[col['displayName']] = col
        # update first value names as well
        for c in dup_headers:
            col_keys[c]['displayName'] = col_keys[c]['displayName'] + '_' + col_keys[c]['name']



================================================
FILE: src/ms_graph/dataobjects.py
================================================
from dataclasses import dataclass, field

from typing import List


@dataclass
class TextColumn:
    allowMultipleLines: bool = True
    appendChangesToExistingText: bool = False
    linesForEditing: int = 1
    maxLength: int = 255
    textType: str = "plain"


@dataclass
class DateTimeColumn:
    displayAs: str = 'default'
    format: str = 'dateTime'


@dataclass
class ColumnDefinition:
    name: str
    displayName: str
    description: str = ''
    text: TextColumn = None
    dateTime: DateTimeColumn = None
    hidden: bool = False
    required: bool = False
    readOnly: bool = False


@dataclass
class ListInfo:
    contentTypesEnabled: bool = False
    hidden: bool = False
    template: str = 'genericList'


@dataclass
class SharepointList:
    displayName: str
    columns: List[ColumnDefinition]
    list: ListInfo = field(default_factory=ListInfo)


def get_col_definition(type, **kwargs):
    col_def = None
    if type == 'text':
        col_def = TextColumn(**kwargs)
    elif type == 'dateTime':
        col_def = DateTimeColumn(format='dateTime')
    elif type == 'date':
        col_def = DateTimeColumn(format='dateOnly')
    else:
        raise ValueError(f'Unsupported column type: {type}')
    return col_def


def get_col_def_name(type):
    if type in ['date', 'dateTime']:
        return 'dateTime'
    else:
        return type



================================================
FILE: src/ms_graph/exceptions.py
================================================
class BaseError(Exception):
    """
    Example:
        error_obj = {
            "error": {
                "code": "invalidRequest",
                "message": "Invalid hostname for this tenancy",
                "innerError": {
                    "request-id": "80fc571a-3262-404b-8a67-22f9cad99016",
                    "date": "2020-01-14T19:01:55"
                }
            }
        }
    """

    def __init__(self, msg, error_obj):
        if isinstance(error_obj.get("error", {}), str):
            Exception.__init__(self, msg + f' Error: {error_obj.get("error", {})}')
            self.error_obj = {}
        else:
            Exception.__init__(self, msg + f' Error: {error_obj.get("error", {}).get("message")}'
                                           f', error code: {error_obj.get("error", {}).get("code")}')
        self.error_obj = error_obj


class UnknownError(BaseError):
    pass


class TokenRequired(BaseError):
    pass


class BadRequest(BaseError):
    pass


class Unauthorized(BaseError):
    pass


class Forbidden(BaseError):
    pass


class NotFound(BaseError):
    pass


class MethodNotAllowed(BaseError):
    pass


class NotAcceptable(BaseError):
    pass


class Conflict(BaseError):
    pass


class Gone(BaseError):
    pass


class LengthRequired(BaseError):
    pass


class PreconditionFailed(BaseError):
    pass


class RequestEntityTooLarge(BaseError):
    pass


class UnsupportedMediaType(BaseError):
    pass


class RequestedRangeNotSatisfiable(BaseError):
    pass


class UnprocessableEntity(BaseError):
    pass


class TooManyRequests(BaseError):
    pass


class InternalServerError(BaseError):
    pass


class NotImplemented(BaseError):
    pass


class ServiceUnavailable(BaseError):
    pass


class GatewayTimeout(BaseError):
    pass


class InsufficientStorage(BaseError):
    pass


class BandwidthLimitExceeded(BaseError):
    pass



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
FILE: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()



================================================
FILE: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on:
  push:
    branches:
      - 'feature/*'
      - 'bug/*'
    tags:
      - '*' # Skip the workflow on the main branch without tags

concurrency: ci-${{ github.ref }} # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: "kds-team.wr-ms-sharepoint-lists" # replace with your component id
  KBC_DEVELOPERPORTAL_VENDOR: "kds-team" # replace with your vendor
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: "kds-team+github"

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }} # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (Optional) Test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: "" # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }} # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo ${raw//origin\//} | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh

