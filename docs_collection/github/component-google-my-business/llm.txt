Directory structure:
└── keboola-component-google-my-business/
    ├── README.md
    ├── change_log.md
    ├── deploy.sh
    ├── docker-compose.yml
    ├── Dockerfile
    ├── flake8.cfg
    ├── LICENSE.md
    ├── requirements.txt
    ├── component_config/
    │   ├── component_long_description.md
    │   ├── component_short_description.md
    │   ├── configSchema.json
    │   ├── configuration_description.md
    │   ├── documentationUrl.md
    │   ├── licenseUrl.md
    │   ├── logger
    │   ├── loggerConfiguration.json
    │   ├── sourceCodeUrl.md
    │   ├── stack_parameters.json
    │   └── sample-config/
    │       ├── config.json
    │       └── in/
    │           ├── state.json
    │           ├── files/
    │           │   └── order1.xml
    │           └── tables/
    │               ├── test.csv
    │               └── test.csv.manifest
    ├── scripts/
    │   ├── build_n_run.ps1
    │   ├── build_n_test.sh
    │   ├── run.bat
    │   ├── run_kbc_tests.ps1
    │   ├── update_dev_portal_properties.sh
    │   └── developer_portal/
    │       ├── fn_actions_md_update.sh
    │       └── update_properties.sh
    ├── src/
    │   ├── component.py
    │   ├── definitions.py
    │   └── google_my_business.py
    ├── tests/
    │   ├── __init__.py
    │   └── test_component.py
    └── .github/
        └── workflows/
            └── push.yml

================================================
FILE: README.md
================================================
# Google My Business Extractor

### API Document

[Google My Business API](https://developers.google.com/my-business/reference/rest)

### Account Authorization

If you are using your personal Google Account to manage locations for your business or company, please ensure your account has the access right to the locations you are interested in. For a list of available locations for you Google Account, please visit [business.google.com/locations](https://business.google.com/locations).

### Configuration

1. Endpoints

    1. Daily Metrics
    2. Reviews
    3. Media
    4. Questions

2. Request Range
    - This configuration will only affect [Daily Metrics] endpoint. 


================================================
FILE: change_log.md
================================================
**0.1.1**

- fix requirements
- add src folder to path for tests

**0.1.0**

- src folder structure
- remove dependency on handler lib - import the code directly to enable modifications until its released

**0.0.2**

- add dependency to base lib
- basic tests

**0.0.1**

- add utils scripts
- move kbc tests directly to pipelines file
- use uptodate base docker image
- add changelog



================================================
FILE: deploy.sh
================================================
#!/bin/sh
set -e

#check if deployment is triggered only in master
if [ $BITBUCKET_BRANCH != "master" ]; then
               echo Deploy on tagged commit can be only executed in master!
               exit 1
fi

# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest
export REPOSITORY=`docker run --rm  \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP}`

eval $(docker run --rm \
    -e KBC_DEVELOPERPORTAL_USERNAME \
    -e KBC_DEVELOPERPORTAL_PASSWORD \
    quay.io/keboola/developer-portal-cli-v2:latest \
    ecr:get-login ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP})

# Push to the repository
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:${BITBUCKET_TAG}
docker tag ${APP_IMAGE}:latest ${REPOSITORY}:latest
docker push ${REPOSITORY}:${BITBUCKET_TAG}
docker push ${REPOSITORY}:latest

# Update the tag in Keboola Developer Portal -> Deploy to KBC
if echo ${BITBUCKET_TAG} | grep -c '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$'
then
    docker run --rm \
        -e KBC_DEVELOPERPORTAL_USERNAME \
        -e KBC_DEVELOPERPORTAL_PASSWORD \
        quay.io/keboola/developer-portal-cli-v2:latest \
        update-app-repository ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} ${BITBUCKET_TAG} ecr ${REPOSITORY}
else
    echo "Skipping deployment to KBC, tag ${BITBUCKET_TAG} is not allowed."
fi



================================================
FILE: docker-compose.yml
================================================
version: "2"
services:
  # for development purposes
  dev:
    build: .
    volumes:
        - ./:/code
        - ./data:/data
    environment:
      - KBC_DATADIR=./data
  test:
    # Use to run flake8 and unittests checks
    build: .
    volumes:
      - ./:/code
      - ./data:/data
    environment:
      - KBC_DATADIR=./data
    command:
      - /bin/sh
      - /code/scripts/build_n_test.sh


================================================
FILE: Dockerfile
================================================
FROM python:3.8.13-slim
ENV PYTHONIOENCODING utf-8

COPY . /code/

# install gcc to be able to build packages - e.g. required by regex, dateparser, also required for pandas
RUN apt-get update && apt-get install -y build-essential

RUN pip install --upgrade pip

RUN pip install flake8

RUN pip install -r /code/requirements.txt

WORKDIR /code/


CMD ["python", "-u", "/code/src/component.py"]



================================================
FILE: flake8.cfg
================================================
[flake8]
exclude =
    .git,
    __pycache__,
    tests
max-line-length = 120

# F812: list comprehension redefines ...
# H101: Use TODO(NAME)
# H202: assertRaises Exception too broad
# H233: Python 3.x incompatible use of print operator
# H301: one import per line
# H306: imports not in alphabetical order (time, os)
# H401: docstring should not start with a space
# H403: multi line docstrings should end on a new line
# H404: multi line docstring should start without a leading new line
# H405: multi line docstring summary not separated with an empty line
# H501: Do not use self.__dict__ for string formatting



================================================
FILE: LICENSE.md
================================================
Copyright (c) 2018 Keboola DS

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files, to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is furnished
to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


================================================
FILE: requirements.txt
================================================
https://bitbucket.org/kds_consulting_team/keboola-python-util-lib/get/0.5.2.zip#egg=kbc
keboola.component==1.4.0
keboola.csvwriter
mock
freezegun
pandas==1.4.3
regex==2019.11.1
backoff==2.2.1
ratelimit==2.2.1



================================================
FILE: component_config/component_long_description.md
================================================
# Google My Business Extractor

### API Documentation

[Google My Business API](https://developers.google.com/my-business/reference/rest)

### Account Authorization

If you are using your personal Google Account to manage locations for your business or company, please ensure your account has the access right to the locations you are interested in. For a list of available locations for you Google Account, please visit [business.google.com/locations](https://business.google.com/locations).

### Configuration

1. Endpoints

    1. Daily Metrics
    2. Reviews
    3. Media
    4. Questions

2. Request Range
    - This configuration will only affect [Daily Metrics] endpoint. 


================================================
FILE: component_config/component_short_description.md
================================================
Allows user to extract their reviews and media related information from their Google My Business location accounts


================================================
FILE: component_config/configSchema.json
================================================
{
   "type":"object",
   "title":"Parameters",
   "required":[
      "endpoints",
      "request_range"
   ],
   "properties":{
"accounts": {
      "type": "array",
       "format": "select",
       "items": {
            "enum": [],
            "type": "string"
          },
      "uniqueItems": true,
      "title": "Accounts",
      "options": {
        "async": {
          "label": "List available accounts",
          "action": "listAccounts"
        }
      },
      "description": "Accounts for which will the component fetch results.",
      "propertyOrder": 2
    },
      "endpoints":{
         "type":"array",
         "format":"checkbox",
         "title":"Endpoints",
         "uniqueItems":true,
         "items":{
            "enum":[
               "dailyMetrics",
               "reviews",
               "media",
               "questions"
            ],
            "type":"string",
            "options":{
               "enum_titles":[
                  "Daily Metrics",
                  "Reviews",
                  "Media",
                  "Questions"
               ]
            }
         },
         "propertyOrder":3
      },
      "request_range":{
         "type":"object",
         "required":[
            "start_date",
            "end_date"
         ],
         "title":"Request Range",
         "description":"This configuration will only affect [Daily Metrics] endpoint.",
         "properties":{
            "start_date":{
               "type":"string",
               "title":"Start Date",
               "default":"7 days ago",
               "propertyOrder":10,
               "description":"default: 7 days ago"
            },
            "end_date":{
               "type":"string",
               "title":"End Date",
               "default":"today",
               "propertyOrder":20,
               "description":"default: today"
            }
         },
         "propertyOrder":4
      },
         "destination": {
      "title": "Destination",
      "type": "object",
      "propertyOrder": 5,
      "required": [
        "load_type"
      ],
      "properties": {
        "load_type": {
          "type": "string",
          "enum": [
            "full_load",
            "incremental_load"
          ],
          "options": {
            "enum_titles": [
              "Full Load",
              "Incremental Load"
            ]
          },
          "default": "full_load",
          "format": "checkbox",
          "title": "Load Type",
          "description": "If Full load is used, the destination table will be overwritten every run. If incremental load is used, data will be upserted into the destination table. Tables with a primary key will have rows updated, tables without a primary key will have rows appended.",
          "propertyOrder": 20
        }
      }
    }
   }
}


================================================
FILE: component_config/configuration_description.md
================================================
1. Endpoints

2. Request Range
    - This configuration will only affect `Location Insights` endpoint. Google My Business API has limited the maximum range of request to 18 months. By default, the component is configured to fech data from 1 week ago to the date of the component run


================================================
FILE: component_config/documentationUrl.md
================================================
[Empty file]


================================================
FILE: component_config/licenseUrl.md
================================================
[Empty file]


================================================
FILE: component_config/logger
================================================
gelf


================================================
FILE: component_config/loggerConfiguration.json
================================================
{
  "verbosity": {
    "100": "normal",
    "200": "normal",
    "250": "normal",
    "300": "verbose",
    "400": "verbose",
    "500": "camouflage",
    "550": "camouflage",
    "600": "camouflage"
  },
  "gelf_server_type": "tcp"
}


================================================
FILE: component_config/sourceCodeUrl.md
================================================
[Empty file]


================================================
FILE: component_config/stack_parameters.json
================================================
{}


================================================
FILE: component_config/sample-config/config.json
================================================
{
  "storage": {
    "input": {
      "files": [],
      "tables": [
        {
          "source": "in.c-test.test",
          "destination": "test.csv",
          "limit": 50,
          "columns": [],
          "where_values": [],
          "where_operator": "eq"
        }
      ]
    },
    "output": {
      "files": [],
      "tables": []
    }
  },
  "parameters": {
    "#api_token": "demo",
    "period_from": "yesterday",
    "endpoints": [
      "deals",
      "companies"
    ],
    "company_properties": "",
    "deal_properties": "",
    "debug": true
  },
  "image_parameters": {
    "syrup_url": "https://syrup.keboola.com/"
  },
  "authorization": {
    "oauth_api": {
      "id": "OAUTH_API_ID",
      "credentials": {
        "id": "main",
        "authorizedFor": "Myself",
        "creator": {
          "id": "1234",
          "description": "me@keboola.com"
        },
        "created": "2016-01-31 00:13:30",
        "#data": "{\"refresh_token\":\"MCWBkfdK9m5YK*Oqahwm6XN6elMAEwcH5kYcK8Ku!bpiOgSDZN9MQIzunpMsh6LyKH0i!7OcwwwajuxPfvm2PrrWYSs*HerDr2ZSJ39pqHJcvwUNIvHdtcgFFr3Em*yhn3GKBwM2p9UrjtgdAriSDny5YgUYGuI3gYJY1ypD*wBaAOzzeeXZx6CdgjruJ7gboTAngbWk3CzO9rORIwXAAlGUH6ZgBQJL3AwkYVMRFV4BvIvDAMF*0DcGDyrcyYDw9X3vYn*Wy!OqgrenKCGowdJk0C0136SUv4PJI383y76UMim6Q7KGDj7Lf!K2N2FDbxsz2iZKZTBr2vHx8pEC1oBc$\"}",
        "oauthVersion": "2.0",
        "appKey": "000000004C184A49",
        "#appSecret": "vBAYak49pVK1zghHAgDH4tCSCNlT-CiN"
      }
    }
  }
}



================================================
FILE: component_config/sample-config/in/state.json
================================================
{"data_delta": "10222018"}


================================================
FILE: component_config/sample-config/in/files/order1.xml
================================================
<?xml version='1.0' ?>
<root_el>
    <orders>
        <order>
            <id>1</id>
            <date>2018-01-01</date>
            <cust_name>David</cust_name>	
            <order-item>
                <price currency="CZK">100</price>
                <item>Umbrella</item>
            </order-item>
            <order-item>
                <price currency="CZK">200</price>
                <item>Rain Coat</item>
            </order-item>
        </order>
    </orders>
</root_el>


================================================
FILE: component_config/sample-config/in/tables/test.csv
================================================
"Type","Campaign_Name","Status","Start_Date","End_Date","Location","Eventbrite_link"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","How to become data driven startup","Complete","2015-10-13","2015-10-13","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711425377"
"Event","How to become data driven startup","Complete","2015-11-04","2015-11-04","United Kingdom","https://www.eventbrite.co.uk/e/how-to-become-data-driven-startup-registration-18711426380"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-01-14","2016-01-14","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20152992142"
"Event","DATAGIRLS PRESENT: HOW TO BECOME DATA-DRIVEN","Complete","2016-02-25","2016-02-25","United Kingdom","https://www.eventbrite.co.uk/e/datagirls-present-how-to-become-data-driven-tickets-20967439175"
"Event","Data Tools for Startups","Complete","2016-03-17","2016-03-17","United Kingdom","https://www.eventbrite.co.uk/e/data-tools-for-startups-tickets-21257426535"
"Event","Data Festival London 2016","Complete","2016-06-24","2016-06-26","United Kingdom","https://www.eventbrite.co.uk/e/data-festival-london-2016-tickets-25192608771"
"Event","Becoming data driven in the high street fashion","Complete","2016-10-12","2016-10-12","United Kingdom","https://www.eventbrite.co.uk/e/becoming-data-driven-in-the-high-street-fashion-tickets-27481268213"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola DataBrunch - Amazon Go a ako s ním v maloobchode “bojovať”","Complete","2017-03-09","2017-03-09","Slovakia","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-ako-s-nim-v-maloobchode-bojovat-tickets-31827553068"
"Event","Keboola DataBrunch - Amazon Go a jak s nim v maloobchodě “bojovat”","Complete","2017-03-29","2017-03-29","Czech Republic","https://www.eventbrite.co.uk/e/keboola-databrunch-amazon-go-a-jak-s-nim-v-maloobchode-bojovat-tickets-32182393405"
"Event","The Data Foundry present: DATAGIRLS Weekend","Complete","2016-10-14","2016-10-16","United Kingdom","https://www.eventbrite.co.uk/e/the-data-foundry-present-datagirls-weekend-tickets-27350069795"
"Event","[NLP] How to analyse text data for knowledge discovery","Complete","2017-04-10","2017-04-10","United Kingdom","https://www.eventbrite.co.uk/e/nlp-how-to-analyse-text-data-for-knowledge-discovery-tickets-32320274812"
"Event","Keboola Data Brunch - KPIs and AmazonGo, budoucnost retailu? ","Complete","2017-06-27","2017-06-27","Czech Republic","https://www.eventbrite.co.uk/e/keboola-data-brunch-kpis-amazongo-budoucnost-retailu-tickets-35257195220"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"
"Event","Conversion Rate Optimisation in Travel Industry","Complete","2018-01-30","2018-01-30","United Kingdom","https://www.eventbrite.co.uk/e/conversion-rate-optimisation-in-travel-industry-tickets-38951076719"
"Event","Learn how to #DoMoreWithData with DataGirls","Complete","2017-10-01","2017-10-01","United Kingdom","https://www.eventbrite.co.uk/e/learn-how-to-domorewithdata-with-datagirls-tickets-36777944823"
"Event","Are You Using Data to Understand Your Customers? ","Complete","2018-02-27","2018-02-27","United Kingdom","https://www.eventbrite.co.uk/e/are-you-using-data-to-understand-your-customers-tickets-42000160611"



================================================
FILE: component_config/sample-config/in/tables/test.csv.manifest
================================================
{
    "id": "in.c-test.test",
    "uri": "https:\/\/connection.keboola.com\/v2\/storage\/tables\/in.c-test.test",
    "name": "test",
    "primary_key": [],
    "indexed_columns": [],
    "created": "2018-03-02T15:36:50+0100",
    "last_change_date": "2018-03-02T15:36:54+0100",
    "last_import_date": "2018-03-02T15:36:54+0100",
    "rows_count": 0,
    "data_size_bytes": 0,
    "is_alias": false,
    "attributes": [],
    "columns": [
        "Type",
        "Campaign_Name",
        "Status",
        "Start_Date",
        "End_Date",
        "Location",
        "Eventbrite_link"
    ],
    "metadata": [
        {
            "id": "18271581",
            "key": "KBC.createdBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271582",
            "key": "KBC.createdBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271583",
            "key": "KBC.createdBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271584",
            "key": "KBC.lastUpdatedBy.component.id",
            "value": "transformation",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271585",
            "key": "KBC.lastUpdatedBy.configuration.id",
            "value": "361585608",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        },
        {
            "id": "18271586",
            "key": "KBC.lastUpdatedBy.configurationRow.id",
            "value": "361585762",
            "provider": "system",
            "timestamp": "2018-03-02T15:37:02+0100"
        }
    ],
    "column_metadata": {
        "Type": [],
        "Campaign_Name": [],
        "Status": [],
        "Start_Date": [],
        "End_Date": [],
        "Location": [],
        "Eventbrite_link": []
    }
}


================================================
FILE: scripts/build_n_run.ps1
================================================
echo Building component...
$COMP_TAG = Read-Host -Prompt 'Input Docker tag name:'
docker build -rm -t $COMP_TAG ../

echo Running component...
Write-host "Would you like to use default data folder? (../data)" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes use: " (join-path (Split-Path -Path (Get-Location).Path) "data"); $DATA_PATH = (join-path (Split-Path -Path (Get-Location).Path) "data") } 
       N {Write-Host "No, I'll specify myself"; $DATA_PATH = Read-Host -Prompt 'Input data folder path:'} 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 

Write-host "Would you like to execute the container to Bash, skipping the execution?" -ForegroundColor Yellow 
    $Readhost = Read-Host " ( y / n ) " 
    Switch ($ReadHost) 
     { 
       Y {Write-host "Yes, get me to the bash"; docker run -ti -v $DATA_PATH`:/data --entrypoint=//bin//bash $COMP_TAG} 
       N {Write-Host "No, execute the app normally"; 
		    echo $DATA_PATH
			docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG
	   } 
       Default {Write-Host "Default, run app"; docker run -v $DATA_PATH`:/data -e KBC_DATADIR=/data $COMP_TAG} 
     } 





================================================
FILE: scripts/build_n_test.sh
================================================
#!/bin/sh
set -e

flake8 --config=flake8.cfg
python -m unittest discover


================================================
FILE: scripts/run.bat
================================================
@echo off

echo Running component...
docker run -v %cd%:/data -e KBC_DATADIR=/data comp-tag


================================================
FILE: scripts/run_kbc_tests.ps1
================================================
echo "Preparing KBC test image"
# set env vars
$KBC_DEVELOPERPORTAL_USERNAME  = Read-Host -Prompt 'Input your service account user name'
$KBC_DEVELOPERPORTAL_PASSWORD  = Read-Host -Prompt 'Input your service account pass'
$KBC_DEVELOPERPORTAL_VENDOR = 'esnerda'
$KBC_DEVELOPERPORTAL_APP = 'esnerda.ex-gusto-export'
$BASE_KBC_CONFIG = '455568423'
$KBC_STORAGE_TOKEN = Read-Host -Prompt 'Input your storage token'


#build app
$APP_IMAGE='keboola-comp-test'
docker build ..\ --tag=$APP_IMAGE
docker images
docker -v
#docker run $APP_IMAGE flake8 --config=./deployment/flake8.cfg
echo "Running unit-tests..."
docker run $APP_IMAGE python -m unittest discover

docker pull quay.io/keboola/developer-portal-cli-v2:latest
$REPOSITORY= docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD quay.io/keboola/developer-portal-cli-v2:latest ecr:get-repository $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP

docker tag $APP_IMAGE`:latest $REPOSITORY`:test

echo 'running login'
$(docker run --rm -e KBC_DEVELOPERPORTAL_USERNAME=$KBC_DEVELOPERPORTAL_USERNAME -e KBC_DEVELOPERPORTAL_PASSWORD=$KBC_DEVELOPERPORTAL_PASSWORD -e KBC_DEVELOPERPORTAL_URL quay.io/keboola/developer-portal-cli-v2:latest ecr:get-login $KBC_DEVELOPERPORTAL_VENDOR $KBC_DEVELOPERPORTAL_APP)

echo 'pushing test image'
docker push $REPOSITORY`:test

echo 'running test config in KBC'
docker run --rm -e KBC_STORAGE_TOKEN=$KBC_STORAGE_TOKEN quay.io/keboola/syrup-cli:latest run-job $KBC_DEVELOPERPORTAL_APP $BASE_KBC_CONFIG test



================================================
FILE: scripts/update_dev_portal_properties.sh
================================================
#!/usr/bin/env bash

set -e
# Obtain the component repository and log in
docker pull quay.io/keboola/developer-portal-cli-v2:latest


# Update properties in Keboola Developer Portal
echo "Updating long description"
value=`cat component_config/component_long_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} longDescription --value="$value"
else
    echo "longDescription is empty!"
    exit 1
fi

echo "Updating config schema"
value=`cat component_config/configSchema.json`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationSchema --value="$value"
else
    echo "configurationSchema is empty!"
fi


echo "Updating config description"

value=`cat component_config/configuration_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} configurationDescription --value="$value"
else
    echo "configurationDescription is empty!"
fi


echo "Updating short description"

value=`cat component_config/component_short_description.md`
echo "$value"
if [ ! -z "$value" ]
then
    docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property ${KBC_DEVELOPERPORTAL_VENDOR} ${KBC_DEVELOPERPORTAL_APP} shortDescription --value="$value"
else
    echo "shortDescription is empty!"
    exit 1
fi


================================================
FILE: scripts/developer_portal/fn_actions_md_update.sh
================================================
#!/bin/bash

# Set the path to the Python script file
PYTHON_FILE="src/component.py"
# Set the path to the Markdown file containing actions
MD_FILE="component_config/actions.md"

# Check if the file exists before creating it
if [ ! -e "$MD_FILE" ]; then
    touch "$MD_FILE"
else
    echo "File already exists: $MD_FILE"
    exit 1
fi

# Get all occurrences of lines containing @sync_action('XXX') from the .py file
SYNC_ACTIONS=$(grep -o -E "@sync_action\(['\"][^'\"]*['\"]\)" "$PYTHON_FILE" | sed "s/@sync_action(\(['\"]\)\([^'\"]*\)\(['\"]\))/\2/" | sort | uniq)

# Check if any sync actions were found
if [ -n "$SYNC_ACTIONS" ]; then
    # Iterate over each occurrence of @sync_action('XXX')
    for sync_action in $SYNC_ACTIONS; do
        EXISTING_ACTIONS+=("$sync_action")
    done

    # Convert the array to JSON format
    JSON_ACTIONS=$(printf '"%s",' "${EXISTING_ACTIONS[@]}")
    JSON_ACTIONS="[${JSON_ACTIONS%,}]"

    # Update the content of the actions.md file
    echo "$JSON_ACTIONS" > "$MD_FILE"
else
    echo "No sync actions found. Not creating the file."
fi


================================================
FILE: scripts/developer_portal/update_properties.sh
================================================
#!/usr/bin/env bash

set -e

# Check if the KBC_DEVELOPERPORTAL_APP environment variable is set
if [ -z "$KBC_DEVELOPERPORTAL_APP" ]; then
    echo "Error: KBC_DEVELOPERPORTAL_APP environment variable is not set."
    exit 1
fi

# Pull the latest version of the developer portal CLI Docker image
docker pull quay.io/keboola/developer-portal-cli-v2:latest

# Function to update a property for the given app ID
update_property() {
    local app_id="$1"
    local prop_name="$2"
    local file_path="$3"

    if [ ! -f "$file_path" ]; then
        echo "File '$file_path' not found. Skipping update for property '$prop_name' of application '$app_id'."
        return
    fi

    # shellcheck disable=SC2155
    local value=$(<"$file_path")

    echo "Updating $prop_name for $app_id"
    echo "$value"

    if [ -n "$value" ]; then
        docker run --rm \
            -e KBC_DEVELOPERPORTAL_USERNAME \
            -e KBC_DEVELOPERPORTAL_PASSWORD \
            quay.io/keboola/developer-portal-cli-v2:latest \
            update-app-property "$KBC_DEVELOPERPORTAL_VENDOR" "$app_id" "$prop_name" --value="$value"
        echo "Property $prop_name updated successfully for $app_id"
    else
        echo "$prop_name is empty for $app_id, skipping..."
    fi
}

app_id="$KBC_DEVELOPERPORTAL_APP"

update_property "$app_id" "isDeployReady" "component_config/isDeployReady.md"
update_property "$app_id" "longDescription" "component_config/component_long_description.md"
update_property "$app_id" "configurationSchema" "component_config/configSchema.json"
update_property "$app_id" "configurationRowSchema" "component_config/configRowSchema.json"
update_property "$app_id" "configurationDescription" "component_config/configuration_description.md"
update_property "$app_id" "shortDescription" "component_config/component_short_description.md"
update_property "$app_id" "logger" "component_config/logger"
update_property "$app_id" "loggerConfiguration" "component_config/loggerConfiguration.json"
update_property "$app_id" "licenseUrl" "component_config/licenseUrl.md"
update_property "$app_id" "documentationUrl" "component_config/documentationUrl.md"
update_property "$app_id" "sourceCodeUrl" "component_config/sourceCodeUrl.md"
update_property "$app_id" "uiOptions" "component_config/uiOptions.md"

# Update the actions.md file
source "$(dirname "$0")/fn_actions_md_update.sh"
# update_property actions
update_property "$app_id" "actions" "component_config/actions.md"


================================================
FILE: src/component.py
================================================
import logging
import os
import json
import dateparser
import requests
import shutil

from keboola.component.base import ComponentBase, sync_action
from keboola.component.exceptions import UserException

from google_my_business import GoogleMyBusiness, GoogleMyBusinessException

# configuration variables
KEY_API_TOKEN = '#api_token'
KEY_PERIOD_FROM = 'period_from'
KEY_ENDPOINTS = 'endpoints'
KEY_ACCOUNTS = 'accounts'
KEY_GROUP_DESTINATION = 'destination'
KEY_LOAD_TYPE = 'load_type'

MANDATORY_PARS = [KEY_ENDPOINTS, KEY_API_TOKEN]


class Component(ComponentBase):

    def __init__(self):
        super().__init__()

    def run(self):
        """
        Main execution code
        """
        params = self.configuration.parameters
        authorization = self.configuration.config_data["authorization"]
        oauth_token = self.get_oauth_token(authorization)

        endpoints = params[KEY_ENDPOINTS]
        logging.info(f"Component will process following endpoints: {endpoints}")
        if not endpoints:
            raise UserException('Please select an endpoint.')

        # Validating input date parameters
        start_date_str = params['request_range'].get('start_date', '7 days ago')
        end_date_str = params['request_range'].get('end_date', 'today')
        start_date_form, end_date_form = dateparser.parse(start_date_str), dateparser.parse(end_date_str)
        if start_date_form > end_date_form:
            raise UserException('Start Date cannot exceed End Date. Please re-enter [Request Range].')
        start_date_str = start_date_form.strftime('%Y-%m-%dT00:00:00.000000Z')
        end_date_str = end_date_form.strftime('%Y-%m-%dT00:00:00.000000Z')

        logging.info('Request Range: {} to {}'.format(start_date_str, end_date_str))
        accounts = params.get(KEY_ACCOUNTS, {})
        if not accounts:
            raise UserException("The authorized account has to have a linked My Google Business account with "
                                "management rights and proper account selected in the component's Accounts parameter.")

        destination_params = params.get(KEY_GROUP_DESTINATION, {})
        incremental = destination_params.get(KEY_LOAD_TYPE) != 'full_load' if destination_params else False

        statefile = self.get_state_file()
        default_columns = statefile or []
        if statefile:
            logging.info(f"Columns loaded from statefile: {default_columns}")

        self.create_temp_folder()

        gmb = GoogleMyBusiness(
            access_token=oauth_token,
            start_timestamp=start_date_str,
            end_timestamp=end_date_str,
            data_folder_path=self.data_folder_path,
            default_columns=default_columns,
            accounts=accounts,
            incremental=incremental
        )
        try:
            gmb.process(endpoints=endpoints)
        except GoogleMyBusinessException as e:
            raise UserException(e)

        self.write_state_file(gmb.tables_columns)
        self.delete_temp_folder()

        logging.info("Extraction finished")

    @staticmethod
    def get_oauth_token(config):
        data = config['oauth_api']['credentials']
        data_encrypted = json.loads(
            config['oauth_api']['credentials']['#data'])
        client_id = data['appKey']
        client_secret = data['#appSecret']
        refresh_token = data_encrypted['refresh_token']

        url = 'https://www.googleapis.com/oauth2/v4/token'
        header = {
            'Content-Type': 'application/x-www-form-urlencoded'
        }
        payload = {
            'client_id': client_id,
            'client_secret': client_secret,
            'grant_type': 'refresh_token',
            'refresh_token': refresh_token,
        }

        response = requests.post(
            url=url, headers=header, data=payload)

        if response.status_code != 200:
            raise UserException(f"Unable to refresh access token. "
                                f"Please reset the account authorization: {response.text}")

        data_r = response.json()
        return data_r["access_token"]

    def create_temp_folder(self):
        temp_path = os.path.join(self.data_folder_path, "temp")
        if not os.path.exists(temp_path):
            logging.info("creating temp folder")
            os.makedirs(temp_path)

    def delete_temp_folder(self):
        temp_path = os.path.join(self.data_folder_path, "temp")
        try:
            shutil.rmtree(temp_path)
        except OSError as e:
            logging.error(f"Error deleting {temp_path}: {e}")

    @sync_action('listAccounts')
    def list_accounts(self):
        authorization = self.configuration.config_data["authorization"]
        oauth_token = self.get_oauth_token(authorization)

        gmb = GoogleMyBusiness(
            access_token=oauth_token,
            data_folder_path=self.data_folder_path)
        try:
            gmb.list_accounts()
        except GoogleMyBusinessException:
            raise UserException("Failed to retrieved Google My Business accounts for which the authorized user has "
                                "management rights.")

        accounts = []
        if gmb.account_list:
            for account in gmb.account_list:
                if account.get("name", None) and account.get("accountName", None):
                    accounts.append(
                        {
                            "label": account.get("accountName"),
                            "value": account.get("name")
                        }
                    )
            return accounts
        else:
            raise UserException("Authorized account does not have any Google My Business accounts with "
                                "management rights.")


"""
        Main entrypoint
"""
if __name__ == "__main__":
    try:
        comp = Component()
        # this triggers the run method by default and is controlled by the configuration.action parameter
        comp.execute_action()
    except UserException as exc:
        logging.exception(exc)
        exit(1)
    except Exception as exc:
        logging.exception(exc)
        exit(2)



================================================
FILE: src/definitions.py
================================================
mapping = {
    "accounts": ["name"],
    "locations": ["name"],
    "reviews": ["reviewId"],
    "media": [],
    "questions": ["name"],
    "daily_metrics": ["location_id", "metric", 'date']
}



================================================
FILE: src/google_my_business.py
================================================
import os
import json
import requests
import logging
from datetime import datetime
import uuid
import backoff
from ratelimit import limits, sleep_and_retry

from keboola.csvwriter import ElasticDictWriter

from definitions import mapping

PAGE_SIZE = 50

AVAILABLE_DAILY_METRICS = ["BUSINESS_IMPRESSIONS_DESKTOP_MAPS", "BUSINESS_IMPRESSIONS_DESKTOP_SEARCH",
                           "BUSINESS_IMPRESSIONS_MOBILE_MAPS", "BUSINESS_IMPRESSIONS_MOBILE_SEARCH",
                           "BUSINESS_CONVERSATIONS", "BUSINESS_DIRECTION_REQUESTS", "CALL_CLICKS",
                           "WEBSITE_CLICKS", "BUSINESS_BOOKINGS", "BUSINESS_FOOD_ORDERS", "BUSINESS_FOOD_MENU_CLICKS"
                           ]


class GoogleMyBusinessException(Exception):
    pass


def get_date_from_string(date_string):
    """
    Extracts the year, month, and day from a string in the format "YYYY-MM-DDTHH:MM:SS.ffffffZ"
    and returns them as a tuple.
    """
    date_format = "%Y-%m-%dT%H:%M:%S.%fZ"
    date_object = datetime.strptime(date_string, date_format)
    year = date_object.year
    month = date_object.month
    day = date_object.day
    return year, month, day


def flatten_dict(d, max_key_length=64):
    flat_dict = {}
    for key, value in d.items():
        if isinstance(value, dict):
            sub_dict = flatten_dict(value, max_key_length)
            for sub_key, sub_value in sub_dict.items():
                full_key = f"{key}_{sub_key}"
                if len(full_key) > max_key_length:
                    # Truncate key if it's too long
                    truncated_key = full_key[:max_key_length]
                    flat_dict[truncated_key] = sub_value
                else:
                    flat_dict[full_key] = sub_value
        elif isinstance(value, list):
            for i, item in enumerate(value):
                if isinstance(item, dict):
                    sub_dict = flatten_dict(item, max_key_length)
                    for sub_key, sub_value in sub_dict.items():
                        full_key = f"{key}_{i}_{sub_key}"
                        if len(full_key) > max_key_length:
                            # Truncate key if it's too long
                            truncated_key = full_key[:max_key_length]
                            flat_dict[truncated_key] = sub_value
                        else:
                            flat_dict[full_key] = sub_value
                else:
                    full_key = f"{key}_{i}"
                    if len(full_key) > max_key_length:
                        # Truncate key if it's too long
                        truncated_key = full_key[:max_key_length]
                        flat_dict[truncated_key] = item
                    else:
                        flat_dict[full_key] = item
        else:
            if len(key) > max_key_length:
                # Truncate key if it's too long
                truncated_key = key[:max_key_length]
                flat_dict[truncated_key] = value
            else:
                flat_dict[key] = value
    return flat_dict


def backoff_custom():
    delays = [15, 30, 45, 61, 61, 61, 61]
    for delay in delays:
        yield delay


class GoogleMyBusiness:
    def __init__(self, access_token, data_folder_path, default_columns=None, start_timestamp=None, end_timestamp=None,
                 accounts=None, incremental=True):
        if default_columns is None:
            default_columns = []
        self.output_columns = None
        self.access_token = access_token
        self.incremental = incremental
        self.base_url = 'https://mybusiness.googleapis.com/v4'
        self.base_url_v1 = "https://mybusiness.googleapis.com/v1"
        self.base_url_profile_performance = "https://businessprofileperformance.googleapis.com/v1"
        self.base_url_quanda = "https://mybusinessqanda.googleapis.com/v1"

        self.start_timestamp = start_timestamp
        self.end_timestamp = end_timestamp
        self.data_folder_path = data_folder_path

        self.temp_table_destination = os.path.join(data_folder_path, "temp/")
        self.default_table_destination = os.path.join(data_folder_path, "out/tables/")

        self.session = requests.Session()
        self.reviews = []
        self.questions = []
        self.media = []
        self.daily_metrics = {}

        self.tables_columns = default_columns if default_columns else {}
        self.selected_accounts = accounts if accounts else []
        self.account_list = []

    def test_connection(self):
        try:
            self.list_accounts()
        except Exception as e:
            raise GoogleMyBusinessException(e)

    @staticmethod
    def select_entries(selected_accounts, all_accounts):

        relevant_entries = []
        for item in all_accounts:
            if item.get("name", "") in selected_accounts:
                relevant_entries.append(item)

        if not relevant_entries:
            raise GoogleMyBusinessException(f"Selected accounts {selected_accounts} is/are not in available accounts: "
                                            f"{all_accounts}")

        return relevant_entries

    def process(self, endpoints=None):
        self.list_accounts()
        if self.selected_accounts:
            self.account_list = self.select_entries(self.selected_accounts, self.account_list)

        logging.info(f'Component will process following accounts: {self.account_list}')

        # Outputting all the accounts found
        logging.info('Outputting Accounts...')
        self.create_temp_files(
            data_in=self.account_list,
            file_name='accounts'
        )

        # Finding all the accounts available for the authorized account
        for account in self.account_list:
            account_id = account['name']
            # Fetching all the locations available for the entered account
            all_locations = self.list_locations(account_id=account_id)
            logging.info('Locations found in Account [{}] - [{}]'.format(
                account['accountName'], len(all_locations)))
            logging.info('Outputting Locations...')
            self.create_temp_files(
                data_in=all_locations,
                file_name='locations'
            )

            if len(all_locations) == 0:
                logging.error(f'There is no location info under the authorized '
                              f'account [{account["accountName"]}].')
                continue

            if 'dailyMetrics' in endpoints:
                for location in all_locations:
                    location_path = location['name']
                    location_title = location['title']
                    location_id = location_path.replace("locations/", "")
                    logging.info(f"Processing endpoint dailyMetrics for {location_title}.")
                    self.daily_metrics[location_id] = self.list_daily_metrics(location_id=location_path)
                self.daily_metrics_parser(data_in=self.daily_metrics)
            self.daily_metrics = {}

            if 'reviews' in endpoints:
                for location in all_locations:
                    location_path = location['name']
                    location_title = location['title']
                    logging.info(f"Processing reviews for {location_title}.")
                    reviews = self.list_reviews(account_id=account_id, location_id=location_path)
                    for review in reviews:
                        self.reviews.append(review)
                self.create_temp_files(data_in=self.reviews, file_name="reviews")
            self.reviews = []

            if 'media' in endpoints:
                for location in all_locations:
                    location_path = location['name']
                    location_title = location['title']
                    logging.info(f"Processing media for {location_title}.")
                    media = self.list_media(location_id=location_path, account_id=account_id)
                    for medium in media:
                        self.media.append(medium)
                self.create_temp_files(data_in=self.media, file_name="media")
            self.media = []

            if 'questions' in endpoints:
                for location in all_locations:
                    location_path = location['name']
                    location_title = location['title']
                    logging.info(f"Processing questions for {location_title}.")
                    questions = self.list_questions(location_id=location_path)
                    for question in questions:
                        self.questions.append(question)
                self.create_temp_files(file_name="questions", data_in=self.questions)
            self.questions = []

        self.save_resulting_files()

    @sleep_and_retry
    @limits(calls=290, period=61)
    @backoff.on_exception(backoff_custom, Exception, max_tries=7)
    def get_request(self, url, headers=None, params=None):
        res = self.session.get(url=url, headers=headers, params=params)
        if res.status_code == 429:
            # Raise an exception to trigger the retry logic
            raise Exception("Rate limit exceeded. Retrying...")
        elif res.status_code in [400, 403, 500]:
            # Ignore error 403 and return the status code and response object
            return res.status_code, res
        elif res.status_code != 200:
            raise Exception(f"Request failed with status code {res.status_code}")
        return res.status_code, res

    def list_accounts(self, nextPageToken=None):
        """
        Fetching all the accounts available in the authorized Google account
        """
        account_url = '{}/accounts'.format(self.base_url_v1)

        params = {
            'access_token': self.access_token
            # 'Authorization': 'Bearer {}'.format(self.access_token)
        }

        if nextPageToken:
            params['pageToken'] = nextPageToken

        # Get Account Lists
        res_status, account_raw = self.get_request(account_url, params=params)
        if res_status != 200:
            raise GoogleMyBusinessException(f'The component cannot fetch list of GMB accounts, '
                                            f'error: {account_raw.text}')

        account_json = account_raw.json()
        if 'accounts' in account_json:
            self.account_list += account_json['accounts']

        # Looping for all the accounts
        if 'nextPageToken' in account_json:
            self.list_accounts(nextPageToken=account_json['nextPageToken'])

        if not self.account_list:
            raise GoogleMyBusinessException("No GMB accounts found for authorized user.")

    @backoff.on_exception(backoff.expo, GoogleMyBusinessException, max_tries=5)
    def list_locations(self, account_id, nextPageToken=None):
        """
        Fetching all locations associated to the account_id
        """

        location_url = '{}/{}/locations'.format(self.base_url_v1, account_id)
        params = {
            'access_token': self.access_token,
            'readMask': 'name,languageCode,storeCode,title,phoneNumbers,categories,storefrontAddress,websiteUri,'
                        'regularHours,specialHours,serviceArea,latlng,openInfo,metadata,profile,relationshipData'
        }
        if nextPageToken:
            params['pageToken'] = nextPageToken

        res_status, location_raw = self.get_request(
            location_url, params=params)
        if res_status != 200:
            raise GoogleMyBusinessException(f'Something wrong with location request. Response: {location_raw.text}')
        location_json = location_raw.json()

        # If the account has no locations under it
        if 'locations' not in location_json:
            out_location_list = []
        else:
            out_location_list = location_json['locations']

        for location in out_location_list:
            location['account_id'] = account_id

        # Looping for all the locations
        if 'nextPageToken' in location_json:
            out_location_list = out_location_list + \
                                self.list_locations(account_id=account_id,
                                                    nextPageToken=location_json['nextPageToken'])

        return out_location_list

    def list_daily_metrics(self, location_id):
        """
        Fetching all the report insights from assigned location.
        https://developers.google.com/my-business/reference/performance/rest/v1/
        locations/getDailyMetricsTimeSeries#DailyRange
        """
        start_year, start_month, start_day = get_date_from_string(self.start_timestamp)
        end_year, end_month, end_day = get_date_from_string(self.end_timestamp)

        parsed_values = {}
        for metric in AVAILABLE_DAILY_METRICS:
            insight_url = self.base_url_profile_performance + f"/{location_id}:getDailyMetricsTimeSeries"
            params = {
                "dailyMetric": metric,
                "dailyRange.startDate.year": start_year,
                "dailyRange.startDate.month": start_month,
                "dailyRange.startDate.day": start_day,
                "dailyRange.endDate.year": end_year,
                "dailyRange.endDate.month": end_month,
                "dailyRange.endDate.day": end_day,
            }

            header = {
                'Content-type': 'application/json',
                'Authorization': 'Bearer {}'.format(self.access_token)
            }

            res_status, insights_raw = self.get_request(url=insight_url, headers=header, params=params)

            if res_status != 200:
                if res_status == 403:
                    logging.error(f"Cannot fetch daily metrics for location with id {location_id}, response: "
                                  f"{insights_raw.text}")
                    return {}
                raise GoogleMyBusinessException(f'Something wrong with report insight request. '
                                                f'Response: {insights_raw.text}')

            response = insights_raw.json()
            if 'timeSeries' in response:
                time_series = response['timeSeries']['datedValues']

                for dated_value in time_series:
                    date = f"{dated_value['date']['year']}-{dated_value['date']['month']:02d}-" \
                           f"{dated_value['date']['day']:02d}"
                    value = int(dated_value.get('value', '0'))
                    if date not in parsed_values:
                        parsed_values[date] = {}
                    parsed_values[date][metric] = value

            else:
                logging.info(f"Metric {metric} did not return any time series.")

        return parsed_values

    def list_reviews(self, account_id, location_id, nextPageToken=None):
        responses = []

        url = self.base_url + "/" + account_id + "/" + location_id + "/reviews"
        params = {
            'access_token': self.access_token,
            'pageSize': PAGE_SIZE
        }
        if nextPageToken:
            params['pageToken'] = nextPageToken

        # Get review for the location
        res_status, data_raw = self.get_request(url, params=params)
        if res_status != 200:
            raise GoogleMyBusinessException(f'Something wrong with request. Response: {data_raw.text}')
        data_json = data_raw.json()
        if 'reviews' in data_json:
            responses.extend(data_json['reviews'])

            if 'nextPageToken' in data_json:
                responses.extend(self.list_reviews(
                    account_id=account_id,
                    location_id=location_id,
                    nextPageToken=data_json['nextPageToken']))

            return responses
        else:
            logging.warning(f'Reviews for location with id {location_id} not found in response: {data_raw.text}')
            return []

    def list_questions(self, location_id, nextPageToken=None):
        responses = []

        url = self.base_url_quanda + "/" + location_id + "/questions"

        params = {
            'access_token': self.access_token
        }
        if nextPageToken:
            params['pageToken'] = nextPageToken

        # Get review for the location
        res_status, data_raw = self.get_request(url, params=params)
        if res_status != 200:
            if res_status == 400:
                if data_raw.json()["error"]["details"][0]["reason"] == "UNVERIFIED_LOCATION":
                    logging.warning(f"Location with id {location_id} is unverified. Cannot fetch questions.")
            else:
                logging.warning(f"Cannot fetch questions for location with id {location_id}. Received response: "
                                f"{data_raw.text}")
            return []

        data_json = data_raw.json()
        if data_json.get("questions", None):
            responses.extend(data_json["questions"])
            if 'nextPageToken' in data_json:
                responses.extend(self.list_questions(
                    location_id=location_id,
                    nextPageToken=data_json['nextPageToken']))
        else:
            logging.info(f"There are no questions for {location_id}")

        return responses

    @backoff.on_exception(backoff.expo, GoogleMyBusinessException, max_tries=20)
    def list_media(self, location_id, account_id, nextPageToken=None):
        responses = []

        url = self.base_url + "/" + account_id + "/" + location_id + "/media"

        params = {
            'access_token': self.access_token
        }
        if nextPageToken:
            params['pageToken'] = nextPageToken

        res_status, data_raw = self.get_request(url, params=params)
        if res_status != 200:
            raise GoogleMyBusinessException(f'Something wrong with request. Response: {data_raw.text}')
        if res_status == 503:
            raise GoogleMyBusinessException("Media service is unavailable at the moment.")

        data_json = data_raw.json()
        if data_json:
            responses.extend(data_json['mediaItems'])
            if 'nextPageToken' in data_json:
                responses.extend(self.list_media(
                    location_id=location_id,
                    account_id=account_id,
                    nextPageToken=data_json['nextPageToken']))
        else:
            logging.info(f"There are no media for {location_id}")

        return responses

    def create_temp_files(self, file_name, data_in):
        """
        Saves data to json files.
        """
        file_output_destination = os.path.join(self.temp_table_destination, file_name)
        if not os.path.exists(file_output_destination):
            os.makedirs(file_output_destination)

        if data_in:
            for row in data_in:
                filename = os.path.join(file_output_destination, str(uuid.uuid4())+".json")
                with open(filename, 'w') as outfile:
                    json.dump(flatten_dict(row), outfile)
        else:
            logging.warning(f"File {file_name} is empty. Results will not be stored.")

    def produce_manifest(self, file_name, primary_key):
        """
        Dummy function for returning manifest
        """

        file = '{}{}.csv.manifest'.format(self.default_table_destination, file_name)

        manifest = {
            'incremental': self.incremental,
            'primary_key': primary_key
        }

        try:
            with open(file, 'w') as file_out:
                json.dump(manifest, file_out)
        except Exception as e:
            logging.error("Could not produce output file manifest.")
            logging.error(e)

    def daily_metrics_parser(self, data_in):
        """
        Parser dedicated for fetching location metrics
        """

        data_out = []
        for location_id, date_data in data_in.items():
            for date, metrics in date_data.items():
                for metric, value in metrics.items():
                    data_out.append({
                        "location_id": location_id,
                        "date": date,
                        "metric": metric,
                        "value": value
                    })

        self.create_temp_files('daily_metrics', data_out)

    def save_resulting_files(self):
        """Produces manifest and saves column names to statefile"""
        filenames = [f.name for f in os.scandir(self.temp_table_destination) if f.is_dir()]

        for file_name in filenames:

            if self.tables_columns.get(file_name, {}):
                fieldnames = self.tables_columns.get(file_name)
            else:
                fieldnames = []

            temp_dir = os.path.join(self.temp_table_destination, file_name)
            temp_files = self.list_json_files(temp_dir)

            if temp_files:
                tgt_path = os.path.join(self.default_table_destination, file_name+".csv")
                with ElasticDictWriter(tgt_path, fieldnames) as wr:
                    wr.writeheader()
                    for file in temp_files:
                        with open(file, 'r') as f:
                            data = json.load(f)
                        wr.writerow(data)

                self.produce_manifest(file_name=file_name, primary_key=mapping[file_name])
                self.tables_columns[file_name] = wr.fieldnames

    @staticmethod
    def list_json_files(target_dir):
        json_files = []
        for filename in os.listdir(target_dir):
            if filename.endswith('.json'):
                json_files.append(os.path.join(target_dir, filename))
        return json_files



================================================
FILE: tests/__init__.py
================================================
import sys
import os
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + "/../src")


================================================
FILE: tests/test_component.py
================================================
'''
Created on 12. 11. 2018

@author: esner
'''
import unittest
import mock
import os
from freezegun import freeze_time

from component import Component


class TestComponent(unittest.TestCase):

    # set global time to 2010-10-10 - affects functions like datetime.now()
    @freeze_time("2010-10-10")
    # set KBC_DATADIR env to non-existing dir
    @mock.patch.dict(os.environ, {'KBC_DATADIR': './non-existing-dir'})
    def test_run_no_cfg_fails(self):
        with self.assertRaises(ValueError):
            comp = Component()
            comp.run()


if __name__ == "__main__":
    # import sys;sys.argv = ['', 'Test.testName']
    unittest.main()



================================================
FILE: .github/workflows/push.yml
================================================
name: Keboola Component Build & Deploy Pipeline
on:
  push:
    branches:
      - 'feature/*'
      - 'bug/*'
    tags:
      - '*' # Skip the workflow on the main branch without tags

concurrency: ci-${{ github.ref }} # to avoid tag collisions in the ECR
env:
  # repository variables:
  KBC_DEVELOPERPORTAL_APP: "kds-team.ex-google-my-business" # replace with your component id
  KBC_DEVELOPERPORTAL_VENDOR: "kds-team" # replace with your vendor
  DOCKERHUB_USER: ${{ secrets.DOCKERHUB_USER }}
  KBC_DEVELOPERPORTAL_USERNAME: "kds-team+github"

  # repository secrets:
  DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }} # recommended for pushing to ECR
  KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}

  # (Optional) Test KBC project: https://connection.keboola.com/admin/projects/0000
  KBC_TEST_PROJECT_CONFIGS: "" # space separated list of config ids
  KBC_STORAGE_TOKEN: ${{ secrets.KBC_STORAGE_TOKEN }} # required for running KBC tests

jobs:
  push_event_info:
    name: Push Event Info
    runs-on: ubuntu-latest
    outputs:
      app_image_tag: ${{ steps.tag.outputs.app_image_tag }}
      is_semantic_tag: ${{ steps.tag.outputs.is_semantic_tag }}
      is_default_branch: ${{ steps.default_branch.outputs.is_default_branch }}
      is_deploy_ready: ${{ steps.deploy_ready.outputs.is_deploy_ready }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Fetch all branches from remote repository
        run: git fetch --prune --unshallow --tags -f

      - name: Get current branch name
        id: current_branch
        run: |
          if [[ ${{ github.ref }} != "refs/tags/"* ]]; then
            branch_name=${{ github.ref_name }}
            echo "branch_name=$branch_name" | tee -a $GITHUB_OUTPUT
          else
            raw=$(git branch -r --contains ${{ github.ref }})
            branch="$(echo ${raw//origin\//} | tr -d '\n')"
            echo "branch_name=$branch" | tee -a $GITHUB_OUTPUT
          fi

      - name: Is current branch the default branch
        id: default_branch
        run: |
          echo "default_branch='${{ github.event.repository.default_branch }}'"
          if [ "${{ github.event.repository.default_branch }}" = "${{ steps.current_branch.outputs.branch_name }}" ]; then
             echo "is_default_branch=true" | tee -a $GITHUB_OUTPUT
          else
             echo "is_default_branch=false" | tee -a $GITHUB_OUTPUT
          fi

      - name: Set image tag
        id: tag
        run: |
          TAG="${GITHUB_REF##*/}"
          IS_SEMANTIC_TAG=$(echo "$TAG" | grep -q '^v\?[0-9]\+\.[0-9]\+\.[0-9]\+$' && echo true || echo false)
          echo "is_semantic_tag=$IS_SEMANTIC_TAG" | tee -a $GITHUB_OUTPUT
          echo "app_image_tag=$TAG" | tee -a $GITHUB_OUTPUT

      - name: Deploy-Ready check
        id: deploy_ready
        run: |
          if [[ "${{ steps.default_branch.outputs.is_default_branch }}" == "true" \
            && "${{ github.ref }}" == refs/tags/* \
            && "${{ steps.tag.outputs.is_semantic_tag }}" == "true" ]]; then
              echo "is_deploy_ready=true" | tee -a $GITHUB_OUTPUT
          else
              echo "is_deploy_ready=false" | tee -a $GITHUB_OUTPUT
          fi

  build:
    name: Docker Image Build
    runs-on: ubuntu-latest
    needs:
      - push_event_info
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest
          outputs: type=docker,dest=/tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar

  tests:
    name: Run Tests
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - build
    steps:
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest flake8 . --config=flake8.cfg
          echo "Running unit-tests..."
          docker run ${{ env.KBC_DEVELOPERPORTAL_APP }}:latest python -m unittest discover

  tests-kbc:
    name: Run KBC Tests
    needs:
      - push_event_info
      - build
    runs-on: ubuntu-latest
    steps:
      - name: Set up environment variables
        run: |
          echo "KBC_TEST_PROJECT_CONFIGS=${KBC_TEST_PROJECT_CONFIGS}" >> $GITHUB_ENV
          echo "KBC_STORAGE_TOKEN=${{ secrets.KBC_STORAGE_TOKEN }}" >> $GITHUB_ENV

      - name: Run KBC test jobs
        if: env.KBC_TEST_PROJECT_CONFIGS != '' && env.KBC_STORAGE_TOKEN != ''
        uses: keboola/action-run-configs-parallel@master
        with:
          token: ${{ secrets.KBC_STORAGE_TOKEN }}
          componentId: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          configs: ${{ env.KBC_TEST_PROJECT_CONFIGS }}

  push:
    name: Docker Image Push
    runs-on: ubuntu-latest
    needs:
      - push_event_info
      - tests
      - tests-kbc
    env:
      DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          path: /tmp

      - name: Load Image & Run Tests
        run: |
          docker load --input /tmp/${{ env.KBC_DEVELOPERPORTAL_APP }}.tar
          docker image ls -a

      - name: Docker login
        if: env.DOCKERHUB_TOKEN
        run: docker login --username "${{ env.DOCKERHUB_USER }}" --password "${{ env.DOCKERHUB_TOKEN }}"

      - name: Push image to ECR
        uses: keboola/action-push-to-ecr@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}
          push_latest: ${{ needs.push_event_info.outputs.is_deploy_ready }}
          source_image: ${{ env.KBC_DEVELOPERPORTAL_APP }}

  deploy:
    name: Deploy to KBC
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Set Developer Portal Tag
        uses: keboola/action-set-tag-developer-portal@master
        with:
          vendor: ${{ env.KBC_DEVELOPERPORTAL_VENDOR }}
          app_id: ${{ env.KBC_DEVELOPERPORTAL_APP }}
          username: ${{ env.KBC_DEVELOPERPORTAL_USERNAME }}
          password: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
          tag: ${{ needs.push_event_info.outputs.app_image_tag }}

  update_developer_portal_properties:
    name: Developer Portal Properties Update
    env:
      KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_DEVELOPERPORTAL_PASSWORD }}
    needs:
      - push_event_info
      - build
      - push
    runs-on: ubuntu-latest
    if: needs.push_event_info.outputs.is_deploy_ready == 'true'
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Update developer portal properties
        run: |
          chmod +x scripts/developer_portal/*.sh
          scripts/developer_portal/update_properties.sh

